{"meta":{"title":"TianyiGu's Blog","subtitle":null,"description":null,"author":"Tianyi Gu","url":"https://coconutmilktaro.top","root":"/"},"pages":[{"title":"分类","date":"2019-12-09T06:58:07.000Z","updated":"2022-05-30T02:51:54.011Z","comments":false,"path":"categories/index.html","permalink":"https://coconutmilktaro.top/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-12-09T06:57:42.000Z","updated":"2022-05-30T02:51:54.013Z","comments":false,"path":"tags/index.html","permalink":"https://coconutmilktaro.top/tags/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-05-30T02:51:54.012Z","updated":"2022-05-30T02:51:54.012Z","comments":false,"path":"repository/index.html","permalink":"https://coconutmilktaro.top/repository/index.html","excerpt":"","text":""},{"title":"友链","date":"2022-05-30T02:51:54.011Z","updated":"2022-05-30T02:51:54.011Z","comments":false,"path":"friends/index.html","permalink":"https://coconutmilktaro.top/friends/index.html","excerpt":"","text":""},{"title":"分类","date":"2020-02-11T13:11:51.000Z","updated":"2022-05-30T02:51:54.012Z","comments":true,"path":"guide/index.html","permalink":"https://coconutmilktaro.top/guide/index.html","excerpt":"","text":"数据库Redis 学习笔记MySQL 学习笔记Influxdb 基础笔记MySQL 高可用笔记MongoDB 基础笔记PostgreSQL 笔记 Python 开发Python 基础完全笔记Django 使用 Whoosh 实现全文检索笔记 Linux 基础服务Postfix 邮件服务器学习笔记无人值守学习笔记FTP 笔记Iptables、Selinux 与防火墙笔记DHCP 笔记Nginx 笔记Apache-Server 笔记samba 笔记DNS 学习笔记Chrony 学习笔记 Linux 运维相关Supervisor 学习笔记《SRE-Google 运维解密》读书笔记Sed 与 Awk 笔记运维小技巧整理Linux 性能监控常用命令 负载均衡与高可用HAProxy-Starter-Guide 翻译Heartbeat 笔记HAProxy 笔记LVS 负载均衡笔记 存储服务MFS 分布式文件系统笔记NFS 高可用笔记Rsync 文件同步服务器学习笔记LVM 逻辑卷与 RAID 磁盘阵列学习笔记NFS 笔记iSCSI 学习笔记 缓存服务Memcached 笔记Squid 笔记Varnish 笔记 JavaJVM 原理与优化笔记Tomcat 学习笔记 网络VLAN 基础笔记控制 IGP 路由PPP 协议基础笔记TCPIP 基础笔记QoS 完全笔记SSH 与 SSL 协议学习笔记IS-IS 学习笔记组播学习笔记常见 VPN 技术笔记生成树协议学习笔记OSPF 学习笔记wireshark 学习笔记HTTP 协议笔记BGP 学习笔记 虚拟化Linux-Namespace 学习笔记KVM 学习笔记 Docker重学 Docker 笔记Docker 网络学习笔记Docker 存储学习笔记Docker-Swarm 学习笔记Docker-Compose 学习笔记 KubernetesKubernetes 网络学习笔记Kubernetes 学习笔记Kubernetes 集群管理与排错笔记 ServerlessServerless 技术笔记 大数据Lucene 与 Solr 笔记ELK 与 EFK 详细笔记 自动化部署SaltStack 学习笔记Ansible 学习笔记Puppet 自动化工具笔记 CI/CDJenkins 学习笔记 监控Prometheus 基础应用笔记Centreon 监控平台搭建笔记zabbix 搭建笔记Cacti 监控学习Nagios 监控搭建 APIPrometheus-python-API 使用笔记Ansible-python-API 使用笔记influxdb-python-API 使用笔记 数据结构与算法PAT-Basic 基础题集PTA 数据结构与算法题目集重学数据结构与算法笔记 消息队列RabbitMQ 基础笔记Kafka 基础笔记I-O 学习笔记 架构分布式系统架构学习笔记 信息安全无线安全渗透笔记Linux 系统安全笔记信息安全学习笔记（软考） 杂项5G-NSA 与 SA 概念笔记PowerShell 实战笔记在 Vultr 上搭建 Shadowsock 记录YAML 学习笔记"},{"title":"关于","date":"2018-04-29T01:01:40.000Z","updated":"2022-06-21T15:16:21.222Z","comments":false,"path":"about/index.html","permalink":"https://coconutmilktaro.top/about/index.html","excerpt":"","text":"&#123; &quot;姓名&quot;: &quot;Tianyi Gu&quot;, &quot;年龄&quot;: 23, &quot;性别&quot;: &quot;男&quot;, &quot;籍贯&quot;: &quot;苏州&quot;, &quot;现居&quot;: &quot;上海 浦东新区&quot;, &quot;github&quot;: &quot;https://github.com/serchaofan&quot;, &quot;blog&quot;: &quot;https://coconutmilktaro.top&quot;, &quot;学习方向&quot;: [ &quot;网络&quot;, &quot;应用运维&quot;, &quot;系统运维&quot;, &quot;云计算&quot;, &quot;大数据&quot;, &quot;DevOps&quot;, &quot;Python开发&quot;, &quot;数据库&quot;, &quot;网站架构&quot;, &quot;一点点前端开发&quot; ]&#125;"}],"posts":[{"title":"Etcd基础笔记","slug":"Etcd基础笔记","date":"2020-04-10T04:28:43.000Z","updated":"2022-05-30T02:51:53.799Z","comments":true,"path":"2020/Etcd基础笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/Etcd%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/","excerpt":"etcd 数据模型 etcd 架构 应用场景 服务注册和发现 消息发布与订阅 负载均衡 分布式通知与协调 分布式锁 分布式队列 集群监控与 Leader 竞选 etcd 与 zookeeper 的区别","text":"etcd 数据模型 etcd 架构 应用场景 服务注册和发现 消息发布与订阅 负载均衡 分布式通知与协调 分布式锁 分布式队列 集群监控与 Leader 竞选 etcd 与 zookeeper 的区别 etcd 以一致和容错的方式存储元数据。分布式系统使用 etcd 作为一致性键值存储，用于配置管理，服务发现和协调分布式工作。etcd 内部采用 raft 协议作为一致性算法，etcd 基于 Go 语言实现。 名字来源：etc 即 linux 的/etc 目录，意为存放配置文件，d 为 distributed 即分布式 etcd 的特点 安装配置简单，且提供了 HTTP API 进行交互 支持 SSL 证书验证 单实例支持每秒 2k+读操作 采用 raft 算法，实现分布式系统数据的可用性和一致性 分布式系统中的数据分为控制数据和应用数据，使用 etcd 的场景默认处理的数据都是控制数据，对于应用数据，只推荐数据量很小，但是更新访问频繁的情况。 常见术语： Node：Raft 状态机实例。 Member：etcd 实例。它管理着一个 Node，并且可以为客户端请求提供服务。 Cluster：由多个 Member 构成可以协同工作的etcd 集群。 Peer：对同一个 etcd 集群中另外一个 Member的称呼。 Client：向 etcd 集群发送 HTTP 请求的客户端。 Proxy：etcd 的一种模式，为 etcd 集群提供反向代理服务。 Leader：Raft 算法中通过竞选而产生的处理所有数据提交的节点。 Follower：竞选失败的节点作为Raft 中的从属节点，为算法提供强一致性保证。 Candidate：当Follower 超过一定时间接收不到 Leader 的心跳时转变为 Candidate 开始竞选。 Term：某个节点成为 Leader 到下一次竞选时间，称为一个 Term。 Index：数据项编号。Raft 中通过 Term 和 Index 来定位数据。 为了保证数据的强一致性，etcd 集群中所有的数据流向都是一个方向，从 Leader（主节点）流向 Follower，也就是所有 Follower 的数据必须与 Leader 保持一致，如果不一致会被覆盖。 读：由于集群所有节点数据是强一致性的，可以从集群中随便哪个节点进行读取数据 写：etcd 集群有 leader，可以直接往 leader 写入，然后 Leader 节点会把写入分发给所有 Follower，如果往 follower 写入，则 Leader 节点会把写入分发给所有 Follower etcd 数据模型etcd 架构 HTTP Server：用于处理用户发送的 API 请求以及其它 etcd 节点的同步与心跳信息请求。 Store：用于处理 etcd 支持的各类功能的事务，包括数据索引、节点状态变更、监控与反馈、事件处理与执行等等，是 etcd 对用户提供的大多数 API 功能的具体实现。 Raft：Raft 强一致性算法的具体实现，是 etcd 的核心。 WAL：预写式日志，etcd 用于持久化存储的日志格式。除了在内存中存有所有数据的状态以及节点的索引以外，etcd 就通过 WAL 进行持久化存储。WAL 中，所有的数据提交前都会事先记录日志。 Snapshot：etcd防止 WAL 文件过多而设置的快照，存储 etcd 数据状态。 Entry：存储的具体日志内容。 应用场景服务注册和发现提供服务发现功能的关键设计： 强一致性、高可用的服务存储目录 注册服务和监控服务健康状态的机制 查找和连接服务的机制 常见服务发现形式： 前后端业务注册发现：中间件和后端服务在 etcd 中注册后，前端和中间件可以从 etcd 中找到相应服务器并根据调用关系进行绑定 多组后端服务器注册发现：后端多个状态相同 APP 可同时注册到 etcd 中，前端可通过 HAProxy 从 etcd 获取到后端 IP 和端口然后请求转发，能够通过动态的配置域名解析实现实例故障重启透明化 消息发布与订阅在分布式系统中，最适用的一种组件间通信方式就是消息发布与订阅，即构建一个配置共享中心，数据提供者在这个配置中心发布消息，而消息使用者则订阅他们关心的主题，一旦主题有消息发布，就会实时通知订阅者。通过这种方式可以做到分布式系统配置的集中式管理与动态更新。 负载均衡分布式通知与协调分布式锁分布式队列集群监控与 Leader 竞选etcd 与 zookeeper 的区别 参考文章： Etcd 集群搭建 Etcd 使用入门 万字长文：etcd 从入门到放弃 etcd 官方文档中文版 etcd：从应用场景到实现原理的全方位解读","categories":[],"tags":[]},{"title":"Istio","slug":"Istio基础笔记","date":"2020-04-05T11:09:24.000Z","updated":"2022-05-30T02:51:53.804Z","comments":true,"path":"2020/Istio基础笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/Istio%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/","excerpt":"本文内容包括Istio的基础概念以及常见用例，大量概念摘抄自《深入浅出Istio：Service Mesh快速入门与实践》一书，仅用作学习参考。Istio 版本：1.9.0","text":"本文内容包括Istio的基础概念以及常见用例，大量概念摘抄自《深入浅出Istio：Service Mesh快速入门与实践》一书，仅用作学习参考。Istio 版本：1.9.0 Service Mesh Istio Istio组件 Istio核心配置对象 Service MeshBuoyant 公司的CEO William , 曾经给出对服务网格的定义：服务网格是一个独立的基础设施层，用来处理服务之间的通信。Istio官方的定义：服务网格常常用来描述构建这些应用以及应用间交互的微服务网络。随着服务网格的大小和复杂性的增长，它变得越来越难以理解和管理。 服务网格的要求可以包括服务发现，负载均衡，故障恢复，指标和监视。服务网格通常还具有更复杂的操作要求，例如A/B测试，金丝雀发布，速率限制，访问控制和端到端身份验证。 也可做出以下总结，ServiceMesh的描述就是： 治理能力独立（sidecar） 应用程序无感知 服务通信的基础设施层 IstioIstio提供了对整个服务网格的行为分析和操作控制，提供了一个完整的解决方案来满足微服务应用程序的各种需求。Istio有助于降低这些部署的复杂性，并减轻开发团队的负担。它是一个完全开源的服务网格，可以透明地分层到现有的分布式应用程序上。它也是一个平台，包括了API，可将其集成到任何日志平台、遥测或策略系统中。 通过在整个环境中部署一个特殊的sidecar代理来拦截微服务之间的所有网络通信，您可以向服务添加Istio支持，然后使用Istio的控制平面功能来配置和管理Istio，其中包括： 对于HTTP、gRPC、TCP流量的自动负载均衡 可通过路由规则、重试次数、故障转移、错误注入进行网络流量行为的细粒度控制 可插拔的策略层和配置API，支持访问控制，速率限制和配额 对集群内的所有流量(包括集群入口和出口)进行自动度量、日志采集和跟踪 通过强身份验证和授权，在集群中进行安全的服务间通信 Istio组件istio在1.5之后回归单体应用，舍弃了mixer，将pilot、citadel、galley封装为一个istiod应用。Istio只包含两个组件： Envoy：Envoy是使用C++开发的高性能代理，可为服务网格中的所有服务调解所有入站和出站流量。Envoy代理是与数据平面流量交互的唯一Istio组件。Envoy代理被部署为服务的Sidecar，包含以下功能： 动态服务发现 负载均衡 TLS终止 HTTP/2和gRPC代理 断路器 健康检查 分阶段推出，并按百分比分配流量（灰度） 故障注入 丰富的指标 Istiod： Istio核心配置对象","categories":[],"tags":[]},{"title":"Kubernetes共享存储笔记","slug":"Kubernetes共享存储笔记","date":"2020-04-05T11:06:54.000Z","updated":"2022-05-30T02:51:53.808Z","comments":true,"path":"2020/Kubernetes共享存储笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/Kubernetes%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8%E7%AC%94%E8%AE%B0/","excerpt":"PV PVC 生命周期 StorageClass","text":"PV PVC 生命周期 StorageClass PVPersistentVolume 是对底层网络共享存储的抽象，将共享存储定义为一种资源，由管理员创建配置，与共享存储的具体实现直接相关，通过插件式的机制完成与共享存储的对接。 PV 主要包含以下参数的设置： 存储能力 capacity：描述存储设备具备的能力（空间大小） 访问模式 accessMode：描述用户的应用对存储资源的访问权限。访问模式有： ReadWriteOnce：RWO，读写权限，只能被单个 Node 挂载 ReadOnlyMany：ROX，只读权限，允许被多个 Node 挂载 ReadWriteMany：读写权限，允许被多个 Node 挂载 存储卷模式 VolumeMode：可选 Filesystem 和 Block，默认为 Filesystem 存储类别 storageClassName：指定一个 StorageClass 资源对象名称，具有特定类别的 PV 只能和请求了该类别的 PVC 进行绑定，未设定类别的 PV 只能与不请求任何类别的 PVC 绑定。 回收策略 persistentVolumeReclaimPolicy：可选项为： 挂载参数 mountOptions 节点亲和性 nodeAffinity PVC生命周期StorageClass","categories":[],"tags":[]},{"title":"Kubernetes集群管理与排错笔记","slug":"Kubernetes集群管理与排错笔记","date":"2020-04-05T11:06:01.000Z","updated":"2022-06-28T18:31:02.505Z","comments":false,"path":"2020/Kubernetes集群管理与排错笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/Kubernetes%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E4%B8%8E%E6%8E%92%E9%94%99%E7%AC%94%E8%AE%B0/","excerpt":"","text":"Node 管理Node 隔离与恢复将 Node 纳入或脱离调度范围 创建 node 类型的 yaml，并设置unschedulable: true apiVersion: v1kind: Nodemetadata: name: kubenode2 labels: kubernetes.io/hostname: kubenode2spec: unschedulable: true 然后kubectl replace -f应用该配置，查看 nodes # kubectl get nodesNAME STATUS ROLES AGE VERSIONkubenode1 Ready master 18d v1.18.0kubenode2 Ready,SchedulingDisabled &lt;none&gt; 18d v1.18.0kubenode3 Ready &lt;none&gt; 18d v1.18.0 对于后续创建的 Pod 就不会再调度到该 Node 了。若要重新将该节点纳入调度范围，只要将该参数改为 false 并应用即可。 也可以直接通过命令cordon和uncordon进行设置 # kubectl cordon kubenode3node/kubenode3 cordoned# kubectl get nodes kubenode3NAME STATUS ROLES AGE VERSIONkubenode3 Ready,SchedulingDisabled &lt;none&gt; 18d v1.18.0# kubectl uncordon kubenode3node/kubenode3 uncordoned Node 扩容只需在新的 Node 上安装 docker、kubelet 和 kube-proxy，并配置启动参数，将 Master URL 指定为当前 k8s 集群 master 的地址。通过 kubelet 默认注册机制，新的 node 就会自动加入到 k8s 集群中。 更新 Label创建一个 deployment，标签为app=nginx # kubectl get pods --show-labelsNAME READY STATUS RESTARTS AGE LABELSnginx-deploy-5bf87f5f59-6r67l 1/1 Running 0 2m24s app=nginx,pod-template-hash=5bf87f5f59nginx-deploy-5bf87f5f59-lt6v2 1/1 Running 0 2m24s app=nginx,pod-template-hash=5bf87f5f59nginx-deploy-5bf87f5f59-sqrs6 1/1 Running 0 2m24s app=nginx,pod-template-hash=5bf87f5f59 可通过-L&lt;label_name&gt;查看指定标签的值 # kubectl get pods -LappNAME READY STATUS RESTARTS AGE APPnginx-deploy-5bf87f5f59-6r67l 1/1 Running 0 3m26s nginxnginx-deploy-5bf87f5f59-lt6v2 1/1 Running 0 3m26s nginxnginx-deploy-5bf87f5f59-sqrs6 1/1 Running 0 3m26s nginx 若要更新标签，则通过kubectl label进行修改 # kubectl get deployments.apps --show-labelsNAME READY UP-TO-DATE AVAILABLE AGE LABELSnginx-deploy 3/3 3 3 8m10s app=nginx_1.17 若要删除该标签，则通过&lt;label_name&gt;-即可 # kubectl label deployments.apps nginx-deploy app-deployment.apps/nginx-deploy labeled# kubectl get deployments.apps --show-labelsNAME READY UP-TO-DATE AVAILABLE AGE LABELSnginx-deploy 3/3 3 3 9m22s &lt;none&gt; NamespaceK8s 通过命名空间和 Context 设置对不同工作组进行区分，使得它们互不干扰。 namespace 的创建 apiVersion: v1kind: Namespacemetadata: name: dev context 运行环境的创建 # 创建集群# kubectl config set-cluster nginx-cluster-1 --server=http://192.168.60.3:8080Cluster &quot;nginx-cluster-1&quot; set.# 配置运行环境# kubectl config set-context dev --namespace=dev --cluster=nginx-cluster-1 --user=devContext &quot;dev&quot; created.# 查看已定义的context# kubectl config viewapiVersion: v1clusters:- cluster: certificate-authority-data: DATA+OMITTED server: https://192.168.60.3:6443 name: kubernetes- cluster: server: http://192.168.60.3:8080 name: nginx-cluster-1contexts:- context: cluster: nginx-cluster-1 namespace: dev user: dev name: dev- context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetescurrent-context: kubernetes-admin@kuberneteskind: Configpreferences: &#123;&#125;users:- name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED kubectl config会在$&#123;HOME&#125;/.kube/下生成一个名为 config 的文件，内容就是以上kubectl config view的内容。 若要将工作组设置在指定 context 环境下工作，则通过kubectl config use-context指定 # kubectl config use-context devSwitched to context &quot;dev&quot;. 此后创建的资源都使用的是该环境。 资源管理若对于 Pod 不定义 CPU Request 和 Memory Request，则 K8s 会认为该 Pod 所需资源很少，可调度到任意可用 Node 上，当集群的计算资源不充足时，就会使某个 Node 资源严重不足。为了避免系统挂掉，Node 会自动清理 pod 释放资源，但有的 Pod 十分重要，无论资源是否充足，也要保障 这些 Pod 的存活。对此 K8s 有以下保障机制： 通过资源配额确保不同 Pod 只占用指定资源 允许集群的资源被超额分配，提高资源利用率 为 Pod 分级，确保不同等级 Pod 有不同的 QoS。资源不足时低等级的 Pod 被清理以确保高等级 Pod 正常运行。 spec: containers: - name: myapp image: &lt;Image&gt; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; requests: cpu: &quot;20m&quot; memory: &quot;55M&quot; CPU的资源值是绝对值，不是相对值，比如0.1CPU在单核或多核机器上都是一样的，都严格等于0.1CPU core。 内存的资源值计量单位为字节数，值的表示方式为整数+国际单位制 国际单位制包含两类： 十进制：E、P、T、G、M、K、m 二进制：Ei、Pi、Ti、Gi、Mi、Ki CPU只能使用十进制的m单位或者不加单位，m表示千分之一单位，即200m表示0.2 cpu core。 对于内存，十进制和二进制区别如下： 1 KB (KiloByte) = 1000 Bytes = 8000 Bits 1 KiB (KibiByte) = 1024 Bytes = 8192 Bits Pod的requests和limits是这个pod中所有容器的requests和limits的总和，例如： spec: containers: - name: myapp1 image: &lt;Image&gt; resources: limits: memory: &quot;200Mi&quot; cpu: &quot;200m&quot; requests: cpu: &quot;100m&quot; memory: &quot;100Mi&quot; - name: myapp2 image: &lt;Image&gt; resources: limits: memory: &quot;200Mi&quot; cpu: &quot;200m&quot; requests: cpu: &quot;100m&quot; memory: &quot;100Mi&quot; 则这个Pod的CPU总体requests为200m，limits为400Mi，内存同理。 Pod的QoS服务质量根据指定资源requests与limits的不同，创建的Pod会有三个不同的QoS（服务质量）属性。K8S提供三种QoS类，并使用QoS类决定Pod的调度与驱逐策略。 Guaranteed 有保证 Burstable 不稳定 BestEffort 尽最大努力 对于 QoS 类为 Guaranteed 的 Pod：Pod 中的每个容器都必须指定CPU与内存的limits和requests，且limits=requests。 spec: containers: - name: myapp image: &lt;Image&gt; resources: limits: cpu: &quot;1&quot; memory: &quot;1Gi&quot; requests: cpu: &quot;1&quot; memory: &quot;1Gi&quot; 查看pod的status #kubectl get po -oyaml xxxx......status: qosClass: Guaranteed 对于 QoS 类为 Burstable 的 Pod：Pod 不符合Guaranteed标准，且Pod 中至少一个容器具有内存或CPU的请求或限制。 spec: containers: - name: myapp image: &lt;Image&gt; resources: limits: cpu: &quot;2&quot; memory: &quot;2Gi&quot; requests: cpu: &quot;1&quot; memory: &quot;1Gi&quot;或 resources: limits: memory: &quot;2Gi&quot; requests: memory: &quot;1Gi&quot; 查看pod的status #kubectl get po -oyaml xxxx......status: qosClass: Burstable 对于 QoS 类为 BestEffort 的 Pod：Pod 中的容器必须没有设置内存和 CPU 限制或请求。 spec: containers: - name: myapp image: &lt;Image&gt; 查看pod的status #kubectl get po -oyaml xxxx......status: qosClass: BestEffort K8S根据QoS进行的回收策略： 当Pod出现资源不足情况时，先会杀死优先级低的pod，而优先级是通过OOM分数值来实现，范围为0-1000。OOM分数值根据OOM_ADJ参数计算得出。 Guaranteed的Pod，OOM_ADJ值为-998 BestEffort的pod，OOM_ADJ值为1000 Burstable的pod，OOM_ADJ值范围为2~999 OOM_ADJ值越大，Pod优先级越低，出现资源不足时会被优先kill掉。 对于docker、kubelet等k8s的保留资源，OOM_ADJ值设置成了-999，即不会被OOM kill干掉。 也就是说，当资源不足时，最先被kill掉的是BestEffort的pod，最晚被安排Kill掉的是Guaranteed的pod 若资源充足，且想要提高资源利用率，资源限制按照Guaranteed配置。 LimitRangeLimitRange用于设置容器的默认资源限制，就是说当pod未设置limits或requests时，则会采用limitrange定义的限制值。 LimitRang支持两种type，一种为container，一种为pod。其中，pod与container都可以设置min、max、maxLimitRequestRatio参数，container可以设置default和defaultRequest参数，而Pod不可以。 对以上几个参数的解释： min指的是requests的下限，max指的是limits的上限。若是在container中指定的，则是每个容器的，若是pod中指定的，则是pod中所有容器的总和。min和max指的就不是默认值了，而是真正的限制，若容器或pod的资源设置超过min和max的范围，则pod根本不会启动，就是kubectl get pod找不到这个pod。 对于同一资源类型，必须满足：min &lt;= default &lt;= defaultRequest &lt;= max container的maxLimitRequestRatio限制的是pod中所有容器的limits值与requests值的比例上限，若是pod则是pod中所有容器的limits值与requests值的总和的比例上限。 例如先创建一个LimitRange类型的资源，设置默认资源限制，内存的requests=256Mi，limits=512Mi ，以下展示两种类型。 apiVersion: v1kind: LimitRangemetadata: name: mem-limit-range-containerspec: limits: - default: memory: 512Mi defaultRequest: memory: 256Mi type: Container# kubectl describe limitrangesName: mem-limit-range-containerNamespace: defaultType Resource Min Max Default Request Default Limit Max Limit/Request Ratio---- -------- --- --- --------------- ------------- -----------------------Container memory - - 256Mi 512Mi - 其中，default指定的是limits值，defaultRequest指定的是requests值。 apiVersion: v1kind: LimitRangemetadata: name: mem-limit-range-podspec: limits: - max: memory: 512Mi min: memory: 256Mi type: Pod# kubectl describe limitrangesName: mem-limit-range-podNamespace: defaultType Resource Min Max Default Request Default Limit Max Limit/Request Ratio---- -------- --- --- --------------- ------------- -----------------------Pod memory 256Mi 512Mi - - - 其中，max指定的是limits值，min指定的是requests值。 再创建一个deployment或者Pod，没有定义resources限制 apiVersion: apps/v1kind: Deploymentmetadata: name: app1 namespace: defaultspec: selector: matchLabels: app: app1 template: metadata: labels: app: app1 spec: containers: - image: nginx imagePullPolicy: Always name: app1 此时再查看pod的信息 # kubectl describe pod app1-668c794b77-wv8spName: app1-668c794b77-wv8spNamespace: default......Containers: app1: ...... Limits: memory: 512Mi Requests: memory: 256Mi Pod 驱逐机制Pod Disruption Budget集群高可用集群监控集群日志管理审计机制Helm常见问题 参考文档Kubernetes 权威指南","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://coconutmilktaro.top/tags/kubernetes/"}]},{"title":"Kubernetes集群安全笔记","slug":"Kubernetes集群安全笔记","date":"2020-04-05T11:05:37.000Z","updated":"2022-05-30T02:51:53.834Z","comments":true,"path":"2020/Kubernetes集群安全笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/Kubernetes%E9%9B%86%E7%BE%A4%E5%AE%89%E5%85%A8%E7%AC%94%E8%AE%B0/","excerpt":"Admission Control Service Account Secret Pod 安全策略配置","text":"Admission Control Service Account Secret Pod 安全策略配置 云原生安全概述云原生安全的 4 个 C 分别是云（Cloud）、集群（Cluster）、容器（Container）和代码（Code）。 云（例如数据中心）是k8s集群的可信计算基（trusted computing base），如果云层容易受到攻击，就不能保证在此基础之上构建的组件是安全的。云基础设施的保护有几个关注的点： 对k8s控制平面的访问不允许在公网上开放，由网络访问控制列表控制（包含管理集群所用的IP地址集合）。 节点仅能从控制平面指定端口来连接，节点不应暴露在公网上。 对于云提供商，最好能给最小特权限制。 仅有控制平面能访问etcd，且使用tls访问 etcd节点的磁盘最好能静态加密数据。 关于集群的保护分两点：一个是集群本身，一个是集群中的应用。保护集群有以下几个关注的点： 所有API交互都用tls加密 所有API客户端必须经过身份验证（如通过ServiceAccount或x509证书） 每个API多用都通过鉴权检查（如RBAC） 控制kubelet访问，默认是允许对API进行未经身份验证的访问。生产环境应启用kubelet身份认证授权。 限制集群资源使用（如ResourceQuota或LimitRange） 控制容器运行的特权，通过设置Pod安全策略，限制哪些用户或服务帐户可以提供危险的安全上下文设置。运行容器最好能用非root用户运行。 限制网络访问，通过编写网络策略（NetworkPolicy）实现。 限制云元数据API访问，在云平台上运行k8s时，需要限制对实例凭据的权限，使用网络策略限制Pod对元数据API的访问，并避免使用配置数据来传递机密信息。 控制Pod可以访问的节点（如NodeSelector或基于污点的Pod放置和放逐） 限制访问etcd，建议将etcd服务器隔离到只有API服务器可以访问的防火墙后面。 启用审计日志，能记录每个用户、使用API的应用以及控制面自身引发的活动。 限制使用 alpha 和 beta 特性 经常轮换基础设施证书，在证书上设置较短的生命期并实现自动轮换。 许多集成到k8s的第三方软件或服务都可能改变集群的安全配置。组件若能够在kube-system这类名字空间中创建 Pod， 则这类组件也可能获得意外的权限，因为这些Pod可以访问服务账户的Secret。 对于容器的安全性，主要关注以下几点： 容器漏洞扫描 镜像签名 禁止特权用户 使用带有较强隔离能力的容器运行时（RuntimeClass） 对于代码的安全性，主要关注以下几点： 加密传输所有内容 限制通信端口范围，只开放必要端口 定期扫描应用程序的第三方库以了解已知的安全漏洞 静态代码分析与动态探测攻击 集群安全性需要考虑几个目标： 保证容器与其所在宿主机的隔离 限制容器给基础设施或其他容器的干扰 最小权限原则：限制单个组件的能力来限制它的权限范围 明确组件间边界划分 划分普通用户与管理员角色 在必要时允许将管理员权限赋给普通用户 允许拥有 Secret 数据的应用在集群中运行 K8S API 访问控制用户使用kubectl、客户端库或构造REST请求来访问K8S API，当请求到达API Server时，会经历以下阶段： 认证（Authentication） 鉴权（Authorization） 准入控制（Admission Control） 认证在API Server与建立TLS连接后，HTTP请求进入认证步骤。API Server上运行着一个或多个身份认证组件，服务器依次尝试每个验证模块，直到其中一个成功。 认证步骤的输入是整个HTTP请求，但通常情况组件只检查头部和客户端证书。 认证模块包含： 客户端证书（HTTPS） 密码（HTTP Base），&quot;用户名:密码&quot;进行 BASE64 编码后的字符串放在 Header 的 Authorization 中 普通令牌（HTTP Token），Token 放在 Header 里 引导令牌 JWT令牌 如果请求认证不通过，服务器将以HTTP状态码401拒绝该请求。反之，该用户被认证为特定的username，并且该用户名可用于后续步骤。 鉴权将请求验证为来自特定的用户后，请求必须被鉴权。 请求必须包含请求者的用户名、请求的行为以及受该操作影响的对象。如果现有策略声明用户有权完成请求的操作，那么该请求被鉴权通过。 Kubernetes 支持多种鉴权模块，例如ABAC模式、RBAC 模式和 Webhook 模式等。 管理员创建集群时会配置应在 API Server中使用的鉴权模块。 如果配置了多个鉴权模块，则 Kubernetes 会检查每个模块，任意一个鉴权模块允许该请求，请求即可继续，如果所有模块拒绝了该请求，请求将会被拒绝，返回HTTP状态码403。 鉴权模块: Node：节点鉴权是一种特殊用途的鉴权模式，专门对 kubelet 发出的 API 请求进行鉴权。 ABAC：基于属性的访问控制，通过使用将属性组合在一起的策略，将访问权限授予用户。策略可以使用任何类型的属性（用户属性、资源属性、 对象，环境属性等） RBAC：基于角色的访问控制，权限是单个用户执行特定任务的能力，例如查看、创建或修改文件。 Webhook：是一个 HTTP 回调：发生某些事情时调用的 HTTP POST，从而通过 HTTP POST 进行简单的事件通知 AlwaysDeny：阻止所有请求。仅用于测试。 AlwaysAllow：允许所有请求。仅在不需要 API 请求的鉴权时才使用。 在API server启动参数中添加一条--authorization-mode=指定鉴权模块，这条也必须配置至少一种，若配置多种则以,分隔，例如--authorization-mode=Node,RBAC，模块按顺序检查，较靠前的模块具有更高的优先级来允许或拒绝请求。 可使用kubectl auth can-i &lt;verb&gt; &lt;resource&gt; 了解当前用户是否有权限做指定操作，可以则为yes，反之为no。 # kubectl auth can-i create deployment -n devyes# kubectl auth can-i delete deployment -n kube-systemyes 准入控制准入控制模块是可以修改或拒绝请求的软件模块。除鉴权模块可用的属性外，准入控制模块还可以访问正在创建或修改的对象的内容。 准入控制器对创建、修改、删除或（通过代理）连接对象的请求进行操作。准入控制器不会对仅读取对象的请求起作用。有多个准入控制器被配置时，服务器将依次调用它们。 与身份认证和鉴权模块不同，如果任何准入控制器模块拒绝某请求，则该请求将立即被拒绝。 鉴权概述Kubernetes 仅审查以下 API 请求属性： 用户：身份验证期间提供的user。 组：经过身份验证的用户所属的组名列表。 额外信息：由身份验证层提供的任意字符串键到字符串值的映射。 API：指示请求是否针对 API 资源。 请求路径：各种非资源端点的路径，如 /api 或 /healthz。 API 请求动词：API 动词 get、list、create、update、patch、watch、proxy、redirect、delete 和 deletecollection 用于资源请求。 HTTP 请求动词：HTTP 动词 get、post、put 和 delete 用于非资源请求。 Resource：正在访问的资源的 ID 或名称（仅限资源请求），对于使用 get、update、patch 和 delete 动词的资源请求必须提供资源名称。 子资源：正在访问的子资源（仅限资源请求）。 名字空间：正在访问的对象的名称空间（仅适用于名字空间资源请求）。 API组：正在访问的API组 （仅限资源请求）。空字符串表示核心API组（apiVersion: v1）。 请求动词非资源请求除/api/v1/...或/apis/&lt;group&gt;/&lt;version&gt;/...之外的请求被视为非资源请求（Non-Resource Requests），并使用该HTTP请求的小写形式作为其请求动词（如get） 资源请求 HTTP动词 请求动词 POST create GET, HEAD get （针对单个资源）、list（针对集合） PUT update PATCH patch DELETE delete（针对单个资源）、deletecollection（针对集合） 有时使用专门的动词以对额外的权限进行鉴权。例如： PodSecurityPolicy：policy API 组中 podsecuritypolicies 资源使用 use 动词 RBAC：对 rbac.authorization.k8s.io API 组中 roles 和 clusterroles 资源的 bind 和 escalate 动词 身份认证：对核心 API 组中 users、groups 和 serviceaccounts 以及 authentication.k8s.io API 组中的 userextras 所使用的 impersonate 动词。 API Server 授权管理客户端发起 API server 调用时，API server 要先内部进行用户认证，然后执行用户授权，通过授权策略决定一个 API 调用是否合法。API server 支持的几种授权策略： API server 收到请求后，会读取请求中的数据，生成一个访问策略对象，然后将这个对象与策略文件中所有访问策略对象逐条匹配，只要至少一个策略通过，则请求被鉴权通过。 ABAC若要启用 ABAC，则需要指定策略文件路径--authorization-policy-file，策略文件中每一行为一个 JSON 对象，称为访问策略对象。一个对象需要包含三块： apiVersion：设为abac.authorization.kubernetes.io/v1beta1 kind：设为Policy spec：详细的策略配置，包含：主体属性、资源属性、非资源属性 主体属性： user：来源于 Token 文件或 base 认证的用户名 group：若为system:authenticated则匹配所有已认证的请求，若为system:unauthenticated则匹配所有未认证的请求 资源属性： apiGroup：匹配哪些 API group namespace：该策略允许访问某个 namespace 的资源 resource：API 资源对象 非资源属性： nonResourcePath：非资源对象类的 URL 路径 readonly：若为 true，则只允许 GET 请求 ABAC 授权算法：API server 收到请求后，先识别请求携带的策略对象的属性，然后根据策略对属性逐条匹配。常见策略配置： 允许所有认证用户做某事：将 group 属性设为system:authenticated 允许所有未认证用户做某事：将 group 属性设为system:unauthenticated 允许一个用户做任何事：将 apiGroup、namespace、resource、nonResourcePath 都设为’*‘ 使用 kubelet 的授权机制：kubelet 通过 apiserver 的/api 和/apis 获取版本信息，要验证kubectl create或update的对象，需要向 OpenAPI 进行查询（/openapi/v2）使用 ABAC 模式时，必须对 nonResourcePath 进行设置： /api、/apis、/api/*、/apis/* /version /swaggerapi/* # 允许jack对所有资源操作&#123; &quot;apiVersion&quot;: &quot;abac.authorization.kubernetes.io/v1beta1&quot;, &quot;kind&quot;: &quot;Policy&quot;, &quot;spec&quot;: &#123; &quot;user&quot;: &quot;jack&quot;, &quot;namespace&quot;: &quot;*&quot;, &quot;resource&quot;: &quot;*&quot;, &quot;apiGroup&quot;: &quot;*&quot; &#125;&#125;# kubelet可读任意pod&#123; &quot;apiVersion&quot;: &quot;abac.authorization.kubernetes.io/v1beta1&quot;, &quot;kind&quot;: &quot;Policy&quot;, &quot;spec&quot;: &#123; &quot;user&quot;: &quot;*&quot;, &quot;namespace&quot;: &quot;*&quot;, &quot;resource&quot;: &quot;pods&quot;, &quot;readonly&quot;: true &#125;&#125;# kubelet可读写event对象&#123; &quot;apiVersion&quot;: &quot;abac.authorization.kubernetes.io/v1beta1&quot;, &quot;kind&quot;: &quot;Policy&quot;, &quot;spec&quot;: &#123; &quot;user&quot;: &quot;kubelet&quot;, &quot;namespace&quot;: &quot;*&quot;, &quot;resource&quot;: &quot;events&quot; &#125;&#125;# bob只能读namespace为dev中的pod&#123; &quot;apiVersion&quot;: &quot;abac.authorization.kubernetes.io/v1beta1&quot;, &quot;kind&quot;: &quot;Policy&quot;, &quot;spec&quot;: &#123; &quot;user&quot;: &quot;bob&quot;, &quot;namespace&quot;: &quot;dev&quot;, &quot;resource&quot;: &quot;pods&quot;, &quot;readonly&quot;: true &#125;&#125;# 任何用户能对nonResourcePath进行只读请求&#123; &quot;apiVersion&quot;: &quot;abac.authorization.kubernetes.io/v1beta1&quot;, &quot;kind&quot;: &quot;Policy&quot;, &quot;spec&quot;: &#123; &quot;group&quot;: &#123; &quot;system&quot;: &quot;authenticated&quot; &#125;, &quot;readonly&quot;: true, &quot;nonResourcePath&quot;: &quot;*&quot; &#125;&#125; RBACRBAC 的优势： 对集群的资源和非资源权限都有完整的覆盖 整个 RBAC 完全由几个 API 对象完成，可用 kubectl 或 API 操作 可在运行时调整，无需重启 API server RBAC 有 4 个顶级资源对象： Role：一个角色是一组权限的集合，都是许可形式。若是集群级别，则需要使用 ClusterRole。apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: namespace: default name: pod-readerrules: - apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] rules 中参数： apiGroups：支持的 API 组列表 resources：支持的资源对象列表 verbs：对资源对象的操作方法列表 ClusterRole：除了 Role 的功能外，还可用于特殊元素的授权： 集群范围的资源，如 Node 非资源型的路径 全部命名空间的资源apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:rules:- apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] ClusterRole 不受限于命名空间，所以无须设置 metadata 的 namespace RoleBinding 和 ClusterRoleBinding：用于将一个角色绑定到一个目标上，目标可以是 user 或 group 或 Service Account Admission ControlService AccountSecretPod 安全策略配置","categories":[],"tags":[]},{"title":"OpenStack基础概念整理","slug":"OpenStack基础概念整理","date":"2020-03-23T08:29:58.000Z","updated":"2022-06-21T15:44:25.557Z","comments":true,"path":"2020/OpenStack基础概念整理/","link":"","permalink":"https://coconutmilktaro.top/2020/OpenStack%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E6%95%B4%E7%90%86/","excerpt":"OpenStack 概述","text":"OpenStack 概述 OpenStack 概述OpenStack 项目是一个开源云计算平台，支持所有类型的云环境。 该项目旨在实现简单的实现，大规模的可伸缩性和丰富的功能。OpenStack 通过各种补充服务提供 IaaS 的解决方案。每个服务都提供各自的 API。 OpenStack 是有多个服务组件组成的一种技术集合，包含以下主要组件： Horizon：提供 Dashboard，一个基于 web 的自服务门户，与 OpenStack 底层服务交互，诸如启动一个实例，分配 IP 地址以及配置访问控制。（可选组件） Nova：提供计算服务，即虚拟机管理，在 OpenStack 环境中计算实例的生命周期管理。按需响应包括生成、调度、回收虚拟机等操作。（核心组件） Neutron：提供网络服务，确保为其它 OpenStack 服务提供网络连接，比如 OpenStack 计算。为用户提供 API 定义网络和使用。基于插件的架构其支持众多的网络提供商和技术。（核心组件） Swift：提供存储服务（备份），通过一个 RESTful，基于 HTTP 的应用程序接口存储和任意检索的非结构化数据对象。它拥有高容错机制，基于数据复制和可扩展架构。它的实现并像是一个文件服务器需要挂载目录。在此种方式下，它写入对象和文件到多个硬盘中，以确保数据是在集群内跨服务器的多份复制。（核心组件） Cinder：为运行实例而提供的持久性块存储。它的可插拔驱动架构的功能有助于创建和管理块存储设备。（核心组件） Keystone：为其他 OpenStack 服务提供认证和授权服务，为所有的 OpenStack 服务提供一个端点目录。（核心组件） Glance：存储和检索虚拟机磁盘镜像，OpenStack 计算会在实例部署时使用此服务。（核心组件） Ceilometer：为 OpenStack 云的计费、基准、扩展性以及统计等目的提供监测和计量。（可选组件） Heat：提供部署编排云应用，Orchestration 服务支持多样化的综合的云应用，通过调用 OpenStack-native REST API 和 CloudFormation-compatible Query API，支持 Heat Orchestration Template (HOT)格式模板或者 AWS CloudFormation 格式模板。（可选组件） 参考文档 OpenStack 官方文档","categories":[],"tags":[]},{"title":"Zookeeper","slug":"Zookeeper","date":"2020-02-29T10:51:48.000Z","updated":"2022-06-30T18:55:36.828Z","comments":false,"path":"2020/Zookeeper/","link":"","permalink":"https://coconutmilktaro.top/2020/Zookeeper/","excerpt":"","text":"概述ZooKeeper是分布式应用的调度服务，通过一组简单的原语，分布式应用程序可以在这些原语的基础上实现更高级别的同步、配置维护、分组以及命名。ZooKeeper被设计为易于编程，并使用一种数据模型，该模型使用文件系统目录树结构。ZooKeeper在Java环境中运行。 zookeeper的设计目的 ZooKeeper允许分布式进程通过共享的分层命名空间（hierarchical namespace）相互协同调度，该命名空间的组织方式类似于标准文件系统。命名空间由数据寄存器组成，称为 znodes，类似于文件和目录。与为存储而设计的典型文件系统不同，ZooKeeper的数据保存在内存中，这意味着 ZooKeeper 可以实现高吞吐量和低延迟。ZooKeeper的实现非常重视高性能、高可用性、严格有序的访问。ZooKeeper的高性能意味着它可以用于大型分布式系统，高可靠性使其不会出现单点故障，严格的排序意味着可以在客户端实现复杂的同步原语。 与它协调的分布式进程一样，ZooKeeper本身旨在通过一组主机进行横向扩展。组成 ZooKeeper集群的服务器必须相互通信，这些服务器在内存中维护集群状态，以及持久存储中的事务日志和快照。只要大多数服务器可用，ZooKeeper 服务就可用。 客户端只需连接到单个 ZooKeeper 服务器，并且客户端会维护一个 TCP 连接，通过它发送请求、获取响应、获取监视事件并发送心跳，如果与服务器的 TCP 连接中断，客户端将连接到不同的服务器。 ZooKeeper 使用数字标记每个更新，通过数字反映所有 ZooKeeper 事务的顺序。后续操作可以使用该顺序来实现更高级别的抽象，例如同步原语。 ZooKeeper 在以读取为主（read-dominant）的工作负载中特别快。ZooKeeper 应用程序能在数千台机器上运行，并且它在读比写多的情况下表现最佳。 数据类型与分层命名空间命名空间的组织方式类似于标准文件系统。名称是由斜杠 (/) 分隔的一系列路径，命名空间中的每个节点都由路径标识。 节点与临时节点与标准文件系统不同，ZooKeeper 命名空间中的每个节点都可以拥有与其关联的数据以及子节点，就像一个允许文件也成为目录的文件系统，原因是ZooKeeper 被设计用于存储协调数据，如状态信息、配置、位置信息等，因此每个节点存储的数据通常很小，在字节到千字节的范围内。ZooKeeper 数据节点称为znode。 Znode 维护一个统计结构，其中包括了数据更改、ACL变更和时间戳的版本号，以允许缓存验证和协调更新。每次 znode 的数据更改时，版本号都会增加。例如，每当客户端检索数据时，它也会收到数据的版本。 存储在命名空间中每个 znode 的数据是原子读取和写入的。读取会获取与 znode 关联的所有数据字节，写入会替换所有数据。 每个节点都有一个访问控制列表 (ACL)，它限制谁可以做什么。 ZooKeeper 也有临时节点（ephemeral nodes）的概念，只要创建 znode 的会话处于活动状态，这些 znode 就存在。当会话结束时，znode 被删除。 有条件的更新和监视ZooKeeper支持 watch，客户端可以在 znode 上设置监视。当 znode 发生变化时，watch 将被触发并移除。当 watch 被触发时，客户端会收到一个数据包告知 znode 已更改。如果客户端和其中一个 ZooKeeper 服务器之间的连接断开，客户端将收到本地通知（local notification）。 保证Guarantees由于ZooKeeper的目标是成为构建更复杂服务（例如同步）的基础，因此它提供了一组保证，如下： 顺序一致性（Sequential Consistency）：来自客户端的更新将按照它们发送的顺序进行应用。 原子性（Atomicity）：更新要么成功，要么失败，没有中间状态的结果。 单一系统映像（Single System Image）：客户端都将看到相同的服务视图，不管它连接到的服务器是什么，也就是说，即使客户端故障转移到具有相同会话的不同服务器，客户端也永远不会看到系统的旧视图。 可靠性（Reliability）：当更新被应用后，它将从那时起持续存在，直到客户端覆盖更新。 及时性（Timeliness）：系统的客户端视图能保证在一定的时间范围内是最新的。 提供接口ZooKeeper 提供非常简单的编程接口，仅支持以下操作： create：在树中的某个位置创建一个节点 delete：删除一个节点 exists：测试节点是否存在于某个位置 get data：从节点读取数据 set data：将数据写入节点 get children：检索节点的子节点列表 sync：等待数据传播 实现如下图，运行 ZooKeeper 的每个服务器都会复制自己的每个组件的副本（除了请求处理器（Request Processor））。 复制数据库（Replicated Database）是包含整个数据树的内存数据库。更新的数据会被记录到磁盘以便恢复，写入的数据在被应用到内存数据库之前会被先序列化到磁盘。 每个 ZooKeeper 服务器都服务于客户端，客户端仅连接到一台服务器以提交请求。每个服务器数据库的本地副本会为读取请求提供服务，而改变服务状态的请求以及写请求是由一致性协议（agreement protocol）处理。 作为一致性协议的一部分，来自客户端的所有写入请求都会被转发到一台服务器，称为领导者（Leader）。其余 ZooKeeper 服务器，称为追随者（followers），会接收来自领导者的消息申请并同意消息传递。消息传递层（messaging layer）负责在失败时替换领导者并将追随者与领导者同步。 ZooKeeper 使用自定义的原子消息传递协议（即Atomic Broadcast）。由于消息传递层具有原子性，因此 ZooKeeper 可以保证本地副本永远不会存在分歧。当领导者收到一个写请求时，它会计算系统在执行写请求时的状态，并将其转换为捕获这个新状态的事务。 Zookeeper内部ZooKeeper 的核心是一个原子消息系统，它使所有服务器保持同步。 Zookeeper 会在消息系统中使用以下特定保证（Guarantees）： 可靠传递（Reliable delivery）：如果某条消息被一个服务器传递了，最终这条消息会被所有服务器都传递一遍。 全序（Total order）：如果一条消息A在另一条消息B之前被一台服务器传递，则这条消息A会在B之前被所有服务器传递，如果A和B都是被传递的消息，则要么A在B前被传递，要么B在A前被传递。 因果序（Causal order）：如果消息B的发送方发送了消息A后又发送了B消息，则A必须被排在B之前。如果发送方在发送消息B之后发送了消息C，则C必须排在B之后。 原子消息系统的协议依赖于TCP的以下属性： 有序传递（Ordered delivery）：数据的下发顺序与消息的下发顺序相同，消息M只有在消息M之前发送的所有消息都被下发后才会下发。(由此得出的推论是，如果消息M丢失，那么M之后的所有消息都将丢失。) 关闭后不再有消息（No message after close）：一旦FIFO（First-In First-Out 先进先出）通道关闭，就不会收到来自它的消息。 FLP（不可能原理）证明，在可能发生故障的异步分布式系统中，共识是无法实现的。因此Zookeeper通过超时时间，来确保在出现故障时达成共识。然而，Zookeeper依靠的是时间来保障集群存活性，而不是正确性。因此，如果超时时间这个组件停止工作(例如时钟故障)，消息传递系统可能会挂起，但不会违反其保证（上面说的三条特定保证）。 FLP：不可能原理。FLP是一篇论文的三个作者名字的首字母合起来组成的。该原理是分布式系统中重要的原理之一。FLP核心观点：在网络可靠，但允许节点失效（即便只有一个）的最小化异步模型系统中，不存在一个可以解决一致性问题的确定性共识算法。 Zookeeper 消息协议中的三个概念： 报文（Packet）：通过FIFO通道发送的字节序列分组 提议（Proposal）：一个协议单元。通过与参与决议的ZooKeeper服务器的交换报文来达成提议，大多数提议包含消息，但是NEW_LEADER提议是不对应于消息的提议的一个例子。 消息（Message）：一个将被原子广播到所有ZooKeeper服务器的字节序列，提议中的消息在被传递之前就已经获得同意。 如上所述，Zookeeper 保证了消息的全序（Total order），并保证了提议的全序。Zookeeper 通过一个事务id（zxid）来公开总排序。所有提议会被打上一个zxid，当这个提议被提出且准确反映总排序。提议会被发送到所有的Zookeeper服务器，并且当达到指定投票参与数的服务器承认这个提议时，这个提议会被提交。承认意味着这个提议会被记录到持久化存储中。对投票者有以下要求，任何一对投票者必须至少有一个共同服务器，通过需要所有投票者拥有 n/2+1的规模（n为Zookeeper集群的服务器个数），来满足上述对投票者的要求。 Zookeeper 消息由两阶段构成： 领导者激活（Leader activation）：在这个阶段，一个领导者会建立系统正确的状态，并准备好开始发起提议。 Active messaging：在这个阶段，一个领导者会接收消息提议并协调消息传递。 ZooKeeper是一个整体协议，并不关注单个提议，而是将提议作为一个整体来看待。通过严格的排序，使Zookeeper能够高效地做到这一点，并极大地简化了Zookeeper的协议，领导者激活体现了这一整体概念。只有当达到一定数量的追随者和领导者同步时，该领导者才会被激活(领导者也算作追随者，可以给自己投票)，他们拥有相同的状态，这个状态包括领导者认为已经提交的所有提议，以及跟随领导者的提议，即NEW_LEADER 提议。 Zookeeper集群安装确保集群的服务器数量为奇数个。 确保java环境已安装完成。 到官网的下载页面下载软件包，解压到集群中每台服务器的指定目录，例如/data/zookeeper。 查看配置文件conf/zoo.cfg，如果没有可以复制同目录下的zoo.cfg.example文件。 tickTime=2000 # Client与Server间或者Server间的通信心跳时间间隔，单位为毫秒。initLimit=10 # Leader与Follower初始通信时限，即初始连接时能容忍的最多心跳数syncLimit=5 # Leader与Follower同步通信时限，即请求与应答间能容忍的最多心跳数dataDir=/var/data/zookeeper # 数据目录clientPort=2181 # 客户端连接端口server.1=192.168.15.xxx:2888:3888server.2=192.168.15.xxx:2888:3888server.3=192.168.15.xxx:2888:3888 当配置完成后，还需要在每个节点的数据目录中创建一个文件，叫myid，里面写入序号，且这个需要要和该集群上配置的server.N的N一致。例如，在server.1这台机器上，需要在myid文件中写入1。 之后启动集群即可，在zookeeper的目录下执行命令bin/zkServer.sh start，如果要前台启动，则执行bin/zkServer.sh start-foreground 如果想使用supervisor托管服务，则可以添加配置/etc/supervisord.d/zookeeper.ini，写入以下内容： [program:zookeeper]command=/data/zookeeper/bin/zkServer.sh start-foregrounduser=rootautostart=trueautorestart=truestartsecs=10stdout_logfile=/data/logs/zookeeper.logredirect_stderr=trueenvironment=JAVA_HOME=/usr/local/jdk1.8.0_241 注：如果使用supervisor托管，必须前台启动，即start-foreground，否则会无法正常启动。 参考文章https://zookeeper.apache.org/doc/r3.4.13/zookeeperOver.htmlhttps://zookeeper.apache.org/doc/r3.4.13/zookeeperInternals.htmlhttps://www.cnblogs.com/skyl/p/4854553.html","categories":[],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://coconutmilktaro.top/tags/zookeeper/"}]},{"title":"《阿森实战网络设计》笔记","slug":"《阿森实战网络设计》笔记","date":"2020-02-21T14:35:44.000Z","updated":"2022-05-30T02:51:53.931Z","comments":true,"path":"2020/《阿森实战网络设计》笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/%E3%80%8A%E9%98%BF%E6%A3%AE%E5%AE%9E%E6%88%98%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1%E3%80%8B%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"区块链概念笔记","slug":"区块链概念笔记","date":"2020-02-21T07:21:53.000Z","updated":"2022-05-30T02:51:53.931Z","comments":true,"path":"2020/区块链概念笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/%E5%8C%BA%E5%9D%97%E9%93%BE%E6%A6%82%E5%BF%B5%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Hive笔记","slug":"Hive笔记","date":"2020-02-21T07:19:58.000Z","updated":"2022-05-30T02:51:53.802Z","comments":true,"path":"2020/Hive笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/Hive%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Hadoop笔记","slug":"Hadoop笔记","date":"2020-02-21T07:19:44.000Z","updated":"2022-05-30T02:51:53.802Z","comments":true,"path":"2020/Hadoop笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/Hadoop%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Go语言笔记","slug":"Go语言笔记","date":"2020-02-21T07:18:37.000Z","updated":"2022-05-30T02:51:53.800Z","comments":true,"path":"2020/Go语言笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/Go%E8%AF%AD%E8%A8%80%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"DevOps概念整理","slug":"DevOps概念整理","date":"2020-02-14T10:39:56.000Z","updated":"2022-06-28T18:31:02.489Z","comments":true,"path":"2020/DevOps概念整理/","link":"","permalink":"https://coconutmilktaro.top/2020/DevOps%E6%A6%82%E5%BF%B5%E6%95%B4%E7%90%86/","excerpt":"DevOps 持续集成、持续交付与持续部署 持续集成 持续交付 持续部署","text":"DevOps 持续集成、持续交付与持续部署 持续集成 持续交付 持续部署 DevOpsDevOps 是使软件开发和 IT 团队之间的流程自动化的一组实践，以便他们可以更快，更可靠地构建，测试和发布软件。 DevOps 的概念建立在建立团队之间协作文化的基础上，这些团队过去一直相对孤立地运作。DevOps 的好处包括增加信任度，更快的软件发布，快速解决关键问题的能力以及更好地管理计划外工作。 从本质上讲，DevOps 是一种文化，一种运动，一种哲学。它是开发和运维之间的有力握手，强调思维方式的转变、更好的协作和更紧密的集成。它将敏捷、持续交付、自动化和更多的东西结合在一起，以帮助开发和运营团队更高效、更快地创新，并向业务和客户交付更高的价值。 DevOps 优势： 速度：可以更快速地针对客户进行创新、更好地适应不断变化的市场，同时更有效地推动业务成果。微服务和持续交付能够让团队充分掌控服务，然后更快速地发布更新。 快速交付：提高发布的频率和速度，以便能够更快速地进行创新并完善产品。发布新功能和修复错误的速度越快，就越能快速地响应客户需求并建立竞争优势。持续集成和持续交付是自动执行软件发布流程（从构建到部署）的两项实践经验。 可靠性：确保应用程序更新和基础设施变更的品质，以便能够在保持最终用户优质体验的同时，更加快速可靠地进行交付。使用持续集成和持续交付等实践经验来测试每次变更是否安全以及能够正常运行。监控和日志记录实践经验能够帮助实时了解当前的性能。 规模：大规模运行和管理基础设施及开发流程。自动化和一致性可在降低风险的同时，帮助有效管理复杂或不断变化的系统。例如，基础设施即代码能够以一种可重复且更有效的方式来管理部署、测试和生产环境。 增强合作：建立一个适应 DevOps 文化模式的更高效的团队，强调主人翁精神和责任感。开发人员和运营团队密切合作，共同承担诸多责任，并将各自的工作流程相互融合。这有助于减少效率低下的工作，同时节约大家的时间（例如，缩短开发人员和运营团队之间的交接时间，编写将运行环境考虑在内的代码）。 安全性：在快速运转的同时保持控制力和合规性。利用自动实施的合规性策略、精细控制和配置管理技术，可以在不牺牲安全性的前提下采用 DevOps 模式。例如，利用基础设施即代码和策略即代码，可以大规模定义并追踪合规性。 持续集成、持续交付与持续部署CI/CD 是一种通过在应用开发阶段引入自动化来频繁向客户交付应用的方法。CI/CD 的核心概念是持续集成、持续交付和持续部署。作为一个面向开发和运营团队的解决方案，CI/CD 主要针对在集成新代码时所引发的问题（亦称：“集成地狱”）。 CI/CD 中的“CI”始终指持续集成，它属于开发人员的自动化流程。成功的 CI 意味着应用代码的新更改会定期构建、测试并合并到共享存储库中。该解决方案可以解决在一次开发中有太多应用分支，从而导致相互冲突的问题。 CI/CD 中的“CD”指的是持续交付和/或持续部署，这些相关概念有时会交叉使用。两者都事关管道后续阶段的自动化，但它们有时也会单独使用，用于说明自动化程度。 持续集成持续集成（Continuous Integration）：在软件开发过程中，频繁地将代码集成到主干上，然后进行自动化测试。持续集成的目的，就是让产品可以快速迭代，同时还能保持高质量。它的核心措施是，代码集成到主干之前，必须通过自动化测试。只要有一个测试用例失败，就不能集成。 持续集成（CI）可以帮助开发人员更加频繁地（有时甚至每天）将代码更改合并到共享分支或“主干”中。一旦开发人员对应用所做的更改被合并，系统就会通过自动构建应用并运行不同级别的自动化测试（通常是单元测试和集成测试）来验证这些更改，确保这些更改没有对应用造成破坏。这意味着测试内容涵盖了从类和函数到构成整个应用的不同模块。如果自动化测试发现新代码和现有代码之间存在冲突，CI 可以更加轻松地快速修复这些错误。 优势： 快速发现错误。每完成一点更新，就集成到主干，可以快速发现错误，定位错误也比较容易。 防止分支大幅偏离主干。如果不是经常集成，主干又在不断更新，会导致以后集成的难度变大，甚至难以集成。 一个 CI 系统的基本功能： 软件构建自动化：配置完成后，CI 系统会依照预先制订的时间表，或针对特定事件，对目标软件进行构建 构建可持续的自动化检查：能持续获取新增或修改后加入的源代码，CI 系统能不断确认这些新代码是否破坏原有软件的成功构建，这也就减少了开发者检查代码变化情况要花费的时间 构建可持续的自动化测试：构建检查的扩展部分，构建后执行预先指定的一套测试规则，完成后触发通知（email，rss 等） 生成后后续过程的自动化：当自动化检查和测试完成后，软件构建的周期中可能需要一些额外的任务，如生成文档、打包软件、部署到运行环境等 CI 系统的基本结构： CI 系统流程： 开发者将代码提交到源代码仓库 CI 为每个项目创建一个单独的工作区，当预设或请求一次新的构建时，CI 会将源代码仓库的源码存放到相应工作区 CI 会在对应工作区内执行构建 构建完成后，CI 会在新的构件中执行一套测试（如果配置的话），完成后通过 RSS 或 Email 通知相关人员 若构建成功，该构件会被打包转移到一个部署目标（如应用服务器），或存储为软件仓库中的一个新版本。（软件仓库可以是 CI 系统内部的一个部分（自己搭），也可以是外部的一个仓库（第三方仓库服务）） CI 通常会根据请求发起相应的操作，如即时构建、生成报告、或检索一些构建好的构件 持续交付持续交付（Continuous Delivery）：频繁地将软件的新版本，交付给质量团队或者用户，以供评审。如果评审通过，代码就进入生产阶段。持续交付可以看作持续集成的下一步。它强调的是，不管怎么更新，软件是随时随地可以交付的。 完成 CI 中构建及单元测试和集成测试的自动化流程后，持续交付可自动将已验证的代码发布到存储库。为了实现高效的持续交付流程，务必要确保 CI 已内置于开发管道。持续交付的目标是拥有一个可随时部署到生产环境的代码库。在持续交付中，每个阶段（从代码更改的合并，到生产就绪型构建版本的交付）都涉及测试自动化和代码发布自动化。在流程结束时，运维团队可以快速、轻松地将应用部署到生产环境中。 持续部署持续部署（Continuous Deployment）：代码通过评审以后，自动部署到生产环境，是持续交付的下一步。持续部署的目标是，代码在任何时刻都是可部署的，可以进入生产阶段。持续部署的前提是能自动化完成测试、构建、部署等步骤。 基础设施即代码（架构即代码） 基础设施即代码是一种实践经验，其中基础设施通过代码和软件部署技术（例如版本控制和持续集成）得以预置和管理。借助云的 API 驱动型模式，开发人员和系统管理员能够以编程方式与基础设施进行大规模互动，而无需手动设置和配置资源。因此，工程师可以使用基于代码的工具来连接基础设施，并且能够以处理应用程序代码的方式来处理基础设施。基础设施和服务器由代码进行定义，因此可以使用标准化模式进行快速部署、使用最新补丁和版本进行更新，或者以可重复的方式进行复制。 配置管理开发人员和系统管理员使用代码将操作系统和主机配置、操作性任务等自动化。代码的使用实现了配置变更的可重复性和标准化。它将开发人员和系统管理员从手动配置操作系统、系统应用程序或服务器软件的任务中解放出来。 策略即代码由于基础设施及其配置全都通过云进行代码编写，所以组织可以动态地大规模监控与实现合规性。因此，组织可以自动跟踪、验证和重新配置由代码描述的基础设施。这样一来，组织能够更加轻松地掌控资源变更，并确保安全措施以分布式方式得到妥善执行（例如，采用 PCI-DSS 或 HIPAA 确保信息安全性或合规性）。这使组织内部的团队能够更快速地运作，因为不合规的资源可能被自动标记为需要进一步调查，甚至被自动纠正为合规资源。 可变架构与不可变架构 可变架构（mutable infrastructure）：可变架构是 IT 服务器基础架构，能够定期直接进行修改和更新。传统上，由于可变方法提供更大的短期灵活性，服务器体系结构是可变的。 但是，可变的基础结构会以不可变的基础结构为可能，但要以不同服务器部署之间的可预测性和一致性为代价。 在可变架构中，将更改应用到现有架构之上，并且随着时间的推移，架构会建立更改历史记录。遵循可变架构范例的工具如 Ansible，Puppet 和 Chef 。 可变架构的优势包括： 架构可以更精确地满足服务器上运行的应用程序的需求。 更新通常更快，并且可以适应每个单独的服务器。 IT 员工无需从头开始创建新服务器，而是可以“个人”级别了解每台服务器，这有时可以帮助更快地解决问题。 可变架构的缺点包括： 技术问题很难诊断或重现，因为每个服务器都具有唯一的配置，这种现象通常称为“配置漂移（Configuration Drift）”。 对服务器的更改不一定会记录在案，从而使版本跟踪更加困难。 由于需要手动配置，配置服务器通常是一个漫长的过程。 不可变架构（immutable infrastructure）指服务器基础设施，一旦部署，就不能修改。它通常与 DevOps 和持续交付相关。如果需要进行更改或更新，则将在服务器上部署一个全新的实例，并进行适当的修改。新的环境可以在几分钟内在云中生成。这使得不可变架构对于 96%的企业来说更加可行。遵循不变基础架构范例的技术如 Terraform 。 不变基础架构的优点包括： 版本跟踪和回滚要容易得多。 IT 部门可以在部署新服务器或虚拟机时保留其标签。 由于不同服务器之间的配置一致，因此测试更易于运行。 不可能出现配置漂移。如果服务器已启动并正在运行，则 IT 员工可以知道该服务器的确切状态，避免任何意外情况。 不变的基础架构的缺点包括： 基础架构完全无法就地修改。例如，如果存在 zero-day 漏洞，则所有具有相同配置的服务器都必须接收安全更新。 不可变架构提高的敏捷性和动态性有时可能与传统的 IT 安全实践不符。 有状态与无状态 Web 浏览器和服务器的网络协议分为两种：无状态（Stateless）协议和有状态（Stateful）协议。根据服务器或服务器端软件保存状态或会话信息的要求，可以区分这两种协议。 无状态协议：无状态协议是网络协议的一种，客户端将请求发送到服务器，服务器根据当前状态返回响应。它不需要服务器为多个请求保留会话信息或有关每个通信对端的状态。如 HTTP，UDP，DNS 都是无状态协议。无状态协议的特点： 简化了服务器的设计。 只要较少的资源，因为系统不需要跟踪多连接通信和会话详细信息。 在无状态协议中，每个信息包都是自己拥有的，而不参考其他任何包。 无状态协议中的每个通信都是离散的，与之前或之后的通信无关。 有状态协议：在有状态协议中，如果客户端将请求发送到服务器，则它期望某种响应，如果未获得任何响应，则它将重新发送该请求。如 FTP，Telnet 都是有状态协议。有状态协议的特点： 通过跟踪连接信息为客户端提供更好的性能。 有状态的应用程序需要后备存储。 有状态请求始终取决于服务器端状态。 TCP 会话遵循有状态协议，因为这两个通信端都在会话生命周期内维护有关会话本身的信息。 有状态与无状态对比表格： 有状态协议 无状态协议 不要求服务器保留服务器信息或会话细节。 要求服务器保存状态和会话信息。 在 Internet 上很容易实现。 在逻辑上很难在 Internet 中实现。 处理事务非常快。 处理事务非常慢。 服务器和客户机之间没有紧密的依赖关系。 服务器和客户端之间存在紧密的依赖关系 简化了服务器设计。 使服务器的设计非常复杂和繁重。 因为没有必须还原的状态，发生故障的服务器可以在崩溃后重新启动。 在崩溃时不能更好地工作，因为有状态服务器必须保留状态信息和内部状态的会话详细信息。 微服务 自动化 参考文章持续集成、持续交付、持续部署简介 持续集成是什么？ [持续交付实践] 开篇：持续集成&amp;持续交付综述 什么是 CI/CD？ Continuous Integration: A “Typical” Process Difference between Stateless and Stateful Protocol A SIDE-BY-SIDE COMPARISON OF IMMUTABLE VS. MUTABLE INFRASTRUCTURE","categories":[],"tags":[]},{"title":"Gitlab服务搭建笔记","slug":"Gitlab服务搭建笔记","date":"2020-02-14T08:14:41.000Z","updated":"2022-05-30T02:51:53.800Z","comments":true,"path":"2020/Gitlab服务搭建笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/Gitlab%E6%9C%8D%E5%8A%A1%E6%90%AD%E5%BB%BA%E7%AC%94%E8%AE%B0/","excerpt":"GitLab 是利用 Ruby On Rails 一个开源的版本管理系统，实现一个自托管的 Git 项目仓库，可通过 Web 界面进行访问公开的或者私人项目。它拥有与 GitHub 类似的功能，能够浏览源代码，管理缺陷和注释。","text":"GitLab 是利用 Ruby On Rails 一个开源的版本管理系统，实现一个自托管的 Git 项目仓库，可通过 Web 界面进行访问公开的或者私人项目。它拥有与 GitHub 类似的功能，能够浏览源代码，管理缺陷和注释。 gitlab 组成： nginx gitlab-shell：用于处理 Git 命令和修改 authorized keys 列表 gitlab-workhorse：智能反向代理服务器，可帮助缓解 Unicorn 的压力 logrotate：日志记录 postgresql：存储应用程序元数据和用户信息 redis：存储会话数据、临时缓存信息、后台作业队列 sidekiq：Sidekiq 是 Ruby 后台作业处理器，可从 Redis 队列中提取作业并进行处理。 后台作业允许 GitLab 通过将工作移至后台来提供更快的请求/响应周期。 unicorn：一个用于运行核心 Rails 应用程序的 Ruby 应用程序服务器，该应用程序提供了 GitLab 中面向用户的功能 gitlab 文件目录： 主配置文件: /etc/gitlab/gitlab.rb 文档根目录: /opt/gitlab 默认存储库位置: /var/opt/gitlab/git-data/repositories Nginx 配置文件: /var/opt/gitlab/nginx/conf/gitlab-http.conf Postgresql 数据目录: /var/opt/gitlab/postgresql/data GitLab 部署搭建环境 Centos7 在安装 Gitlab 前需要先安装的软件 openssh-server：用于各个主机与 gitlab 进行 ssh 连接 postfix：邮件通知需要 yum install -y openssh-server postfixsystemctl enable postfix sshdsystemctl start postfix sshd 若开启了防火墙，需要放行服务 http 和 https 由于 Gitlab 的官方脚本可能用不了，可以使用清华的源进行安装。先配置/etc/yum.repos.d/gitlab.repo [gitlab-ce]name=Gitlab CE Repositorybaseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el$releasever/gpgcheck=0enabled=1# 最好将$releasever直接改为版本号，即 7baseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/ 然后安装 gitlab yum install -y gitlab-ce 修改配置文件/etc/gitlab/gitlab.rb # 修改external_url，改为本机域名external_url &#x27;http://gitlab.example.com&#x27;# 设置邮箱gitlab_rails[&#x27;gitlab_email_enabled&#x27;] = truegitlab_rails[&#x27;gitlab_email_from&#x27;] = &#x27;xxx@163.com&#x27;user[&#x27;git_user_email&#x27;] = &quot;xxx@163.com&quot;# 设置smtp邮件服务器gitlab_rails[&#x27;smtp_enable&#x27;] = truegitlab_rails[&#x27;smtp_address&#x27;] = &quot;smtp.163.com&quot;gitlab_rails[&#x27;smtp_port&#x27;] = 25gitlab_rails[&#x27;smtp_user_name&#x27;] = &quot;xxx@163.com&quot;gitlab_rails[&#x27;smtp_password&#x27;] = &quot;xxxxxxxxx&quot;gitlab_rails[&#x27;smtp_domain&#x27;] = &quot;163.com&quot;gitlab_rails[&#x27;smtp_authentication&#x27;] = &quot;login&quot;gitlab_rails[&#x27;smtp_enable_starttls_auto&#x27;] = truegitlab_rails[&#x27;smtp_tls&#x27;] = false 执行gitlab-ctl reconfigure，gitlab 开始自动安装配置。浏览器直接访问服务器 ip 或域名即可进入 gitlab，首次登录会要求设置用户 root 的密码（要复杂的），然后登录即可，使用与 github 类似。 常用命令Gitlab 实现 CI 持续集成Gitlab 自带持续集成方案即 Gitlab CI，因此无需额外配置或搭建额外的 CI 系统。Gitlab CI 的触发为 Git 提交检索.gitlab-ci.yaml文件触发，而执行过程是在客户端的Gitlab-Runner上执行的。 Gitlab-Runner就是一台安装了Gitlab-Runner软件的主机，只要该服务器在Gitlab Server端注册，则该Gitlab Runner就能在Server端的.gitlab-ci.yaml中使用该Runner执行pipeline的stage。Runner可分布在不同服务器上，一台服务器上也可以有多个Runner。 Gitlab Runner有以下分类： Shared Runner：共享型，所有项目都能使用该Runner执行job Specfic Runner：特定型，该Runner只为特定项目服务 Group Runner：若将该Runner注册到项目组中，则该项目组中的项目可使用该Runner Gitlab Runner状态： Locked：被某个项目锁定，只服务该项目，不能用于其他项目 Paused：被暂停，不能接受任何job 流程： 开发提交代码并发出Merge Request，触发pipeline Server分配Runner执行pipeline 通过公共Runner部署代码到生产服务器 注册时需要为Runner指定Executor执行器，用于在不同场景下进行构建任务，不同Executor支持不同平台及执行不同方法。以下为常见Executor： Shell：直接将Runner注册到Server端口，后期所有运行的job所需依赖都需要手动安装在这台注册的系统上。 Docker：Docker镜像作为执行器，容易进行依赖管理 Docker Machine：使用Docker Machine构建机，支持自动缩放 Virtualbox：虚拟机执行job SSH：类似跳板，连接到外部服务器，并在那进行构建 Kubernetes：放在k8s集群中构建，集群为每个gitlab作业创建一个pod来构建任务，构建完成后再释放掉。 .gitlab-ci.yaml中的重要概念： pipeline：一套构建任务的集合，包含很多流程，如依赖安装、编译、测试、部署等 stages：步骤，pipeline中包含多个stage，所有stage都从上到下顺序执行，若其中一个stage执行失败，则不再继续执行。只有所有stage都执行成功，pipeline状态才为成功。 jobs：每个stage由多个job组成，同一个stage中的job是并行执行的，任一个job失败导致stage失败导致pipeline失败。只有所有job都执行成功，stage才为成功。 Gitlab CI/CD流程： 校验：完成CI，包含代码扫描、性能测试、单元测试、容器扫描、依赖扫描等，stage失败就打回给开发者修复，直到分支的pipeline都构建测试正常。 打包：gitlab提供多种打包方式：Docker镜像（Container Registry）、Npm包（NPM Registry）、Maven工件（Maven Repository）、Conan包（Conan Repository） 发布：完成CD持续部署，将通过测试和构建的分支merge到主分支，手动确认应用部署进入正式环境，进行应用发布。 .gitlab-ci.yaml中的常用关键字： stages jobs variables：变量，在jobs中起作用 environment：用于定义job部署到特殊环境中，若没有该环境则自动创建 scirpts：runner执行的命令和脚本，必填 tags：指定注册在该项目下的runner（runner也要设置tag） except：在一个job下，指定一个git分支不进行此构建，若only和except在一个job中同时存在，以only为准 only：一个job中指定一个git分支可执行此构建 when：何时开始job。可设为on_success、on_failure、always、manual before_script：在job执行前的命令 after_script：定义作业部署完成后的环境名称 cache：定义一组文件列表，可在后续stage使用 image：指定stage使用的docker镜像 gitlab-ci.yaml文件样例： stages: - build - test - deployvariables: BASE_DIR: &quot;/usr/proj/&quot;cache: paths: - node_modules/ - dist/before_script: - cp $&#123;BASE_DIR&#125;.env.example $&#123;BASE_DIR&#125;.env - yarn install - yarn buildbuild_job: # 创建job，名为build_job stage: build script: - yarn build tags: - dev only: - develop when: alwaystest_job: stage: test script: - yarn test tags: -dev only: - develop when: manualdeploy_job: stage: deploy only: - master script: - pm2 delete api || true - pm2 start dist/index.js --name api tags: - dev when: always 参考文章： 从零开始搭建 Gitlab 服务器 搭建 GitLab 服务器 ( CTO 必会) GitLab 的安装及使用教程 GitLab Architecture Overview 敏捷无敌之 Gitlab CI 持续集成","categories":[],"tags":[]},{"title":"PostgreSQL笔记","slug":"PostgreSQL笔记","date":"2020-02-13T11:56:48.000Z","updated":"2022-05-30T02:51:53.864Z","comments":true,"path":"2020/PostgreSQL笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/PostgreSQL%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Serverless技术笔记","slug":"Serverless技术笔记","date":"2020-02-13T11:19:50.000Z","updated":"2022-05-30T02:51:53.870Z","comments":true,"path":"2020/Serverless技术笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/Serverless%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/","excerpt":"Serverless 简介 Serverless 技术特点","text":"Serverless 简介 Serverless 技术特点 Serverless 简介Serverless（无服务器架构）指的是服务端逻辑由开发者实现，运行在无状态的计算容器中，由事件触发，完全被第三方管理，而业务层面的状态则记录在数据库或存储资源中。用户无需管理服务器等基础设施，只需编写代码和选择触发器（trigger)，比如 RPC 请求、定时器等并上传，其余的工作（如实例选择、扩缩容、部署、容灾、监控、日志、安全补丁等）全部由 serverless 系统托管。用户只需要为代码实际运行消耗的资源付费——代码未运行则不产生费用（Pay as you go）。 Serverless 相比 Serverful，有以下 3 个改变： 弱化了存储和计算之间的联系。服务的储存和计算被分开部署和收费，存储不再是服务本身的一部分，而是演变成了独立的云服务，这使得计算变得无状态化，更容易调度和扩缩容，同时也降低了数据丢失的风险。 代码的执行不再需要手动分配资源。不需要为服务的运行指定需要的资源（比如使用几台机器、多大的带宽、多大的磁盘等），只需要提供一份代码，剩下的交由 serverless 平台去处理就行了。当前阶段的实现平台分配资源时还需要用户方提供一些策略，例如单个实例的规格和最大并发数，单实例的最大 CPU 使用率。理想的情况是通过某些学习算法来进行完全自动的自适应分配。 按使用量计费。Serverless 按照服务的使用量（调用次数、时长等）计费，而不是像传统的 serverful 服务那样，按照使用的资源（ECS 实例、VM 的规格等）计费。 Serverless 两个重要的产品： FaaS（Function as a Service， 函数即服务，也称为云函数）：FaaS 提供了一个计算平台，在这个平台上，应用以一个或多个函数的形式开发、运行和管理。FaaS 平台提供了函数式应用的运行环境，一般支持多种主流的编程语言，如 Java、PHP 及 Python 等。FaaS 可以根据实际的访问量进行应用的自动化动态加载和资源的自动化动态分配。大多数 FaaS 平台基于事件驱动（Event Driven）的思想，可以根据预定义的事件触发指定的函数应用逻辑。 BaaS（Backend as a Service，后端即服务）：通过 BaaS 平台将应用所依赖的第三方服务，如数据库、消息队列及存储等服务化并发布出来，用户通过向 BaaS 平台申请所需要的服务进行消费，而不需要关心这些服务的具体运维。 FaaS 与 PaaS 的比较：大部分 PaaS 应用无法针对每个请求启动和停止整个应用程序，而 FaaS 平台生来就是为了实现这样的目的。FaaS 和 PaaS 在运维方面最大的差异在于缩放能力。对于大部分 PaaS 平台，用户依然需要考虑缩放。但是对于 FaaS 应用，这种问题完全是透明的。就算将 PaaS 应用设置为自动缩放，依然无法在具体请求的层面上进行缩放，而 FaaS 应用在成本方面效益就高多了。 Serverless 架构的优点 降低运营成本：Serverless 是非常简单的外包解决方案。它可以让您委托服务提供商管理服务器、数据库和应用程序甚至逻辑，否则您就不得不自己来维护。由于这个服务使用者的数量会非常庞大，于是就会产生规模经济效应。在降低成本上包含了两个方面，即基础设施的成本和人员（运营/开发）的成本。 降低运维需求：Serverless 使得应用与服务器解耦，业务上线前无需预估资源，无需进行服务器购买、配置 Serverless 也使得底层运维工作量进一步降低，业务上线后，也无需担忧服务器运维，而是全部交给了云平台或云厂商 降低开发成本：IaaS 和 PaaS 存在的前提是，服务器和操作系统管理可以商品化。Serverless 作为另一种服务的结果是整个应用程序组件被商品化。 缩短迭代周期、上线时间：Serverless 架构带来的是进一步的业务解耦，应用功能被解构成若干个细颗粒度的无状态函数，开发可以聚焦在单功能的快速开发和上线。同时，拆解后的云函数，也都可以进行独立的迭代升级，更快速的实现业务迭代，缩减功能的上线时间 扩展能力：Serverless 架构一个显而易见的优点即“横向扩展是完全自动的、有弹性的、且由服务提供者所管理”。从基本的基础设施方面受益最大的好处是，您只需支付您所需要的计算能力。 更简单的管理：Serverless 架构明显比其他架构更简单。更少的组件，就意味着您的管理开销会更少。 快速试错：利用 Serverless 架构的简单运维、低成本及快速上线能力，可以来快速尝试业务的新形态、新功能，利用 Serverless 产品的强弹性扩容能力，在业务获得成功时，也无需为资源扩容而担心 “绿色”的计算：按照《福布斯》杂志的统计，在商业和企业数据中心的典型服务器仅提供 5%～ 15%的平均最大处理能力的输出。这无疑是一种资源的巨大浪费。随着 Serverless 架构的出现，让服务提供商提供我们的计算能力最大限度满足实时需求。这将使我们更有效地利用计算资源。 Serverless 架构的缺点 状态管理：要想实现自由的缩放，无状态是必须的，而对于有状态的服务，使用 serverless 这就丧失了灵活性，有状态服务需要与存储交互就不可避免的增加了延迟和复杂性。 延迟：应用程序中不同组件的访问延迟是一个大问题，我们可以通过使用专有的网络协议、RPC 调用、数据格式来优化，或者是将实例放在同一个机架内或同一个主机实例上来优化以减少延迟。而 serverless 应用程序是高度分布式、低耦合的，这就意味着延迟将始终是一个问题，单纯使用 serverless 的应用程序是不太现实的。 本地测试：Serverless 应用的本地测试困难是一个很棘手的问题。虽然可以在测试环境下使用各种数据库和消息队列来模拟生产环境，但是对于无服务应用的集成或者端到端测试尤其困难，很难在本地模拟应用程序的各种连接，并与性能和缩放的特性结合起来测试，并且 serverless 应用本身也是分布式的，简单的将无数的 FaaS 和 BaaS 组件粘合起来也是有挑战性的。 Serverless 适用的场景： 异步的并发，组件可独立部署和扩展 应对突发或服务使用量不可预测（主要是为了节约成本，因为 Serverless 应用在不运行时不收费） 短暂、无状态的应用，对冷启动时间不敏感 需要快速开发迭代的业务（因为无需提前申请资源，因此可以加快业务上线速度） Serverless 常见的应用： WEB 及移动后端通过结合使用云函数和 API 网关或 HTTP 触发器，可以对外提供 URL 访问地址，成为 Web、小程序、或移动应用等的后端服务。Serverless 架构既可以直接用于构建后台来服务应用，也可以通过类似 BFF 模式，构建中台和应用间的桥梁。Serverless 架构提供的强弹性能力，使得可以支撑业务或应用的暴涨；而提供的低运维需求，使得开发者可以专注于业务实现和优化；同时，按实际使用量的付费方式，使得开发者无需预配置资源，无需担心预配置资源的浪费。 消息处理Serverles 架构的应用本身是由事件触发的，因此极其适合于进行消息处理。无论是消息队列中传递的业务消息，还是 Kafka 中采集应用日志，均可以对接到云函数上，进行实时的消息处理、分析。 对象存储文件处理在 Serverless 应用场景中，由对象存储中的文件上传事件，来触发云函数的运行，也是一种常见场景。针对图片文件的上传，可以借助云函数完成图片的缩略图生成、二维码或水印标记、图片优化处理；而针对数据文件的上传，可以启动数据的自动化分析 物联网物联网意味着成千上万的设备会连入网络，时刻在不断的产生数据，这对数据的分析、处理的及时性提出了很高的挑战。通过使用 Serverless 架构，物联网设备所采集的数据将可以作为云函数的触发事件，而实现数据的实时处理、分析和应用。随着物联网设备计算能力的进一步提升，云函数作为最小粒度的计算单元，有机会被调度到设备端运行，实现边缘计算，达到「端 - 云」联合的 Serverless 架构。 运维及集成通过对接云函数以及云上的各个产品、日志服务、监控告警系统，云时代的运维也都可以用云函数来构建。定时触发的云函数，将可以方便地替代需要在主机上来运行的定时任务；而日志或告警触发的云函数，将可以对云中的事件作出立刻回应及处理。 Serverless 技术特点 事件驱动 云函数的运行，是由事件驱动起来的，在有事件到来时，云函数会启动运行 Serverless 应用不会类似于原有的「监听 - 处理」类型的应用一直在线，而是按需启动 事件的定义可以很丰富，一次 http 请求，一个文件上传，一次数据库条目修改，一条消息发送，都可以定义为事件 单事件处理 云函数由事件触发，而触发启动的一个云函数实例，一次仅处理一个事件 无需在代码内考虑高并发高可靠性，代码可以专注于业务，开发更简单 通过云函数实例的高并发能力，实现业务高并发 自动弹性伸缩 由于云函数事件驱动及单事件处理的特性，云函数通过自动的伸缩来支持业务的高并发 针对业务的实际事件或请求数，云函数自动弹性合适的处理实例来承载实际业务量 在没有事件或请求时，无实例运行，不占用资源 无状态开发 云函数运行时根据业务弹性，可能伸缩到 0，无法在运行环境中保存状态数据 分布式应用开发中，均需要保持应用的无状态，以便于水平伸缩 可以利用外部服务、产品，例如数据库或缓存，实现状态数据的保存 参考文档 Serverless 基本概念入门 Serverless 的运行原理与组件架构 Serverless Framework 产品概述 Serverless 架构应用开发指南 从 IaaS 到 FaaS—— Serverless 架构的前世今生 Serverless Handbook——无服务架构实践手册 Serverless（无服务）基础知识","categories":[],"tags":[]},{"title":"Prometheus-python-API使用笔记","slug":"Prometheus-python-API使用笔记","date":"2020-01-07T14:49:26.000Z","updated":"2022-05-30T02:51:53.864Z","comments":true,"path":"2020/Prometheus-python-API使用笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/Prometheus-python-API%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Ansible-python-API使用笔记","slug":"Ansible-python-API使用笔记","date":"2020-01-07T14:49:09.000Z","updated":"2022-05-30T02:51:53.769Z","comments":true,"path":"2020/Ansible-python-API使用笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/Ansible-python-API%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/","excerpt":"Ansible python API v2.9.4 , Ansible v2.9.4","text":"Ansible python API v2.9.4 , Ansible v2.9.4 API官方案例import jsonimport shutilfrom ansible.module_utils.common.collections import ImmutableDictfrom ansible.parsing.dataloader import DataLoaderfrom ansible.vars.manager import VariableManagerfrom ansible.inventory.manager import InventoryManagerfrom ansible.playbook.play import Playfrom ansible.executor.task_queue_manager import TaskQueueManagerfrom ansible.plugins.callback import CallbackBasefrom ansible import contextimport ansible.constants as Cclass ResultCallback(CallbackBase): def v2_runner_on_ok(self, result, **kwargs): host = result._host print(json.dumps(&#123;host.name: result._result&#125;, indent=4))context.CLIARGS = ImmutableDict(connection=&#x27;local&#x27;, module_path=[&#x27;/to/mymodules&#x27;], forks=10, become=None, become_method=None, become_user=None, check=False, diff=False)loader = DataLoader()passwords = dict(vault_pass=&#x27;secret&#x27;)results_callback = ResultCallback()inventory = InventoryManager(loader=loader, sources=&#x27;localhost,&#x27;)variable_manager = VariableManager(loader=loader, inventory=inventory)play_source = dict( name = &quot;Ansible Play&quot;, hosts = &#x27;localhost&#x27;, gather_facts = &#x27;no&#x27;, tasks = [ dict(action=dict(module=&#x27;shell&#x27;, args=&#x27;ls&#x27;), register=&#x27;shell_out&#x27;), dict(action=dict(module=&#x27;debug&#x27;, args=dict(msg=&#x27;&#123;&#123;shell_out.stdout&#125;&#125;&#x27;))) ] )play = Play().load(play_source, variable_manager=variable_manager, loader=loader)tqm = Nonetry: tqm = TaskQueueManager( inventory=inventory, variable_manager=variable_manager, loader=loader, passwords=passwords, stdout_callback=results_callback, ) result = tqm.run(play)finally: if tqm is not None: tqm.cleanup() shutil.rmtree(C.DEFAULT_LOCAL_TMP, True) API详解CallbackBaseCallbackBase是需要重写的结果返回类。类的定义位于ansible.plugins.plugins.callback的__init__.py中。因为ansible版本是&gt;2的，所以可以重写的方法开头是带有v2的。一些可重写的方法： v2_runner_on_failed：当执行失败后执行的方法 v2_runner_on_ok：当执行成功后执行的方法 v2_runner_on_unreachable：当出现无法连接的情况执行的方法 参考文献 Ansible 官方文档","categories":[{"name":"Python开发","slug":"Python开发","permalink":"https://coconutmilktaro.top/categories/Python%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"API","slug":"API","permalink":"https://coconutmilktaro.top/tags/API/"},{"name":"python","slug":"python","permalink":"https://coconutmilktaro.top/tags/python/"},{"name":"ansible","slug":"ansible","permalink":"https://coconutmilktaro.top/tags/ansible/"}]},{"title":"Kubernetes网络学习笔记","slug":"Kubernetes网络学习笔记","date":"2020-01-07T14:48:17.000Z","updated":"2022-06-21T15:16:21.206Z","comments":true,"path":"2020/Kubernetes网络学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/Kubernetes%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"K8s网络基础架构 Ingress K8s网络策略 K8s网络故障定位 K8s 网络实现机制 K8s DNS Flannel Calico Weave Cilium","text":"K8s网络基础架构 Ingress K8s网络策略 K8s网络故障定位 K8s 网络实现机制 K8s DNS Flannel Calico Weave Cilium K8s网络基础架构IngressK8s网络策略K8s网络故障定位K8s 网络实现机制K8s DNSFlannelCalicoWeaveCilium","categories":[],"tags":[]},{"title":"分布式系统架构","slug":"分布式系统架构","date":"2020-01-07T14:46:27.000Z","updated":"2022-05-30T02:51:53.931Z","comments":false,"path":"2020/分布式系统架构/","link":"","permalink":"https://coconutmilktaro.top/2020/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/","excerpt":"","text":"分布式系统基础 什么是分布式系统？ 分布式系统是若干独立计算机的集合，这些计算机对于用户来说就是单个系统。 分布式系统面临什么挑战？ 异构性：分布式系统由不通网络、操作系统、硬件和编程语言组成，必须由一种通用协议来屏蔽异构系统间的差异。往往用中间件来处理差异。 缺乏全球时钟：交换消息依赖对于程序动作发生时间的共识，计算机同步时钟准确性受到极大限制。 一致性：如何保证各主机间数据的一致性 故障独立性：应该允许部分故障而不会影响整个系统的正常使用 并发：系统中每个资源必须被设计为是并发环境中安全的 透明性：系统中任何组件故障对于用户应是不可见的 开放性：所有组件的接口必须遵循一定规范并能理解维护 安全性：对网络上所有敏感信息进行加密 可扩展性：系统要能随业务量增加而相应扩展 集群概念几种服务器性能增强方式： Scale on：向上扩展。也称垂直扩展，在服务器硬件上扩展 Scale out：向外扩展。也称水平扩展，在服务器数量上扩展 LB 集群与 HA 集群的着眼点： LB 集群是为了增加请求的并发处理能力，而 HA 集群是为了增加服务的可用性 RAID 阵列与 NFS 的区别： NFS 是文件系统服务器，前端 web 对数据的请求是文件级别的。属于 NAS NAS 具有锁机制，因为 NAS 是服务器，是有操作系统的，这样就不会造成数据的不一致了。 RAID 阵列是磁盘，前端 web 对数据的请求是块级别的。属于 DAS DAS 就是磁盘，无法设置磁盘锁，因为读写操作是在内存中执行的，读取的数据都是在 web 服务器中执行操作，不同的 web 服务器读取同一段数据会造成数据的不一致 但是 DAS 的速度远高于 NAS 一个文件包含多个数据块 资源粘性：资源更倾向于运行在哪个节点 节点间通过 Messaging Layer 传递资源粘性值，但 Messaging Layer 并不进行比较，而是在 CRM（Cluster Resource Manager 集群资源管理器）上比较，指定资源应该运行在哪个节点上。 资源约束：Constraint ​ 排列约束 ​ 位置约束（location） Session 共享： 基于 Cookie 的 Session 共享： 将全站用户的 Session 信息加密并序列化后以 Cookie 的方式统一存放在根域名下，当浏览器访问该根域名下的所有二级域名时，会将域名相对应的所有 Cookie 内容传递给子服务器，实现用户的 Cookie 化 Session 在多个服务器间共享。 优点：无需额外服务器资源。 缺点：受 HTTP 协议头长度限制，仅存储小部分用户信息。 基于数据库的 Session 共享： 实用性强，但比较复杂，Session 的逻辑淘汰也需要自己实现，并且 Session 的并发读写能力取决于数据库的性能。 Session 复制： 将用户的 Session 复制到每个要访问的服务器，Tomcat 和 Weblogic 都带有这种机制，但随机器增加，网络负担会成指数上升。 基于 Memcached 或 Redis 的 Session 共享： Memcached 和 Redis 适合用于存放 Session，Memcached 和 Redis 中的 Hash 表具有 Expires 数据淘汰机制，符合 Session 的要求。并且这两款数据存储系统的性能都很强，能够应对高并发场景。 会话保持： 会话保持不是 Session 共享。网站中有时一次操作需要与服务器进行多次交互，并且这几次的交互都是紧密联系的，这要求相关操作都要在一台服务器上完成，不能被负载到其他服务器上。 会话保持就是指在负载均衡器上的机制，可识别客户和服务器间交互的关联性，在做负载均衡的同时，还能保持一系列相关的访问都分配到同一台服务器上。 负载均衡器的会话保存机制： 基于源 IP 的持续性保持：主要用于四层负载均衡，如 Nginx 的 ip_hash、HAProxy 的 source 算法。 基于 Cookie 的持续性保持：主要用于七层负载均衡，同一会话的被分配到同一台服务器上。 根据应答报文中是否带有Set_Cookie字段，可分为 Cookie 插入保持和 Cookie 截取保持。 基于 HTTP 报文头的持续性保持：主要用于七层负载均衡，负载均衡器首次收到客户端的请求时，会建立该客户端的表项，记录为该客户端分配服务器的情况。在会话表项生存期内，后续具有相同 HTTP 报文头的连接都发往同一服务器。","categories":[{"name":"架构","slug":"架构","permalink":"https://coconutmilktaro.top/categories/%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://coconutmilktaro.top/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"Kafka基础笔记","slug":"Kafka基础笔记","date":"2020-01-07T14:45:38.000Z","updated":"2022-05-30T02:51:53.807Z","comments":true,"path":"2020/Kafka基础笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/Kafka%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/","excerpt":"消息队列 Kafka 介绍 架构 搭建","text":"消息队列 Kafka 介绍 架构 搭建 消息队列消息队列（Message Queue）是一种进程间通信或同一进程的不同线程间的通信方式，软件的队列用来处理一系列的输入，通常是来自用户。消息队列提供了异步的通信协议，每一个队列中的纪录包含详细说明的数据，包含发生的时间，输入设备的种类，以及特定的输入参数，也就是说：消息的发送者和接收者不需要同时与消息队列交互。消息会保存在队列中，直到接收者取回它。 消息队列各个角色： Producer：消息生产者。负责产生和发送消息到 Broker Broker：消息处理中心。负责消息存储、确认、重试等，一般其中会包含多个 queue Consumer：消息消费者。负责从 Broker 中获取消息，并进行相应处理 消息队列种类： Peer—To-Peer：消息生产者生产消息发送到 queue 中，然后消息消费者从 queue 中取出并且消费消息。消息被消费以后，queue 中不再有存储，所以消息消费者不可能消费到已经被消费的消息。queue 支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 一般基于 Pull 或 Polling 接收消息 支持异步”即发即弃“的消息传送方式，支持同步请求/应答传送方式 支持一对一、一对多、多对多、多对一等多种配置方式，支持树状、网状等多种拓扑结构。 发布/订阅：消息生产者将消息发布到 topic 中，同时有多个消息消费者消费该消息。和点对点方式不同，发布到 topic 的消息会被所有订阅者消费。 发布到一个主题的信息，可被多个订阅者所接收 发布/订阅可基于 Push 消费数据，也可基于 Polling 消费数据 解耦能力比 P2P 模型更强 多点广播：能够将消息发送到多个目标站点 (Destination List)。可以使用一条 MQ 指令将单一消息发送到多个目标站点，并确保为每一站点可靠地提供信息。 MQ 不仅提供了多点广播的功能，而且还拥有智能消息分发功能，在将一条消息发送到同一系统上的多个用户时，MQ 将消息的一个复制版本和该系统上接收者的名单发送到目标 MQ 系统。目标 MQ 系统在本地复制这些消息，并将它们发送到名单上的队列，从而尽可能减少网络的传输量。 群集 (Cluster)：为了简化点对点通讯模式中的系统配置，MQ 提供 Cluster(群集) 的解决方案。群集类似于一个域 (Domain)，群集内部的队列管理器之间通讯时，不需要两两之间建立消息通道，而是采用群集 (Cluster) 通道与其它成员通讯，从而大大简化了系统配置。 此外，群集中的队列管理器之间能够自动进行负载均衡，当某一队列管理器出现故障时，其它队列管理器可以接管它的工作，从而大大提高系统的高可靠性。 消息队列特性： 异步性：将耗时的同步操作，通过以发送消息的方式，进行了异步化处理。减少了同步等待的时间。 松耦合：消息队列减少了服务之间的耦合性，不同的服务可以通过消息队列进行通信，而不用关心彼此的实现细节，只要定义好消息的格式就行。 分布式：通过对消费者的横向扩展，降低了消息队列阻塞的风险，以及单个消费者产生单点故障的可能性（当然消息队列本身也可以做成分布式集群）。 可靠性：消息队列一般会把接收到的消息存储到本地硬盘上（当消息被处理完之后，存储信息根据不同的消息队列实现，有可能将其删除），这样即使应用挂掉或者消息队列本身挂掉，消息也能够重新加载。 可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 顺序保证：在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。 缓冲：在任何重要的系统中，都会有需要不同的处理时间的元素。例如，加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行———写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。 Kafka 介绍Kafka 最初是由 LinkedIn 采用 scala 编写的多分区、多副本且基于 Zookeeper 协调的分布式消息系统，目前为 Apache 的项目。目前，kafka 定位为一个分布式流式处理平台，可提供消息持久化，即使 TB 级别的消息存储也能保持长时间的稳定性能，以高吞吐、可持久化、可水平扩展、支持流数据处理等特性被广泛使用。 Kafka 扮演了三大角色： 消息系统：Kafka 和传统的消息系统（消息中间件）都具备系统解耦、冗余存储、流量削峰、缓存、异步通信、扩展性、可恢复性等功能。同时，kafka 还提供大多数消息系统难以实现的消息顺序性保障及回溯消费功能 存储系统：Kafka 把消息持久化到磁盘，相比于其他基于内存存储的系统而言，有效降低了数据丢失的风险，得益于 Kafka 的消息持久化和多副本，可以把 Kafka 作为长期的数据存储系统来使用，只需要把对应的数据保留策略设置为永久或启动主题的日志压缩功能即可。 流式处理平台：Kafka 不仅为每个流行的流式处理框架提供了可靠的数据来源，还提供了一个完整的流式处理类库，如窗口、连接、变换和聚合等操作。 Kafka 设计目标： 高吞吐率：在廉价的商用机器上单机可支持每秒 100 万条消息的读写 消息持久化：所有消息均被持久化到磁盘，无消息丢失，支持消息重放 完全分布式：Producer，Broker，Consumer 均支持水平扩展 同时适应在线流处理和离线批处理 架构 常见术语： Topic：主题，特指 Kafka 处理的消息源（feeds of messages）的不同分类，Kafka 中消息以主题为单位进行归类，生产者负责将消息发送到特定的主题，然后消费者负责订阅主题并消费。物理上不同 topic 的消息分开存储，逻辑上一个 topic 的消息虽然保存于一个或多个 broker 上但用户只需指定消息的 topic 即可生产或消费数据而不必关心数据存于何处。 Partition：分区，Topic 物理上的分组，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列。 分区在存储层面可以看作一个可追加的日志文件，消息被追加到分区日志文件时都会分配一个特定的偏移量（offset），offset 是消息在分区中的唯一标识，Kafka 通过它保证消息在分区内的有序性，且 offset 不跨分区，就是说 Kafka 保证分区有序而不是主题有序。 创建 topic 时可指定 parition 数量。每个 partition 对应于一个文件夹，该文件夹下存储该 partition 的数据和索引文件。 Kafka 的分区可分布在不同的服务器（Broker）上，也就是说，一个主题可横跨多个 Broker，一次提供比单个 Broker 更强大的性能。 Message：消息，是通信的基本单位，每个 producer 可以向一个 topic（主题）发布一些消息。 Producers：一个动词，向 Kafka 的一个 topic 发布消息的过程叫做 producers。 Consumers：一个动词，订阅 topics 并处理其发布的消息的过程叫做 consumers。 Consumer Group：消费者组，可以并行消费 Topic 中 partition 的消息 每条消息被发送到 Broker 前，会根据分区规划选择存储到哪个具体分区。若分区规则设置地合理，则所有消息都可均匀地分配到不同分区中。若一个主题只对应一个文件，那么这个文件所在的机器 I/O 将会成为这个主题的性能瓶颈，而分区解决了这个问题。 搭建需要先安装 JAVA 环境 从官网下载最新的 Kafka 的 tar 包，解压放到/usr/local/下。 先配置启动 Zookeeper，Kafka 包简易集成了 Zookeeper。首先修改config/中的&quot;zookeeper.properties&quot;配置文件 dataDir=/data/zookeeper # zookeeper的数据存放，最好专门放一个目录clientPort=2181 # 启动端口，默认2181 kafka 包的bin/中有一个脚本启动 zookeeper，为zookeeper-server-start.sh。在 kafka 包中，执行以下命令即可启动单机 zookeeper bin/zookeeper-server-start.sh config/zookeeper.properties 查看 zookeeper 是否启动 # lsof -i:2181COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEjava 21999 root 116u IPv6 771206 0t0 TCP *:eforward (LISTEN) 然后启动 kafka，先配置文件config/server.properties broker.id=0 # Broker的idlisteners=PLAINTEXT://localhost:9092 # 监听端口，默认9092log.dirs=/data/kafka-logs # 将数据持久化到哪个目录num.partitions=3 # 默认的partition数量 然后通过脚本kafka-server-start.sh启动 Kafka，并指定-daemon后台启动 bin/kafka-server-start.sh -daemon config/server.properties 再查看一下 Kafka 是否启动 # lsof -i:9092COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEjava 5199 root 121u IPv6 81847 0t0 TCP localhost:XmlIpcRegSvc (LISTEN)java 5199 root 137u IPv6 82158 0t0 TCP localhost:54104-&gt;localhost:XmlIpcRegSvc (ESTABLISHED)java 5199 root 138u IPv6 82159 0t0 TCP localhost:XmlIpcRegSvc-&gt;localhost:54104 (ESTABLISHED)# lsof -i:2181COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEjava 3080 root 116u IPv6 72020 0t0 TCP *:eforward (LISTEN)java 3080 root 122u IPv6 82155 0t0 TCP localhost:eforward-&gt;localhost:55208 (ESTABLISHED)java 5199 root 116u IPv6 82154 0t0 TCP localhost:55208-&gt;localhost:eforward (ESTABLISHED)# Zookeeper和Kafka也建立了连接 创建一个 Topic # bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test--bootstrap-server 指定Kafka位置（也可以 --zookeeper 指定zookeeper的位置，两者只能选一个）--create 创建topic--topic 指定topic的名字--partitions 指定分区数量--replication-factor 指定复制因子查看该topic# bin/kafka-topics.sh --describe --bootstrap-server localhost:9092Topic: test PartitionCount: 1 ReplicationFactor: 1 Configs: segment.bytes=1073741824 Topic: test Partition: 0 Leader: 0 Replicas: 0 Isr: 0必须指定zookeeper和topic名--describe 查看具体topic的情况 启动一个 Console Consumer，是一个挂起状态的监听 Consumer # bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning 然后在另一个终端启动一个 Console Producer，是一个消息的发送端 # bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test&gt;hello # 发送一个hello消息于是，consumer也能接受到这个消息 参考资料Kafka 设计解析（一）：Kafka 背景及架构介绍 Kafka 深度解析 Apache kafka 工作原理介绍","categories":[],"tags":[]},{"title":"RabbitMQ基础笔记","slug":"RabbitMQ基础笔记","date":"2020-01-07T14:44:24.000Z","updated":"2022-05-30T02:51:53.866Z","comments":true,"path":"2020/RabbitMQ基础笔记/","link":"","permalink":"https://coconutmilktaro.top/2020/RabbitMQ%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"JVM内存分配与垃圾回收","slug":"JVM内存分配与垃圾回收","date":"2020-01-07T13:50:08.000Z","updated":"2022-06-30T19:34:45.086Z","comments":false,"path":"2020/JVM内存分配与垃圾回收/","link":"","permalink":"https://coconutmilktaro.top/2020/JVM%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","excerpt":"本篇笔记不包含具体如何调优，只记录相关JVM内存分配与垃圾回收机制概念，以及常见的jvm工具使用方法。","text":"本篇笔记不包含具体如何调优，只记录相关JVM内存分配与垃圾回收机制概念，以及常见的jvm工具使用方法。 JVM概念JVM（Java Virtual Machine，Java虚拟机）为Java的核心技术，所有Java程序都运行在JVM内部，JVM相当于一个虚拟计算机，常见的JVM为Sun公司Hotspot VM，是jdk与openjdk的默认自带的虚拟机。 JVM是JRE（Java Runtime Environment）的一部分，因此安装了Jre后，也就有了JVM。 JVM运行时内存区结构JVM中每个区域都有各自的用途，存储各自的数据类型，但都有一个本质，就是存储程序的运行时数据。 JVM中的内存区可根据受访权限的不同定义为线程共享与线程私有两大类，线程共享内存区指的是可以允许被所有线程共享访问的一类内存区，包含堆区、方法区和运行时常量池三个内存区。 堆区Java堆区在JVM启动的时候被创建，且在实际的内存空间中可以是不连续的，堆区是一块用于存储对象实例的内存区，也是GC（Garbage Collection，垃圾收集器）执行垃圾回收的重点区域。 存储在JVM中的java对象可以被划分为两类，一类是生命周期较短的瞬时对象，这类对象的创建与消亡都非常迅速；另一类是生命周期较长的对象。对于不同生命周几的java对象，应该采取不用的垃圾收集策略，因此分代收集诞生。","categories":[{"name":"Java","slug":"Java","permalink":"https://coconutmilktaro.top/categories/Java/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://coconutmilktaro.top/tags/JVM/"},{"name":"Java","slug":"Java","permalink":"https://coconutmilktaro.top/tags/Java/"}]},{"title":"Influxdb基础笔记","slug":"Influxdb基础笔记","date":"2019-12-15T06:37:59.000Z","updated":"2022-05-30T02:51:53.803Z","comments":true,"path":"2019/Influxdb基础笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/Influxdb%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/","excerpt":"InfluxDB 是一个开源时间序列平台，包括用于存储和查询数据，在后台处理数据以实现 ETL 或监视和警报目的的 API，用户仪表板以及可视化和探索数据的 API 等。本笔记根据 influxdb v1.7 文档作翻译整理，跳过一些难点，仅作基础的一些应用","text":"InfluxDB 是一个开源时间序列平台，包括用于存储和查询数据，在后台处理数据以实现 ETL 或监视和警报目的的 API，用户仪表板以及可视化和探索数据的 API 等。本笔记根据 influxdb v1.7 文档作翻译整理，跳过一些难点，仅作基础的一些应用 安装与启动 概念 特性 设计见解与权衡 influx 命令行 配置管理 全局配置 元存储 数据 请求管理 保留策略 碎片预创建 监控 HTTP 端点 日志 InfluxQL 要点整理 数据库监控 HTTP_API 查询 debug ping query write 认证授权 日志追踪 备份与恢复 安装与启动首先，安装 influxdb 需要 root 权限，TCP 8086 和 8088 端口需要开启且空闲。 TCP 端口 8086 可用于使用 InfluxDB API 的客户端-服务器通信。 TCP 端口 8088 可用于 RPC 服务执行备份和还原操作。 除了上面的端口外，InfluxDB 还提供了多个插件，这些插件可能需要自定义端口。 可以通过配置文件修改所有端口映射，对于默认安装，该文件位于/etc/influxdb/influxdb.conf中。 需要安装 NTP 或 chrony 服务同步计算机时间，InfluxDB 使用主机 UTC 本地时间为数据分配时间戳。使用 NTP 在主机之间同步时间，如果主机的时钟未与 NTP 同步，则写入 InfluxDB 的数据上的时间戳可能不准确。 ubuntu 安装 wget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add -source /etc/lsb-releaseecho &quot;deb https://repos.influxdata.com/$&#123;DISTRIB_ID,,&#125; $&#123;DISTRIB_CODENAME&#125; stable&quot; | sudo tee /etc/apt/sources.list.d/influxdb.list debian 安装 wget -qO- https://repos.influxdata.com/influxdb.key | sudo apt-key add -source /etc/os-releaseecho &quot;deb https://repos.influxdata.com/debian $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/influxdb.list 然后：sudo apt-get update &amp;&amp; sudo apt-get install influxdb centos 安装 cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo[influxdb]name = InfluxDB Repository - RHEL \\$releaseverbaseurl = https://repos.influxdata.com/rhel/\\$releasever/\\$basearch/stableenabled = 1gpgcheck = 1gpgkey = https://repos.influxdata.com/influxdb.keyEOF 然后yum install influxdb 系统对每个配置文件设置都有内部默认值。 使用influxd config命令查看默认配置设置。启动 influxdb 也可使用指定配置文件influxd -config &lt;configfile&gt;也可配置环境变量INFLUXDB_CONFIG_PATH进行设置 确保存储数据和预写日志（WAL）的目录对于运行 Influxd 服务的用户是可写的。如果 data 和 WAL 目录不可写，那么 influxd 服务将不会启动。 概念在 influxdb 中所有数据都会有一列 time，用于存储时间戳，时间戳以RFC3339 UTC的形式显示。 字段（field）由字段键和字段值组成。字段键是字符串。字段值是数据，它们可以是字符串，浮点数，整数或布尔值，并且字段值始终与时间戳关联。字段键和字段值对的集合构成了一个字段集（field set）。 字段是 InfluxDB 数据结构必不可少的部分，如果没有字段，则 InfluxDB 中就不会有数据。同时要注意，字段未编入索引。使用字段值作为过滤器的查询一定会扫描与查询中其他条件匹配的所有值，对比标签查询来说性能不高。所以，通常字段不应包含常用查询的元数据。 标签（tag）由标签键和标签值组成。标记键和标记值都存储为字符串并记录元数据。标签集（tag set）是所有标签键值对的不同组合。标签是可选的，无需在数据结构中包含标签，但通常最好使用它们，因为与字段不同，标签是被索引的，这意味着对标签的查询速度更快，并且标签非常适合存储常见查询的元数据。为了优化查询，建议重新安排架构，使字段成为标签，而标签成为字段，这样当 InfluxDB 执行查询时，不必扫描字段的每个值，查询会更快。 度量（measurement）充当标签、字段和时间列的容器，度量名称是对关联字段中存储的数据的描述。度量名称是字符串，度量在概念上类似于 SQL 的表。单个度量可以属于不同的保留策略（retention policies）。保留策略描述 InfluxDB 保留数据多长时间（DURATION），以及该数据在集群中存储多少副本（REPLICATION）。 复制因子（replication factors）不适用于单节点实例。 InfluxDB 自动创建保留策略，它具有无限的持续时间，并且复制因子设置为 1。 序列（series）是共享保留策略、度量和标记集的数据集合。序列包含四列：Arbitrary series number、Retention policy、Measurement、Tag set，在设计架构以及在 InfluxDB 中处理数据时，了解序列的概念至关重要。 点（point）代表具有四个组成部分的单个数据记录：测量，标签集，字段集和时间戳。点由其序列和时间戳唯一标识。 特性数据库可以具有多个用户，连续查询，保留策略和度量。InfluxDB 是无模式（schemaless）的数据库，这意味着可以随时轻松添加新的度量，标签和字段。 在 InfluxDB 中，时间戳标识任何给定数据序列中的单个点。这就像一个 SQL 数据库表，其中主键由系统预先设置，并且始终为时间。 InfluxDB 还认识到架构首选项可能会随着时间而改变，因此在 InfluxDB 中无需预先定义架构。数据点可以具有度量中的一个字段、所有字段或介于两者之间的任何数字，只需为新字段编写一个点即可将其添加到度量中。 Influxdb 与 SQL 比对 InfluxDB 度量类似于 SQL 数据库表。 InfluxDB 标签类似于 SQL 数据库中的索引列。 InfluxDB 字段类似于 SQL 数据库中的未索引列。 InfluxDB 点类似于 SQL 行。 InfluxDB 连续查询和保留策略类似于 SQL 数据库中的存储过程。 SQL JOIN 不适用于 InfluxDB 度量，架构设计应反映出这种差异。而且，度量就像一个 SQL 表，其中主索引始终被预设为时间，InfluxDB 时间戳必须采用 UNIX 纪元（GMT）或格式化为在 RFC3339 下有效的日期时间字符串。 InfluxQL 是一种类似于 SQL 的查询语言，用于与 InfluxDB 进行交互。InfluxQL 允许在 WHERE 子句中指定查询的时间范围。可以使用用单引号引起来的日期时间字符串，其格式为YYYY-MM-DD HH：MM：SS.mmm（mmm 是毫秒，是可选的，还可以指定微秒或纳秒）。还可以使用now()获取计算机当前时间戳。 时间单位 单位 时间单位 单位 ns 纳秒 m 分 u or µ 微秒 h 时 ms 毫秒 d 天 s 秒 w 周 InfluxQL 还支持正则表达式，表达式中的算术，SHOW语句和GROUP BY语句。 时间序列数据通常只写入一次，很少更新。因此，InfluxDB 不是完整的 CRUD 数据库，而是更像 CR-ud，将创建和读取数据的性能置于更新和销毁之上，并阻止某些更新和销毁行为来使创建和读取性能更高。 若要更新一个点，需插入一个具有相同测量值，标签集和时间戳的点 可以删除或删除序列，但不能删除或删除基于字段值的单个点。解决方法是：可以搜索字段值，检索时间，然后根据时间字段删除 还不能更新或重命名标签，要修改一点序列的标签，先找到带有问题的标签值的点，将值更改为所需的点，将点写回，然后删除具有旧标签值的序列 无法通过标签键（而不是值）删除标签 设计见解与权衡InfluxDB 是一个时间序列数据库。为此用例进行优化需要权衡取舍，主要是以功能为代价来提高性能。 假设多次发送相同的数据，则它仅是客户端刚刚发送几次的完全相同的数据。优点：简化的冲突解决方案可提高写入性能。缺点：无法存储重复数据，在极少数情况下可能会覆盖数据。 删除很少见。当删除确实发生时，几乎总是会遇到大量旧数据，这些旧数据对于写入而言是无用的。优点：限制对删除的访问可以提高查询和写入性能。缺点：删除功能受到严重限制。 对现有数据的更新很少发生，而有争议的更新则永远不会发生。时间序列数据主要是永远不会更新的新数据。优点：限制对更新的访问可以提高查询和写入性能。缺点：更新功能受到严重限制。 绝大多数写操作是针对具有最近时间戳记的数据，并且数据按时间升序添加。优点：按时间升序添加数据的性能明显更高。缺点：用随机时间或不按升序排列的时间写点的性能明显降低。 规模（scale，或者说性能）至关重要。数据库必须能够处理大量的读写操作。优点：数据库可以处理大量的读写操作。缺点：InfluxDB 开发团队被迫进行权衡以提高性能。 能够写入和查询数据比拥有高度一致的视图更为重要。优点：编写和查询数据库可以由多个客户端以高负载完成。缺点：如果数据库负载沉重，查询返回可能不包含最新点。 许多时间序列都是短暂的。通常，时间序列只会出现几个小时，然后消失，例如：一个新的主机，该主机开始启动并报告一段时间，然后关闭。优点：InfluxDB 擅长管理不连续的数据。缺点：无模式设计意味着某些数据库功能不受支持，例如：没有交叉表联接。 点（point）是不太重要的。优点：InfluxDB 具有非常强大的工具来处理聚合数据和大型数据集。缺点：点没有传统意义上的 ID，它们通过时间戳和序列来区分。 influx 命令行influx是 influxDB 的 shell API，在启动了influxd后，通过influx进入 influx shell。 influx [选项] -host &lt;hostname&gt; # 指定主机，默认localhost -username &lt;username&gt; # 指定数据库用户 -password &lt;password&gt; # 指定该用户密码 -version # 查看版本 -type # 指定交互的shell，默认为influxql，也可以设为flux -pretty # 将json格式的输出进行格式化，需要-format设为json -format &#x27;json|csv|column&#x27; # 指定输出格式，默认为column -execute &lt;command&gt; # 直接执行influxql命令 -port &lt;port&gt; # 指定连接端口，默认为8086 -database &lt;database&gt; # 指定数据库 -precision &#x27;rfc3339|h|m|s|ms|u|ns&#x27; # 指定时间的输出格式，rfc3339会比较好看 -import # 从文件中导入一个库 -path &lt;path&gt; # 导入的文件路径，与-import一起用 -compressed # 若import的文件是压缩的就加上这个，与-import一起用 -pps # 每秒导入的点的个数，默认为0，不限流，与-import一起用 -consistency &#x27;any|one|quorum|all&#x27; # 设置写的一致性等级 -ssl # 使用https请求 -unsafeSsl # 禁用ssl的证书验证，使用自验证 导入文件分为两部分： DDL（数据定义语言）：包含 InfluxQL 命令，用于创建相关数据库和管理保留策略。如果数据库和保留策略已经存在，则文件可以跳过此部分。 DML（数据操作语言）：列出相关的数据库和（如果需要）保留策略，并包含在线协议中的数据。 对于大型数据集，influx 每 10 万个点写出一条状态消息。如果在导入期间保持活动状态的 InfluxDB Enterprise 群集上使用此命令，则建议将每秒的点数（-pps）限制为 5000 至 10000。导入可处理.gz文件，只需在命令中包含-compressed。在数据文件中包括时间戳。InfluxDB 将为没有时间戳的点分配相同的时间戳。这可能会导致意外的覆盖行为。如果数据文件具有 5000 个以上的点，则可能有必要将该文件拆分为几个文件，以便将数据批量写入 InfluxDB。建议分 5000 至 10000 分写作分数。较小的批次和更多的 HTTP 请求将导致次优性能。默认情况下，HTTP 请求在 5 秒钟后超时。超时后，InfluxDB 仍将尝试写入这些点，但是不会确认它们已成功写入。可以在导入命令后加上2&gt; failure.log 生成错误日志，方便排错 进入 influxdb shell 后的命令，很多都是可以直接通过上面的选项直接设置的 auth # 提示输入用户名和密码。Influx在查询数据库时使用这些凭据。 # 或使用INFLUX_USERNAME和INFLUX_PASSWORD环境变量设置CLI的用户名和密码。connect &lt;host:port&gt; # 连接指定的数据库use [&lt;database_name&gt; | &lt;database_name&gt;.&lt;retention policy_name&gt;] # 设置当前数据库and/or保留策略。 # 一旦Influx设置了当前数据库和/或保留策略，就无需在查询中指定该数据库和/或保留策略 # 若未指定保留策略，那么Influx会自动查询使用的数据库的DEFAULT保留策略insert # 插入数据，类似SQLconsistency &lt;level&gt; # 一致性等级，参考上面format &lt;format&gt; # 输出格式，同上precision &lt;format&gt; # 时间格式，同上pretty # 格式化jsonsettings # 输出设置信息history # 显示指令历史chunked # 开启块响应，默认已开启chunk &lt;size&gt; # 设置块响应的大小。 默认大小为10000，设置为0会将块大小重置为其默认值。clear [database| db| retention policy| rp] # 清除数据库或保留策略的当前上下文。 配置管理全局配置 reporting-disabled = false 把数据报告给 influxData，默认关 bind-address = &quot;127.0.0.1:8088&quot; 用于备份和恢复的端口，也可通过环境变量INFLUXDB_BIND_ADDRESS设置 元存储[meta]本部分控制 InfluxDB 元存储的参数，该元存储存储有关用户，数据库，保留策略，分片和连续查询的信息。 dir = &quot;/var/lib/influxdb/meta&quot;元数据数据库的存储目录。 元目录中的文件包括meta.db（InfluxDB 元存储文件）。环境变量INFLUXDB_META_DIR reserved-autocreate = true创建数据库时启用默认保留策略自动生成的自动创建。保留策略自动生成具有无限的持续时间，并且还被设置为数据库的 DEFAULT 保留策略，当写入或查询未指定保留策略时使用。禁用此设置以防止在创建数据库时创建此保留策略。环境变量INFLUXDB_META_RETENTION_AUTOCREATE logging-enabled = true启用元服务消息的日志记录。环境变量INFLUXDB_META_LOGGING_ENABLED 数据[data]设置控制 InfluxDB 的实际分片数据在何处以及如何从预写日志（WAL）中清除。 dir可能需要更改为适合系统的位置，但是 WAL 设置是高级配置。默认值适用于大多数系统。 dir = &quot;/var/lib/influxdb/data&quot;TSM 引擎存储 TSM 文件的 InfluxDB 目录。 此目录可能会更改。环境变量INFLUXDB_DATA_DIR wal-dir = &quot;/var/lib/influxdb/wal&quot;预写日志（WAL）文件的目录位置。环境变量INFLUXDB_DATA_WAL_DIR query-log-enabled = true在执行之前启用已解析查询的日志记录。查询日志对于故障排除很有用，但会记录查询中包含的所有敏感数据。环境变量INFLUXDB_DATA_QUERY_LOG_ENABLED 请求管理[coordinator] write-timeout = &quot;10s&quot; 写入请求等待直到“超时”错误返回给调用方的持续时间。默认值为 10 秒。环境变量：INFLUXDB_COORDINATOR_WRITE_TIMEOUT max-concurrent-queries = 0 实例上允许的最大运行查询数。 默认设置为 0，允许无限数量的查询。环境变量：INFLUXDB_COORDINATOR_MAX_CONCURRENT_QUERIES query-timeout = &quot;0s&quot; 终止查询之前，允许执行查询的最大持续时间。 默认设置 0，允许查询不受时间限制地运行。环境变量：INFLUXDB_COORDINATOR_QUERY_TIMEOUT max-select-point = 0 SELECT 语句可以处理的最大点数。 默认设置 0，允许 SELECT 语句处理无限数量的点。环境变量：INFLUXDB_COORDINATOR_MAX_SELECT_POINT max-select-series = 0 SELECT 语句可以处理的最大序列数。默认设置 0，允许 SELECT 语句处理无限数量的序列。环境变量：INFLUXDB_COORDINATOR_MAX_SELECT_SERIES max-select-buckets = 0 查询可以处理的 GROUP BY time()存储桶的最大数量。默认设置 0，允许查询处理无限数量的存储桶。环境变量：INFLUXDB_COORDINATOR_MAX_SELECT_BUCKETS 保留策略[retention]设置控制用于收回旧数据的保留策略的实施。 enabled = true 设置为 false 可以防止 InfluxDB 强制执行保留策略。环境变量：INFLUXDB_RETENTION_ENABLED check-interval = &quot;30m0s&quot; InfluxDB 检查以强制执行保留策略的时间间隔。环境变量：INFLUXDB_RETENTION_CHECK_INTERVAL 碎片预创建[shard-precreation]设置控制分片的增量，以便在数据到达之前可以使用分片。只有在创建后将在未来具有开始时间和结束时间的分片才会被创建。永远不会预先创建全部或部分过去的碎片。 enabled = true 确定是否启用碎片预创建服务。环境变量：INFLUXDB_SHARD_PRECREATION_ENABLED check-interval = &quot;10m&quot; 运行检查以预创建新分片的时间间隔。环境变量：INFLUXDB_SHARD_PRECREATION_CHECK_INTERVAL advance-period = &quot;30m&quot; 预先为其创建碎片的最长期限。 默认的 30m 应该适用于大多数系统。 将来将此设置增加太多会导致效率低下。环境变量：INFLUXDB_SHARD_PRECREATION_ADVANCE_PERIOD 监控[monitor]默认情况下，InfluxDB 将数据写入_internal数据库。如果该数据库不存在，InfluxDB 会自动创建它。 _internal数据库上的 DEFAULT 保留策略为 7 天。如果要使用 7 天保留策略以外的保留策略，则必须创建它。 store-enabled = true设置为 false 可在内部禁用记录统计信息。 如果设置为 false，将大大增加诊断安装问题的难度。环境变量：INFLUXDB_MONITOR_STORE_ENABLED store-database = &quot;_internal&quot;记录的统计信息的目标数据库。环境变量：INFLUXDB_MONITOR_STORE_DATABASE store-interval = &quot;10s&quot;InfluxDB 记录统计信息的时间间隔。 默认值为每十秒钟（10 秒）。环境变量：INFLUXDB_MONITOR_STORE_INTERVAL HTTP 端点[http]控制 InfluxDB 如何配置 HTTP 端点。 enabled = true 确定是否启用 HTTP 端点。环境变量：INFLUXDB_HTTP_ENABLED bind-address = &quot;:8086&quot; HTTP 服务使用的绑定地址（端口）。环境变量：INFLUXDB_HTTP_BIND_ADDRESS auth-enabled = false 确定是否通过 HTTP 和 HTTPS 启用用户身份验证。 要要求身份验证，请将值设置为 true。环境变量：INFLUXDB_HTTP_AUTH_ENABLED log-enabled = true 确定是否启用 HTTP 请求日志记录。环境变量：INFLUXDB_HTTP_LOG_ENABLED access-log-path = &quot;&quot; 访问日志的路径。 如果未指定或无法访问指定的路径，将退回到 stderr，这会将 HTTP 日志与内部 InfluxDB 日志混合在一起。 max-connection-limit = 0 一次可打开的最大连接数。超出限制的新连接将被删除。默认值 0 将禁用该限制。 日志[logging]控制记录器如何将日志发送到输出。 format = &quot;auto&quot; 确定要用于日志的日志编码器。 可选 auto（默认），logfmt 和 json。 使用默认的自动选项，如果输出是输出到 TTY 设备（例如终端），则使用更加用户友好的控制台编码。 如果输出是文件，则 auto 选项使用 logfmt 编码。 logfmt 和 json 选项对于与外部工具集成很有用。环境变量：INFLUXDB_LOGGING_FORMAT level = &quot;info&quot; 要发出的日志级别。 可选为 error、warn、info(默认值)和 debug。等于或高于指定级别的日志将被发出。环境变量：INFLUXDB_LOGGING_LEVEL InfluxQL 要点整理数据库监控InfluxDB 可以显示有关每个节点的统计和诊断信息。 要查看节点统计信息，可执行命令SHOW STATS，返回的统计信息仅存储在内存中。 &gt; show statsname: runtime-------------Alloc Frees HeapAlloc HeapIdle HeapInUse HeapObjects HeapReleased HeapSys Lookups Mallocs NumGC NumGoroutine PauseTotalNs Sys TotalAlloc4136056 6684537 4136056 34586624 5816320 49412 0 40402944 110 6733949 83 44 36083006 46692600 439945704name: graphitetags: proto=tcpbatches_tx bytes_rx connections_active connections_handled points_rx points_tx --- 159 3999750 0 1 158110 158110 要查看节点诊断信息，可执行命令SHOW DIAGNOSTICS，返回诸如构建信息，正常运行时间，主机名，服务器配置，内存使用情况和 Go 运行时诊断之类的信息。 name: buildBranch Build Time Commit Version---1.7 23bc63d43a8dc05f53afa46e3526ebb5578f3d88 1.7.9name: configbind-address reporting-disabled---127.0.0.1:8088 false...... InfluxDB 还将统计和诊断信息写入名为_internal的数据库，该数据库记录有关内部运行时和服务性能的指标。可以像其他任何 InfluxDB 数据库一样查询和操作_internal数据库。其中有以下度量： cqdatabasehttpdqueryExecutorruntimeshardsubscribertsm1_cachetsm1_enginetsm1_filestoretsm1_walwrite 可通过 HTTP API 访问 8086 端口/metrics，获取当前信息 HTTP_API 查询debug/debug/pprof：运行时的一些参数信息。包含以下参数： allocs：过去所有内存分配的样本 block：导致跟踪原语阻塞的堆栈跟踪 cmdline：当前程序的命令行调用 goroutine：所有当前 goroutine 的堆栈跟踪 heap：活动对象的内存分配的采样。可以指定 gc GET 参数以在获取堆样本之前运行 GC。 mutex：竞争互斥锁持有人的堆栈痕迹 profile：CPU 配置文件。 可以在 GET 参数中指定持续时间。 获取概要文件后，使用go tool pprof命令调查概要文件。 threadcreate：堆栈跟踪，导致创建新的 OS 线程 trace：当前程序执行的跟踪。可以在 GET 参数中指定持续时间。 获取跟踪文件后，使用go tool trace命令调查跟踪。 可直接通过/debug/pprof访问，或/debug/pprof/&lt;profile&gt;查看指定参数，或 go tool 命令查看go tool pprof http://localhost:8086/debug/pprof/heap还可以通过/debug/pprof/all获取信息的 tar.gz 包，包含所有参数信息的 txt 文本，curl -o profiles.tar.gz &quot;http://localhost:8086/debug/pprof/all&quot; /debug/requests：可跟踪对/write和/query的 HTTP 客户端请求，会按用户名和 IP 地址返回写入和查询的次数。 curl http://localhost:8086/debug/requests&#123; &quot;user1:123.45.678.91&quot;: &#123;&quot;writes&quot;:1,&quot;queries&quot;:0&#125;,&#125; 可设置seconds设置客户端收集信息的持续时间（以秒为单位）。默认持续时间为 10 秒。 curl http://localhost:8086/debug/requests?seconds=60&#123; &quot;user1:123.45.678.91&quot;: &#123;&quot;writes&quot;:3,&quot;queries&quot;:0&#125;, &quot;user1:000.0.0.0&quot;: &#123;&quot;writes&quot;:0,&quot;queries&quot;:16&#125;, &quot;user2:xx.xx.xxx.xxx&quot;: &#123;&quot;writes&quot;:4,&quot;queries&quot;:0&#125;&#125; /debug/vars可获取统计信息 curl http://localhost:8086/debug/vars&#123; &quot;system&quot;: &#123;&quot;currentTime&quot;:&quot;2019-12-19T09:11:19.458636472Z&quot;,&quot;started&quot;:&quot;2019-12-16T08:46:03.377469744Z&quot;,&quot;uptime&quot;:260716&#125;, &quot;cmdline&quot;: [&quot;/usr/bin/influxd&quot;,&quot;-config&quot;,&quot;/etc/influxdb/influxdb.conf&quot;],...&#125; ping/ping可获取当前 Influxdb 的实例信息，默认情况下，/ping返回一个简单的 HTTP 204 状态响应，以使客户端知道服务器正在运行。当 verbose 选项设置为 true 时（/ping？verbose=true）（默认 erbose 为 false），返回 HTTP 200 状态。 curl http://localhost:8086/ping?verbose=true&#123;&quot;version&quot;:&quot;1.7.9&quot;&#125; query/query可用于查询数据和管理数据库、保留策略和用户。支持 GET 和 POST curl -G &#x27;http://localhost:8086/query?db=mydb&#x27; --data-urlencode &#x27;q=SELECT * FROM &quot;mymeas&quot;&#x27;&#123;&quot;results&quot;:[&#123;&quot;statement_id&quot;:0,&quot;series&quot;:[&#123;&quot;name&quot;:&quot;mymeas&quot;,&quot;columns&quot;:[&quot;time&quot;,&quot;myfield&quot;,&quot;mytag1&quot;,&quot;mytag2&quot;],&quot;values&quot;:[[&quot;2017-03-01T00:16:18Z&quot;,33.1,null,null],[&quot;2017-03-01T00:17:18Z&quot;,12.4,&quot;12&quot;,&quot;14&quot;]]&#125;]&#125;]&#125; 若要使用带插入的INTO语句，则需要使用 POST curl -XPOST &#x27;http://localhost:8086/query?db=mydb&#x27; --data-urlencode &#x27;q=SELECT * INTO &quot;newmeas&quot; FROM &quot;mymeas&quot;&#x27;&#123;&quot;results&quot;:[&#123;&quot;statement_id&quot;:0,&quot;series&quot;:[&#123;&quot;name&quot;:&quot;result&quot;,&quot;columns&quot;:[&quot;time&quot;,&quot;written&quot;],&quot;values&quot;:[[&quot;1970-01-01T00:00:00Z&quot;,2]]&#125;]&#125;]&#125; 若要创建数据库 curl -XPOST &#x27;http://localhost:8086/query&#x27; --data-urlencode &#x27;q=CREATE DATABASE &quot;mydb&quot;&#x27;&#123;&quot;results&quot;:[&#123;&quot;statement_id&quot;:0&#125;]&#125; 一些请求参数： db=：查询（大部分 select 和 show）需要此参数，设置数据库 epoch=：返回具有指定精度的纪元时间戳。 默认情况下，InfluxDB 以 RFC3339 格式返回时间戳（以纳秒为单位）。 p=：设置登录密码（若开启了验证） u=：设置登录用户（若开启了验证） pretty=true：开启美化过的 json 格式输出 q=：InfluxQL 查询语句 若要一次上传多个请求，查询语句间用;分隔 curl -G http://localhost:8086/query\\?db\\=test --data-urlencode &quot;q=select * from go_info;select * from node_cpu_seconds_total&quot; 若要返回 csv 格式，可加上参数-H &quot;Accept: application/csv&quot;，系统以纪元格式返回时间戳而不是 RFC3339 格式。 若要读取文件中的查询语句，可加上参数-F &quot;q=@&lt;path_to_file&gt;&quot; -F &quot;async=true&quot;，文件中的语句必须以;分隔。 write/write写入数据库，支持 POST建议在写入时，将时间的精度降低，例如设为precision=s，这会在压缩方面有所改善。 写入一个点 curl -XPOST &quot;http://localhost:8086/write?db=mydb&amp;precision=s&quot; --data-binary &#x27;mymeas,mytag=1 myfield=90 1463683075&#x27; 写入一个点并指定保留策略，可使用参数rp指定策略 curl -XPOST &quot;http://localhost:8086/write?db=mydb&amp;rp=myrp&quot; --data-binary &#x27;mymeas,mytag=1 myfield=90&#x27; 其余参数与 query 类似，可参照 query 的参数从文件读入指令需要用参数--data-binary @data.txt，要注意文件中的指令间要用换行分隔 认证授权日志追踪备份与恢复","categories":[{"name":"数据库","slug":"数据库","permalink":"https://coconutmilktaro.top/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"influxdb","slug":"influxdb","permalink":"https://coconutmilktaro.top/tags/influxdb/"},{"name":"数据库","slug":"数据库","permalink":"https://coconutmilktaro.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"influxdb-python-API使用笔记","slug":"influxdb-python-API使用笔记","date":"2019-12-15T06:37:33.000Z","updated":"2022-05-30T02:51:53.878Z","comments":true,"path":"2019/influxdb-python-API使用笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/influxdb-python-API%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Supervisor使用","slug":"Supervisor使用","date":"2019-12-15T06:34:32.000Z","updated":"2022-06-28T18:31:02.506Z","comments":false,"path":"2019/Supervisor使用/","link":"","permalink":"https://coconutmilktaro.top/2019/Supervisor%E4%BD%BF%E7%94%A8/","excerpt":"Supervisor 是一个 python 编写的 C/S 系统，允许其用户监视和控制 UNIX/Linux 上的多个进程。它不会作为“进程 ID 1”替代 init 运行，相反，它旨在用于控制与某个项目或客户相关的进程，并且在启动时像任何其他程序一样启动。","text":"Supervisor 是一个 python 编写的 C/S 系统，允许其用户监视和控制 UNIX/Linux 上的多个进程。它不会作为“进程 ID 1”替代 init 运行，相反，它旨在用于控制与某个项目或客户相关的进程，并且在启动时像任何其他程序一样启动。 特点 rc.d脚本是进程初始化、自动启动、管理的一个很好的形式，但是编写和维护很麻烦。 此外，rc.d脚本无法自动重启崩溃的进程，许多程序在崩溃时无法正确地自行重启。Supervisord 将进程作为其子进程启动，并且可以配置为在崩溃时自动重启，也可以将其自动配置为自行调用启动进程。 在 UNIX 上，通常很难获得准确的启动和关闭状态信息。 Pidfile 经常出错。 Supervisord 将流程作为子流程启动，因此它始终知道其子级的真实的启动和关闭状态，并且可以方便地查询该数据。 不需要或不需要完整的 Shell 访问运行这些进程的计算机。 监听小的 TCP 端口的进程通常需要以 root 用户身份启动和重新启动。 通常情况下，可以允许普通用户停止或重新启动这样的进程，但是为他们提供 shell 访问权限通常是不切实际的，并且为他们提供 root 访问权限或 sudo 访问权限通常是不可能的。Supervisorctl 允许以非常有限的方式访问计算机，实质上是允许用户通过简单的 Shell 或 Web UI 发出停止、启动、重新启动命令来查看进程状态并控制受监督的子进程。 进程通常需要成组地启动和停止，有时甚至需要按优先级顺序启动和停止。Supervisor 允许为进程分配优先级，允许用户通过 superviseorctl 发出命令，比如 start all 和 restart all，它们按照预先分配的优先级顺序启动进程。此外，可以将流程分组到流程组中，并且可以作为一个单元来停止和启动一组逻辑相关的进程。 Supervisor 通过 fork/exec 启动它的子进程，且子进程不做守护进程。当进程终止时，操作系统会立即向 Supervisor 发出信号。 通过简单易懂的 INI 样式配置文件配置 Supervisor。 它提供了许多进程的选项，例如重新启动失败的进程和自动日志轮换。 Supervisor 能启动、停止和监视进程，可以单独控制，也可以分组控制。可以配置 Supervisor 来提供本地或远程命令行和 web 接口。 Supervisor 具有一个简单的事件通知协议，该协议可以使用任何语言编写的程序对其进行监视，并且具有用于控制的 XML-RPC 接口。它还使用扩展点构建，Python 开发人员可以利用这些扩展点。 组成 supervisord：负责自行调用启动子程序，响应来自客户端的命令，重新启动崩溃或退出的子进程，记录其子进程 stdout 和 stderr 输出以及生成和处理与子进程生命周期中的点相对应的“事件”。配置文件/etc/supervisord.conf，是“ Windows-INI”样式的配置文件。因为可能包含未加密的用户名和密码，最好通过适当的文件系统权限来确保此文件的安全 supervisorctl：提供了类似 shell 的界面，与 supervisord 结合使用。 通过超级用户，用户可以连接到不同的超级用户进程（一次一个），获取由超级用户控制的子进程的状态，停止和启动该超级用户的子进程，以及获取超级用户正在运行的进程的列表。命令行客户端通过 UNIX 域套接字或 Internet（TCP）套接字与服务器交互。 服务器可以声明客户端用户应在允许客户端执行命令之前出示身份验证凭据。客户端进程通常使用与服务器相同的配置文件，但是其中需包含[supervisorctl] 激活配置文件的[inet_http_server]部分后，可访问浏览器的 localhost:9001，通过 web 界面查看操作 开启 web ui 后，还会自动提供 XML-RPC 接口，该接口可用于询问和控制管理程序及其运行的程序。 安装使用pip install supervisor，之后运行命令echo_supervisord_conf，该命令会打印出 supervisor 的配置案例。 echo_supervisord_conf &gt; /etc/supervisord.conf 启动 supervisord，可使用-c指定配置文件 supervisord -c /etc/supervisord.conf 若要配置出supervisord.service，可创建 [Unit]Description=Supervisor daemon[Service]Type=forkingExecStart=/usr/bin/supervisord -c /etc/supervisord.confExecStop=/usr/bin/supervisorctl shutdownExecReload=/usr/bin/supervisorctl reloadKillMode=processRestart=on-failure[Install]WantedBy=multi-user.target 添加程序在配置文件中添加 [program:program_name]command= 官方案例 [program:foo]command=/bin/cat 启动 supervisord，然后可通过supervisorctl查看操作进程 # supervisorctl status allfoo RUNNING pid 27977, uptime 0:00:44 web 管理需要在配置中将[inet_http_server]的注释去除 [inet_http_server]port=127.0.0.1:9001username=testpassword=test# 若是需要其他主机能访问，需要将127.0.0.1改为* 重新启动 supervisord，查看 9001 端口是否开启然后通过浏览器访问 9001 端口。 supervisorctl 动作 help：获取动作的列表help &lt;action&gt;能获取指定 action 的帮助 add &lt;name&gt;：激活配置中进程/组的任何更新，可指定多个 remove &lt;name&gt;：从当前配置中删除进程/组，可指定多个 update或update all：重新加载配置并根据需要添加/删除，并将重新启动受影响的程序update &lt;gname&gt;：更新特定的组，并将重新启动受影响的程序，可指定多个 clear &lt;name&gt;：清除进程日志文件，可指定多个。clear all可清除所有 fg &lt;process&gt;：连接到前台模式下的进程 pid：获取 supervisord 的 PIDpid &lt;name&gt;：通过名称获取单个子进程的 PID。pid all reload：重启远端 supervisord reread：重新加载守护程序的配置文件，而无需重新启动 restart &lt;name&gt;：重启一个进程，restart 不重新读取配置文件。 可指定多个。restart &lt;gname&gt;:*restart all start &lt;name&gt;：启动一个进程，可指定多个。start &lt;gname&gt;:*start all status：查看所有进程信息status &lt;name&gt; stop &lt;name&gt;：停止一个进程，可指定多个。stop &lt;gname&gt;:*stop all 配置文件详解[unix_http_server]监听一个 unix 域 socket 的 http 服务器的配置 file：socket 文件 chmod：socket 文件权限，默认 0700 chown：socket 文件的所属者 username：需要验证的用户名 password：需要验证的用户密码 示例： [unix_http_server]file = /tmp/supervisor.sockchmod = 0777chown= nobody:nogroupusername = userpassword = 123 [inet_http_server]监听一个 TCP socket 的 http 服务器的配置 port：supervisor 监听 http/xml-rpc 请求的端口。supervisorctl 会在这个端口使用 xml-rpc 与 supervisord 交互 username password 示例： [inet_http_server]port = 127.0.0.1:9001username = userpassword = 123 [supervisord]supervisord 进程的全局配置 logfile：supervisord 进程的日志 logfile_maxbytes：活动日志文件在轮换之前可能消耗的最大字节数（需加单位） logfile_backups：活动日志文件轮换导致要保留的备份数。 如果设置为 0，将不保留任何备份。 loglevel：日志等级。critical, error, warn, info, debug, trace, blather。默认 info pidfile：supervisord 进程的 pidfile umask：supervisord 进程的 umask nodaemon：如果是 true，则不会在后台运行 minfds：在主目录成功启动之前必须可用的文件描述符的最小数量。默认 1024 minprocs：在主进程成功启动之前必须可用的进程描述符的最小数量。默认 200 nocleanup childlogdir user：当执行操作时，将身份切换到指定用户。如果超级用户无法切换到指定的用户，它将向 stderr 写入一条错误消息，然后立即退出。 directory：当 supervisord 守护进程时，切换到该目录。 strip_ansi：从子日志文件中删除所有 ANSI 转义序列。 environment：键值对环境变量 identifier：标记 supervisor 进程的字符串，供 RPC 使用 示例： [supervisord]logfile = /tmp/supervisord.loglogfile_maxbytes = 50MBlogfile_backups=10loglevel = infopidfile = /tmp/supervisord.pidnodaemon = falseminfds = 1024minprocs = 200umask = 022user = chrismidentifier = supervisordirectory = /tmpnocleanup = truechildlogdir = /tmpstrip_ansi = falseenvironment = KEY1=&quot;value1&quot;,KEY2=&quot;value2&quot; [supervisorctl]客户端命令 supervisorctl 的配置 serverurl：通过 UNIX socket 连接 supervisord，路径与 unix_http_server 部分的 file 一致 username password prompt：提示字符串 示例： [supervisorctl]serverurl = unix:///tmp/supervisor.sockusername = chrispassword = 123prompt = mysupervisor [program:x]子进程配置 directory command autostart：supervisor 启动的时候是否随着同时启动，默认 True autorestart：当程序 exit 的时候，这个 program 是否会自动重启，默认 unexpected，设置子进程挂掉后自动重启的情况，有三个选项，false,unexpected 和 true。如果为 false 的时候，无论什么情况下，都不会被重新启动，如果为 unexpected，只有当进程的退出码不在下面的 exitcodes 里面定义的 startsecs：子进程启动多少秒之后，此时状态如果是 running，则认为启动成功 user stderr_logfile stdout_logfile redirect_stderr：把 stderr 重定向到 stdout，默认 false stdout_logfile_maxbytes：stdout 日志文件大小，默认 50MB stdout_logfile_backups：stdout 日志文件备份数 [include]添加其他配置文件 files：文件路径，多个就以空格分隔 [group:x]进程组配置 programs：该组的进程，以逗号分隔 priority：优先级 示例： [group:foo]programs=bar,bazpriority=999 常见服务配置示例kafka和zookeeper[program:kafka]command=/data/kafka/bin/kafka-server-start.sh /data/kafka/config/server.propertiesdirectory=/data/kafkauser=rootautostart=trueautorestart=truestartsecs=10stdout_logfile=/data/logs/kafka.logredirect_stderr=trueenvironment=JAVA_HOME=/usr/local/jdk1.8.0_261[program:zookeeper]command=/data/zookeeper/bin/zkServer.sh start-foregrounddirectory=/data/zookeeperuser=rootautostart=trueautorestart=truestartsecs=10stdout_logfile=/data/logs/zookeeper.logredirect_stderr=trueenvironment=JAVA_HOME=/usr/local/jdk1.8.0_261 prometheus[program:prometheus]command=/data/prometheus/prometheus --web.enable-lifecycle --storage.tsdb.path=/data/prometheus/data --config.file=&quot;/data/prometheus/prometheus.yml&quot; --storage.tsdb.retention.time=30dredirect_stderr=truestdout_logfile=/data/logs/prometheus.logautostart=trueautorestart=truestartsecs=10[program:node_exporter]command=/data/node_exporter/node_exporter --web.listen-address=&quot;:9100&quot;directory=/data/node_exporterredirect_stderr=truestdout_logfile=/data/logs/node_export.logautostart=trueautorestart=truestartsecs=10 rocketmq[program:mqnamesrv]command=/usr/local/rocketmq/bin/mqnamesrvprocess_name=mqnamesrvnumprocs=1autostart=trueautorestart=truestartsecs=10startretries=3stopasgroup=truekillasgroup=truestopsignal=TERMstopwaitsecs=5[program:broker]command=/usr/local/rocketmq/bin/mqbroker -n xxx:9876;xxx:9876;xxx:9876 -c /usr/local/rocketmq/conf/2m-2s-async/broker-a.propertiesnumprocs=1autostart=trueautorestart=truestartsecs=10startretries=3stopasgroup=truestopsignal=TERMstopwaitsecs=5killasgroup=true","categories":[],"tags":[{"name":"Supervisor","slug":"Supervisor","permalink":"https://coconutmilktaro.top/tags/Supervisor/"}]},{"title":"《SRE Google运维解密》读书笔记","slug":"《SRE-Google运维解密》读书笔记","date":"2019-12-15T05:44:19.000Z","updated":"2022-05-30T02:51:53.930Z","comments":true,"path":"2019/《SRE-Google运维解密》读书笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/%E3%80%8ASRE-Google%E8%BF%90%E7%BB%B4%E8%A7%A3%E5%AF%86%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"sre","slug":"sre","permalink":"https://coconutmilktaro.top/tags/sre/"}]},{"title":"5G NSA与SA概念笔记","slug":"5G-NSA与SA概念笔记","date":"2019-12-13T03:46:36.000Z","updated":"2022-05-30T02:51:53.768Z","comments":true,"path":"2019/5G-NSA与SA概念笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/5G-NSA%E4%B8%8ESA%E6%A6%82%E5%BF%B5%E7%AC%94%E8%AE%B0/","excerpt":"5G 组网方案 SA 组网 NSA 组网 3 系组网 7 系组网 4 系组网 第五代移动通信技术（5th generation mobile networks 或 5th generation wireless systems，简称 5G）是最新一代蜂窝移动通信技术，5G 的性能目标是高数据速率、减少延迟、节省能源、降低成本、提高系统容量和大规模设备连接。ITU IMT-2020 规范要求速度高达 20 Gbit/s，可以实现宽信道带宽和大容量 MIMO。第三代合作伙伴计划（3GPP）将提交 5G NR（新无线电）作为其 5G 通信标准提案。5G NR 可包括低频（FR1），低于 6 GHz 和更高频率（FR2），高于 2.4 GHz 和毫米波范围。","text":"5G 组网方案 SA 组网 NSA 组网 3 系组网 7 系组网 4 系组网 第五代移动通信技术（5th generation mobile networks 或 5th generation wireless systems，简称 5G）是最新一代蜂窝移动通信技术，5G 的性能目标是高数据速率、减少延迟、节省能源、降低成本、提高系统容量和大规模设备连接。ITU IMT-2020 规范要求速度高达 20 Gbit/s，可以实现宽信道带宽和大容量 MIMO。第三代合作伙伴计划（3GPP）将提交 5G NR（新无线电）作为其 5G 通信标准提案。5G NR 可包括低频（FR1），低于 6 GHz 和更高频率（FR2），高于 2.4 GHz 和毫米波范围。 多输入多输出系统（Multi-input Multi-output ; MIMO）是一种用来描述多天线无线通信系统的抽象数学模型，核心概念为利用多根发射天线与多根接收天线所提供之空间自由度来有效提升无线通信系统之频谱效率，以提升传输速率并改善通信质量。第三代合作伙伴计划（3rd Generation Partnership Project，即 3GPP）是一个成立于 1998 年 12 月的标准化机构。当前其成员包括欧洲的 ETSI、日本的 ARIB 和 TTC、中国的 CCSA、韩国的 TTA、北美洲的 ATIS 和印度的电信标准开发协会。 下一代移动网络联盟（Next Generation Mobile Networks Alliance）定义了 5G 网络的以下要求： 以 10Gbps 的数据传输速率支持数万用户； 以 1Gbps 的数据传输速率同时提供给在同一楼办公的许多人员； 支持数十万的并发连接以用于支持大规模传感器网络的部署； 频谱效率应当相比 4G 被显著增强； 覆盖率比 4G 有所提高； 信令效率应得到加强； 延迟应显著低于 LTE。 5G 组网方案从 2017 年开始，已经开始 5G 的部署，但是部署方式为 NSA（Non-Standalone，非独立组网），而从 2019 年开始，开始 SA（Standalone，独立组网）的部署。NSA 与 SA 的最大区别：是否就一种核心网，还有一种基站。只要 4G 基站和 5G 基站共存，就是 NSA 其中 NSA 有约 8 种组网方案，分为 3 系 选项 3 选项 3a 选项 3x 4 系 选项 4 选项 4a 7 系 选项 7 选项 7a 选项 7x SA 有两种组网方案 选项 2 选项 5 SA 组网SA 就两种方式： 选项 2：5G 核心网下直接建设 5G 基站，有钱的运营商会直接用这个方案 选项 2 的缺点：5G 频点相对 LTE 较高，初期部署难以实现连续覆盖，会存在大量的 5G 与 4G 系统间的切换，用户体验不好 选项 5：5G 核心网下改进 4G 基站（LTE eNB），改为增强型 4G 基站（eLTE eNB），一般有大量 4G 基站的运营商会这样，利用旧基站省钱，但是这样本质上仍为 4G，所以这方案不会用的多 NSA 组网NSA 组网围绕的就是三个问题： 基站走 4G 核心网还是 5G 核心网？ 控制指令是走 4G 基站还是 5G 基站？ 数据分流是在 4G 基站还是 5G 基站还是核心网？ 3 系组网在 3 系组网方式中，参考的是 LTE 双连接架构。 双连接架构：在 LTE 双连接构架中，UE（用户终端）在连接态下可同时使用至少两个不同基站的无线资源(分为主站和从站)。 选项 3：5G 基站是无法直接连在 4G 核心网上面的，所以，它会通过 4G 基站接到 4G 核心网，而传统 4G 基站由于处理能力有限，所以只能改进 4G 基站，变为增强型，再连 5G 基站。 选项 3a：若无法改进 4G 基站（可能是资金原因或技术原因），只能继续使用老的 4G 基站。5G 基站直通 4G 核心网，所以整体仍为 4G 选项 3x：同样不改进 4G 基站，让 5G 网既通 4G 核心网，又接收处理 4G 基站超负载的流量 7 系组网运营商若要真正 5G 流量，需要将核心网变为 5G 核心网。而 7 系就是 3 系的升级版本，所有 4G 基站升级为增强型 选项 7：与 3 系的 3 类似 选项 7a：与 3 系的 3a 类似 选项 7x：与 3 系的 3x 类似 4 系组网4G 基站和 5G 基站共用 5G 核心网，5G 基站为主站，4G 基站为从站。 选项 4：4G 的用户流量从 5G 基站过 选项 4a：4G 和 5G 用户流量通过各自基站直通核心网 所以整个 NSA5G 网络的部署过程应为： 3 系：5G 部署初期，能快速实现 5G 商用，推荐选项 3x 7 系：5G 部署初期及中期场景，由升级后的增强型 4G 基站提供连续覆盖、5G 仍然作为热点覆盖提高容量，推荐选项 7x 4 系：5G 商用中后期部署场景，推荐选项 4 最后淘汰 4G 基站，真正变为 SA 选项 2 中国的三大运营商直接上了 SA 选项 2 参考：5G 维基百科 &gt; 说清楚，5G SA 和 NSA 到底有啥区别？ &gt; 三分钟看懂 5G NSA 和 SA","categories":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"5G","slug":"5G","permalink":"https://coconutmilktaro.top/tags/5G/"}]},{"title":"Prometheus基础应用笔记","slug":"Prometheus基础应用笔记","date":"2019-12-13T03:41:10.000Z","updated":"2022-05-30T02:51:53.865Z","comments":true,"path":"2019/Prometheus基础应用笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/Prometheus%E5%9F%BA%E7%A1%80%E5%BA%94%E7%94%A8%E7%AC%94%E8%AE%B0/","excerpt":"Prometheus 是最初在 SoundCloud 上构建的开源系统监视和报警工具包。Prometheus 于 2016 年加入了 CNCF，这是继 Kubernetes 之后的第二个托管项目。","text":"Prometheus 是最初在 SoundCloud 上构建的开源系统监视和报警工具包。Prometheus 于 2016 年加入了 CNCF，这是继 Kubernetes 之后的第二个托管项目。 Prometheus 基本概念 特点 组成 工作流程 数据模型 四种 Metric 类型 数据采集 instance 和 jobs Prometheus Server 使用 PromQL 查询监控数据 常用函数 Prometheus Server 配置文件详细说明 抓取配置 将 prometheus 数据自动写入 influxdb 常见exporter Node exporter AlertManager 报警 HTTP API 表达式请求 请求元数据 targets rules alerts alertmanager status Prometheus 基本概念特点强大的多维度数据模型： 时间序列数据通过 metric 名和键值对来区分。 所有的 metrics 都可以设置任意的多维标签。 数据模型更随意，不需要刻意设置为以点分隔的字符串。 可以对数据模型进行聚合，切割和切片操作。 支持双精度浮点类型，标签可以设为全 unicode。 灵活而强大的查询语句（PromQL）：在同一个查询语句，可以对多个 metrics 进行乘法、加法、连接、取分数位等操作。 易于管理： Prometheus server 是一个单独的二进制文件，可直接在本地工作，不依赖于分布式存储。 高效：平均每个采样点仅占 3.5 bytes，且一个 Prometheus server 可以处理数百万的 metrics。 使用 pull 模式采集时间序列数据，这样不仅有利于本机测试而且可以避免有问题的服务器推送坏的 metrics。 可以采用 push gateway 的方式把时间序列数据推送至 Prometheus server 端。 可以通过服务发现或者静态配置去获取监控的 targets。 有多种可视化图形界面。 易于伸缩。 由于数据采集可能会有丢失，所以 Prometheus 不适用对采集数据要 100% 准确的情形 组成 Prometheus Server: 用于收集和存储时间序列数据。Prometheus Server 本身就是一个时序数据库，将采集到的监控数据按照时间序列的方式存储在本地磁盘当中。 Client Library: 客户端库，为需要监控的服务生成相应的 metrics 并暴露给 Prometheus server。当 Prometheus server 来 pull 时，直接返回实时状态的 metrics。 Exporters: Exporter 将监控数据采集的端点通过 HTTP 服务的形式暴露给 Prometheus Server，Prometheus Server 通过访问该 Exporter 提供的 Endpoint 端点，即可获取到需要采集的监控数据一般来说可以将 Exporter 分为 2 类： 直接采集：这一类 Exporter 直接内置了对 Prometheus 监控的支持，比如 cAdvisor，Kubernetes，Etcd，Gokit 等，都直接内置了用于向 Prometheus 暴露监控数据的端点。 间接采集：间接采集，原有监控目标并不直接支持 Prometheus，因此我们需要通过 Prometheus 提供的 Client Library 编写该监控目标的监控采集程序。例如： Mysql Exporter，JMX Exporter，Consul Exporter 等。 Alertmanager: 在 Prometheus Server 中支持基于 PromQL 创建告警规则，如果满足 PromQL 定义的规则，则会产生一条告警，而告警的后续处理流程则由 AlertManager 进行管理。在 AlertManager 中我们可以与邮件，Slack 等等内置的通知方式进行集成，也可以通过 Webhook 自定义告警处理方式。AlertManager 即 Prometheus 体系中的告警处理中心。从 Prometheus server 端接收到 alerts 后，会进行去除重复数据，分组，并路由到对收的接受方式，发出报警。常见的接收方式有：电子邮件，pagerduty，OpsGenie, webhook 等。 Push Gateway: 主要用于短期的 jobs。由于这类 jobs 存在时间较短，可能在 Prometheus 来 pull 之前就消失了。为此，这次 jobs 可以直接向 Prometheus server 端推送它们的 metrics。这种方式主要用于服务层面的 metrics，对于机器层面的 metrices，需要使用 node exporter。 工作流程Prometheus server 定期从配置好的 jobs 或者 exporters 中拉 metrics，或者接收来自 Pushgateway 发过来的 metrics，或者从其他的 Prometheus server 中拉 metrics。Prometheus server 在本地存储收集到的 metrics，并运行已定义好的 alert.rules，记录新的时间序列或者向 Alertmanager 推送报警。Alertmanager 根据配置文件，对接收到的报警进行处理，发出告警。在图形界面中，可视化采集数据。 数据模型Prometheus 中存储的数据为时间序列，是由 metric 的名字和一系列的标签（键值对）唯一标识的，不同的标签则代表不同的时间序列。 metric name：该名字应该具有语义，一般用于表示 metric 的功能，例如：http*requests_total, 表示 http 请求的总数。其中，metric 名字由 ASCII 字符，数字，下划线，以及冒号组成，且必须满足正则表达式 [a-zA-Z*:][a-zA-Z0-9_:]\\_。 标签 label：使同一个时间序列有了不同维度的识别。例如 http*requests_total&#123;method=&quot;Get&quot;&#125; 表示所有 http 请求中的 Get 请求。当 method=”post” 时，则为新的一个 metric。标签中的键由 ASCII 字符，数字，以及下划线组成，且必须满足正则表达式 [a-zA-Z*:][a-zA-Z0-9_:]\\_。 样本 timestamp + value：实际的时间序列，每个序列包括一个 float64 的值和一个毫秒级的时间戳。 格式：&lt;metric name&gt;&#123;&lt;label name&gt;=&lt;label value&gt;, …&#125;，例如：http_requests_total&#123;method=&quot;POST&quot;,endpoint=&quot;/api/tracks&quot;&#125;。 四种 Metric 类型Prometheus 客户端库主要提供四种主要的 metric 类型： Counter一种累加的 metric，只增不减（除非系统发生重置），典型的应用如：http_requests_total，node_cpu 等等。例如，查询 http_requests_total&#123;method=&quot;get&quot;, job=&quot;Prometheus&quot;, handler=&quot;query&quot;&#125; 返回 8，10 秒后，再次查询，则返回14。一般在定义 Counter 类型指标的名称时推荐使用_total 作为后缀。 Gauge一种常规的 metric，典型的应用如：温度，运行的 goroutines 的个数。Gauge 类型的指标侧重于反应系统的当前状态。可以任意加减。例如：go_goroutines&#123;instance=&quot;172.17.0.2&quot;, job=&quot;Prometheus&quot;&#125; 返回值 147，10 秒后返回 124。 Histogram可以理解为柱状图，典型的应用如：请求持续时间，响应大小。可以对观察结果采样，分组及统计。 Summary类似于 Histogram, 典型的应用如：请求持续时间，响应大小。提供观测值的 count 和 sum 功能。提供百分位的功能，即可以按百分比划分跟踪结果。 Histogram 和 Summary 主用用于统计和分析样本的分布情况。 在大多数情况下人们都倾向于使用某些量化指标的平均值，例如 CPU 的平均使用率、页面的平均响应时间。这种方式的问题很明显，以系统 API 调用的平均响应时间为例：如果大多数 API 请求都维持在 100ms 的响应时间范围内，而个别请求的响应时间需要 5s，那么就会导致某些 WEB 页面的响应时间落到中位数的情况，而这种现象被称为长尾问题。 为了区分是平均的慢还是长尾的慢，最简单的方式就是按照请求延迟的范围进行分组。例如，统计延迟在 010ms 之间的请求数有多少而 1020ms 之间的请求数又有多少。通过这种方式可以快速分析系统慢的原因。Histogram 和 Summary 都是为了能够解决这样问题的存在，通过 Histogram 和 Summary 类型的监控指标，我们可以快速了解监控样本的分布情况。 与 Summary 类型的指标相似之处在于 Histogram 类型的样本同样会反应当前指标的记录的总数(以_count 作为后缀)以及其值的总量（以_sum 作为后缀）。不同在于 Histogram 指标直接反应了在不同区间内样本的个数，区间通过标签 len 进行定义。 不同在于 Histogram 通过 histogram_quantile 函数是在服务器端计算的分位数。 而 Sumamry 的分位数则是直接在客户端计算完成。因此对于分位数的计算而言，Summary 在通过 PromQL 进行查询时有更好的性能表现，而 Histogram 则会消耗更多的资源。反之对于客户端而言 Histogram 消耗的资源更少。 数据采集Prometheus支持两种数据采集方式：Pull和Push。 pull：prometheus server去agent拉取数据 push：agent主动上报数据到prometheus server 对比： Push时效性较好，可将采集数据立即上报。Pull采用周期性采集。若对实时性要求非常高，最好采用Push。 Push采集后立刻上报，本地不会保存采集数据。Pull正好相反，agent本身需要有一定的数据存储能力，prometheus仅负责简单拉取。 Push方式，控制方为agent，agent来决定上报周期和内容。Pull的话，prometheus来控制周期和内容。 Push方式必须在agent配置prometheus的地址。pull方式prometheus可做到与agent解耦，agent不用感知prometheus的存在。 Prometheus的两种配置更新方式： 调用prometheus的reload接口，即localhost:9090/-/reload，需要先开启--web.enable-lifecycle，推荐这个方式。 发送HUP信号给prometheus，即kill -HUP &lt;prometheus进程ID&gt;，需要获取进程id，并不方便。 instance 和 jobs instance: 一个单独 scrape 的目标， 一般对应于一个进程。 jobs: 一组同种类型的 instances（主要用于保证可扩展性和可靠性） Prometheus Server下载安装：https://prometheus.io/download/#prometheus解压后将目录中的命令（prometheus、promtool、tsdb）放到/usr/local/bin 中。其采集的数据会以文件的形似存储在本地中，默认的存储路径为当前目录的 data/中。若不存在会自动创建。用户也可以通过参数--storage.tsdb.path=&quot;data/&quot;修改本地数据存储的路径。但是启动一定要 prometheus.yml 文件，若没有就启动不了 默认配置文件： global:scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.alerting:alertmanagers:- static_configs: - targets:rule_files:scrape_configs:- job_name: &#x27;prometheus&#x27; static_configs: - targets: [&#x27;localhost:9090&#x27;] 配置为 systemd 服务： [Unit]Description=Prometheus ServerAfter=network.target[Service]Type=simpleRestart=on-failureExecStart=/usr/local/bin/prometheus --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/var/lib/prometheus[Install]WantedBy=multi-user.target docker 启动： docker run -p 9090:9090 -v /etc/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus 启动后，可通过 9090 端口访问 web 页面，或/metrics 访问纯数据 为了能够让 Prometheus Server 能够从当前 node exporter 获取到监控数据，这里需要修改 Prometheus 配置文件。编辑prometheus.yml 并在 scrape_configs 节点下添加以下内容 scrape_configs:- job_name: &#x27;prometheus&#x27; static_configs: - targets: [&#x27;localhost:9090&#x27;]# 采集 node exporter 监控数据- job_name: &#x27;node&#x27; static_configs: - targets: [&#x27;localhost:9100&#x27;] 常见指标： node*boot_time：系统启动时间 node_cpu：系统 CPU 使用量 nodedisk*：磁盘 IO nodefilesystem*：文件系统用量 node_load1：系统负载 nodememeory*：内存使用量 nodenetwork*：网络带宽 node_time：当前系统时间 go*_：node exporter 中 go 相关指标 process__：node exporter 自身进程相关运行指标 使用 PromQL 查询监控数据PromQL 是 Prometheus 自定义的一套强大的数据查询语言，除了使用监控指标作为查询关键字以为，还内置了大量的函数，帮助用户进一步对时序数据进行处理。 可通过node_cpu_seconds_total&#123;mode=&#39;idle&#39;&#125;查找具体的某项的值rate(node_cpu_seconds_total&#123;mode=&#39;idle&#39;&#125;[2m]) 查看过去 2min 的增长速率 在 Prometheus 中，每一个暴露监控样本数据的 HTTP 服务称为一个实例。例如在当前主机上运行的 node exporter 可以被称为一个实例(Instance)。当前在每一个 Job 中主要使用了静态配置(static_configs)的方式定义监控目标。除了静态配置每一个 Job 的采集 Instance 地址以外，Prometheus 还支持与 DNS、Consul、E2C、Kubernetes 等进行集成实现自动发现 Instance 实例，并从这些 Instance 上获取监控数据。除了通过使用“up”表达式查询当前所有 Instance 的状态以外，还可以通过 Prometheus UI 中的 Targets 页面查看当前所有的监控采集任务，以及各个任务下所有实例的状态。 Prometheus 会将所有采集到的样本数据以时间序列（time-series）的方式保存在内存数据库中，并且定时保存到硬盘上。time-series 是按照时间戳和值的序列顺序存放的，我们称之为向量(vector). 每条 time-series 通过指标名称(metrics name)和一组标签集(labelset)命名。 在 time-series 中的每一个点称为一个样本（sample），样本由以下三部分组成： 指标(metric)：metric name 和描述当前样本特征的 labelsets; 时间戳(timestamp)：一个精确到毫秒的时间戳; 样本值(value)： 一个 float64 的浮点型数据表示当前样本的值。 http_request_total&#123;status=&quot;200&quot;, method=&quot;GET&quot;&#125;@1434417560938 =&gt; 94355|----metric name--|----------labelsets-------|---timestamp--|--value-| 其中__作为前缀的标签，是系统保留的关键字，只能在系统内部使用。标签的值则可以包含任何 Unicode 编码的字符。在 Prometheus 的底层实现中指标名称实际上是以__name__=&lt;metric name&gt;的形式保存在数据库中的 http_request_total&#123;status=&quot;200&quot;, method=&quot;GET&quot;&#125; == &#123;__name__=&quot;http_request_total&quot;, status=&quot;200&quot;, method=&quot;GET&quot;&#125; PromQL 支持用户根据时间序列的标签匹配模式来对时间序列进行过滤，目前主要支持两种匹配模式：完全匹配和正则匹配。PromQL 支持使用=和!=两种完全匹配模式PromQL 还可以支持使用正则表达式作为匹配条件，多个表达式之间使用|进行分离： 使用label=~regx表示选择那些标签符合正则表达式定义的时间序列； 反之使用label!~regx进行排除； 直接查找时，返回值中只会包含该时间序列中的最新的一个样本值，这样的返回结果我们称之为瞬时向量。而相应的这样的表达式称之为瞬时向量表达式。如果我们想过去一段时间范围内的样本数据时，我们则需要使用区间向量表达式，时间范围通过时间范围选择器[]进行定义，通过区间向量表达式查询到的结果我们称为区间向量。时间选择器单位：s - 秒、m - 分钟、h - 小时、d - 天、w - 周、y - 年如果想查询，5 分钟前的瞬时样本数据，或昨天一天的区间内的样本数据，可以使用位移操作，位移操作的关键字为 offset。 http_request_total&#123;&#125; offset 5mhttp_request_total&#123;&#125;[1d] offset 1d 如果描述样本特征的标签(label)在并非唯一的情况下，通过 PromQL 查询数据，会返回多条满足这些特征维度的时间序列。而 PromQL 提供的聚合操作可以用来对这些时间序列进行处理，形成一条新的时间序列 # 查询系统所有 http 请求的总量sum(http_request_total)# 按照 mode 计算主机 CPU 的平均使用时间avg(node_cpu) by (mode)# 按照主机查询各个主机的 CPU 使用率sum(sum(irate(node_cpu&#123;mode!=&#x27;idle&#x27;&#125;[5m])) / sum(irate(node_cpu[5m]))) by (instance) PromQL 还直接支持用户使用标量(Scalar)和字符串(String)。 标量（Scalar）：一个浮点型的数字值，标量只有一个数字，没有时序。 字符串（String）：一个简单的字符串值，直接使用字符串，作为 PromQL 表达式，则会直接返回字符串。 PromQL 还支持丰富的操作符，用户可以使用这些操作符对进一步的对事件序列进行二次加工。这些操作符包括：数学运算符，逻辑运算符，布尔运算符等等。数学运算工作方式：依次找到与左边向量元素匹配（标签完全一致）的右边向量元素进行运算，如果没找到匹配元素，则直接丢弃。同时新的时间序列将不会包含指标名称。布尔运算则支持用户根据时间序列中样本的值，对时间序列进行过滤。瞬时向量与标量进行布尔运算时，PromQL 依次比较向量中的所有时间序列样本的值，如果比较结果为 true 则保留，反之丢弃。瞬时向量与瞬时向量直接进行布尔运算时，同样遵循默认的匹配模式：依次找到与左边向量元素匹配（标签完全一致）的右边向量元素进行相应的操作，如果没找到匹配元素，则直接丢弃。 布尔运算符的默认行为是对时序数据进行过滤。而在其它的情况下我们可能需要的是真正的布尔结果。这时可以使用 bool 修饰符改变布尔运算的默认行为。 http_requests_total &gt; bool 1000 使用 bool 修改符后，布尔运算不会对时间序列进行过滤，而是直接依次瞬时向量中的各个样本数据与标量的比较结果 0 或者 1。从而形成一条新的时间序列。 http_requests_total&#123;code=&quot;200&quot;,handler=&quot;query&quot;,instance=&quot;localhost:9090&quot;,job=&quot;prometheus&quot;,method=&quot;get&quot;&#125; 1http_requests_total&#123;code=&quot;200&quot;,handler=&quot;query_range&quot;,instance=&quot;localhost:9090&quot;,job=&quot;prometheus&quot;,method=&quot;get&quot;&#125; 0 如果是在两个标量之间使用布尔运算，则必须使用 bool 修饰符 通过集合运算，可以在两个瞬时向量与瞬时向量之间进行相应的集合操作。支持以下集合运算符：and (并且)， or (或者) ，unless (排除) vector1 and vector2 会产生一个由 vector1 的元素组成的新的向量。该向量包含 vector1 中完全匹配 vector2 中的元素组成。vector1 or vector2 会产生一个新的向量，该向量包含 vector1 中所有的样本数据，以及 vector2 中没有与 vector1 匹配到的样本数据。vector1 unless vector2 会产生一个新的向量，新向量中的元素由 vector1 中没有与 vector2 匹配的元素组成。 在 PromQL 操作符中优先级由高到低依次为： ^ *, /, % +, - ==, !=, &lt;=, &lt;, &gt;=, &gt; and, unless or PromQL 中有两种典型的匹配模式：一对一（one-to-one）,多对一（many-to-one）或一对多（one-to-many）。 vector1 &lt;operator&gt; vector2 在操作符两边表达式标签不一致的情况下，可以使用 on(label list)或者 ignoring(label list）来修改便签的匹配行为。使用 ignoring 可以在匹配时忽略某些便签。而 on 则用于将匹配行为限定在某些便签之内。 &lt;vector expr&gt; &lt;bin-op&gt; ignoring(&lt;label list&gt;) &lt;vector expr&gt;&lt;vector expr&gt; &lt;bin-op&gt; on(&lt;label list&gt;) &lt;vector expr&gt; 对于样本： method_code:http_errors:rate5m&#123;method=&quot;get&quot;, code=&quot;500&quot;&#125; 24method_code:http_errors:rate5m&#123;method=&quot;get&quot;, code=&quot;404&quot;&#125; 30method_code:http_errors:rate5m&#123;method=&quot;put&quot;, code=&quot;501&quot;&#125; 3method_code:http_errors:rate5m&#123;method=&quot;post&quot;, code=&quot;500&quot;&#125; 6method_code:http_errors:rate5m&#123;method=&quot;post&quot;, code=&quot;404&quot;&#125; 21method:http_requests:rate5m&#123;method=&quot;get&quot;&#125; 600method:http_requests:rate5m&#123;method=&quot;del&quot;&#125; 34method:http_requests:rate5m&#123;method=&quot;post&quot;&#125; 120 若要对两个关键字进行操作，必须进行限制一方的标签，否则无法匹配，通过 ignoring，将 code 标签忽略 method_code:http_errors:rate5m&#123;code=&quot;500&quot;&#125; / ignoring(code) method:http_requests:rate5m 匹配的结果 &#123;method=&quot;get&quot;&#125; 0.04 // 24 / 600 第一行与第七行&#123;method=&quot;post&quot;&#125; 0.05 // 6 / 120 第四行与第九行 多对一和一对多两种匹配模式指的是“一”侧的每一个向量元素可以与”多”侧的多个元素匹配的情况。在这种情况下，必须使用 group 修饰符：group_left 或者 group_right 来确定哪一个向量具有更高的基数（充当“多”的角色）。 &lt;vector expr&gt; &lt;bin-op&gt; ignoring(&lt;label list&gt;) group_left(&lt;label list&gt;) &lt;vector expr&gt;&lt;vector expr&gt; &lt;bin-op&gt; ignoring(&lt;label list&gt;) group_right(&lt;label list&gt;) &lt;vector expr&gt;&lt;vector expr&gt; &lt;bin-op&gt; on(&lt;label list&gt;) group_left(&lt;label list&gt;) &lt;vector expr&gt;&lt;vector expr&gt; &lt;bin-op&gt; on(&lt;label list&gt;) group_right(&lt;label list&gt;) &lt;vector expr&gt; 多对一和一对多两种模式一定是出现在操作符两侧表达式返回的向量标签不一致的情况。因此需要使用 ignoring 和 on 修饰符来排除或者限定匹配的标签列表。同样用上面的例子，右向量中的元素可能匹配到多个左向量中的元素 因此该表达式的匹配模式为多对一，需要使用 group 修饰符 group_left 指定左向量具有更好的基数。 method_code:http_errors:rate5m / ignoring(code) group_left method:http_requests:rate5m 匹配结果： &#123;method=&quot;get&quot;, code=&quot;500&quot;&#125; 0.04 // 24 / 600&#123;method=&quot;get&quot;, code=&quot;404&quot;&#125; 0.05 // 30 / 600&#123;method=&quot;post&quot;, code=&quot;500&quot;&#125; 0.05 // 6 / 120&#123;method=&quot;post&quot;, code=&quot;404&quot;&#125; 0.175 // 21 / 120 group 修饰符只能在比较和数学运算符中使用。在逻辑运算 and,unless 和 or 才注意操作中默认与右向量中的所有元素进行匹配。 PromQL 聚合可以将瞬时表达式返回的样本数据进行聚合，形成一个具有较少样本值的新的时间序列。 sum (求和) min (最小值) max (最大值) avg (平均值) stddev (标准差) stdvar (标准差异) count (计数) count_values (对 value 进行计数) bottomk (样本值最小的 k 个元素) topk (样本值最大的 k 个元素) quantile (分布统计) 这些操作符被用于聚合所有标签维度，或者通过 without 或者 by 子语句来保留不同的维度。 &lt;aggr-op&gt;([parameter,] &lt;vector expression&gt;) [without|by (&lt;label list&gt;)] 其中只有 count_values, quantile, topk, bottomk 支持参数(parameter)。without 用于从计算结果中移除列举的标签，而保留其它标签。by 则正好相反，结果向量中只保留列出的标签，其余标签则移除。通过 without 和 by 可以按照样本的问题对数据进行聚合。 如果指标 http_requests_total 的时间序列的标签集为 application, instance, 和 group，我们可以通过以下方式计算所有 instance 中每个 application 和 group 的请求总量： sum(http_requests_total) without (instance) 等价于 sum(http_requests_total) by (application, group) 如果只需要计算整个应用的 HTTP 请求总量，可以直接使用表达式： sum(http_requests_total) count_values 用于时间序列中每一个样本值出现的次数。count_values会为每一个唯一的样本值输出一个时间序列，并且每一个时间序列包含一个额外的标签。这个标签的名字由聚合参数指定，同时这个标签值是唯一的样本值。例如要计算运行每个构建版本的二进制文件的数量： count_values(&quot;version&quot;, build_version) 返回结果如下： &#123;count=&quot;641&quot;&#125; 1&#123;count=&quot;3226&quot;&#125; 2&#123;count=&quot;644&quot;&#125; 4 topk 和 bottomk 则用于对样本值进行排序，返回当前样本值前 n 位，或者后 n 位的时间序列。获取 HTTP 请求数前 5 位的时序样本数据，可以使用表达式： topk(5, http_requests_total) quantile 用于计算当前样本数据值的分布情况 quantile(φ, express) ，其中 0 ≤ φ ≤ 1。例如，当 φ 为 0.5 时，即表示找到当前样本数据中的中位数： quantile(0.5, http_requests_total) 返回结果如下： &#123;&#125; 656 常用函数 函数名 作用 函数名 作用 abs 返回绝对值 absent 检查是否有样本数据 ceil 向上取整 floor 向下取整 changes 区间向量内每个样本数据值变化的次数 clamp_max 样本数据值若大于 max，则改为 max clamp_min 样本数据值若小于 min，则改为 min delta 区间向量第一个元素和最后一个元素之间的差值 increase 区间向量中的第一个和最后一个样本并返回其增长量 irate 计算区间向量的增长率 rate 计算区间向量在时间窗口内平均增长速率 round 返回向量中所有样本值的最接近的整数 sort 对向量按元素的值进行升序排序 sort_desc 对向量按元素的值进行降序排序 Prometheus Server 配置文件详细说明global: # 查询目标的频率 [ scrape_interval: &lt;duration&gt; | default = 1m ] # 查询失败的超时时间 [ scrape_timeout: &lt;duration&gt; | default = 10s ] # 匹配规则的频率 [ evaluation_interval: &lt;duration&gt; | default = 1m ] # 当与外部系统（外部存储、Alertmanager）联系时，可附加到任意时间序列或告警的标签 external_labels: [ &lt;labelname&gt;: &lt;labelvalue&gt; ... ]# 指定规则文件rule_files: [ - &lt;filepath_glob&gt; ... ]# 抓取配置scrape_configs: [ - &lt;scrape_config&gt; ... ]# 与alertmanager相关的告警配置alerting: alert_relabel_configs: [ - &lt;relabel_config&gt; ... ] alertmanagers: [ - &lt;alertmanager_config&gt; ... ]# 配置远端写入特性remote_write: [ - &lt;remote_write&gt; ... ]# 配置远端读取特性remote_read: [ - &lt;remote_read&gt; ... ] 抓取配置scrape_configs: - job_name: &#x27;prometheus&#x27; static_configs: - targets: [&#x27;localhost:9090&#x27;] # 采集node exporter监控数据 - job_name: &#x27;node&#x27; static_configs: - targets: [&#x27;localhost:9100&#x27;, &quot;192.168.1.13:9100&quot;] 将 prometheus 数据自动写入 influxdb 安装influxdb，安装1.8版本，非2.0以上版本wget https://dl.influxdata.com/influxdb/releases/influxdb-1.8.10.x86_64.rpmsudo yum localinstall influxdb-1.8.10.x86_64.rpmsystemctl start influxdb 进入influxdb，创建prometheus库# influxConnected to http://localhost:8086 version 1.8.10InfluxDB shell version: 1.8.10&gt; create database prometheus;&gt; show databases;name: databasesname----_internalprometheus&gt; exit 配置prometheus，在prometheus.yml中添加remote_write: - url: &quot;http://192.168.13.41:8086/api/v1/prom/write?db=prometheus&quot;remote_read: - url: &quot;http://192.168.13.41:8086/api/v1/prom/read?db=prometheus&quot; prometheus如何存储influxdb官方文档 常见exporter node_expoter mysqld_exporter redis_exporter blackbox_exporter elasticsearch_exporterNode exporter 主要用于暴露 metrics 给 Prometheus，其中 metrics 包括：cpu 的负载，内存的使用情况，网络等。下载安装：https://prometheus.io/download/#node_exporter解压后将 node_exporter 放到/usr/local/bin中，然后执行。可通过浏览器访问 9100 端口/metrics查看 可创建 systemd 服务： [Unit]Description=node_exporterAfter=network.target[Service]Type=simpleExecStart=/usr/local/bin/node_exporterRestart=on-failure[Install]WantedBy=multi-user.target docker 启动： docker run -d -p 9100:9100 -v /proc:/host/proc -v /sys:/host/sys -v /:/rootfs prom/node-exporter AlertManager 报警Prometheus 的报警分为两个部分。 Prometheus 服务器中的报警规则将报警发送到 Alertmanager。 然后，报警管理器将重复数据删除，分组，再通过电子邮件，通话通知系统和聊天平台等方法管理这些报警，包括静默，禁止，聚合和发出通知。 分组（Grouping）将性质相似的警报归类为单个通知。当许多系统同时发生故障，并且可能同时发出数百到数千个警报时，这种方法尤其有用。警报分组、分组通知的定时以及这些通知的接收者由配置文件中的路由树配置。 抑制（Inhibition）是当某些其他警报已经触发时，抑制某些警报的通知。发出警报，通知整个群集不可访问。 可以将 Alertmanager 配置为使与该群集有关的所有其他警报静音，这样可以防止与实际问题无关的数百或数千个触发警报的通知。通过 Alertmanager 的配置文件配置抑制。 静默（Sliences）是一种简单地在给定时间内静音警报的简单方法。静默是基于匹配器（matchers）配置的，就像路由树一样。将检查传入警报是否与活动静默的所有相等或正则表达式匹配。如果这样做了，将不会发送该警报的通知。静默是在 Alertmanager 的 web 界面配置的。 Alertmanager 支持配置以创建高可用性集群。这可以使用——cluster-*标志进行配置。重点是不要在 Prometheus 和它的 Alertmanagers 之间进行负载平衡，而是将 Prometheus 指向一个所有 Alertmanagers 的列表。 下载：https://prometheus.io/download/#alertmanager解压后执行 alertmanager 命令，需要当前目录有配置文件，默认配置文件为： global: resolve_timeout: 5mroute: group_by: [&#x27;alertname&#x27;] group_wait: 10s group_interval: 10s repeat_interval: 1h receiver: &#x27;web.hook&#x27;receivers:- name: &#x27;web.hook&#x27; webhook_configs: - url: &#x27;http://127.0.0.1:5001/&#x27;inhibit_rules: - source_match: severity: &#x27;critical&#x27; target_match: severity: &#x27;warning&#x27; equal: [&#x27;alertname&#x27;, &#x27;dev&#x27;, &#x27;instance&#x27;] 启动 alertmanager，通过 web 的 9093 端口访问 配置 systemd 服务/usr/lib/systemd/system/alertmanager.service： [Unit]Description=AlertmanagerAfter=network.target[Service]Type=simpleRestart=on-failureExecStart=/usr/local/bin/alertmanager --config.file=/etc/prometheus/alertmanager.yml --storage.path=/var/lib/alertmanager[Install]WantedBy=multi-user.target HTTP APIPrometheus API 使用了 JSON 格式的响应内容。 当 API 调用成功后将会返回 2xx 的 HTTP 状态码。当 API 调用失败时可能返回以下几种不同的 HTTP 状态码： 404 Bad Request：参数错误或者缺失 422 Unprocessable Entity：表达式无法执行 503 Service Unavailiable：请求超时或者被中断 表达式请求瞬时数据请求：/api/v1/query，有 GET 和 POST 两种方法。有以下参数，参数间用&amp;连接： query=：PromQL 表达式 time=&lt;rfc3339 | unix_timestamp&gt;：用于指定用于计算 PromQL 的时间戳。可选参数，默认情况下使用当前系统时间。 timeout=：超时设置。可选参数，默认情况下使用-query.timeout 的全局设置。 请求结果的data段的结构如下： &#123; &quot;resultType&quot;: &quot;matrix&quot; | &quot;vector&quot; | &quot;scalar&quot; | &quot;string&quot;, &quot;result&quot;: &lt;value&gt;&#125; 示例： curl &#x27;http://localhost:9090/api/v1/query?query=up&amp;time=2015-07-01T20:10:51.781Z&#x27;&#123; &quot;status&quot; : &quot;success&quot;, &quot;data&quot; : &#123; &quot;resultType&quot; : &quot;vector&quot;, &quot;result&quot; : [ &#123; &quot;metric&quot; : &#123; &quot;__name__&quot; : &quot;up&quot;, &quot;job&quot; : &quot;prometheus&quot;, &quot;instance&quot; : &quot;localhost:9090&quot; &#125;, &quot;value&quot;: [ 1435781451.781, &quot;1&quot; ] &#125;, ... ] &#125;&#125; 区间数据请求：/api/v1/query_range，支持 GET 和 POST。有以下参数： query=: PromQL 表达式。 start=: 起始时间。 end=: 结束时间。 step=: 查询步长。 timeout=: 超时设置。可选参数，默认情况下使用-query.timeout的全局设置。 请求结果的data段结构： &#123; &quot;resultType&quot;: &quot;matrix&quot;, &quot;result&quot;: &lt;value&gt;&#125; 示例： curl &#x27;http://localhost:9090/api/v1/query_range?query=up&amp;start=2015-07-01T20:10:30.781Z&amp;end=2015-07-01T20:11:00.781Z&amp;step=15s&#x27;&#123; &quot;status&quot; : &quot;success&quot;, &quot;data&quot; : &#123; &quot;resultType&quot; : &quot;matrix&quot;, &quot;result&quot; : [ &#123; &quot;metric&quot; : &#123; &quot;__name__&quot; : &quot;up&quot;, &quot;job&quot; : &quot;prometheus&quot;, &quot;instance&quot; : &quot;localhost:9090&quot; &#125;, &quot;values&quot; : [ [ 1435781430.781, &quot;1&quot; ], [ 1435781445.781, &quot;1&quot; ], [ 1435781460.781, &quot;1&quot; ] ] &#125;, ... ] &#125;&#125; 请求元数据请求序列信息：/api/v1/series，有 GET 和 POST 方法。有以下参数： match[] = &lt;series_selector&gt;：重复的序列选择器参数，用于选择要返回的序列。必须至少提供一个match[]参数。 start = &lt;rfc3339 | unix_timestamp&gt;：开始时间戳。 end = &lt;rfc3339 | unix_timestamp&gt;：结束时间戳。 查询结果的data部分由一个对象列表组成，这些对象包含标识每个系列的标签名/值对。 curl -g &#x27;http://localhost:9090/api/v1/series?&#x27; --data-urlencode=&#x27;match[]=up&#x27; --data-urlencode=&#x27;match[]=process_start_time_seconds&#123;job=&quot;prometheus&quot;&#125;&#x27;&#123; &quot;status&quot; : &quot;success&quot;, &quot;data&quot; : [ &#123; &quot;__name__&quot; : &quot;up&quot;, &quot;job&quot; : &quot;prometheus&quot;, &quot;instance&quot; : &quot;localhost:9090&quot; &#125;, ... ]&#125; 获取标签（label）名：/api/v1/labels，支持 GET 和 POST。 curl &#x27;localhost:9090/api/v1/labels&#x27;&#123; &quot;status&quot;: &quot;success&quot;, &quot;data&quot;: [ &quot;__name__&quot;, &quot;call&quot;, &quot;code&quot;, &quot;config&quot;, &quot;dialer_name&quot;, &quot;endpoint&quot;, ... ]&#125; 获取标签值：/api/v1/label/&lt;label_name&gt;/values，仅支持 GET curl http://localhost:9090/api/v1/label/job/values&#123; &quot;status&quot; : &quot;success&quot;, &quot;data&quot; : [ &quot;node&quot;, &quot;prometheus&quot; ]&#125; targets获取 targets：/api/v1/targets，支持 GET。活动（active）目标和已删除（dropped）目标都是响应的一部分。 labels表示在重新标记后的标签集。discoverLabel表示在重新标记之前在服务发现期间检索到的未修改的标签。 curl http://localhost:9090/api/v1/targets&#123; &quot;status&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;activeTargets&quot;: [ &#123; &quot;discoveredLabels&quot;: &#123; &quot;__address__&quot;: &quot;127.0.0.1:9090&quot;, &quot;__metrics_path__&quot;: &quot;/metrics&quot;, &quot;__scheme__&quot;: &quot;http&quot;, &quot;job&quot;: &quot;prometheus&quot; &#125;, &quot;labels&quot;: &#123; &quot;instance&quot;: &quot;127.0.0.1:9090&quot;, &quot;job&quot;: &quot;prometheus&quot; &#125;, &quot;scrapeUrl&quot;: &quot;http://127.0.0.1:9090/metrics&quot;, &quot;lastError&quot;: &quot;&quot;, &quot;lastScrape&quot;: &quot;2017-01-17T15:07:44.723715405+01:00&quot;, &quot;health&quot;: &quot;up&quot; &#125; ], &quot;droppedTargets&quot;: [ &#123; &quot;discoveredLabels&quot;: &#123; &quot;__address__&quot;: &quot;127.0.0.1:9100&quot;, &quot;__metrics_path__&quot;: &quot;/metrics&quot;, &quot;__scheme__&quot;: &quot;http&quot;, &quot;job&quot;: &quot;node&quot; &#125;, &#125; ] &#125;&#125; rules获取 rules：/api/v1/rules，支持 GET。返回当前加载的报警和记录规则的列表，还返回由每个报警规则的 Prometheus 实例触发的当前活动报警。 curl http://localhost:9090/api/v1/rules&#123; &quot;data&quot;: &#123; &quot;groups&quot;: [ &#123; &quot;rules&quot;: [ &#123; &quot;alerts&quot;: [ &#123; &quot;activeAt&quot;: &quot;2018-07-04T20:27:12.60602144+02:00&quot;, &quot;annotations&quot;: &#123; &quot;summary&quot;: &quot;High request latency&quot; &#125;, &quot;labels&quot;: &#123; &quot;alertname&quot;: &quot;HighRequestLatency&quot;, &quot;severity&quot;: &quot;page&quot; &#125;, &quot;state&quot;: &quot;firing&quot;, &quot;value&quot;: &quot;1e+00&quot; &#125; ], &quot;annotations&quot;: &#123; &quot;summary&quot;: &quot;High request latency&quot; &#125;, &quot;duration&quot;: 600, &quot;health&quot;: &quot;ok&quot;, &quot;labels&quot;: &#123; &quot;severity&quot;: &quot;page&quot; &#125;, &quot;name&quot;: &quot;HighRequestLatency&quot;, &quot;query&quot;: &quot;job:request_latency_seconds:mean5m&#123;job=\\&quot;myjob\\&quot;&#125; &gt; 0.5&quot;, &quot;type&quot;: &quot;alerting&quot; &#125;, &#123; &quot;health&quot;: &quot;ok&quot;, &quot;name&quot;: &quot;job:http_inprogress_requests:sum&quot;, &quot;query&quot;: &quot;sum(http_inprogress_requests) by (job)&quot;, &quot;type&quot;: &quot;recording&quot; &#125; ], &quot;file&quot;: &quot;/rules.yaml&quot;, &quot;interval&quot;: 60, &quot;name&quot;: &quot;example&quot; &#125; ] &#125;, &quot;status&quot;: &quot;success&quot;&#125; alerts获取 alerts：/api/v1/alerts，支持 GET。返回所有启用的报警的列表。 curl http://localhost:9090/api/v1/alerts&#123; &quot;data&quot;: &#123; &quot;alerts&quot;: [ &#123; &quot;activeAt&quot;: &quot;2018-07-04T20:27:12.60602144+02:00&quot;, &quot;annotations&quot;: &#123;&#125;, &quot;labels&quot;: &#123; &quot;alertname&quot;: &quot;my-alert&quot; &#125;, &quot;state&quot;: &quot;firing&quot;, &quot;value&quot;: &quot;1e+00&quot; &#125; ] &#125;, &quot;status&quot;: &quot;success&quot;&#125; alertmanager获取 alertmanager 信息：/api/v1/alertmanagers。支持 GET。返回 Prometheus alertmanager 发现的当前状态的概述。 curl http://localhost:9090/api/v1/alertmanagers&#123; &quot;status&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;activeAlertmanagers&quot;: [ &#123; &quot;url&quot;: &quot;http://127.0.0.1:9090/api/v1/alerts&quot; &#125; ], &quot;droppedAlertmanagers&quot;: [ &#123; &quot;url&quot;: &quot;http://127.0.0.1:9093/api/v1/alerts&quot; &#125; ] &#125;&#125; status获取该结点的当前的 Prometheus 配置：/api/v1/status/config，支持 GET。配置作为转储的 YAML 文件返回。 由于 YAML 库的限制，不包括 YAML 注释。 curl http://localhost:9090/api/v1/status/config&#123; &quot;status&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;yaml&quot;: &quot;&lt;content of the loaded config file in YAML&gt;&quot;, &#125;&#125; 获取配置 Prometheus 的标志值：/api/v1/status/flags，支持 GET。 curl http://localhost:9090/api/v1/status/flags&#123; &quot;status&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;alertmanager.notification-queue-capacity&quot;: &quot;10000&quot;, &quot;alertmanager.timeout&quot;: &quot;10s&quot;, &quot;log.level&quot;: &quot;info&quot;, &quot;query.lookback-delta&quot;: &quot;5m&quot;, &quot;query.max-concurrency&quot;: &quot;20&quot;, ... &#125;&#125; 获取关于 Prometheus 服务器的各种运行时信息属性：/api/v1/status/runtimeinfo，支持 GET。根据运行时属性的性质，返回的值具有不同的类型。 curl http://localhost:9090/api/v1/status/runtimeinfo&#123; &quot;status&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;startTime&quot;: &quot;2019-11-02T17:23:59.301361365+01:00&quot;, &quot;CWD&quot;: &quot;/&quot;, &quot;reloadConfigSuccess&quot;: true, &quot;lastConfigTime&quot;: &quot;2019-11-02T17:23:59+01:00&quot;, &quot;chunkCount&quot;: 873, &quot;timeSeriesCount&quot;: 873, &quot;corruptionCount&quot;: 0, &quot;goroutineCount&quot;: 48, &quot;GOMAXPROCS&quot;: 4, &quot;GOGC&quot;: &quot;&quot;, &quot;GODEBUG&quot;: &quot;&quot;, &quot;storageRetention&quot;: &quot;15d&quot; &#125;&#125; 获取关于 Prometheus 服务器的各种构建信息属性：/api/v1/status/buildinfo，支持 GET。 curl http://localhost:9090/api/v1/status/buildinfo&#123; &quot;status&quot;: &quot;success&quot;, &quot;data&quot;: &#123; &quot;version&quot;: &quot;2.13.1&quot;, &quot;revision&quot;: &quot;cb7cbad5f9a2823a622aaa668833ca04f50a0ea7&quot;, &quot;branch&quot;: &quot;master&quot;, &quot;buildUser&quot;: &quot;julius@desktop&quot;, &quot;buildDate&quot;: &quot;20191102-16:19:59&quot;, &quot;goVersion&quot;: &quot;go1.13.1&quot; &#125;&#125;","categories":[{"name":"云计算","slug":"云计算","permalink":"https://coconutmilktaro.top/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"监控","slug":"监控","permalink":"https://coconutmilktaro.top/tags/%E7%9B%91%E6%8E%A7/"},{"name":"云计算","slug":"云计算","permalink":"https://coconutmilktaro.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://coconutmilktaro.top/tags/Prometheus/"}]},{"title":"PowerShell实战笔记","slug":"PowerShell实战笔记","date":"2019-06-01T13:25:28.000Z","updated":"2022-06-21T15:16:21.217Z","comments":true,"path":"2019/PowerShell实战笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/PowerShell%E5%AE%9E%E6%88%98%E7%AC%94%E8%AE%B0/","excerpt":"本笔记主要参照《Windows PowerShell 实战指南》的步骤学习，以及知识点补充。目录参照该书。","text":"本笔记主要参照《Windows PowerShell 实战指南》的步骤学习，以及知识点补充。目录参照该书。 帮助系统 运行命令 提供程序 管道 终止进程或停止服务 读取 csv 或 XML 文件 命令示例 收集计算机信息 帮助系统查看当前的 PowerShell 版本，输入$PSVersionTable PS C:\\WINDOWS\\system32&gt; $PSVersionTableName Value---- -----PSVersion 5.1.17134.765PSEdition DesktopPSCompatibleVersions &#123;1.0, 2.0, 3.0, 4.0...&#125;BuildVersion 10.0.17134.765CLRVersion 4.0.30319.42000WSManStackVersion 3.0PSRemotingProtocolVersion 2.3SerializationVersion 1.1.0.1 查看指定命令的帮助文档 help 命令或get-help PS C:\\WINDOWS\\system32&gt; help Get-Service语法 Get-Service [[-Name] &lt;string[]&gt;] [&lt;CommonParameters&gt;] ...... 更新帮助文档 update-help 可使用( )指定操作的顺序，括号中的会先执行。 例：先创建一个文件叫names.txt，写入要操作的目标主机名 PS C:\\Users\\gutianyi&gt; Get-EventLog Application -ComputerName (Get-Content names.txt) Index Time EntryType Source InstanceID Message ----- ---- --------- ------ ---------- ------- 13446 6月 01 21:46 Information ESENT 916 svchost (16532,G,98) 由于 Beta 网站模式设置 0x8... 13445 6月 01 21:39 Information ESENT 916 svchost (3084,G,98) 由于 Beta 网站模式设置 0x80... 13444 6月 01 21:38 Information ESENT 916 svchost (4488,G,98) 由于 Beta 网站模式设置 0x80... ...... 查看命令示例 help 命令 -examples PS C:\\Users\\gutianyi&gt; help Get-EventLog -Examples名称 Get-EventLog摘要 Gets the events in an event log, or a list of the event logs, on the local or remote computers. Example 1: Get event logs on a computer PS C:\\&gt;Get-EventLog -List This command gets the event logs on the computer. Example 2: Get the five most recent entries from a specific event log ...... 运行命令Cmdlet 是一个原生的 PowerShell 命令行工具，是以 Powershell 自己脚本语言编写的。 Cmdlet 的命令规范，以一个动词开始，后面跟上一个-，之后是一个单数名词。动词可通过Get-Verb查看，大概有 100 个。 PS C:\\Users\\gutianyi&gt; Get-VerbVerb Group---- -----Add CommonClear CommonClose CommonCopy CommonEnter CommonExit CommonFind Common...... 查看命令别称 get-alias -Definition &#39;命令&#39; PS C:\\Users\\gutianyi&gt; Get-Alias -Definition &#x27;Get-Service&#x27;CommandType Name Version Source----------- ---- ------- ------Alias gsv -&gt; Get-Service 提示命令帮助信息 show-command 命令，会弹出图形提示框，填入信息后自动生成命令。 提供程序提供程序（PSProvider）本质上是一个适配器，可以访问某些数据存储介质，并使这些截至看起来像是磁盘驱动器一样。可通过模块或管理单元将提供程序添加到 PowerShell 中。 查看当前 shell 中存在的提供程序： PS C:\\Users\\gutianyi&gt; Get-PSProviderName Capabilities Drives---- ------------ ------Registry ShouldProcess, Transactions &#123;HKLM, HKCU&#125;Alias ShouldProcess &#123;Alias&#125;Environment ShouldProcess &#123;Env&#125;FileSystem Filter, ShouldProcess, Credentials &#123;C&#125;Function ShouldProcess &#123;Function&#125;Variable ShouldProcess &#123;Variable&#125; 提供程序的功能（Capabilities）： ShouldProcess：支持-WhatIf和-Confirm参数，保证在正式执行这部分脚本之前能对他们进行测试。 Filter：支持-Filter参数 Credentials：允许使用可变更的凭据去连接数据存储，即支持-Credentials参数。 Transactions：支持事务，即允许在该提供程序中将多个变更作为一个原子操作进行提交或回滚。 可使用某个提供程序去创建一个 PSDrive，PSDrive 可通过一个特定的提供程序去连接到某些存储介质，类似资源管理器，本质是创建一个驱动器映射。可查看当前已连接的驱动器 PS C:\\Users\\gutianyi&gt; Get-PSDriveName Used (GB) Free (GB) Provider Root CurrentLocation---- --------- --------- -------- ---- ---------------Alias AliasC 204.70 31.68 FileSystem C:\\ Users\\gutianyiCert Certificate \\Env EnvironmentFunction FunctionHKCU Registry HKEY_CURRENT_USERHKLM Registry HKEY_LOCAL_MACHINEVariable VariableWSMan WSMan Windows 文件系统主要由三部分组成： 磁盘驱动器：是最上层的对象，包含文件夹与文件 文件夹：是一种容器对象，可包含文件或其他文件夹 文件：是最小的对象 在 PowerShell 中，并不使用文件和文件夹的说法，而是使用“项（Item）”指代文件或文件夹，因为 PSDrive 还可能映射到注册表（并不是文件系统）。 查看一个项的属性 PS C:\\Users\\gutianyi&gt; Get-ItemProperty -Path C:\\ 目录:Mode LastWriteTime Length Name---- ------------- ------ ----d--hs- 2019/6/1 12:39 C:\\PS C:\\Users\\gutianyi&gt; Get-ItemProperty -Path C:\\Users\\gutianyi\\Documents\\ISOs\\CentOS-7-x86_64-DVD-1804.iso 目录: C:\\Users\\gutianyi\\Documents\\ISOsMode LastWriteTime Length Name---- ------------- ------ -----a---- 2018/11/24 18:33 4470079488 CentOS-7-x86_64-DVD-1804.iso 变更当前路径 Set-Location 路径，即进行文件夹切换，类似cd。 PS C:\\Users\\gutianyi&gt; Set-Location C:\\MyProgramsPS C:\\MyPrograms&gt; 新建一个项，可以是文件、文件夹、注册表等，需要-ItemType指定类型 PS C:\\Users\\gutianyi\\Documents&gt; New-Item -ItemType Directory powershell-test 目录: C:\\Users\\gutianyi\\DocumentsMode LastWriteTime Length Name---- ------------- ------ ----d----- 2019/6/2 11:12 powershell-testPS C:\\Users\\gutianyi\\Documents\\powershell-test&gt; New-Item -ItemType File test1 目录: C:\\Users\\gutianyi\\Documents\\powershell-testMode LastWriteTime Length Name---- ------------- ------ -----a---- 2019/6/2 11:15 0 test1 查看指定项的子项（即子目录或文件），get-childitem与dir一致，允许使用通配符。 PS C:\\Users\\gutianyi\\Documents&gt; Get-ChildItem .\\powershell-test\\ 目录: C:\\Users\\gutianyi\\Documents\\powershell-testMode LastWriteTime Length Name---- ------------- ------ -----a---- 2019/6/2 11:15 0 test1 若路径中本身就包含了通配符，需要忽略，则需要使用参数-LiteralPath，不会解释任何字符为通配符。 修改注册表，以 HKEY_CURRENT_USER 为例。 PS C:\\Users\\gutianyi\\Documents&gt; Set-Location -Path HKCU:PS HKCU:\\&gt; Set-Location .\\Software\\Microsoft\\Windows\\PS HKCU:\\Software\\Microsoft\\Windows\\&gt; Get-ChildItem Hive: HKEY_CURRENT_USER\\Software\\Microsoft\\WindowsName Property---- --------CurrentVersionDWM Composition : 1 ColorizationColor : 3288365271 ColorizationColorBalance : 89 ColorizationAfterglow : 3288365271 ColorizationAfterglowBalance : 10 ColorizationBlurBalance : 1 ColorizationGlassAttribute : 1 AccentColor : 4292311040 ColorPrevalence : 0 EnableAeroPeek : 1ShellShellNoRoamTabletPCWindows Error Reporting LastQueuePesterTime : 131958139657061437 LastRateLimitedDumpGenerationTime : 132034409468928434WinlogonPS HKCU:\\Software\\Microsoft\\Windows&gt; Set-ItemProperty -Path .\\DWM\\ -PSProperty EnableAeroPeek -Value 0 管道将输出结果通过管道导到 CSV 或 XML 文件，通过export-csv 文件或export-clixml 文件 PS C:\\Users\\gutianyi\\Documents&gt; Get-Process | Export-Csv process.csvPS C:\\Users\\gutianyi\\Documents&gt; Get-Process | Export-Clixml process.xml 比较两个文件或同个命令输出结果的不同，需要用到命令compare-object或diff（compare-object的别名） PS C:\\Users\\gutianyi\\Documents&gt; diff -ReferenceObject (Import-Clixml .\\process.xml) -DifferenceObject (Get-Process) -Property namename SideIndicator---- -------------cloudmusic =&gt;cloudmusic =&gt;cloudmusic =&gt;firefox =&gt;firefox =&gt;firefox =&gt;SearchProtocolHost =&gt;DocToPDF &lt;=QQ &lt;=TXPlatform &lt;=YNoteCefRender &lt;=YNoteCefRender &lt;=YNoteCefRender &lt;=YoudaoNote &lt;= 若要直接将命令输出结果重定向到一个文件，可直接通过&gt;导出。&gt;是 powershell 为兼容 cmd 的一个快捷方式，实际在处理时使用的是命令 | Outfile xxx.txt。默认Outfile输出的文件最大列数为 80，若命令的列数超过了 80，则需要限制命令输出的参数。 还可以将输出转为 HTML，使用命令ConvertTo-HTML，后面最好再管道输出为文件，否则会直接将所有结果输出到终端。也可以转换为 CSV 文件或 XML 文件。 PS C:\\Users\\gutianyi\\Documents&gt; Import-Clixml .\\process.xml | ConvertTo-Html | Out-File process.htmlPS C:\\Users\\gutianyi\\Documents&gt; Import-Clixml .\\process.xml | ConvertTo-Csv | Out-File process.csv 终止进程或停止服务使用Stop-Process终止进程。 PS C:\\Users\\gutianyi&gt; Get-Process -Name FoxitReader | Stop-Process Cmdlets 内部定义有影响级别，且不允许修改，可通过变量$ConfirmPreference查看，默认为 High PS C:\\Users\\gutianyi&gt; $ConfirmPreferenceHigh 当 Cmdlet 内部影响等级大于等于 Shell 的$confirmpreference时，不管 cmdlet 做什么，shell 都会询问Are you Sure?。若内部影响级别小于$confirmpreference时，则不会询问。但若要在操作时弹出确认，则可以添加参数-confirm PS C:\\Users\\gutianyi&gt; Get-Process -Name chrome | Stop-Process -Confirm确认是否确实要执行此操作?正在目标“chrome (1432)”上执行操作“Stop-Process”。[Y] 是(Y) [A] 全是(A) [N] 否(N) [L] 全否(L) [S] 暂停(S) [?] 帮助 (默认值为“Y”): y确认 若要查看停止进程或服务时，会执行哪些操作，减小误操作风险，可以加上-whatif参数，仅是查看，并不会真正执行。 PS C:\\Users\\gutianyi&gt; Get-Process -Name chrome | Stop-Process -whatifWhatIf: 正在目标“chrome (1676)”上执行操作“Stop-Process”。WhatIf: 正在目标“chrome (2412)”上执行操作“Stop-Process”。WhatIf: 正在目标“chrome (4856)”上执行操作“Stop-Process”。WhatIf: 正在目标“chrome (9912)”上执行操作“Stop-Process”。WhatIf: 正在目标“chrome (11848)”上执行操作“Stop-Process”。..... 读取 csv 或 XML 文件可通过get-content 文件 读取文件，但是该指令并不会对数据进行过滤及解析，排版不友好，会存在一些垃圾信息，如提示信息或文件信息。 而使用指令import-csv则会进行解析，清除掉无用或重复的信息，并格式化数据，便于查看。 命令示例收集计算机信息 参考文章或书目： Windows PowerShell 实战指南（第二版）（Learn Windows Powershell In A Month Of Launches） Microsoft PowerShell 文档 PowerShell 在线教程","categories":[],"tags":[{"name":"PowerShell","slug":"PowerShell","permalink":"https://coconutmilktaro.top/tags/PowerShell/"}]},{"title":"控制IGP路由笔记","slug":"控制IGP路由","date":"2019-03-21T12:20:08.000Z","updated":"2022-05-30T02:51:53.952Z","comments":true,"path":"2019/控制IGP路由/","link":"","permalink":"https://coconutmilktaro.top/2019/%E6%8E%A7%E5%88%B6IGP%E8%B7%AF%E7%94%B1/","excerpt":"","text":"","categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"华三","slug":"华三","permalink":"https://coconutmilktaro.top/tags/%E5%8D%8E%E4%B8%89/"},{"name":"路由","slug":"路由","permalink":"https://coconutmilktaro.top/tags/%E8%B7%AF%E7%94%B1/"}]},{"title":"园区网管理维护笔记","slug":"园区网管理维护笔记","date":"2019-03-21T12:10:08.000Z","updated":"2022-05-30T02:51:53.931Z","comments":true,"path":"2019/园区网管理维护笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/%E5%9B%AD%E5%8C%BA%E7%BD%91%E7%AE%A1%E7%90%86%E7%BB%B4%E6%8A%A4%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"华三","slug":"华三","permalink":"https://coconutmilktaro.top/tags/%E5%8D%8E%E4%B8%89/"},{"name":"园区网","slug":"园区网","permalink":"https://coconutmilktaro.top/tags/%E5%9B%AD%E5%8C%BA%E7%BD%91/"}]},{"title":"园区网安全技术笔记","slug":"园区网安全技术笔记","date":"2019-03-21T12:00:08.000Z","updated":"2022-05-30T02:51:53.931Z","comments":true,"path":"2019/园区网安全技术笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/%E5%9B%AD%E5%8C%BA%E7%BD%91%E5%AE%89%E5%85%A8%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"华三","slug":"华三","permalink":"https://coconutmilktaro.top/tags/%E5%8D%8E%E4%B8%89/"},{"name":"园区网","slug":"园区网","permalink":"https://coconutmilktaro.top/tags/%E5%9B%AD%E5%8C%BA%E7%BD%91/"}]},{"title":"VLAN基础笔记","slug":"VLAN基础笔记","date":"2019-03-21T11:50:08.000Z","updated":"2022-05-30T02:51:53.876Z","comments":true,"path":"2019/VLAN基础笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/VLAN%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"华三","slug":"华三","permalink":"https://coconutmilktaro.top/tags/%E5%8D%8E%E4%B8%89/"},{"name":"VLAN","slug":"VLAN","permalink":"https://coconutmilktaro.top/tags/VLAN/"}]},{"title":"PPP协议基础笔记","slug":"PPP协议基础笔记","date":"2019-03-21T11:40:08.000Z","updated":"2022-05-30T02:51:53.863Z","comments":true,"path":"2019/PPP协议基础笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/PPP%E5%8D%8F%E8%AE%AE%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/","excerpt":"PPP 协议基础，根据华三网络整理 PPP 协议概述 PPP MP PPPOE 华三 PPP 环境搭建 PAP 单向验证配置 PAP 双向验证配置 CHAP 验证配置 IP 地址协商 直接指定对端 IP 地址 配置地址池供对端选择 ISP 域关联 IP 地址池 PPP 常见配置项 MP 配置 虚拟模板接口配置 MP 将链路直接绑定到 VT 上 按用户名查找 VT MP-group 接口配置 MP","text":"PPP 协议基础，根据华三网络整理 PPP 协议概述 PPP MP PPPOE 华三 PPP 环境搭建 PAP 单向验证配置 PAP 双向验证配置 CHAP 验证配置 IP 地址协商 直接指定对端 IP 地址 配置地址池供对端选择 ISP 域关联 IP 地址池 PPP 常见配置项 MP 配置 虚拟模板接口配置 MP 将链路直接绑定到 VT 上 按用户名查找 VT MP-group 接口配置 MP PPP 协议概述Point to Point Protocol 点对点协议 特点： 支持动态分配 IP 地址，允许连接时协商 IP 地址。 支持同步与异步线路。支持多种网络层协议（TCP/IP 等） 支持错误检测及纠错，支持数据压缩 支持身份验证。无重传机制，网络开销小 组成： 链路控制协议 LCP：用于建立、配置、测试管理数据链路连接 网络控制协议 NCP：协商链路上所传输的数据包格式等参数，建立配置不同网络层协议 认证协议：用于对用户进行认证，包括 PAP（Password Authentication Protocol 密码认证协议）、CHAP（Challenge Handshake Authentication Protocol 质询握手认证协议）、MS-CHAP（Microsoft CHAP 微软 CHAP 协议） PPP 会话过程：PPP 的初始状态为不活动（dead）状态 链路建立阶段：当物理层可用时，进入 Establish 阶段，发送 LCP 报文检测链路可用情况（LCP 协商），若可用（协商成功）就成功建立，LCP 进入 Opened 状态并上报 up 事件，否则失败，上报 fail 事件，进入 Dead 阶段。 验证阶段（若配置了验证）：根据 PPP 帧中验证选项字段确定是否验证，若配置了验证，就进入 Authenticate 阶段，选择 PAP 或 CHAP 等验证。该阶段仅支持 LCP 和验证协议报文，其他报文都被丢弃。若验证失败进入 Terminate 状态，链路拆除，LCP 变为 Down。若成功就进入 Network 阶段 网络层协商阶段（若配置了网络层协议）：PPP 双方发送 NCP 报文协商网络层协议（如 IPCP）及地址，NCP 状态从 Initial 变为 Request。协商成功后 NCP 状态变为 Opened，链路建立成功。 此后，PPP 链路将一直保持通信，直至有明确的 LCP 或 NCP 消息关闭这条链路，或发生了某些外部事件（例如用户的干预）。 两种验证： PAP：两次握手。被验证方发起验证请求，向对端发送用户名和密码（明文），主验证方通过查询本地用户列表或 RADIUS 服务器，然后回应通过或拒绝。 PAP 支持双向认证。 CHAP：三次握手。主验证方发起验证请求 Challenge，发送本端主机名和随机报文。被验证方收到后查询本地密码，若本端配置了 CHAP 默认密码，就选用此密码。否则在用户表查找主验证方用户名对应的密码，并选用。被验证方通过 MD5 对报文 ID、被验证方密码、原随机数生成一个摘要，回复 Response。主验证方对本端密码、相同随机数、报文 ID 进行 MD5 摘要，并进行比对，若相同则验证成功，回复 Acknowledge，否则回复失败。 CHAP 不直接传输用户密码，而是传输通过 MD5 将密码与随机报文 ID 一起计算的结果，安全性高。认证方最多允许被认证方重传 3 次。 PPP 帧格式： PPP MPMP 是 MultiLink PPP 的简写。将多条 PPP 链路捆绑后当作一条链路，实现带宽增加，负载分担，降低报文延时，备份。 MP 会将报文分片（小于最小分片包长时不分片）后，从 MP 链路下的多个 PPP 通道发送到对端，对端将这些分片组装起来传递给网络层处理。 通过配置虚拟模板（virtual-template）（华三设备）实现。可用用户名捆绑或通过一个 VT 口派生多个捆绑，也可通过 MP-Group 实现。MP-Group 是 MP 专用接口，一个 MP-Group 只能对应一个绑定 PPPOEPPPoE 描述了在以太网上建立 PPPoE 会话及封装 PPP 报文的方法。要求通信双方建立的是点到点关系，而不是在以太网中所出现的点到多点关系。 PPPoE 利用以太网将大量主机组成网络，然后通过一个远端接入设备为以太网上的主机提供互联网接入服务，并对接入的每台主机实现控制、计费功能。由于很好地结合了以太网的经济性及 PPP 良好的可扩展性与管理控制功能，PPPoE 被广泛应用于小区组网等环境中。 PPPoE 协议将 PPP 报文封装在以太网帧之内，在以太网上提供点对点的连接。 PPPoE 使用 Client/Server 模型。PPPoE Client 向 PPPoE Server 发起连接请求，两者之间会话协商通过后，就建立 PPPoE 会话，此后 PPPoE Server 向 PPPoE Client 提供接入控制、认证等功能。 根据 PPPoE 会话的起点所在位置的不同，有两种组网结构： 第一种方式是在两台路由器之间建立 PPPoE 会话，所有主机通过同一个 PPPoE 会话传送数据，主机上不用安装 PPPoE 客户端拨号软件，一般是一个企业共用一个账号接入网络（图中 PPPoE Client 位于企业/公司内部，PPPoE Server 是运营商的设备）。 第二种方式是将 PPPoE 会话建立在 Host 和运营商的路由器之间，为每一个 Host 建立一个 PPPoE 会话，每个 Host 都是 PPPoE Client，每个 Host 使用一个帐号，方便运营商对用户进行计费和控制。Host 上必须安装 PPPoE 客户端拨号软件。 华三 PPP 环境搭建实验环境：两台路由器 RT1 和 RT2，RT1 为主验证方，RT2 为被验证方 RT1:[RT1]interface Serial 1/0[RT1-Serial1/0]ip address 192.168.1.1 24RT2:[RT2]interface Serial 1/0[RT2-Serial1/0]ip address 192.168.1.2 24 PAP 单向验证配置RT1:[RT1-Serial1/0]link-protocol ppp # 在串口配置封装的链路层协议为PPP# 缺省情况下，除以太网接口、VLAN接口、ATM接口外，其它接口封装的链路层协议均为PPP# 所以在串口可以不配这条[RT1-Serial1/0]ppp authentication-mode pap # 设置验证方式为pap[RT1]local-user zhangsan class network # 为RT2添加验证用户zhangsanNew local user added.[RT1-luser-network-zhangsan]password simple 123[RT1-luser-network-zhangsan]service-type ppp # 设置服务类型为pppRT2:[RT2-Serial1/0]link-protocol ppp[RT2-Serial1/0]ppp pap local-user zhangsan password simple 123 # 在串口配置验证信息 查看串口的端口信息，检查是否配置成功 [RT1]display interface SerialSerial1/0Current state: UP # 端口状态UPLine protocol state: UP # 链路协议UPDescription: Serial1/0 InterfaceBandwidth: 64kbpsMaximum Transmit Unit: 1500Hold timer: 10 seconds, retry times: 5Internet Address is 192.168.1.1/24 PrimaryLink layer protocol: PPPLCP: opened, IPCP: opened # LCP和IPCP都开启了Output queue - Urgent queuing: Size/Length/Discards 0/100/0Output queue - Protocol queuing: Size/Length/Discards 0/500/0Output queue - FIFO queuing: Size/Length/Discards 0/75/0Last link flapping: 0 hours 45 minutes 27 secondsLast clearing of counters: Never 此时互相 ping，能够 ping 通 PAP 双向验证配置互相配置验证用户 RT1:[RT1]local-user zhangsan class network[RT1-luser-network-zhangsan]password simple 123[RT1-luser-network-zhangsan]service-type pppRT2:[RT2]local-user lisi class networkNew local user added.[RT2-luser-network-lisi]password simple 321[RT2-luser-network-lisi]service-type ppp 双方都要在串口配置 PPP 验证 RT1:[RT1-Serial1/0]ppp authentication-mode pap[RT1-Serial1/0]ppp pap local-user zhangsan password simple 123RT2:[RT2-Serial1/0]ppp authentication-mode pap[RT2-Serial1/0]ppp pap local-user lisi password simple 321 同理检查串口状态，并互相 ping 检查。 CHAP 验证配置CHAP 认证分为两种：认证方配置了用户名和认证方没有配置用户名。 当认证方配置了用户名： RT1（验证方）:# 配置对端的验证用户zhangsan[RT1]local-user zhangsan class networkNew local user added.[RT1-luser-network-zhangsan]password simple 123[RT1-luser-network-zhangsan]service-type ppp[RT1-Serial1/0]ppp authentication-mode chap# 配置对端验证本端的用户，即RT1对应了用户lisi[RT1-Serial1/0]ppp chap user lisi[RT1-Serial1/0]ppp chap password simple 321 # 密码，可选RT2:# 配置对端验证用户lisi[RT2]local-user lisi class networkNew local user added.[RT2-luser-network-lisi]password simple 321[RT2-luser-network-lisi]service-type ppp[RT2-Serial1/0]ppp authentication-mode chap[RT2-Serial1/0]ppp chap user zhangsan[RT2-Serial1/0]ppp chap password simple 123即：RT1的本地用户zhangsan是给RT2来验证的，RT2的本地用户lisi是给RT1来验证的 当验证方没有配置用户名： RT1:[RT1]local-user zhangsan class networkNew local user added.[RT1-luser-network-zhangsan]password simple 123[RT1-luser-network-zhangsan]service-type ppp[RT1-Serial1/0]ppp authentication-mode chapRT2:[RT2-Serial1/0]ppp chap user zhangsan[RT2-Serial1/0]ppp chap password simple 123 IP 地址协商直接指定对端 IP 地址RT1:[RT1-Serial1/0]remote address 192.168.1.2RT2:[RT2-Serial1/0]ip address ppp-negotiate 然后查看 RT2 的串口端口 IP [RT2-Serial1/0]display interface Serial 1/0 briefBrief information on interface(s) under route mode:Link: ADM - administratively down; Stby - standbyProtocol: (s) - spoofingInterface Link Protocol Main IP DescriptionSer1/0 UP UP 192.168.1.2 配置地址池供对端选择RT1:[RT1]ip pool pool-1 192.168.1.10 192.168.1.20[RT1-Serial1/0]remote address pool pool-1RT2:[RT2-Serial1/0]ip address ppp-negotiate 可在 RT1 上查看地址池的分配情况 [RT1]display ip pool pool-1Group name: default Pool name Start IP address End IP address Free In use pool-1 192.168.1.10 192.168.1.20 10 1In use IP addresses: IP address Interface 192.168.1.10 Ser1/0 ISP 域关联 IP 地址池RT1:[RT1]ip pool pool-1 192.168.1.10 192.168.1.20[RT1]local-user zhangsan class networkNew local user added.[RT1-luser-network-zhangsan]password simple 123[RT1-luser-network-zhangsan]service-type ppp[RT1]domain domain-1[RT1-isp-domain-1]authorization-attribute ip-pool pool-1[RT1-Serial1/0]ip address 192.168.1.1 24[RT1-Serial1/0]ppp authentication-mode pap domain domain-1RT2:[RT2-Serial1/0]ppp pap local-user zhangsan password simple 123[RT2-Serial1/0]ip address ppp-negotiate 之后链路会断开，等待一段时间后会再次 up，RT2 的 IP 也会分配好。 PPP 常见配置项PPP 协议可以为每条 PPP 链路提供基于流量的计费统计功能，具体统计内容包括出入两个方向上流经本链路的报文数和字节数。AAA 可以获取这些流量统计信息用于计费控制。 缺省情况下，PPP 计费统计功能处于关闭状态。 ppp account-statistics enable 轮询时间间隔指的是接口发送 keepalive 报文的周期。当接口上封装的链路层协议为 PPP 时，链路层会周期性地向对端发送 keepalive 报文。如果接口在10 个 keepalive 周期内无法收到对端发来的 keepalive 报文，链路层会认为对端故障，上报链路层 Down。 用户可以通过**timer-hold命令修改 keepalive 报文轮询的时间间隔。如果将轮询时间间隔配置为 0 秒，则不发送 keepalive 报文。** 在速率非常低的链路上，轮询时间间隔不能配置过小。因为在低速链路上，大报文可能会需要很长的时间才能传送完毕，这样就会延迟 keepalive 报文的发送与接收。而接口如果在 10 个 keepalive 周期之后仍然无法收到对端的 keepalive 报文，它就会认为链路发生故障。如果 keepalive 报文被延迟的时间超过接口的这个限制，链路就会被认为发生故障而被关闭。 缺省情况下，轮询时间间隔为 10 秒。 timer-hold [period] MP 配置MP 的配置主要有两种方式，一种是通过虚拟模板（Virtual Template，VT）接口，一种是通过 MP-group 接口。 通过虚拟模板接口配置 MP VT 是用于配置一个 VA（Virtual Access，虚拟访问）接口的模板。将多个 PPP 链路捆绑成 MP 链路之后，需要创建一个 VA 接口与对端交换数据。此时，系统将选择一个 VT，以便动态地创建一个 VA 接口。 虚拟模板接口配置方式可以与认证相结合，可以根据对端的用户名找到指定的虚拟模板接口，从而利用模板上的配置，创建相应的捆绑（Bundle），以对应一条 MP 链路。 由一个虚拟模板接口可以派生出若干个捆绑，每个捆绑对应一条 MP 链路。从网络层看来，这若干条 MP 链路会形成一个点对多点的网络拓扑。系统可以根据接口接收到的认证用户名或终端标识符来进行 MP 捆绑，并以此来区分虚模板接口下的多个捆绑（对应多条 MP 链路）。 系统支持 3 种绑定方式： authentication：根据 PPP 的认证用户名进行 MP 捆绑，每个认证用户名对应一个捆绑。认证用户名是指 PPP 链路进行 PAP、CHAP、MS-CHAP 或 MS-CHAP-V2 认证时所接收到的对端用户名。 descriptor：根据 PPP 的终端描述符进行 MP 捆绑，每个终端描述符对应一个捆绑。终端标识符是用来唯一标识一台设备的标志，是指进行 LCP 协商时所接收到的对端终端标识符。 both：同时根据 PPP 的认证用户名和终端描述符进行 MP 捆绑。 通过 MP-group 接口配置 MP MP-group 接口是 MP 的专用接口，不支持其它应用，也不能利用对端的用户名来指定捆绑，同时也不能派生多个捆绑。与虚拟模板接口配置方式相比，MP-group 接口配置方式更加快速高效、配置简单、容易理解。 虚拟模板接口配置 MP通过虚拟模板接口配置 MP 时，又可以细分为两种配置方式： 将物理接口与虚拟模板接口直接关联：使用命令**ppp mp virtual-template**直接将链路绑定到指定的虚拟模板接口上，这时可以配置认证也可以不配置认证。如果不配置认证，系统将通过对端的终端描述符捆绑出 MP 链路；如果配置了认证，系统将通过用户名和/或对端的终端描述符捆绑出 MP 链路。 将用户名与虚拟模板接口关联：根据认证通过后的用户名查找相关联的虚拟模板接口，然后根据用户名和对端终端描述符捆绑出 MP 链路。这种方式需在要绑定的接口下配置**ppp mp**及双向认证（PAP、CHAP、MS-CHAP 或 MS-CHAP-V2），否则链路协商不通。 配置时需要注意： ppp mp和ppp mp virtual-template命令互斥，即同一个接口只能采用一种配置方式。 对于需要绑在一起的接口，必须采用同样的配置方式。 实际使用中也可以配置单向认证，即一端直接将物理接口绑定到虚拟模板接口，另一端则通过用户名查找虚拟模板接口。 不推荐使用同一个虚拟模板接口配置多种业务（如 MP、L2TP、PPPoE 等）。 实验环境：两台路由器，连着两根串口线 将链路直接绑定到 VT 上RT1:[RT1]interface Virtual-Template 1 # 进入虚模板，号码可选1-8[RT1-Virtual-Template1]ip address 192.168.1.1 24[RT1-Serial1/0]link-protocol ppp[RT1-Serial1/0]ppp mp Virtual-Template 1[RT1-Serial2/0]link-protocol ppp[RT1-Serial2/0]ppp mp Virtual-Template 1在端口配置完后就会自动重启端口同理在RT2上:[RT2]interface Virtual-Template 1[RT2-Virtual-Template1]ip address 192.168.1.2 24[RT2-Serial1/0]link-protocol ppp[RT2-Serial1/0]ppp mp Virtual-Template 1[RT2-Serial2/0]link-protocol ppp[RT2-Serial2/0]ppp mp Virtual-Template 1 可以查看 ppp mp 的状态 [RT1]dis ppp mp----------------------Slot0----------------------Template: Virtual-Template1max-bind: 16, fragment: enabled, min-fragment: 128 Master link: Virtual-Access0, Active members: 2, Bundle RT2 Peer&#x27;s endPoint descriptor: RT2 Sequence format: long (rcv)/long (sent) Bundle Up Time: 2019/03/24 04:55:11:467 0 lost fragments, 0 reordered, 0 unassigned, 0 interleaved Sequence: 0 (rcv)/0 (sent) Active member channels: 2 members Serial1/0 Up-Time:2019/03/24 04:55:11:467 Serial2/0 Up-Time:2019/03/24 04:55:21:892 查看 VA 状态 [RT1]dis interface Virtual-AccessVirtual-Access0Current state: UPLine protocol state: UPDescription: Virtual-Access0 InterfaceBandwidth: 128kbpsMaximum Transmit Unit: 1500Hold timer: 10 seconds, retry times: 5Internet Address is 192.168.1.1/24 PrimaryLink layer protocol: PPPLCP: opened, MP: opened, IPCP: openedPhysical: MP, baudrate: 128000 bpsMain interface: Virtual-Template1...... 按用户名查找 VTRT1:# 创建用户供RT2认证，需要为每个线路创一个[RT1]local-user rt2-user1 class networkNew local user added.[RT1-luser-network-rt2-user1]password simple rt2-user1[RT1-luser-network-rt2-user1]service-type ppp[RT1]local-user rt2-user2 class networkNew local user added.[RT1-luser-network-rt2-user2]password simple rt2-user2[RT1-luser-network-rt2-user2]service-type ppp# 用户绑定虚模板[RT1]ppp mp user rt2-user1 bind Virtual-Template 1[RT1]ppp mp user rt2-user2 bind Virtual-Template 1# 虚模板配置[RT1]interface Virtual-Template 1[RT1-Virtual-Template1]ip address 192.168.1.1 24[RT1-Virtual-Template1]ppp mp binding-mode authentication# 串口配置，填写对端提供给本端的用户# s1/0指定rt1-user1，s2/0指定rt1-user2[RT1-Serial1/0]link-protocol ppp[RT1-Serial1/0]ppp authentication-mode pap[RT1-Serial1/0]ppp pap local-user rt1-user1 password simple rt1-user1[RT1-Serial1/0]ppp mp[RT1-Serial2/0]link-protocol ppp[RT1-Serial2/0]ppp authentication-mode pap[RT1-Serial2/0]ppp pap local-user rt1-user2 password simple rt1-user2[RT1-Serial2/0]ppp mp同理RT2配置:[RT2]local-user rt1-user1 class networkNew local user added.[RT2-luser-network-rt1-user1]password simple rt1-user1[RT2-luser-network-rt1-user1]service-type ppp[RT2]local-user rt1-user2 class networkNew local user added.[RT2-luser-network-rt1-user2]password simple rt1-user2[RT2-luser-network-rt1-user2]service-type ppp[RT2]ppp mp user rt1-user1 bind Virtual-Template 1[RT2]ppp mp user rt1-user2 bind Virtual-Template 1[RT2]int Virtual-Template 1[RT2-Virtual-Template1]ip address 192.168.1.2 24[RT2-Virtual-Template1]ppp mp binding-mode authentication[RT2-Serial1/0]ppp authentication-mode pap[RT2-Serial1/0]ppp pap local-user rt2-user1 password simple rt2-user1[RT2-Serial1/0]ppp mp[RT2-Serial2/0]ppp authentication-mode pap[RT2-Serial2/0]ppp pap local-user rt2-user2 password simple rt2-user2[RT2-Serial2/0]ppp mp 查看 ppp mp 信息 [RT1]dis ppp mp----------------------Slot0----------------------Template: Virtual-Template1max-bind: 16, fragment: enabled, min-fragment: 128 Master link: Virtual-Access0, Active members: 1, Bundle rt2-user1 # VA0，用户绑定 Peer&#x27;s endPoint descriptor: RT2 Sequence format: long (rcv)/long (sent) Bundle Up Time: 2019/03/24 05:27:20:244 0 lost fragments, 0 reordered, 0 unassigned, 0 interleaved Sequence: 0 (rcv)/0 (sent) Active member channels: 1 members Serial1/0 Up-Time:2019/03/24 05:27:20:244 Master link: Virtual-Access1, Active members: 1, Bundle rt2-user2 # VA1 Peer&#x27;s endPoint descriptor: RT2 Sequence format: long (rcv)/long (sent) Bundle Up Time: 2019/03/24 05:27:48:932 0 lost fragments, 0 reordered, 0 unassigned, 0 interleaved Sequence: 0 (rcv)/0 (sent) Active member channels: 1 members Serial2/0 Up-Time:2019/03/24 05:27:48:932 MP-group 接口配置 MPRT1:[RT1]interface MP-group 1[RT1-MP-group1]ip address 192.168.1.1 24[RT1-Serial1/0]ppp mp MP-group 1[RT1-Serial2/0]ppp mp MP-group 1RT2:[RT2]interface MP-group 1[RT2-MP-group1]ip address 192.168.1.2 24[RT2-Serial1/0]ppp mp MP-group 1[RT2-Serial2/0]ppp mp MP-group 1 查看 PPP MP 信息 [RT1]dis ppp mp----------------------Slot0----------------------Template: MP-group1max-bind: 16, fragment: enabled, min-fragment: 128 Master link: MP-group1, Active members: 2, Bundle Multilink Peer&#x27;s endPoint descriptor: MP-group1 Sequence format: long (rcv)/long (sent) Bundle Up Time: 2019/03/24 05:41:40:229 0 lost fragments, 0 reordered, 0 unassigned, 0 interleaved Sequence: 0 (rcv)/0 (sent) Active member channels: 2 members Serial1/0 Up-Time:2019/03/24 05:41:40:229 Serial2/0 Up-Time:2019/03/24 05:41:49:213","categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"华三","slug":"华三","permalink":"https://coconutmilktaro.top/tags/%E5%8D%8E%E4%B8%89/"},{"name":"PPP","slug":"PPP","permalink":"https://coconutmilktaro.top/tags/PPP/"}]},{"title":"IPv6基础笔记","slug":"IPv6基础笔记","date":"2019-03-21T11:30:08.000Z","updated":"2022-05-30T02:51:53.803Z","comments":true,"path":"2019/IPv6基础笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/IPv6%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"IPv6","slug":"IPv6","permalink":"https://coconutmilktaro.top/tags/IPv6/"},{"name":"华三","slug":"华三","permalink":"https://coconutmilktaro.top/tags/%E5%8D%8E%E4%B8%89/"}]},{"title":"QoS完全笔记","slug":"QoS完全笔记","date":"2019-03-21T11:20:08.000Z","updated":"2022-05-30T02:51:53.866Z","comments":true,"path":"2019/QoS完全笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/QoS%E5%AE%8C%E5%85%A8%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"华三","slug":"华三","permalink":"https://coconutmilktaro.top/tags/%E5%8D%8E%E4%B8%89/"},{"name":"QoS","slug":"QoS","permalink":"https://coconutmilktaro.top/tags/QoS/"}]},{"title":"园区高可靠技术笔记","slug":"高可靠性技术笔记","date":"2019-03-21T11:10:08.000Z","updated":"2022-05-30T02:51:54.010Z","comments":true,"path":"2019/高可靠性技术笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/%E9%AB%98%E5%8F%AF%E9%9D%A0%E6%80%A7%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/","excerpt":"基于华三网络","text":"基于华三网络 概述可靠性： MTBF（Mean Time Between Failure，平均无故障时间），衡量稳定程度 MTTR（Mean Time To Repair，平均故障修复时间），衡量故障响应修复速度 可靠性=MTBF/(MTBF+MTTR) 可靠性 99.99%，说明年故障时间不超过 8 小时 可靠性 99.999%，说明年故障时间不超过 5 分钟 可靠性 99.9999%，说明年故障时间不超过 30 秒 链路聚合链路聚合是将多条物理链路聚合在一起，形成一条逻辑链路，不经提供了链路冗余，也提高了链路带宽。 链路聚合在 IEEE 802.3 中的位置：位于 MAC Client 和 MAC 之间，是可选的子层，称为 Link Aggregation Sublayer 链路聚合使用了 LACP（Link Aggregation Control Protocol）交换信息。 链路聚合模式： 静态聚合 端口不与对端设备交互信息 选择参考端口根据本端设备信息 用户命令创建和删除静态聚合组 动态聚合 端口的 LACP 协议自动使能，与对端设备交互 LACP 报文 选择参考端口根据本段设备与对端设备交互信息 用户命令创建和删除动态聚合组 Smart Link 与 Monitor LinkVRRPVRRP（Virtual Router Redundancy Protocol，虚拟路由器冗余协议）可以将多台路由器加入备份组，形成一台虚拟路由器，承担网关功能。只要备份组中仍有一台路由器正常工作，则虚拟路由器正常工作，避免由于网关单点故障而导致的网络中断。版本为 v2 VRRP 是一种容错协议，在提高可靠性的同时，简化了主机的配置。 VRRP 使用组播地址224.0.0.18发送报文。 不同的备份组有不同的编号，范围 1-255，称为虚拟路由器号（VRID），有相同 VRID 的一组路由器组成一个 VRRP 备份组。 VRRP 负载分担：将多台路由器同时承担业务，形成多台虚拟路由器，分担内网和外网之间的流量 虚拟路由器由 LAN 上的唯一 Virtual Router ID 标识，具有虚 MAC 地址：00-00-5E-00-01-&#123;vrid&#125; 路由器角色：Master 路由器承担网关功能，由所有路由器根据优先级选出，其他路由器为 Backup 路由器。 Master 会周期发送 VRRP 通告报文，告知备份路由器自身的状况 IP 地址拥有者：接口 IP 与虚拟 IP 一致的路由器 优先级：范围 0-255，默认 100，可配置范围 1-254，0 用作特殊用途，255 保留给 IP 地址拥有者。 若优先级相同，比较时 IP 地址大的优先 两种模式： 抢占模式：若备份组中路由器发现自身优先级比当前 master 高，则会对外发送 VRRP 通告报文，导致备份组内重新选举 master，原来的 master 则变为 backup 非抢占模式：只要 master 没有出现故障，即使备份路由器被配置了更高的优先级也不会成为 master 认证方式：无认证、简单字符认证、MD5 认证 VRRP 监视接口：当 Master 上行链路接口变为 DOWN 或 REMOVED，则 Master 主动降低自身优先级，使备份组其他路由器成为 Master。 VRRP 状态机 Initialize： 路由器启动后进入该状态，收到接口的 Startup 消息后，路由器转入 Master 或 Backup。 该状态不会对 VRRP 报文做任何处理。 当 Master 或 Backup 路由器收到端口的 shutdown 事件时，会转入 Initialize 状态。 Master： 定期发送 VRRP 广播报文。 响应对虚拟 IP 的 ARP 请求，且响应的是虚拟 MAC 地址，不是接口的真实 MAC 地址。 转发目的 MAC 为虚拟 MAC 的 IP 报文，否则丢弃。 Backup： 接收 Master 发送的 VRRP 广播 对虚拟 IP 的 ARP 请求不响应 丢弃目的 MAC 为虚拟 MAC 的 IP 报文 丢弃目的 IP 为虚拟 IP 的 IP 报文 RRPPIRF 堆叠","categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"高可用","slug":"高可用","permalink":"https://coconutmilktaro.top/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"},{"name":"华三","slug":"华三","permalink":"https://coconutmilktaro.top/tags/%E5%8D%8E%E4%B8%89/"}]},{"title":"TCP/IP基础笔记","slug":"TCPIP基础笔记","date":"2019-03-21T11:00:08.000Z","updated":"2022-06-21T15:34:09.620Z","comments":true,"path":"2019/TCPIP基础笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/TCPIP%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/","excerpt":"本篇包含计算机网络和 TCP/IP 结构的重要知识点，以及华三设备的一些操作。 网络概述 TCP 与 UDP 概述 TCP UDP IP 基本原理 DHCP 概述 FTP 概述 TFTP 简单文件传输协议 DNS 概述 Telnet 概述 华三设备操作 SSH 概述","text":"本篇包含计算机网络和 TCP/IP 结构的重要知识点，以及华三设备的一些操作。 网络概述 TCP 与 UDP 概述 TCP UDP IP 基本原理 DHCP 概述 FTP 概述 TFTP 简单文件传输协议 DNS 概述 Telnet 概述 华三设备操作 SSH 概述 网络概述两种数据交换方式： 电路交换：基于电话网的电路交换 主要通过交换机连接，两台计算机独占线路 分组交换：以分组为单位存储转发 原理：将大数据分隔为一个一个包传输，每个包都打上报文头（标有分组序号、源目 IP 等），接收端收到后按照分组序号进行拼接 衡量计算机网络的指标： 带宽（一定时间内两个节点间数据量，单位 bps） 延迟（数据在两个节点间传输的时间） OSI 七层模型： 物理层：比特流传输，比特流与电子信号之间的转换，规定连接器与网线的规格 数据链路层：协商比特流一致性，数据帧与比特流之间的转换，流量控制，差错校验，编帧与识别帧 网络层：寻址，地址管理，路由选择，拥塞控制，异种网络互连 传输层：端到端逻辑连接的建立维护与断开、差错重传、数据排序、多路复用、流量控制、对应用层数据进行分段与封装 会话层：通信管理。主机间通信、建立维护及终止程序间会话、决定连接方式（SQL、NFS、RPC 等） 表示层：定义数据格式与结构、协议上层数据格式、将设备固有的数据格式转化为网络标准传输格式（编码格式、图片格式等） 应用层：为程序进程提供网络服务，针对特定应用 可路由协议：定义数据包内各个字段格式与用途，对数据进行网络层封装（即 IP 协议） 路由协议：在路由器间传递信息，计算并形成路由表，为可路由协议选择路径（RIP、OSPF、BGP） 面向连接的服务（可靠连接）： 通信前建立连接，完成后断开连接 有序传递 应答确认 差错重传 无连接服务（不可靠连接）： 尽力而为 无需建立连接 无序列号机制 无确认机制 无重传机制 TCP/IP 模型： 网络接口层：物理线路与链路层通信 网络层：数据包路由转发，路由表的维护 （IP、ICMP、IGMP） 传输层：端到端通信，数据完整性校验，差错重传，数据排序 （TCP、UDP） 应用层：处理特定应用细节 （Telnet、FTP、SMTP、HTTP 等） 局域网 CSMA/CD 载波侦听机制：用于防止总线型局域网冲突。 实现：先听后发，边听边发，冲突停发，退避再发 广域网三种连接方式： 专线：点到点永久独占线路，带宽固定，链路层使用 SDLC、HDLC、PPP 等协议 电路交换：按需拨号建立连接，独占线路，带宽固定，链路层用 PPP 分组交换：通过虚电路连接到多个对端，如帧中继、ATM 等 ARP（Address Resolution Protocol）地址解析协议，根据 IP 地址解析出 MAC 地址。 ARP 原理： 请求：广播发送 ARP 请求报文，请求获取目标 IP 的 MAC 地址 应答：单播回应，将自身 MAC 发送给请求方 RARP：反向地址转换协议，根据 MAC 地址解析出 IP 地址 代理 ARP：路由器充当 ARP 中间代理去广播 ARP 请求 ICMP 协议：Internet 控制报文协议，用于检测网络是否通畅、主机是否可达等 TCP 与 UDP 概述TCPTransmission Control Protocol 传输控制协议，协议号 6 特点： 面向连接，可靠，基于字节流 确认机制：应答接收 端口号：多路复用 序列号：丢失检测、乱序重排 完整性校验：差错校验 窗口机制：流量控制 三次握手：可靠连接 TCP 报文头： Source Port(16 bit)：源端口 Destination Port(16 bit) ：目的端口 Sequence Number(32 bit)：数据包中第一个字节的序列号，取值范围[0,2^32-1]，是 mod2^32 运算的值 Acknowledge Number(32 bit) ：确认序列号，期望收到对方下一报文的第一个数据字段的序号 Data Offset(4 bit) ：数据偏移，值为 TCP 头的长度除以 4 标志位(6 bit) #主要字段： URG：紧急，需要尽快传送 ACK：确认，建立连接后的报文确认 PSH：推送，接收方要尽快将此报文给上层处理 RST：复位，重新连接 SYN：同步，发起连接 FIN：终止，释放连接 Windows Size(16 bit)：接受缓冲区空闲空间，告诉对端自己能接收的最大数据长度 Checksum(16 bit)：校验和 常见服务：Telnet(23)、FTP(20\\21)、SSH(22)、HTTP(80)、SMTP(25) 三次握手：连接建立 A 向 B 发送 SYN（seq=x），进入 SYN_SEND 状态 B 收到 SYN 报文，回应 SYN（seq=y）、ACK（ack=x+1）报文，进入 SYN_RECV 状态 A 收到 B 的 SYN 报文，回应 ACK（ack=y+1）进入 Established 状态，TCP 连接建立 四次挥手：连接终止 A 向 B 发送 FIN，表示数据传输完毕 B 收到后执行被动关闭，确认回复 ACK B 关闭套接字，向 A 发送 FIN A 接收到后向 B 确认，发送 ACK 滑动窗口：限制每次发送的包数 1.A 向 B 以默认发包数发送数据包 2.B 回复 ACK 报文，其中除了 ack 外还带有 win 限制每次发送的数据长度 3.此后 A 便会按照要求长度发送数据包 UDPUser Datagram Protocol 用户数据报协议，协议号 17 特点：无连接，不可靠传输，无流量控制，直接封装应用层数据（不分段） 报文头： Source Port(16 bit) #源端口 Destination Port(16 bit) #目的端口 Length(16 bit) #数据包长度 Checksum(16 bit) #校验和 常见服务：DNS(53)、TFTP(69)、SNMP(161)、NFS、DHCP(67\\68\\546) IP 基本原理IP 作用： 标识结点与链路 用唯一 IP 地址标识每个节点 用唯一 IP 网络号标识每个链路 寻址与转发 确定节点所在网络位置进而确定节点位置 IP 路由器选择适当路径将 IP 包转发到目的节点 适应各种数据链路 根据链路的 MTU 对 IP 包进行分片与重组（MTU：最大传输单元，能通过的最大数据包大小（以字节为单位）） 为了通过实际的数据链路传递信息，须建立 IP 地址到数据链路层地址的映射 IP 头（20 字节） Version(4 bit)：当前 IP 版本 IHL(4 bit) ：IP 报文头长度，等同于数据字段的偏移量，最小为 5（即 5*32），最大为 15 Type-of-Service(8 bit) ：上层协议对处理当前数据报所期望的服务质量，并对数据报按照重要性级别进行分配，分配优先级、延迟、吞吐量以及可靠性。 Total Length(16 bit) ：整个 IP 数据包的字节长度（数据+IP 头），最大 65535 字节 Identification(16 bit)：用于识别当前数据包 Flags(3 bit)：低位控制分片，中位指出数据包是否可分片，高位保留 Fragment Offset(13 bit)：指出与源数据包的起始端相关的分片数据位置 Time-to-Live(8 bit) ：生存时间，每经过一节点就减少 1 直到为 0 Protocol(8 bit) ：指出接收数据包的上层协议 Header Checksum(16 bit) ：头部校验和，保证 IP 头完整 Source Address(32 bit)：源 IP Destination Address(32 bit) ：目 IP VLSM：可变长子网掩码。使同一 IP 地址能划分为多个子网，能按照子网要求定制，每个子网都可自定义大小。 CIDR：无类域间路由。基于 VLSM，CIDR 使用网络前缀，可有各种长度，由掩码标识。可进行网段聚合。 DHCP 概述UDP 协议，采用 C/S 模式，服务器端口 67，客户端端口 68 三种分配方式： 手动分配：静态绑定固定 IP，这些 IP 固定给特定设备使用（打印机，DNS，web 服务器等） 自动分配：服务器给客户端分配租期无限长的 IP 地址，只有客户释放，其他客户才能使用该地址 动态分配：服务器给客户端分配租期有限长的 IP 地址，一旦租期到期而未续约，地址就会释放。 基本原则：尽可能为客户端分配原来使用的地址。 分配顺序： 静态分配的 客户端曾经会用过的 最先找到的可用 IP DHCP 报文： Discover：客户端第一次向服务器发送的请求报文，广播发送 Offer：服务器对客户端 Discover 的回应，包含分配的 IP、掩码、网关等信息，广播或单播发送 Request：客户端发送给服务器的请求报文，包括服务器的选择与租期更新等，单播或广播发送（根据客户端状态） Release：客户端若想释放当前地址，则单播发送给服务器 Ack/Nak：服务器对客户端的回应，请求报文正确时回复 Ack，否则回复 Nak Decline：客户端收到服务器的 Ack 后，对获取的 IP 进行确认，使用 ARP，若发现该 IP 已被使用，则广播向服务器发送 Decline 报文，拒绝使用该 IP。 Inform：当客户端通过其他方式已获取了 IP，若还需要向服务器索取其他配置信息时，会向服务器发送 Inform，若服务器能根据要求分配则会回复 Ack，否则不操作。 DHCP 续约： 更新状态：使用时间达到租约的 50%，客户端进入更新状态，单播向服务器发送 Request，服务器若同意续约则回复 Ack，否则回复 Nak 重新绑定状态：使用时间达到租约的 87.5%，客户端进入重新绑定状态。客户端广播 Request 请求，请求对有效租期进行更新。 进入该状态的原因：客户端未收到服务器对续约 Request 的回应。 若 Request 未收到回应，客户端会在一定时间内重发 Request 报文，若直到租期结束也未更新租期，则被迫释放 IP 地址。 DHCP 中继：DHCP 只适用于客户端与服务器在同网段（原因：广播请求）。可以通过中继使客户端可向其他网段的 DHCP 服务器请求。 ​ 实现：中继路由器收到请求广播报文，便向服务器单播发送，同理服务器也单播回应中继，中继再广播回应客户端。 FTP 概述TCP 协议，采用 C/S 模式，控制连接端口 21，数据连接端口 20 控制连接：负责 FTP 客户端与服务器交互命令与信息的传输，在整个会话过程中始终打开。 数据连接：负责客户端与服务器数据的传输，传输完毕就会关闭 文件传输模式： ASCII：默认模式，发送方将文件转为 ASCII 码传输，适合文本文件传输 二进制：也称图像文件传输模式，按比特流传输，适合程序文件传输 数据传输方式： 主动 PORT 过程： 首先客户端（随机端口）与服务器（21 端口）TCP 三次握手建立连接，建立控制连接通道 客户端向服务器发送 PORT 命令，告知服务器使用主动模式。 其中 PORT 命令携带参数（客户端 IP 地址, P1, P2），P1 与 P2 用于标识客户端数据连接的临时端口号，具体为 256*P1+P2，IP 地址也是四段，每段用逗号分隔 服务器收到 PORT 命令后按照参数用 20 端口与客户端指定端口三次握手建立数据传输通道。 数据传输完毕，发送方发送 FIN 报文，关闭数据连接 问题：若客户端在防火墙内部网络，主动方式会出现问题，因为客户端提供的端口是随机的，防火墙若未放行该端口，则无法建立 FTP 连接。 此时需要使用被动方式建立连接 被动 PASV 过程： 首先客户端（随机端口）与服务器（21 端口）TCP 三次握手建立连接，建立控制连接通道 客户端向服务器发送 PASV 命令，参数与 PORT 一致。但 IP 是服务器的，标识的是服务器端的临时端口号。 客户端用随机端口与服务器的指定临时端口 TCP 三次握手建立数据连接通道。 数据传输完毕，发送方发送 FIN 报文，关闭数据连接 TFTP 简单文件传输协议UDP 协议，端口 69 特点： 仅提供简单文件传输功能（上传，下载） 无存取授权与认证机制，无目录功能 由客户端发起 下载过程： 客户端向服务器发送读请求 服务器根据请求回应数据报文（块编号从 1 开始） 客户端收到数据后回应确认报文。 重复 2.3 步直至完成下载 上传过程： 客户端向服务器发送写请求 服务器回应确认报文（块编号为 0） 客户端发送数据报文（块编号从 1 开始） 服务器收到后回应确认报文。 重复 3.4 步直至上传完成 文件传输时，将文件分成多个文件块，封装到数据报文中并打上文件块编号 传输文件模式： netASCII：对应 FTP 的 ASCII 模式 octet：对应 FTP 二进制模式 协议报文： RRQ 读请求 WRQ 写请求 数据报文 确认正确/错误报文 报文的头两个字节是操作码字段，1 为读请求，2 为写请求，3 为数据报文，4 为确认正确，5 为错误。 文件传输过程中读写出错就发送差错报文，数据传输就停止，差错报文不会被确认也不会重传 TFTP 每次传输的数据报文中文件块大小固定为 512 字节，若文件大小刚好是 512 字节的整数倍，则传完文件后还要再发一个空文件块的数据报文表明文件传输完成。 DNS 概述TCP 或 UDP（基本是 UDP）协议，端口号 53，采用 C/S 模式，解析域名与 IP 映射。 查询方式： 递归：服务器收到请求时，若不能解析，则把请求转发到下一台服务器直到有一台解析成功 迭代：服务器收到请求时，若不能解析，则按根域 -&gt; 一级域名 -&gt; 二级域名 -&gt; 三级域名依次询问，直到解析成功 反向查询：根据 IP 解析域名，使用特殊域 in-addr.arpa 域，该域的子域是按照点分十进制表示法编号的 IP 地址相反顺序构造，即 IP 地址的四段倒置形成该域。 域名服务器类型： 本地域名服务器：自行管理的域名服务器，与客户端很近 根域名服务器：管理顶级域，本身不对域名解析，但知道相关域名服务器的地址 授权域名服务器：每个主机都必须在某个授权域名服务器上注册，通常该服务器就是本地域名服务器，最好有两个以防单点故障 主域名服务器：完成一个或多个域的域名解析的主用域名服务器 辅助域名服务器：协助主域名服务器，分担主服务器的压力，且能作为冗余服务器，本身不建立区域地址信息文件，而是获取主服务器上最新副本（两种获取方式：1.辅服务器启动或配置刷新时间到期后主动向主服务器获取 2.主服务器启动通知功能，区域数据变化后，将变化通知给辅服务器，辅服务器更新副本） DNS 既可以 TCP 查询也可以 UDP 查询的原因： DNS 响应报文中有特殊位–删减标志位 TC，当响应报文采用 UDP 封装且长度大于 512 字节时，服务器仅回复前 512 字节，同时 TC 置位，表示报文进行了删减。当客户端收到 TC 置位的报文，客户端将用 TCP 重新封装请求报文并发送，此时服务器返回 TCP 封装的回应。 DNS 特性 静态域名解析 设备上手动建立域名与 IP 的映射关系 动态域名解析 设备查询 DNS 服务器，由服务器完成解析 DNS 代理 在客户端与服务器间转发 DNS 请求与应答报文。客户端将 DNS 代理当做服务器，代理再将请求转发给服务器，应答同理。 Telnet 概述基于 TCP，端口号 23，采用 C/S 模式。使用 Telnet 进行远程访问设备进行配置维护。 telnet 安全问题：无安全认证机制，数据以明文传输。 实现 Telnet 的条件： 服务器端： 内核命令行接口：操作系统内核与虚拟终端间的适配层 虚拟终端：类似实体终端的驱动程序。通过虚拟终端与内核交换信息 Telnet 服务器进程 TCP/IP 协议栈 客户端： Telnet 客户端程序 TCP/IP 协议栈 工作过程： 客户端与设备端 23 端口进行 TCP 连接 系统将客户端命令以 NVT 网络虚拟终端格式传送到服务器并执行 服务器端将 NVT 格式的命令执行结果再转化为客户端接受的格式传回客户端 客户端发送命令进行 TCP 断开连接。 华三路由器的 Telnet 默认关闭。 华三设备操作实验环境：两台交换机（需要配 IP 地址）或路由器 SW2 作为 telnet server，SW1 作为 telnet client SW2:[sw2]interface Vlan-interface 1[sw2-Vlan-interface1]ip address 192.168.1.2 24SW1:[sw1]interface Vlan-interface 1[sw1-Vlan-interface1]ip address 192.168.1.1 24 配置 telnet SW2:[sw2]telnet server enable # 开启telnet[sw2]user-interface vty 0 10 # vty用户界面。# 两个数字是确定vty号的范围第一个数字范围0-63，第二个范围1-63[sw2-line-vty0-10]authentication-mode scheme # 设置验证方式，这里选了用户名密码方式[sw2-line-vty0-10]set authentication password simple 123456 #设置验证密码[sw2-line-vty0-10]user-role level-3 # 设置用户权限，权限等级0-15[sw2]local-user zhangsan # 创建用户zhangsanNew local user added.[sw2-luser-manage-zhangsan]service-type telnet # 设置为telnet服务，不设置无法登录[sw2-luser-manage-zhangsan]password simple 123456 # 设置提权密码 从 SW1 登录 SW2 telnet 192.168.1.2 SSH 概述基于 TCP，端口号 22，安全的远程登录协议 特点： 数据机密性：支持 DES、3DES 加密算法，会对用户名与密码及数据进行加密。 支持多种认证方式：支持公钥验证方式（必支持）、密码验证方式（可选支持）、不验证方式（可选） 支持 RSA 认证：RSA 非对称加密 SSH 协议基本框架： 主要包含三个协议：1.传输层协议 2.用户认证协议 3.连接协议 连接建立过程： 版本号协商： 客户端与服务器端 22 端口 TCP 连接。连接建立后，服务器向客户端发送报文，包含版本标志字符串（SSH-&lt;主协议版本号&gt;.&lt;次协议版本号&gt;-&lt;软件版本号&gt;） 客户端收到后解析报文，若版本号比自己的低，就使用低版本号 客户端回应服务器，包含客户端决定的协议版本号 服务器端比较版本号，若协商成功，则进入密钥算法协商阶段，否则断开 TCP 连接 密钥与算法协商： 客户端与服务器端互相交换密钥算法协商报文，包含支持的公钥算法列表、加密算法列表、MAC（消息验证码）算法列表、压缩算法列表 通过对比双方都得出最终使用的算法 双方通过 DH 算法交换，生成会话密钥与会话 ID 认证： 客户端向服务器发送认证请求，包含用户名、认证方式等 服务器端对客户端进行认证，若失败发送失败信息，包含可再次认证的方法列表 失败的情况下：客户端从认证方法中选一种再次认证 直到认证成功或次数达到上限服务器关闭连接为止 两种认证方式： password 认证：客户端向服务器发送 password 认证请求并将用户名密码加密后发送，服务器收到解密并比对，返回成功或失败信息 publickey 认证：采用数字签名认证。 会话请求： 认证通过后，客户端向服务器发送会话请求，服务器若成功处理请求就回复 SUCCESS 包，否则回复 FAILURE 包 交互会话： 会话请求通过后，进行双向数据传输，客户端发送加密的命令，服务器接收解密处理命令，将结果加密返回 SFTP：安全文件传输协议。建立在 SSH 基础上，默认采用加密方式传输数据。","categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"TCP/IP","slug":"TCP-IP","permalink":"https://coconutmilktaro.top/tags/TCP-IP/"}]},{"title":"无线安全渗透笔记","slug":"无线安全渗透笔记","date":"2019-02-18T14:09:29.000Z","updated":"2022-06-28T18:51:34.866Z","comments":true,"path":"2019/无线安全渗透笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/%E6%97%A0%E7%BA%BF%E5%AE%89%E5%85%A8%E6%B8%97%E9%80%8F%E7%AC%94%E8%AE%B0/","excerpt":"无线相关知识点 WEP/WPA/WPA2 常用无线工具使用以及密码破解操作 字典工具 crunch 字典工具 cupp airmon-ng 工具 kismet 工具 airodump-ng 工具 aircrack-ng 破解 WEP 与 WPA 加密 自动化破解工具 gerix-wifi-cracker airgeddon 工具 DOS 攻击简单操作 握手包工具操作 自动化破解工具 wifite hirte 工具伪造 AP 不指定字典破解密码 hashcat 工具跑包 cowpatty 工具破解密码 hash-table 加速破解 batch-table 加速破解 pyrit 工具补充 airolib-ng 生成彩虹表 解除验证 DoS 攻击 Easy-Creds 工具创建伪 AP Evil Twin 攻击 架设无赖 AP 误关联攻击 Caffe Latte 攻击 解除对无线客户端的验证 不碰 AP 的情况下破解 WPA","text":"无线相关知识点 WEP/WPA/WPA2 常用无线工具使用以及密码破解操作 字典工具 crunch 字典工具 cupp airmon-ng 工具 kismet 工具 airodump-ng 工具 aircrack-ng 破解 WEP 与 WPA 加密 自动化破解工具 gerix-wifi-cracker airgeddon 工具 DOS 攻击简单操作 握手包工具操作 自动化破解工具 wifite hirte 工具伪造 AP 不指定字典破解密码 hashcat 工具跑包 cowpatty 工具破解密码 hash-table 加速破解 batch-table 加速破解 pyrit 工具补充 airolib-ng 生成彩虹表 解除验证 DoS 攻击 Easy-Creds 工具创建伪 AP Evil Twin 攻击 架设无赖 AP 误关联攻击 Caffe Latte 攻击 解除对无线客户端的验证 不碰 AP 的情况下破解 WPA 无线相关知识点WLAN 无线局域网技术主要采用 IEEE 802.11 标准，包含 802.11a/b/g/n/ac，是 IEEE 规定的数据链路层无线协议。以下是常见的一些无线标准。 IEEE 802.11 ，1997 年，原始标准（2Mbit/s，工作在 2.4GHz）。 IEEE 802.11a，1999 年，物理层补充（54Mbit/s，工作在 5GHz）。 IEEE 802.11b，1999 年，物理层补充（11Mbit/s 工作在 2.4GHz）。 IEEE 802.11g，2003 年，物理层补充（54Mbit/s，工作在 2.4GHz）。 IEEE 802.11n，更高传输速率的改善，支持多输入多输出技术（Multi-Input Multi-Output，MIMO）。 提供标准速度 300M，最高速度 600M 的连接速度 IEEE 802.11ac 是 802.11 家族的一项无线网上标准，由 IEEE 标准协会制定，透过 5GHz 频带提供高通量的无线局域网（WLAN），俗称 5G WiFi （5th Generation of Wi-Fi）。 IEEE 定义了两种无线网络拓扑结构，一种是基础设施网络（Infrastructure Networking），一种是特殊网络（Ad Hoc Networking）。 在基础设施网络中，无线终端通过接入点 AP 接入访问骨干网。接入点负责在 802.11 和 802.3 MAC 间转换。 一个接入点覆盖的区域称为基本服务区（Basic Service Area，BSA），接入点控制的所有终端组成一个基本服务集（Basic Service Set，BSS） 多个基本服务集相互连接就形成了分布式系统（Distributed System，DS），DS 支持的所有服务称为扩展服务集（Extended Service Set，ESS），由两个以上 BSS 组成。 AD Hoc 是一种点对点网络，不需要有线网络和接入点的支持，终端设备间通过无线网卡可直接通信。 无线接入点 AP（access point）： 扩展型 AP，也称胖 AP：类似家用无线路由器，能三层路由 单纯型 AP，也称瘦 AP：类似集线器、交换机，仅转发电信号与无线信号，通过将网络信号通过双绞线传送，再经过无线 AP 编译，将电信号转变为无线信号，形成 wifi 覆盖。一般无线 AP 最大覆盖范围 400m AP 的工作模式： 纯 AP 模式（无线漫游模式）：最常用、最基本的工作模式，用于构建以 AP 为中心的集中控制式网络，所有通信由 AP 转发 网桥模式（无线客户端模式）：分布一台主 AP 和多台从 AP，从 AP 作为主 AP 的客户端（类似无线网卡的地位），形成主从关系。主 AP 工作在纯 AP 模式，从 AP 工作在客户端模式，此时从 AP 管理的网络对于主 AP 就相当于一个客户端，并且从 AP 只能接入有线网络，不能为其他无线客户端提供服务 点对点模式：两台 AP 无线设备，通过这两台 AP 连接两个有线局域网，实现两个局域网间的无线互联和资源共享，或实现有线网络的扩展。点对点连接的距离较远，最好采用定向天线。此时两个 AP 不会向其他客户端发送无线信号，仅仅向着对端 AP 互发信号。 点对多点模式：多个 AP 组成网络，其中一个 AP 设为点对多点桥接模式，其余 AP 设为点对点桥接模式。用于一定区域内的多个远端点对一个中心点的访问，将多个网络连成一体 中继模式：连接几个无线 AP，实现信号的中继和放大，延伸无线网络覆盖范围。中继 AP 设备有两个模块，一个模块采用客户端模式，接收前一站 AP 的信号，另一个模块采用标准 AP 覆盖模式，供无线设备通信 Wi-Fi 是 WECA（无线以太网兼容性联盟）为普及 IEEE 802.11 的标准而打造的一个品牌。 Wi-Fi 工作原理：wifi 至少需要设置一个 AP 和一个以上的客户端，AP 每 100ms 会将 SSID 通过 beacons（信号台）封包广播一次。beacons 包的传输速率是 1Mb/s，长度短。wifi 规定的最小传输速率为 1Mb/s，所以所有客户端都能收到该 SSID 广播包。 SSID（Service Set Identifier）服务集标识，就是显示的无线网名称，用于区分网络，最多 32 字符。主要包含两种：ESSID 和 BSSID。 信道（channel），也称频段。通常有 13 个信道。信道频率范围从 2413MHz 到 2472MHz，从小到大，每个信道都有一定的频率范围，范围也会有重叠。其中 1,6,11 三个信道之间是完全没有重叠的，就是不会相互干扰，同理 2,7,12 和 3,8,13 和 4,9,14 也是互不重叠的信道组。 频段带宽：发送无线信号频率的标准，频率越高越容易失真。在 11n 模式中，包含 20MHZ 和 40MHZ 两个频带，20MHZ 能达到 144Mbps 带宽，距离 100M，40MHZ 能达到 300Mbps，而穿透性差，仅有 50M。 WDS：Wireless Distribution System 无线分布式系统，是多个 AP 通过无线互联的系统，将无线网通过多个 AP 扩展。 无线网卡的几种模式： 广播模式 多播模式 直接模式：只收目的 MAC 是本身的帧 混杂模式：接收所有流过网卡的帧 网卡默认工作模式为广播和直接。 WEP/WPA/WPA2WLAN 委员会制定了三种数据加密协议： WEP（有线等效保密） WPA（WiFi 保护访问） WPA2（WiFi 保护访问 v2） WEP 的关键缺陷是它使用了 RC4 和短 IV 值（每 224 个帧就会循环使用一次），虽然 IV 值是很大的数字，但每 5000 个数据包重用 4 个 IV 值的概率为 50%。因此只要能生成密集的流量，便能显著增加重用 IV 值的概率。 WPA 主要采用临时密钥完整性协议（TKIP）加密算法，旨在改进 WEP。而 WPA2 强制使用 AES-CCMP 算法加密，比 TKIP 更强大。 WPA 和 WPA2 支持两种身份验证机制： 基于 EAP 的身份验证（采用 RADIUS，用于企业） 基于预共享密钥（Pre-shared Key，用于个人） WPA 和 WPA2 主要通过抓取握手包，再用字典进行穷举破解。 WPA/WPA2 PSK 验证的 4 次握手过程： 申请者（WiFi 客户端）与验证者（AP）验证并关联 RR，并协商产生 256 位的预共享密钥 验证者向申请者发送消息（包括预共享密钥、网络的 SSID、验证者随机数 ANonce、申请者随机数 SNonce、验证者 MAC 地址（AP 的 MAC）、申请者 MAC 地址（WiFi 客户端的 MAC）） 申请者计算出成对临时密钥（PTK）作为每会话密钥（Per-session Key），用于加密 AP 和客户端间的数据。 申请者再向验证方发送申请者随机数以及一个 MIC（消息完整性检查）。验证方收到后，进行 MIC 验证，若验证无误，则用申请者随机数生成一个 PTK 验证方再向申请方发送确认信息，确认进行密钥安装。申请方安装后返回密钥安装确认信息。 常用无线工具使用以及密码破解操作实验使用了 HAYSENSE 厂的 HS-8515NS 无线网卡，驱动为 RT3070。 在虚拟机导入该网卡 实验虚拟机为 kali linux。 字典工具 crunch用法：crunch 密码最短长度 密码最长长度 字符集 [选项]选项： -f 指定字典配置文件，后面需要跟上该文件中的指定字符集 crunch自带的字符集配置文件为/usr/share/crunch/charset.lst -o 指定输出的字典文件 生成字典大小随密码位数指数增长，并且与指定的字符集长度有关。 常见用法：crunch 6 10 123456abcdefg 生成包含前面字符集的所有密码可能的字典crunch 6 10 -f /usr/share/crunch/charset.lst lalpha -o ./wordlst-lalpha 字典工具 cupp会根据输入的信息，生成可能的密码字典。需要安装 cupp cupp [options]选项： -i 进行交互式的信息输入 -w 改善已存在的字典 airmon-ng 工具airmon-ng 是 aircrack-ng 套件中的一个工具，用于开启无线网卡的监听模式。 airmon-ng &lt;start|stop&gt; &lt;interface&gt; [channel]airmon-ng &lt;check&gt; [kill] 可使用iwconfig命令查看无线网卡的列表以及信息 kismet 工具kismet 是一个无线网卡监控工具。 打开 kismet 后会有一个是否选择灰色界面，最好选 NO，否则界面是黑白的，没有彩色的图像 然后会有要求填写一个监控源。 可以看到监控到的信息，就是能看到的无线信号 点右下角的 close console window 即可进入图像界面 上面是网络信息，下面是客户端信息，其中 MAC 就是客户端的 MAC 地址 双击网络可查看该网络的详细信息 其中主要有以下参数信息： BSSID：该网络 AP 的 MAC 地址 Manuf：制造厂商 Type：类型，此处说明是 AP Channel：信道为 11 SSID：网络名称 Type：Beacon。被动信标帧 Encryption：WPA PSK 加密算法 airodump-ng 工具属于 aircrack-ng 套件，用于无线网络抓包与分析，将无线网络数据传送到 PCAP 或 IVS 文件并显示网络信息。 将无线网卡置于监听模式后开始抓包。airodump-ng wlan0mon 主要有以下参数： BSSID：AP 的 MAC PWR：信号强度取决于驱动，值越高，与该 AP 的距离越近。 若 BSSID 的 PWR 为-1，则说明网卡驱动不支持报告信号水平。 若客户端的 PWR 为-1，则说明该客户端不在能探测到的范围内，但能捕获 AP 发往客户端的数据。 Beacons：AP 发的通告 #Data：抓到的数据包量 CH：channel（是从 Beacons 中获取） MB：最大传输速率。 若为 11，则协议为 802.11b。 若为 22，则协议为 802.11b+。 若更高，则为 802.11g。 e 表示有 802.11e（QoS）启用， . 表示短前导码。前导码是数据包的一组比特组，让接收者同步并准备接收实际的数据 ENC：加密方式 OPN（无加密） WEP?（WEP 或 WPA\\WPA2） WEP（静态或动态 WEP） TKIP 或 CCMP（WPA\\WPA2） CIPHER：加密算法。WPAAP、TKIP、WEP、CCMP、WEP104。TKIP 与 WPA 结合使用，CCMP 与 WPA2 结合使用 AUTH：认证。 MGT（WPA/WPA2 使用独立认证服务器（802.1x、redius、eap 等）） PSK（WPA/WPA2 的 pre-shared key） OPN（无认证） SKA（WEP 的共享密钥） ESSID：wifi 名 STATION：每一个已连接或者正尝试连接用户的 MAC 地址 Rate：传输率 Lost：最近 10s 内的丢包数，基于序列号检测 Frames：客户端的数据帧数量 Probe：被客户端探查的 ESSID。若客户端试图连接一个 AP 但没连上，则会显示在这 aircrack-ng 破解 WEP 与 WPA 加密要对指定的 wifi 抓包 airodump-ng -c 该wifi的channel --ivs -w ~/WEP --bssid 要抓的wifi的BSSID 如果信道正确，#Data 的量会增长的比较快。-w 会在指定位置生成握手包抓包文件，后面的破解就需要这个文件。 可使用aireplay-ng -0 5 -a B4:0B:44:93:11:C4 -c 18:01:F1:30:00:42 wlan0mon造成目标网络掉线，使设备不断发送 arp 请求，能更容易抓到有用的密码数据。需要连接的设备开启了自动重连，或者等待目标的用户再次输入密码。 生成密码字典 cupp -i 填入可能的密码信息，然后aircrack-ng -w 字典文件 抓包文件。开始暴力破解。 自动化破解工具 gerix-wifi-crackerairgeddon 工具工具介绍地址：https://github.com/v1s1t0r1sh3r3/airgeddon git clone 后，进入目录执行 airgeddon.sh 脚本。经过几次 enter 确认后，先会要求选择无线网卡。 选择后进入主菜单 有以下主要功能：DOS 攻击、握手包工具、离线 WPA 解密、evil twin 攻击、WPS 攻击、WEP 攻击、Enterprise 协议攻击 DOS 攻击简单操作首先会提示输入要攻击的 wifi 的 BSSID，以及信道 可以选择攻击方式：Deauth 攻击、mdk3 攻击、WIDS 攻击等。就会自动开始攻击 握手包工具操作同理填入目标 BSSID 和信道。 可直接选择 5，开始抓取握手包. 关闭窗口后，会显示所有目标，按照开头标号选择，其中标号后面有*的说明有客户端连接着。 进入握手包抓取选项界面，选择攻击方式，与 DOS 一致。可直接选择 deauth aireplay attack，与 aireplay-ng 效果一致，断开目标网络，抓取 arp 握手包。可以填写要抓取的时间，默认 20s。 若抓取成功，就会在主目录生成握手包抓包文件，然后同理使用 aircrack-ng 破解密码。 自动化破解工具 wifite执行 wifite 后会自动开始扫描范围内的无线网络，ctrl+c 停止扫描 选择一个目标后，开始自动检测 然后直接就能看到破解的 wifi 密码 hirte 工具伪造 AP不指定字典破解密码仍然是使用字典工具暴力破解握手包。 crunch 最小长度 最大长度 字符集 | aircrack-ng 握手包 -e ESSID -w -示例：crunch 8 8 12345678 | aircrack-ng ~/wpa-B4\\:0B\\:44\\:93\\:11\\:C4-02.cap -e gty123 -w - hashcat 工具跑包aircrack-ng 握手包 -J 输出hash文件 会生成一个.hccap 文件 aircrack-ng ~/wpa-B4\\:0B\\:44\\:93\\:11\\:C4-02.cap -J ~/wpahash 需要 hashcat 工具，可–help 查看加密方式对应的号码 # hashcat --help | grep WPA 2500 | WPA-EAPOL-PBKDF2 | Network Protocols 2501 | WPA-EAPOL-PMK | Network Protocols 16800 | WPA-PMKID-PBKDF2 | Network Protocols 16801 | WPA-PMKID-PMK | Network Protocols 使用-m指定编号 用法：hashcat -m 2500 hccap文件 字典文件# hashcat -m 2500 ~/wpahash.hccap /usr/share/dict/wordlist-top4800-probable.txt cowpatty 工具破解密码cowpatty 功能与 aircrack-ng 类似 cowpatty -f 字典文件 -r 握手包文件 -s ESSID hash-table 加速破解可通过对字典文件生成 hashtable，加快破解速度。但是对字典生成 hash 表文件需要消耗大量时间。 genpmk -f 字典文件 -d 输出的hash表文件 -s ESSID 然后再用 cowpatty 破解 cowpatty -d hash表文件 -r 握手包文件 -s ESSID 在本机的速度提升约有 100 倍。 batch-table 加速破解生成 batch-table 与 hash-table 一样，需要耗费大量时间 先载入字典文件 pyrit -i 字典文件 import_passwords 添加 ESSID，可添加多个 pyrit -e ESSID create_essid 生成 batch-table pyrit batch 最后开始破解 pyrit -e ESSID -r 握手包 attack_batch pyrit 工具补充pyrit 也可以仅仅使用字典跑密码，功能仍然类似 aircrack-ng 可以先通过 pyrit 的 analyze 模块分析，获取 AP 的 BSSID pyrit -r 握手包 -i 字典文件 -b AP的BSSID（或者-e AP的ESSID）跑字典破解 pyrit 也可以通过 hash-table 进行破解，同样使用-i指定输入 hash 表文件 airolib-ng 生成彩虹表可通过彩虹表加速破解，可使用 airolib-ng 生成彩虹表 airolib-ng &lt;database&gt; &lt;operation&gt; [options] --import [essid|passwd] &lt;file&gt; 传入ESSID或字典文件 --export cowpatty &lt;essid&gt; &lt;file&gt; 传出为一个cowpatty文件 需要先将目标 ESSID 写入一个文件 还需要将字典文件写入 最后生成 batch-table 导出表供 cowpatty 使用 使用 cowpatty 破解密码 解除验证 DoS 攻击WLAN 容易受到 DoS（拒绝服务）攻击，攻击手段包括以下： 解除验证（Deauthentication） 取消关联（Disassociation） CTS-RTS 攻击 信号干扰或频谱干扰 Easy-Creds 工具创建伪 APEasy-Creds 是一款欺骗嗅探为主的攻击脚本工具，具备 ARP 毒化，DNS 毒化等一些嗅探攻击模式，具有 fakeAP 功能，比一般自行搭建的 fake AP 要稳定的多，而且里面还包含了针对 802.1x 的攻击模式。 GitHub 地址 下载后执行目录中的installer.sh安装即可。然后执行easy-creds命令。 ____ ____ ____ ____ ____ ____ ____ ____ ____ ____||e |||a |||s |||y |||- |||c |||r |||e |||d |||s ||||__|||__|||__|||__|||__|||__|||__|||__|||__|||__|||/__\\|/__\\|/__\\|/__\\|/__\\|/__\\|/__\\|/__\\|/__\\|/__\\| Version 3.8-dev - Garden of New JerseyAt any time, ctrl+c to cancel and return to the main menu1. Prerequisites &amp; Configurations # 配置2. Poisoning Attacks # 毒化攻击3. FakeAP Attacks # 伪AP攻击4. Data Review # 数据检查5. Exitq. Quit current poisoning session 选择 3，进入伪 AP 攻击 1. FakeAP Attack Static # 静态2. FakeAP Attack EvilTwin # eviltwin3. Karmetasploit Attack4. FreeRadius Attack5. DoS AP Options6. Previous Menu 选择 1，静态伪 AP 攻击 Would you like to include a sidejacking attack? [y/N]: N # 是否要包劫持攻击Interface connected to the internet (ex. eth0): eth0Wireless interface name (ex. wlan0): wlan0ESSID you would like your rogue AP to be called, example FreeWiFi: gty123Channel you would like to broadcast on: 10Enter your monitor enabled interface name, (ex: mon0): wlan0monWould you like to change your MAC address on the mon interface? [y/N]: N #是否修改监听接口MAC地址Enter your tunnel interface, example at0: at0 # 隧道接口Do you have a dhcpd.conf file to use? [y/N]: N # 是否使用dhcpd.confNetwork range for your tunneled interface, example 10.0.0.0/24: 192.168.1.0/24 # 隧道接口网络范围The following DNS server IPs were found in your /etc/resolv.conf file: &lt;&gt; 192.168.1.2Enter the IP address for the DNS server, example 8.8.8.8: 192.168.1.2 # 设置DNS服务器 之后系统会自动开启一些程序，如 DHCP、SSLStrip、Etterp、Dsniff 等，弹出几个窗口 Evil Twin 攻击Evil Twin（克隆 AP）攻击是一种针对 WLAN 基础设施的攻击。在受攻击的 WLAN 网络附近部署受攻击者控制的双胞胎 AP，该无赖 AP 会通告与受攻击的网络完全相同的 SSID，导致许多用户在不经意间连接到此 AP。然后攻击者就可发动中间人攻击。 架设无赖 AP无赖 AP 是指未经授权便连接到需授权网络的 AP，攻击者会将这样的 AP 作为私开的后门，从而绕过需授权网络中的安全控制机制。 大多数情况下，无赖 AP 会被设置为开放验证且不启用加密。可以通过部署实际的物理设备作为无赖 AP，也可以通过软件创建无赖 AP，将其桥接到需授权网络的本地以太网网络。 误关联攻击Caffe Latte 攻击解除对无线客户端的验证不碰 AP 的情况下破解 WPA","categories":[],"tags":[{"name":"安全","slug":"安全","permalink":"https://coconutmilktaro.top/tags/%E5%AE%89%E5%85%A8/"},{"name":"无线","slug":"无线","permalink":"https://coconutmilktaro.top/tags/%E6%97%A0%E7%BA%BF/"},{"name":"渗透","slug":"渗透","permalink":"https://coconutmilktaro.top/tags/%E6%B8%97%E9%80%8F/"}]},{"title":"Python基础完全笔记","slug":"Python基础完全笔记","date":"2019-02-03T04:56:22.000Z","updated":"2022-06-21T15:16:21.218Z","comments":true,"path":"2019/Python基础完全笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/Python%E5%9F%BA%E7%A1%80%E5%AE%8C%E5%85%A8%E7%AC%94%E8%AE%B0/","excerpt":"函数式编程 高阶函数 map 与 reduce filter sorted 装饰器 偏函数 面向对象编程 特殊方法与特殊变量 静态方法、类方法与继承 __new__和 metaclass 反射 测试 进程与线程 正则表达式 网络编程 数据库编程 文本处理 协程与异步 IO Web 编程 常见内建模块","text":"函数式编程 高阶函数 map 与 reduce filter sorted 装饰器 偏函数 面向对象编程 特殊方法与特殊变量 静态方法、类方法与继承 __new__和 metaclass 反射 测试 进程与线程 正则表达式 网络编程 数据库编程 文本处理 协程与异步 IO Web 编程 常见内建模块 函数式编程高阶函数map 与 reducefiltersorted装饰器偏函数面向对象编程特殊方法与特殊变量所有保留属性： Class.__doc__：类的帮助信息，显示类的帮助信息 &gt;&gt;&gt; class Person():... &#x27;&#x27;&#x27;... Person 类... &#x27;&#x27;&#x27;... def __init__(self, name):... self.name = name...&gt;&gt;&gt; peter = Person(&quot;peter&quot;)&gt;&gt;&gt; Person.__doc__ Person 类 Class.__name__：类名 &gt;&gt;&gt; Person.__name__Person Class.__module__：类所在模块 &gt;&gt;&gt; Person.__module____main__ Class.__bases__：类所继承的基类 &gt;&gt;&gt; Person.__base__&lt;class &#x27;object&#x27;&gt; Class.__dict__：类型字典，存储所有类的成员信息（类属性） &gt;&gt;&gt; Person.__dict__&#123;&#x27;__module__&#x27;: &#x27;__main__&#x27;, &#x27;__doc__&#x27;: &#x27;\\n Person 类\\n &#x27;, &#x27;__init__&#x27;: &lt;function Person.__init__ at 0x0000027E01DAC7B8&gt;, &#x27;__dict__&#x27;: &lt;attribute &#x27;__dict__&#x27; of &#x27;Person&#x27; objects&gt;, &#x27;__weakref__&#x27;:&lt;attribute &#x27;__weakref__&#x27; of &#x27;Person&#x27; objects&gt;&#125; Class().__class__：对象的类 &gt;&gt;&gt; peter.__class__&lt;class &#x27;__main__.Person&#x27;&gt; Class().__module__：对象的实例类所在模块 &gt;&gt;&gt; peter.__module__&#x27;__main__&#x27; Class().__dict__：对象字典，存储所有实例成员信息（实例属性，不包括类属性） &gt;&gt;&gt; peter.__dict__&#123;&#x27;name&#x27;: &#x27;peter&#x27;&#125; Class().__call__：当对象后加了()，则会触发执行 在类中添加方法__call__... def __call__(self, *args, **kwargs):... print(&quot;person&quot;, args, kwargs)&gt;&gt;&gt; peter = Person(&quot;peter&quot;)&gt;&gt;&gt; peter()person () &#123;&#125;&gt;&gt;&gt; peter(&quot;peter&quot;, 1, 2, 3)person (&#x27;peter&#x27;, 1, 2, 3) &#123;&#125;&gt;&gt;&gt; peter(&quot;peter&quot;, 1, 2, 3, name=&quot;peter&quot;)person (&#x27;peter&#x27;, 1, 2, 3) &#123;&#x27;name&#x27;: &#x27;peter&#x27;&#125; Class().__str__：在打印对象时再同时打印__str__定义的默认输出值 在类中添加__str__方法... def __str__(self):... return &quot;&lt;obj: %s&gt;&quot; % self.name&gt;&gt;&gt; print(peter)&lt;obj: peter&gt; Class.__getitem__、class.__setitem__、class.__delitem__：将对象当作字典，但可以进行调用权限控制 class Dog(): def __init__(self): self.data = &#123;&#125; def __getitem__(self, key): print(&quot;__getitem__ key: %s&quot; % key) return self.data.get(key) def __setitem__(self, key, value): print(&quot;__setitem__ key: %s, value: %s&quot; % (key,value)) self.data[key] = value def __delitem__(self, key): print(&quot;__delitem__&quot;, &quot;key: %s&quot; % key)&gt;&gt;&gt; d = Dog()&gt;&gt;&gt; d[&#x27;name&#x27;] = &#x27;tom&#x27;__setitem__ key: name, value: tom&gt;&gt;&gt; d[&#x27;name&#x27;]__getitem__ key: name&#x27;tom&#x27;&gt;&gt;&gt; del d[&#x27;name&#x27;]__delitem__ key: name 静态方法、类方法与继承静态方法虽然定义在类中，但与类没有实际关系。需要在定义时，添加@staticmethod，而调用时，仍然是通过类的对象或类调用。 class Person(object): def __init__(self, name=&quot;AAA&quot;): self.name = name def eat(self): print(&quot;&#123;&#125; is eating&quot;.format(self.name)) @staticmethod def drink(self): print(&quot;&#123;&#125; is drinking&quot;.format(self.name))p = Person()# 普通方法可不需要参数p.eat()# 静态方法必须传入一个该类的对象print(&quot;通过对象p调用静态方法：&quot;)p.drink(p)print(&quot;-------------------------&quot;)print(&quot;通过类直接调用静态方法：&quot;)Person.drink(p)# 执行结果AAA is eating通过对象p调用静态方法：AAA is drinking-------------------------通过类直接调用静态方法：AAA is drinking 静态方法只是名义上归类管理，实际上静态方法中无法访问类或实例中的任何属性。 类方法只能访问类变量，不能访问实例变量。在定义方法前加上@classmethod标记，可通过类的对象或类直接调用。 class Person(object): name = &quot;BBB&quot; # 初始化方法对类方法的参数无效 def __init__(self, name=&quot;AAA&quot;): self.name = name @classmethod def drink(self): print(&quot;&#123;&#125; is drinking&quot;.format(self.name))p = Person()p.drink()Person.drink()# 执行结果BBB is drinkingBBB is drinking 属性方法会把一个方法变为一个静态属性，即不能作为一个函数（有括号的）。只能通过对象调用，不能直接类名调用。 class Person(object): def __init__(self, name=&quot;AAA&quot;): self.name = name @property def drink(self): print(&quot;&#123;&#125; is drinking&quot;.format(self.name))p = Person()p.drink# 如果方法是带参数的，则这样调用会出错。 @property def drink(self, something): print(&quot;&#123;&#125; is drinking &#123;&#125;&quot;.format(self.name, self.something)) 解决方法：设置一个专门的设置属性值的函数 setter，而原来的属性方法就不需要带参数了，而让 setter 方法进行设置参数值。setter 方法就是类似 java 的 setter 方法 @property def drink(self): print(&quot;&#123;&#125; is drinking &#123;&#125;&quot;.format(self.name, self.something)) @drink.setter def setDrink(self, something): self.something = something print(&quot;set drink to : &#123;&#125;&quot;.format(self.something))p = Person()# 先调用setter方法设置属性值p.setDrink = &quot;coffee&quot;p.drink# 执行结果set drink to : coffeeAAA is drinking coffee 同理，删除对象属性的方法 deleter 也可定义。 @drink.deleter def deleteDrink(self): print(&quot;delete drink &#123;&#125;&quot;.format(self.something)) del self.somethingp = Person()p.drink = &quot;coffee&quot;del p.deleteDrinktry: print(p.something)except: print(&quot;p.something属性已被删除&quot;)# 执行结果set drink to : coffeedelete drink coffeep.something属性已被删除 __new__和 metaclass&gt;&gt;&gt; class Foo(object):... def __init__(self, name):... self.name = name&gt;&gt;&gt; f = Foo(&quot;tom&quot;) # 普通方式创建类的实例&gt;&gt;&gt; print(type(f)) # 对象实例的类型是Foo&lt;class &#x27;__main__.Foo&#x27;&gt;&gt;&gt;&gt; print(type(Foo)) # 类Foo的类型是type&lt;class &#x27;type&#x27;&gt;特殊方法创建类实例&gt;&gt;&gt; def func():... print(&quot;func_1&quot;)...&gt;&gt;&gt; Foo = type(&#x27;Foo&#x27;, (), &#123;&#x27;func_1&#x27;: func&#125;)&gt;&gt;&gt; print(type(Foo))&lt;class &#x27;type&#x27;&gt;&gt;&gt;&gt; f = Foo&gt;&gt;&gt; f.func_1() # Foo对象调用方法funcfunc_1&gt;&gt;&gt; def __init__(self, name):... self.name = name... print(&quot;__init__ %s&quot; % name)...&gt;&gt;&gt; def func(self):... print(&quot;func_1&quot;, self.name)...&gt;&gt;&gt;&gt;&gt;&gt; Foo = type(&#x27;Foo&#x27;, (object,), &#123;&#x27;func_1&#x27;: func, &#x27;__init__&#x27;: __init__&#125;)&gt;&gt;&gt; foo = Foo(&quot;tom&quot;) # 实例创建完成__init__ tom&gt;&gt;&gt; foo.func_1()func_1 tom 因此可知，类是由type类实例化产生的。那么 type 类又是如何创建类的呢、如何创建对象的？ 类中有一个属性__metaclass__，用来表示该类由谁来实例化创建，通过__metaclass__设置一个 type 类的派生类 class Foo(object): def __init__(self, name): self.name = name print(&quot;Foo __init__&quot;) def __new__(cls, *args, **kwargs): print(&quot;Foo __new__&quot;)先创建一个对象&gt;&gt;&gt; foo = Foo(&quot;tom&quot;)Foo __new__ 可以知道创建对象时只调用了__new__，而且没有执行__init__，并没有实例化。因此，可以得知__new__是真正实现实例化的方法。并且当创建类时，会自动已经创建好__new__，并不需要重写。 因为此时__new__还没有真正创建好实例，所以要添加一条 def __new__(cls, *args, **kwargs): print(&quot;Foo __new__&quot;) # 返回类的实例，cls即为Foo类本身，继承父类的__new__方法 return object.__new__(cls) 反射反射就是通过字符串映射或修改程序运行时的状态、属性、方法。 hasattr(object, func)：判断对象是否有指定方法、属性、状态，若有则返回 True，否则返回 False &gt;&gt;&gt; class Dog(object):... def __init__(self, name):... self.name = name... def eat(self):... print(&quot;%s is eating&quot; % self.name)...&gt;&gt;&gt; dog = Dog(&quot;tom&quot;) # 创建对象&gt;&gt;&gt;&gt;&gt;&gt; choice = input(&quot;input func name: &quot;)input func name: eat&gt;&gt;&gt; print(hasattr(dog, choice)) # 若对象有该方法，就返回TrueTrue&gt;&gt;&gt; choice = input(&quot;input func name: &quot;)input func name: walk&gt;&gt;&gt; print(hasattr(dog, choice)) # 若对象没有该方法，返回FalseFalse getattr(obj, func)：获取该状态、属性、方法 &gt;&gt;&gt; choice = input(&quot;input func name: &quot;)input func name: eat&gt;&gt;&gt; getattr(dog, choice) # 得到方法的内存地址&lt;bound method Dog.eat of &lt;__main__.Dog object at 0x00000201A8B0A3C8&gt;&gt;&gt;&gt;&gt; getattr(dog, choice)() # 执行方法tom is eating# 因此可以这样写&gt;&gt;&gt; if hasattr(dog, choice):... getattr(dog, choice)()...tom is eating setattr(obj, name, value)：设置状态、属性、方法 &gt;&gt;&gt; def inputError():... print(&quot;input wrong func name&quot;)...&gt;&gt;&gt; if hasattr(dog, choice):... getattr(dog, choice)()... else:... setattr(dog, choice, inputError) # 若输入不存在，则将choice设为一个指定方法...input wrong func name delattr(obj, name)：删除指定的属性或方法 &gt;&gt;&gt; choice = input(&quot;input func name: &quot;)input func name: eat&gt;&gt;&gt; if hasattr(dog, choice):... delattr(dog, choice) # 删除choice... else:... setattr(dog, choice, inputError)...Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 2, in &lt;module&gt;AttributeError: eat # 报错，已没有choice 测试进程与线程正则表达式网络编程套接字 socket 是计算机网络数据结构，在任何类型的通信开始之前，网络应用程序必须创建套接字。 Socket API 的调用顺序和 TCP 的数据流： python 网络编程需要使用 socket 库。 socket 具有很多的地址簇，其中常见的有： AF_UNIX：被绑定到一个文件系统的节点。会返回 Linux 的抽象命名空间中的地址的字节对象，此命名空间中的套接字可以与普通文件系统套接字通信，因此打算在 Linux 上运行的程序可能需要处理这两种类型的地址。 AF_INET：IPv4 的地址信息，是一对(host, port)，host 可以是 IP 地址或是域名 AF_INET6：IPv6 的地址信息，是一对(host, port, flowinfo, scopeid) 客户端简单实现： import socketclient = socket.socket() # 生成socket对象（连接）client.connect((&#x27;localhost&#x27;, 9000)) # 连接端口，参数为主机和端口的元组client.send(b&quot;hello&quot;) # python3规定必须为字节流，所以必须转换print(&quot;开始接收server端返回信息&quot;)server_data = client.recv(1024) # 接收server的返回信息print(&quot;recv:&quot;, server_data) 服务器端简单实现： import socket, osserver = socket.socket()server.bind((&#x27;localhost&#x27;, 9000)) # 绑定本地端口print(&quot;server开始监听&quot;)server.listen() # 开始监听conn, addr = server.accept() # 准备接收数据，会返回连接对象和对端IP地址# conn就是客户端连接服务器时，服务器为其生成的一个连接对象print(&quot;conn: &quot;, conn, &quot;,addr: &quot;, addr)print(&quot;开始接收client信息&quot;)client_data = conn.recv(1024) # 接收客户端数据print(&quot;recv: &quot;, client_data)conn.send(client_data)server.close() 先开启 server 程序，之后执行 client 程序 # 开启server端python sock_server.pyserver开始监听conn: &lt;socket.socket fd=524, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=(&#x27;127.0.0.1&#x27;, 9000), raddr=(&#x27;127.0.0.1&#x27;, 12215)&gt; ,addr: (&#x27;127.0.0.1&#x27;, 12215)开始接收client信息recv: b&#x27;hello&#x27;# 开启client端python sock_client.py开始接收server端返回信息recv: b&#x27;hello&#x27; 而此时若传输中文就会报错 python3规定必须为字节流，所以必须转换SyntaxError: bytes can only contain ASCII literal characters. 需要在客户端修改 client.send(&quot;你好&quot;.encode(&#x27;utf-8&#x27;)) 然后服务器端会收到： recv: b&#x27;\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd&#x27; 因此需要在服务器端解码 print(&quot;recv: &quot;, client_data.decode()) 若要服务器端不断监听，客户端不断发送 # 客户端只要在send嵌套在while中即可while True: msg = input(&quot;&gt;&gt; &quot;).strip() client.send(msg.encode(&quot;utf-8&quot;))# 服务器端只要在recv嵌套在while中即可# accept一定要在while外面，否则会出错卡住while True: client_data = conn.recv(1024) # 接收客户端数据 print(&quot;recv: &quot;, client_data.decode()) 数据库编程文本处理协程与异步 IOWeb 编程常见内建模块","categories":[],"tags":[{"name":"Python","slug":"Python","permalink":"https://coconutmilktaro.top/tags/Python/"},{"name":"lang","slug":"lang","permalink":"https://coconutmilktaro.top/tags/lang/"}]},{"title":"Django使用Whoosh实现全文检索笔记","slug":"Django使用Whoosh实现全文检索笔记","date":"2019-02-01T07:33:18.000Z","updated":"2022-06-21T15:46:20.359Z","comments":true,"path":"2019/Django使用Whoosh实现全文检索笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/Django%E4%BD%BF%E7%94%A8Whoosh%E5%AE%9E%E7%8E%B0%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%E7%AC%94%E8%AE%B0/","excerpt":"在 Django 编写博客时添加简单的全文搜索的功能。使用到了 haystack、whoosh 和 jieba 三个项目","text":"在 Django 编写博客时添加简单的全文搜索的功能。使用到了 haystack、whoosh 和 jieba 三个项目 haystack 是一个与 whoosh 交互的连接工具，属于一种全文检索的框架。官网 Whoosh 是一个全文检索引擎，由 python 编写。官方文档 jieba 是一个中文分词工具。github 库 首先需要安装这三个库 pip install django-heystack whoosh jieba 这三个库的版本 django-haystack==2.8.1jieba==0.39Whoosh==2.7.4 简单实现然后在 Django 的项目 settings.py 中添加配置。首先将&#39;haystack&#39;添加到INSTALLED_APPS列表中 INSTALLED_APPS = [ &#x27;django.contrib.admin&#x27;,...... &#x27;blog&#x27;, &#x27;account&#x27;, &#x27;haystack&#x27;,] 并添加一端配置 # 用于haystack的连接HAYSTACK_CONNECTIONS = &#123; &#x27;default&#x27;: &#123; &#x27;ENGINE&#x27;: &#x27;haystack.backends.whoosh_cn_backend.WhooshEngine&#x27;, &#x27;PATH&#x27;: os.path.join(os.path.dirname(__file__), &#x27;whoosh_index&#x27;), &#125;,&#125;# 注这里一定要将官方文档中的whoosh_backend改为whoosh_cn_backend# 因为是需要使用whoosh_cn_backend中的配置。否则无法检索中文# 用于自动生成索引HAYSTACK_SIGNAL_PROCESSOR = &#x27;haystack.signals.RealtimeSignalProcessor&#x27; 官网配置文档 基本配置：https://django-haystack.readthedocs.io/en/v2.8.1/tutorial.html#configuration 所有设置字段：https://django-haystack.readthedocs.io/en/v2.8.1/settings.html# 在项目目录下的 urls.py 中添加 url。 urlpatterns = [...... url(r&#x27;^search/&#x27;, include(&#x27;haystack.urls&#x27;)),] 在应用目录下创建search_indexes.py from haystack import indexesfrom .models import Articleclass ArticleIndex(indexes.SearchIndex, indexes.Indexable): text = indexes.CharField(document=True, use_template=True) def get_model(self): return Article def index_queryset(self, using=None): return self.get_model().objects.all() 在 templates 下创建search/indexes/应用名目录，并在该应用名目录下创建 模型类_text.txt文件。 mkdir -p templates/search/indexes/blogtouch templates/search/indexes/blog/article_text.txt# txt文件的格式必须是： 要检索的模型类的小写_text.txt 在该 txt 文件中列出所有要检索的字段，是该模型类中定义的字段，一定要加上 object &#123;&#123; object.title &#125;&#125;&#123;&#123; object.body &#125;&#125;# 对标题和文章内容进行检索 在 search 目录下创建一个search.html &#123;% block content %&#125; &#123;% if query %&#125; &lt;h3&gt;搜索结果：&lt;/h3&gt; &#123;% for result in page.object_list %&#125; &lt;p&gt;&lt;a href=&quot;&#123;&#123; result.object.get_absolute_url &#125;&#125;&quot;&gt;&#123;&#123; result.object.title &#125;&#125;&lt;/a&gt;&lt;/p&gt; &#123;% empty %&#125; &lt;p&gt;未找到&lt;/p&gt; &#123;% endfor %&#125; &#123;% if page.has_previous or page.has_next %&#125; &lt;div&gt; &#123;% if page.has_previous %&#125;&lt;a href=&quot;?q=&#123;&#123; query &#125;&#125;&amp;amp;page=&#123;&#123; page.previous_page_number &#125;&#125;&quot;&gt;&#123;% endif %&#125;&amp;laquo; Previous&#123;% if page.has_previous %&#125;&lt;/a&gt;&#123;% endif %&#125; | &#123;% if page.has_next %&#125;&lt;a href=&quot;?q=&#123;&#123; query &#125;&#125;&amp;amp;page=&#123;&#123; page.next_page_number &#125;&#125;&quot;&gt;&#123;% endif %&#125;Next &amp;raquo;&#123;% if page.has_next %&#125;&lt;/a&gt;&#123;% endif %&#125; &lt;/div&gt; &#123;% endif %&#125; &#123;% endif %&#125;&#123;% endblock content %&#125; 在 haystack 安装的目录下中建立ChineseAnalyzer.py，路径例如：venv/lib/python3.7/site-packages/haystack/backends。不需要改，直接复制即可。 import jiebafrom whoosh.analysis import Tokenizer, Tokenclass ChineseTokenizer(Tokenizer): def __call__(self, value, positions=False, chars=False, keeporiginal=False, removestops=True, start_pos=0, start_char=0, mode=&#x27;&#x27;, **kwargs): t = Token(positions, chars, removestops=removestops, mode=mode, **kwargs) seglist = jieba.cut(value, cut_all=True) for w in seglist: t.original = t.text = w t.boost = 1.0 if positions: t.pos = start_pos + value.find(w) if chars: t.startchar = start_char + value.find(w) t.endchar = start_char + value.find(w) + len(w) yield tdef ChineseAnalyzer(): return ChineseTokenizer() 将该目录中的whoosh_backend.py复制后改名为whoosh_cn_backend.py 并添加和修改以下内容 from .ChineseAnalyzer import ChineseAnalyzer约164行的analyzer=StemmingAnalyzer() 修改为 analyzer=ChineseAnalyzer() 重建索引 python3 manage.py rebuild_index # 会询问是否继续，y确认即可Removing all documents from your index because you said so.All documents removed.Indexing 11 articlesBuilding prefix dict from the default dictionary ...Loading model from cache /tmp/jieba.cacheLoading model cost 0.945 seconds.Prefix dict has been built succesfully. 会在项目目录下生成一个whoosh_index目录，以后的检索就是基于这个目录。 在应用目录下的 urls 中添加 url(r&#x27;^mysearch/$&#x27;, views.mysearch) 这个名字可随便取，是普通的视图实现，即通过此 url 访问搜索表单。然后在 views 中添加实现方法 def mysearch(request): return render(request, &#x27;blog/mysearch.html&#x27;) 在 templates 目录的应用目录下创一个mysearch.html &#123;% block content %&#125;&lt;form method=&quot;get&quot; action=&quot;/search&quot; target=&quot;_self&quot;&gt; &lt;input type=&quot;text&quot; name=&quot;q&quot;&gt; # name必须是q &lt;input type=&quot;submit&quot;&gt;&lt;/form&gt;&#123;% endblock content %&#125; 运行项目，添加几篇文章，进行测试。通过/mysearch访问表单页面 先进行英文单词的检索 检索成功，跳转到结果页面。再进行中文单词检索 改进 1不需要 mysearch.html，不用在应用的 urls 中配置，不用配置 views 的 mysearch 方法。 注：使用的是 MDUI 前端 UI 直接在网页中的搜索框中建立表单 &lt;div class=&quot;mdui-col-xs-3&quot;&gt; &lt;div class=&quot;mdui-textfield mdui-textfield-expandable mdui-float-right&quot;&gt; &lt;button class=&quot;mdui-textfield-icon mdui-btn mdui-btn-icon&quot;&gt; &lt;i class=&quot;mdui-icon material-icons&quot;&gt;search&lt;/i&gt; &lt;/button&gt; &lt;form method=&quot;get&quot; target=&quot;_self&quot; action=&quot;/search&quot;&gt; &lt;input name=&quot;q&quot; class=&quot;mdui-textfield-input&quot; type=&quot;text&quot; placeholder=&quot;Search&quot; /&gt; &lt;/form&gt; &lt;button class=&quot;mdui-textfield-close mdui-btn mdui-btn-icon&quot;&gt; &lt;i class=&quot;mdui-icon material-icons&quot;&gt;close&lt;/i&gt; &lt;/button&gt; &lt;/div&gt;&lt;/div&gt; 注：在简单实现中，搜索出的结果是无法进行超链接的。需要进行小的修改 对搜索结果界面进行美化，使用与首页一致的卡片 &#123;% block content %&#125;&#123;% if query %&#125; &lt;div class=&quot;mdui-typo-headline-opacity&quot; style=&quot;margin-top: 20px;&quot;&gt;搜索结果：&lt;/div&gt; &lt;ul class=&quot;mdui-list mdui-center&quot;&gt; &#123;% for result in page.object_list %&#125; &lt;li class=&quot;mdui-card mdui-ripple mdui-shadow-2&quot; style=&quot;margin-bottom: 5px&quot;&gt; &lt;a href=&quot;/&#123;&#123; page.number &#125;&#125;/&#123;&#123; result.object.id &#125;&#125;&quot;&gt; &lt;div class=&quot;mdui-card-primary&quot;&gt; &lt;div class=&quot;mdui-card-primary-title&quot;&gt;&#123;&#123; result.object.title &#125;&#125;&lt;/div&gt; &lt;div class=&quot;mdui-card-primary-subtitle&quot;&gt;&#123;&#123; result.object.author &#125;&#125;&lt;/div&gt; &lt;div class=&quot;mdui-card-primary-subtitle&quot;&gt;&#123;&#123; result.object.publish &#125;&#125;&lt;/div&gt; &lt;/div&gt; &lt;/a&gt; &lt;/li&gt; &#123;% empty %&#125; &lt;div class=&quot;mdui-typo-headline-opacity&quot;&gt;未找到&lt;/div&gt; &#123;% endfor %&#125; &lt;/ul&gt;# 下面的内容不改......&#123;% endblock content %&#125; 参考文章： django-haystack(全文检索-jieba 分词) Django–全文检索功能","categories":[{"name":"后端开发","slug":"后端开发","permalink":"https://coconutmilktaro.top/categories/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"python","slug":"python","permalink":"https://coconutmilktaro.top/tags/python/"},{"name":"Django","slug":"Django","permalink":"https://coconutmilktaro.top/tags/Django/"},{"name":"Whoosh","slug":"Whoosh","permalink":"https://coconutmilktaro.top/tags/Whoosh/"},{"name":"全文检索","slug":"全文检索","permalink":"https://coconutmilktaro.top/tags/%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2/"}]},{"title":"信息安全概述与Linux系统安全","slug":"Linux系统安全与信息安全笔记","date":"2019-01-21T15:57:43.000Z","updated":"2022-05-30T02:51:53.844Z","comments":true,"path":"2019/Linux系统安全与信息安全笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/Linux%E7%B3%BB%E7%BB%9F%E5%AE%89%E5%85%A8%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E7%AC%94%E8%AE%B0/","excerpt":"Linux 后门入侵检测 rootkit 检测 服务器受到攻击后的处理","text":"Linux 后门入侵检测 rootkit 检测 服务器受到攻击后的处理 Linux 后门入侵检测rootkit 检测rootkit 是一种木马后门工具，主要通过替换系统文件达到入侵和隐蔽目的，攻击能力强，攻击者能隐蔽行迹并获取 root 权限。 rootkit 分为两种： 文件级别：通过程序或系统漏洞进入系统后，修改系统文件隐蔽自己。通常会将程序替换为木马，例如 login、ls、ps、ifconfig 等。 内核级别：比文件级别更加高级，攻击者能获得系统底层的完全控制权，即可以修改内核。内核级 rootkit 主要依附在内核上，并不对系统文件做任何修改，一般检测工具无法检测。 可使用 rkhunter 工具检测 rootkit 威胁。官网下载。进入解压目录后 ./installer.sh --install rkhunter 命令参数 -c # 检测本地系统 服务器受到攻击后的处理 切断网络 查找攻击源：分析系统日志和用户登录日志、开放端口、进程服务 分析入侵原因和途径：可能是系统漏洞、程序漏洞 检查锁定可疑用户：通过 w，或日志/var/log/secure查看异常登录 检查并关闭系统可疑进程：pidof 命令、/proc/[pid]/fd|exe目录 检查文件系统完好性：rpm -Va命令 备份用户数据 重新安装系统 修复程序或系统漏洞 恢复数据和连接网络","categories":[{"name":"系统运维","slug":"系统运维","permalink":"https://coconutmilktaro.top/categories/%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"运维","slug":"运维","permalink":"https://coconutmilktaro.top/tags/%E8%BF%90%E7%BB%B4/"},{"name":"安全","slug":"安全","permalink":"https://coconutmilktaro.top/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"Jenkins学习笔记","slug":"Jenkins学习笔记","date":"2019-01-21T15:28:13.000Z","updated":"2022-05-30T02:51:53.806Z","comments":true,"path":"2019/Jenkins学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2019/Jenkins%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"jenkins 介绍 jenkins 搭建 war 部署 apt 安装 Docker 部署 配置系统 系统设置 配置 JDK、Maven、Ant 配置发件人地址 配置邮件通知 安全设置 用户设置 插件设置 项目构建设置 Pipeline","text":"jenkins 介绍 jenkins 搭建 war 部署 apt 安装 Docker 部署 配置系统 系统设置 配置 JDK、Maven、Ant 配置发件人地址 配置邮件通知 安全设置 用户设置 插件设置 项目构建设置 Pipeline jenkins 介绍jenkins 是一个基于 java 开发的在自动化服务器，是一款开源 CI&amp;CD 软件，用于自动化各种任务，包括构建、测试和部署软件。 CI 系统内容可参考DevOps 概念整理 特性： 易于安装 易于配置 集成 RSS/Email，通过 RSS 发布构建结果或通过 email 通知 生成 Junit/TestNG 测试报告 支持分布式构建，让多台计算机一起构建 文件识别，能跟踪哪次构建了哪些 jar 及哪个版本的 jar 插件支持，大量官方插件以及可自定义插件 jenkins 搭建war 部署下载最新的 jenkins war 包 下载地址 两种启动方法： 直接java -jar jenkins.war，然后通过 8080 端口访问 将 war 包放到 tomcat 的 webapps 目录下，然后启动 tomcat，通过本机域名/jenkins访问 apt 安装对于 Debian/Ubuntu 系统，通过 apt 安装 wget -q -O - https://pkg.jenkins.io/debian/jenkins.io.key | sudo apt-key add -sudo sh -c &#x27;echo deb http://pkg.jenkins.io/debian-stable binary/ &gt; /etc/apt/sources.list.d/jenkins.list&#x27;sudo apt-get updatesudo apt-get install jenkins 安装软件包会自动完成以下内容： 将 Jenkins 设置为启动时启动的守护进程。查看/etc/init.d/jenkins获取更多细节 创建一个’jenkins’用户来运行此服务 直接将控制台日志输出到文件/var/log/jenkins/jenkins.log。如果您正在解决 Jenkins 问题，请检查此文件 /etc/default/jenkins为启动填充配置参数，例如 JENKINS_HOME 将 Jenkins 设置为在端口 8080 上进行监听。 Docker 部署docker run \\ -u root \\ --rm \\ -d \\ -p 8080:8080 \\ -p 50000:50000 \\ -v jenkins-data:/var/jenkins_home \\ -v /var/run/docker.sock:/var/run/docker.sock \\ jenkinsci/blueocean 官方推荐使用 jenkinsci/blueocean 镜像，该镜像包含当前的长期支持 (LTS) 的 Jenkins 版本 （可以投入使用） ，捆绑了所有 Blue Ocean 插件和功能，不需要单独安装 Blue Ocean 插件。也可以使用 jenkins tls 版本 jenkins/jenkins:tls 因为需要获取登录密码，所以必须先在本机创建/var/jenkins_home作 jenkins 的 volume，即 jenkins 的默认存放密码的目录。因为需要配置 jdk、maven 等目录，所以也需要作 volume 映射。因此，需要添加几项参数： docker run -u root -d \\ -p 8080:8080 -p 50000:50000 \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /usr/lib/jvm/java-8-openjdk-amd64:/usr/lib/jvm/java-8-openjdk-amd64 \\ -v /var/jenkins_home:/var/jenkins_home \\ --name jenkins \\ jenkinsci/blueocean 需要修改/var/jenkins_home的权限，使当前用户能有管理权限（所属人），这样才能查看 secrets 下的 initialAdminPassword 文件。secrets目录的默认权限为：700 配置系统系统设置配置 JDK、Maven、Ant进入全局工具配置（Global Tool Configuration），进行工具配置 JDK 配置最好不要选自动配置，JAVA_HOME 要与 volume 设置的路径一致。 同理，Maven 和 Ant，要先有 volume 映射，再配置。 配置发件人地址进入配置系统（Configure System），找到 Jenkins Location 配置 Jenkins URL 改为域名/jenkins，Email 改成 jenkins 要发送报告到的邮箱 配置邮件通知进入配置系统（Configure System），找到 E-mail Notification 配置 安全设置用户设置插件设置项目构建设置Pipeline","categories":[],"tags":[]},{"title":"Linux-Namespace学习笔记","slug":"Linux-Namespace学习笔记","date":"2018-12-30T06:42:04.000Z","updated":"2022-06-21T15:16:21.209Z","comments":true,"path":"2018/Linux-Namespace学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Linux-Namespace%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"学习 Namespace 的笔记与 C 语言 Demo 参考书目：《Docker 容器与容器云》","text":"学习 Namespace 的笔记与 C 语言 Demo 参考书目：《Docker 容器与容器云》 简介Linux 提供了 6 种 Namespace 隔离。 namespace 系统调用参数 隔离内容 UTS CLONE_NEWUTS 主机名与域名 IPC CLONE_NEWIPC 信号量、消息队列、共享内存 PID CLONE_NEWPID 进程编号 Network CLONE_NEWNET 网络设备、网络栈、端口等 Mount CLONE_NEWNS 挂载点 User CLONE_NEWUSER 用户与用户组 namespace 有以下 API：clone()、setns()、unshare() clone()是 Linux 系统调用 fork()的一种更通用的方式 int clone(int (_child_func)(void _), void *child_stack, int flags, void *arg);child*func 传入子进程运行的程序主函数child_stack 传入子程序使用的栈空间flags 表示使用哪些 CLONE*\\*标志位args 传入用户参数 从 3.8 版本开始，可通过/proc/[pid]/ns 目录查看指向不同 namespace 号的文件，[]中的就是 namespace 号。 ll /proc/\\$\\$/nslrwxrwxrwx. 1 root root 0 1 月 9 05:31 ipc -&gt; ipc:[4026531839]lrwxrwxrwx. 1 root root 0 1 月 9 05:31 mnt -&gt; mnt:[4026531840]lrwxrwxrwx. 1 root root 0 1 月 9 05:31 net -&gt; net:[4026531956]lrwxrwxrwx. 1 root root 0 1 月 9 05:31 pid -&gt; pid:[4026531836]lrwxrwxrwx. 1 root root 0 1 月 9 05:31 user -&gt; user:[4026531837]lrwxrwxrwx. 1 root root 0 1 月 9 05:31 uts -&gt; uts:[4026531838] 设置链接的作用：一旦 link 文件被打开，只要打开的文件描述符存在，则即使该 namespace 下的所有进程都结束，这个 namespace 仍会继续存在，后续的进程仍能加入进来。 通过 setns()加入一个已存在的 namespace int setns(int fd, int nstype);fd 为要加入 namespace 的文件描述符nstype 让调用者检查 fd 指向的 namespace 类型是否符合实际要求。若为 0 表示不检查 通过 unshare()在原先进程上进行 namespace 隔离，而不用启动新的进程。Docker 暂时还没有使用此调用。 int unshare(int flags); fork()系统调用，fork 函数不属于 namespace 的 API，属于 Linux 系统自带的调用。能创建新的进程，并分配资源。fork()调用一次却能返回两次，父进程和子进程各返回一次。可能有三种返回值： 在父进程中，返回新创建的子进程的进程 ID 在子进程中，返回 0 若出现错误，返回一个负值 一个实例代码： #include &lt;unistd.h&gt;#include &lt;stdio.h&gt;int main()&#123; pid_t fpid; int count = 0; fpid = fork(); if(fpid &lt; 0) printf(&quot;error&quot;); else if(fpid == 0) printf(&quot;child process id: %d\\n&quot;, getpid()); else printf(&quot;parent process id: %d\\n&quot;, getpid()); return 0;&#125; 编译后输出： parent process id: 33008child process id: 33009 在使用 fork()函数后，父进程有义务监视子进程的运行状态，并在子进程退出后自己才能正常退出，否则子进程就会孤儿进程。对于孤儿进程，会直接由上级的父进程接管。 以下是 6 种隔离的具体实现，以下为学习顺序 简介 UTS IPC PID Mount Network User UTS简单实现 UTS 隔离，实现代码uts.c #define _GNU_SOURCE#include&lt;sys/types.h&gt;#include&lt;sys/wait.h&gt;#include&lt;stdio.h&gt;#include&lt;sched.h&gt;#include&lt;signal.h&gt;#include&lt;unistd.h&gt;#define STACK_SIZE (1024*1024)static char child_stack[STACK_SIZE];char* const child_args[] = &#123;&quot;/bin/bash&quot;, NULL&#125;;int child_main(void* args)&#123; printf(&quot;在子进程中！\\n&quot;); sethostname(&quot;newnamespace&quot;, 12); execv(child_args[0], child_args); return 1;&#125;/*sethostname函数来自unistd.h库，用于设置主机名int sethostname(const char *name, size_t len);execv函数来自unistd.h库，用于运行其他程序int execv(const char *pathname, char * const argv[]);*/int main()&#123; printf(&quot;程序开始\\n&quot;); int child_pid = clone(child_main, child_stack + STACK_SIZE, CLONE_NEWUTS | SIGCHLD, NULL); waitpid(child_pid, NULL, 0); printf(&quot;已退出\\n&quot;); return 0;&#125;/*waitpid来自sys/wait.h头文件，暂停执行调用进程，直到pid参数指定的子进程发生更改状态。 默认情况下，waitpid()仅等待已终止的子项。pid_t waitpid(pid_t pid, int *status, int options);*/ 编译运行，实现了域名隔离 [root@s1 uts]# gcc -Wall uts.c -o uts[root@s1 uts]# ./uts程序开始在子进程中！[root@newnamespace uts]# exitexit已退出[root@s1 uts]# IPC简单实现 IPC 隔离，代码与 uts.c 基本一致，只需要添加一个标识位CLONE_NEWIPC int child_pid = clone(child_main, child_stack + STACK_SIZE, CLONE_NEWIPC | CLONE_NEWUTS | SIGCHLD, NULL); 先创建一个消息队列 [root@s1 ipc]# ipcmk -QMessage queue id: 0[root@s1 ipc]# ipcs -q------ Message Queues --------key msqid owner perms used-bytes messages0xf1646a41 0 root 644 0 0 编译后运行 ipc [root@s1 ipc]# gcc -Wall ipc.c -o ipc[root@s1 ipc]# ./ipc程序开始在子进程中！[root@newnamespace ipc]# ipcs -q------ Message Queues --------key msqid owner perms used-bytes messages 已实现了 IPC 隔离，在子进程中查看不到原先创建的消息队列 PID每个 PID namespace 都有自己的计数器，内核为所有 PID namespace 维护一个树状结构，最顶层是系统初始化时创建的，称为 root namespace，新的 Namespace 称为 child namespace。所属父进程能看到子节点的进程，并可通过信号等方式对子节点的进程产生影响。而子节点不能看到父进程 PID namespace 中的内容。 同样代码仍然使用ipc.c 的，只要添加一个标志位CLONE_NEWPID，保存为pid.c int child_pid = clone(child_main, child_stack + STACK_SIZE, CLONE_NEWPID | CLONE_NEWIPC | CLONE_NEWUTS | SIGCHLD, NULL); 首先查看当前的 shell 的 PID，再运行 pid，查看 shell 的 PID [root@s1 pid]# echo $$35580[root@s1 pid]# ./pid程序开始在子进程中！[root@newnamespace pid]# echo $$1 init 进程的 PID 为 1，作为所有进程的父进程，维护一张进程表，检查进程状态，一旦有子进程因父进程的错误变为孤儿进程，init 会收养该子进程，并回收资源，结束进程。 内核为 PID namespace 中的 init 进程赋予了信号屏蔽的特权，若 init 中没有编写处理某个信号的代码，则与 init 同一个 PID namespace 的进程发送给 init 的该信号就会被屏蔽。主要用于防止 init 进程被误杀。 父节点向子节点发送的除SIGKILL或SIGSTOP信号外的其他信号也会被子节点屏蔽。而对于SIGKILL和SIGSTOP信号，子节点会强制执行，因此，父节点有权终止子节点的进程。 当子节点的 init 进程被销毁，即使/proc/[pid]/ns/pid处于挂载或打开状态，导致 namespace 保存下来，但该 namespace 也无法通过setns()或fork()创建进程。 当一个容器中存在多个进程时，容器内的 init 进程可对信号捕获，对子进程做信息保存、资源回收等处理。 在新的 PID namespace 中用ps查看，仍然会看到所有进程。因为与 PID 相关的/proc文件系统profs没有挂载到一个与原/proc不同的位置 [root@newnamespace pid]# ps -a PID TTY TIME CMD 36025 pts/1 00:00:00 pid 36026 pts/1 00:00:00 bash 36060 pts/1 00:00:00 ps 挂载文件系统 proc 到/proc [root@newnamespace pid]# mount -t proc proc /proc[root@newnamespace pid]# ps a PID TTY STAT TIME COMMAND 1 pts/1 S 0:00 /bin/bash 38 pts/1 R+ 0:00 ps a 但此时并没有进行 mount namespace 的隔离，所以该操作实际上影响了 root namespace 的文件系统。因此退出后，仍要重新挂载 proc：mount -t proc proc /proc 调用unshare()和setns()创建新的 PID namespace 时，该调用程序并不进入，而是之后创建的子进程才会进入 namespace，成为 init。因为调用getpid()函数得到的 PID 是根据调用者所在的 PID namespace 决定返回哪个 PID，进入新的 PID namespace 会导致 PID 变动，造成进程错误。 Mount通过隔离文件系统挂载点对隔离文件系统提供支持，是第一个支持的 Linux namespace。 进程创建 mount namespace 时，会把当前的文件结构复制给新的 namespace，并且子节点中的挂载只影响自身，不影响外部。但也会有例外，因此，2006 年引入的挂载传播解决了该问题，定义了挂载对象（mount object）间的关系： 共享挂载（share relationship）：若两个挂载对象具有共享关系，则一个挂载对象中的挂载事件会传播到另一个挂载对象，反之亦然。 从属关系（slave relationship）：若两个挂载对象形成从属关系，则一个挂载对象中的挂载事件会传播到另一个挂载对象，反之不行。从属对象是事件的接受者。 有可能出现的挂载状态： 共享挂载（share）：传播事件的挂载对象 从属挂载（slave）：接收传播事件的挂载对象 共享/从属挂载（shared and slave）：既传播又接收事件的挂载对象 私有挂载（private）：既不传播也不接收事件的挂载对象 不可绑定挂载（unbindable）：类似私有挂载，但不允许执行绑定挂载，即这块文件对象不可被复制 mount提供以上的各种挂载方式： --make-shared mark a subtree as shared--make-slave mark a subtree as slave--make-private mark a subtree as private--make-unbindable mark a subtree as unbindable--make-rshared recursively marka whole subtree as shared--make-rslave recursively marka whole subtree as slave--make-rprivate recursively marka whole subtree as private--make-runbindable recursively marka whole subtree as unbindable 代码仍然只需要添加标志位CLONE_NEWNS #define _GNU_SOURCE#include&lt;sys/types.h&gt;#include&lt;sys/wait.h&gt;#include&lt;stdio.h&gt;#include&lt;sched.h&gt;#include&lt;signal.h&gt;#include&lt;unistd.h&gt;#define STACK_SIZE (1024*1024)static char child_stack[STACK_SIZE];char* const child_args[] = &#123;&quot;/bin/bash&quot;, NULL&#125;;int child_main(void* args)&#123; printf(&quot;在子进程中！\\n&quot;); sethostname(&quot;newnamespace&quot;, 12); execv(child_args[0], child_args); return 1;&#125;int main()&#123; printf(&quot;程序开始\\n&quot;); int child_pid = clone(child_main, child_stack + STACK_SIZE, CLONE_NEWNS | CLONE_NEWPID | CLONE_NEWIPC | CLONE_NEWUTS | SIGCHLD, NULL); waitpid(child_pid, NULL, 0); printf(&quot;已退出\\n&quot;); return 0;&#125; 编译后运行，挂载 CDROM [root@newnamespace mount]# mount /dev/cdrom /mntmount: /dev/sr0 is write-protected, mounting read-only[root@newnamespace mount]# dfFilesystem 1K-blocks Used Available Use% Mounted on/dev/mapper/centos-root 17811456 1836940 15974516 11% /....../dev/sda1 1038336 129608 908728 13% /boot/dev/sr0 4414592 4414592 0 100% /mnt Network网络隔离包括：网络设备、IPv4 和 IPv6 协议栈、IP 路由表、防火墙、/proc/net、/sys/class/net、套接字等。 通过创建 veth pair（虚拟网络设备对）实现容器内外的通信。当创建的 network namespace 被释放时，这个 namespace 中的物理网卡会返回到 root namespace，而不是创建 namespace 的父进程。 在建立好 veth pair 前，新旧 namespace 间通过管道（pipe）通信。以 docker 为例，docker daemon 在宿主进程上创建 veth pair，把一端绑定在 docker0 网桥上，另一端接入新建的 network namespace，在 veth pair 建立完成前，docker daemon 和 init 通过管道通信，直到 init 接收到 docker daemon 发送的 veth 设备信息，然后关闭管道，并启动 eth0。 代码仅需要在 mount.c 上添加标志位CLONE_NEWNET，保存为network.c int child_pid = clone(child_main, child_stack + STACK_SIZE, CLONE_NEWNET | CLONE_NEWNS | CLONE_NEWPID | CLONE_NEWIPC | CLONE_NEWUTS | SIGCHLD, NULL); 编译运行后，查看网络设备，只有一个环回口 [root@s1 network]# ./network程序开始在子进程中！[root@newnamespace network]# ifconfig[root@newnamespace network]# ip addr1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 Useruser namespace 主要隔离安全相关的标识符和属性，包括用户（组）ID、root 目录、密钥和特殊权限。user namespace 也类似 PID namespace，是树状结构。 为了能查看用户权限，需要在代码中添加内容。需要导入sys/capability.h，该包需要安装libcap-dev（debian）或libcap-devel（redhat）。 在child_main函数中添加 int child_main(void* args)&#123; cap_t caps; printf(&quot;在子进程中！\\n&quot;); sethostname(&quot;newnamespace&quot;, 12); printf(&quot;eUID=%ld, eGID=%ld\\n&quot;, (long)geteuid(), (long)getegid()); caps = cap_get_proc(); printf(&quot;capabilities: %s\\n&quot;, cap_to_text(caps, NULL)); execv(child_args[0], child_args); return 1;&#125; 并在 main 函数添加标志位CLONE_NEWUSER，并且需要删除以前的CLONE标识符，否则可能无法正常显示。 int child_pid = clone(child_main, child_stack + STACK_SIZE, CLONE_NEWUSER | SIGCHLD, NULL); 因为添加了 capability 头文件，在编译时需要添加选项-lcap。gcc -Wall -lcap userns.c -o userns root:user &gt; ./userns程序开始在子进程中！eUID=65534, eGID=65534capabilities: = cap_chown,cap_dac_override,......nobody:user &gt; 子节点的 UID 和 GID 都变为了 65534（默认的），并且用户名变为 nobody。capabilities 为子节点中该用户的权限，已经有所有权限了。65534 表示尚未与外部的 namespace 的用户（组）映射。 进行用户绑定，通过在/proc/[pid]/uid_map和/proc/[pid]/gid_map写入对应绑定信息即可。 ID-inside-ns ID-outside-ns length 注： 这两个文件仅允许拥有该 user namespace 中 CAP_SETUID 的进程写入一次，不允许修改 写入的进程必须是 user namespace 的父 namespace 或子 namespace 第一个字段ID-inside-ns表示新建的 user namespace 中对应的 UID/GID，第二个字段ID-outside-ns表示外部映射的 UID/GID，length表示映射范围，通常就填 1，表示只映射一个。 创建函数设置 UID 和 GID void set_uid_map(pid_t pid, int inside_id, int outside_id, int length)&#123; char path[256]; sprintf(path, &quot;/proc/%d/uid_map&quot;, getpid()); FILE* uid_map = fopen(path, &quot;w&quot;); fprintf(uid_map, &quot;%d %d %d&quot;, inside_id, outside_id, length); fclose(uid_map);&#125;void set_gid_map(pid_t pid, int inside_id, int outside_id, int length)&#123; char path[256]; sprintf(path, &quot;/proc/%d/gid_map&quot;, getpid()); FILE* gid_map = fopen(path, &quot;w&quot;); fprintf(gid_map, &quot;%d %d %d&quot;, inside_id, outside_id, length); fclose(gid_map);&#125; 然后在 child_main 函数中添加对这两个函数的调用 int child_main(void* args)&#123; cap_t caps; printf(&quot;在子进程中！\\n&quot;); set_uid_map(getpid(), 0, 1000, 1); set_gid_map(getpid(), 0, 1000, 1); printf(&quot;eUID=%ld, eGID=%ld\\n&quot;, (long)geteuid(), (long)getegid()); caps = cap_get_proc(); printf(&quot;capabilities: %s\\n&quot;, cap_to_text(caps, NULL)); execv(child_args[0], child_args); return 1;&#125; userns.c完整文件： #define _GNU_SOURCE#include&lt;sys/types.h&gt;#include&lt;sys/wait.h&gt;#include&lt;stdio.h&gt;#include&lt;sched.h&gt;#include&lt;signal.h&gt;#include&lt;unistd.h&gt;#include&lt;sys/capability.h&gt;#define STACK_SIZE (1024*1024)static char child_stack[STACK_SIZE];char* const child_args[] = &#123;&quot;/bin/bash&quot;, NULL&#125;;void set_uid_map(pid_t pid, int inside_id, int outside_id, int length)&#123; char path[256]; sprintf(path, &quot;/proc/%d/uid_map&quot;, getpid()); FILE* uid_map = fopen(path, &quot;w&quot;); fprintf(uid_map, &quot;%d %d %d&quot;, inside_id, outside_id, length); fclose(uid_map);&#125;void set_gid_map(pid_t pid, int inside_id, int outside_id, int length)&#123; char path[256]; sprintf(path, &quot;/proc/%d/gid_map&quot;, getpid()); FILE* gid_map = fopen(path, &quot;w&quot;); fprintf(gid_map, &quot;%d %d %d&quot;, inside_id, outside_id, length); fclose(gid_map);&#125;int child_main(void* args)&#123; cap_t caps; printf(&quot;在子进程中！\\n&quot;); // sethostname(&quot;newnamespace&quot;, 12); // printf(&quot;------------&quot;); set_uid_map(getpid(), 0, 1000, 1); set_gid_map(getpid(), 0, 995, 1); printf(&quot;eUID=%ld, eGID=%ld\\n&quot;, (long)geteuid(), (long)getegid()); caps = cap_get_proc(); printf(&quot;capabilities: %s\\n&quot;, cap_to_text(caps, NULL)); execv(child_args[0], child_args); return 1;&#125;int main()&#123; printf(&quot;程序开始\\n&quot;); int child_pid = clone(child_main, child_stack + STACK_SIZE, CLONE_NEWUSER | SIGCHLD, NULL); waitpid(child_pid, NULL, 0); printf(&quot;已退出\\n&quot;); return 0;&#125; 重新编译执行，结果如下 gutianyi:user &gt; ./userns程序开始在子进程中！eUID=0, eGID=0capabilities: = cap_chown......root:user &gt; 注：执行的用户必须是set_uid_map和set_gid_map中 outside_id 对应的用户。 Linux 内核将原来与超级用户相关的高级权限划分为不同的单元，称为 Capability。管理员可独立对特定的 Capability 进行使用或禁用。在 Docker 中同时使用了 user namespace 和 Capability，加强了容器安全性。","categories":[{"name":"Linux原理","slug":"Linux原理","permalink":"https://coconutmilktaro.top/categories/Linux%E5%8E%9F%E7%90%86/"}],"tags":[{"name":"namespace","slug":"namespace","permalink":"https://coconutmilktaro.top/tags/namespace/"}]},{"title":"在Vultr上搭建Shadowsocks记录","slug":"在Vultr上搭建Shadowsock记录","date":"2018-12-25T14:46:01.000Z","updated":"2022-05-30T02:51:53.932Z","comments":true,"path":"2018/在Vultr上搭建Shadowsock记录/","link":"","permalink":"https://coconutmilktaro.top/2018/%E5%9C%A8Vultr%E4%B8%8A%E6%90%AD%E5%BB%BAShadowsock%E8%AE%B0%E5%BD%95/","excerpt":"使用的是Vultr的云服务器，选择的节点在日本，服务器是最便宜的3.5$每月的。","text":"使用的是Vultr的云服务器，选择的节点在日本，服务器是最便宜的3.5$每月的。 在服务器页面会有提示密码，先用ssh-copy-id拷贝一下密钥，方便以后登录。 登录服务器后，下载shadowsocks脚本，并增加执行权限，然后运行，最好记录日志 wget --no-check-certificate https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.shchmod +x shadowsocks.sh./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log 在安装过程中，会依次提示输入ss的密码 Please enter password for shadowsocks-python(Default password: teddysun.com):---------------------------password = XXXX--------------------------- 输入服务器端端口号 Please enter a port for shadowsocks-python [1-65535](Default port: 12305):---------------------------port = 12305--------------------------- 输入加密算法，为了使iphone能用，选第七个aes-256-cfb Please select stream cipher for shadowsocks-python:1) aes-256-gcm2) aes-192-gcm3) aes-128-gcm4) aes-256-ctr5) aes-192-ctr6) aes-128-ctr7) aes-256-cfb8) aes-192-cfb9) aes-128-cfb10) camellia-128-cfb11) camellia-192-cfb12) camellia-256-cfb13) chacha20-ietf-poly130514) chacha20-ietf15) chacha2016) rc4-md5Which cipher you&#x27;d select(Default: aes-256-gcm):---------------------------cipher = aes-256-cfb--------------------------- 最后开始安装，安装完成后就会提示ss的信息 至此，服务器端配置完成。 在自己的主机上安装shadowsocks客户端 apt-get install shadowsocks shadowsocks-client 然后配置ss服务器信息，修改配置文件/etc/shadowsocks/config.json &#123; &quot;server&quot;:&quot;XXX&quot;, # 服务器地址 &quot;server_port&quot;:12305, # 服务器端端口 &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, # 本地端口，用于建立vpn隧道 &quot;password&quot;:&quot;XXXX&quot;, # ss服务器的密码 &quot;timeout&quot;:300, &quot;method&quot;:&quot;aes-256-cfb&quot;, # 加密算法 &quot;fast_open&quot;: false, &quot;workers&quot;: 1, &quot;prefer_ipv6&quot;: false&#125; 最后使用命令启动shadowsocks-client sshlocal -c /etc/shadowsocks/config.jsonINFO: loading config from /etc/shadowsocks/config.json2018-12-25 23:37:46 INFO loading libcrypto from libcrypto.so.1.12018-12-25 23:37:46 INFO starting local at 127.0.0.1:1080 在浏览器上设置代理，选自动检测代理设置。 然后安装插件switchyomega进行配置 代理协议一定要是sock5，因为ss只支持sock5，不支持HTTP。 代理服务器须是本地，因为是ss远端与本地建立隧道，指向的端口也是本地端口，就是ss本地的配置中指定的端口。 应用选项后即可访问外网。还可以使用BBR加速，BBR是谷歌开发的内核模块，可使用脚本一键安装，需要内核版本4.9以上。脚本会自动检测内核版本，并安装最新的内核（通过elrepo源）。注：此脚本是在服务器上执行，而不是在本主机上执行。 wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.shchmod +x bbr.sh 安装完成后查看bbr模块是否已加载 lsmod | grep bbrtcp_bbr 20480 9 至此所有配置完成，可直接上外网了。 参考文章 Vultr vps搭建属于自己的ss 代理 使用BBR一键脚本为你的CentOS/Debian/Ubuntu系统加速","categories":[],"tags":[{"name":"Vultr","slug":"Vultr","permalink":"https://coconutmilktaro.top/tags/Vultr/"},{"name":"Shadowsocks","slug":"Shadowsocks","permalink":"https://coconutmilktaro.top/tags/Shadowsocks/"}]},{"title":"MFS分布式文件系统笔记","slug":"MFS分布式文件系统笔记","date":"2018-12-06T09:13:31.000Z","updated":"2022-06-21T17:00:44.301Z","comments":true,"path":"2018/MFS分布式文件系统笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/MFS%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0/","excerpt":"分布式文件系统 MFS 概述 MFS 读过程 MFS 写过程 MFS 简单部署 启动 master mfsmaster.cfg mfsexports.cfg 启动操作 启动 metalogger 启动 chunkserver 启动 client master 宕机切换 MFS 集群内各角色的启动与停止 MFS 高可用 keepalived+DRBD 实现高可用","text":"分布式文件系统 MFS 概述 MFS 读过程 MFS 写过程 MFS 简单部署 启动 master mfsmaster.cfg mfsexports.cfg 启动操作 启动 metalogger 启动 chunkserver 启动 client master 宕机切换 MFS 集群内各角色的启动与停止 MFS 高可用 keepalived+DRBD 实现高可用 分布式文件系统Distributed File System（DFS）分布式文件系统是开放软件基金会（OSF）的分布式计算环境（DCE）中的文件系统部分，指文件系统管理的物理存储资源不一定直接连接在本地节点上，具有以下特性： 只读共享：任何客户端只能访问文件，不能修改。 受控写操作：允许多个用户打开同一个文件，但只有一个用户能进行写操作，并且该用户的修改不能立刻在其他用户上体现。 并发写操作：允许多个用户同时读写一个文件，需要操作系统进行大量监控工作防止文件重写，并保证用户看到最新消息。 传统的 DFS，如 NFS，所有的数据和元数据存放在一起，通过单一的存储服务器提供服务，这种模式称为In-band Mode，随着客户端数量增加，服务器成为整个文件系统的瓶颈。 新型的 DFS 利用存储区域网络（SAN），将应用服务器直接与存储设备连接，提升了传输能力，应用服务器直接访问 SAN 中的数据，只有关于信息的元数据才经过元数据服务器处理，减少了数据传输的中间环节，减轻了元数据服务器的负载。这种模式称为Out-band Mode。 区分两种模式的依据：元数据操作的控制信息是否与文件数据一起通过服务器转发。In-band Mode 需要服务器转发，Out-band Mode 可直接访问。 MFS 概述MooseFS 是一个具备容错功能的高可用分布式网络文件系统，MFS 将数据分散在多台服务器上，确保一份数据有多份备份，且备份分布在不同服务器上，而用户看到的只是一个资源，即对用户透明。MFS 就类似一个 unix 的文件系统，包含层级结构、文件属性等。 分层目录树结构 存储支持 POSIX 标准的文件属性 支持特殊文件，如块设备、字符设备、管道、套接字、链接文件 支持基于 IP 地址和密码的方式访问文件系统 MFS 特性： 高可靠性：一份数据多份副本 高可扩展性：轻松实现横向或纵向扩展 高可容错性：可实现回收站机制 高数据一致性：即使文件被写入或访问时，仍可以进行一致性快照 MFS 应用场景： 大规模高并发的线上数据存储及访问 大规模数据处理，如日志分析。 尽量在文件系统服务器前架设缓存服务器，而不是一味地扩容 MFS 组成： 管理服务器（managing servers）：也称 master servers，管理整个文件系统的主机，存储每个文件的元数据（大小信息、属性、文件路径） 数据存储服务器（data servers）：也称 chunk servers，存储文件并进行同步的服务器，如果确定的话，文件要做多于一份的备份。此服务器是真正的数据载体，最好组 raid5 或 raid0 或 raid10。实际环境中应该至少有三台以上。 元数据备份服务器（metadata backup servers）：也称 metalogger server，存储元数据修改日志并周期下载主元数据文件 客户端：使用mfsmount进程与管理服务器或数据存储服务器通信以获取与修改元数据信息的主机 MFS 读过程 客户端向元数据服务器（master）发出读请求。 元数据服务器把所需数据的存放位置（chunk server IP 和 chunk 编号）告知客户端 客户端向已知的 chunk server 发送请求 chunk server 向客户端发送数据。 数据传输并不经过元数据服务器，减轻了元数据服务器的压力，也增大了整个系统的吞吐能力。 MFS 写过程 客户端向元数据服务器发送写请求 元数据服务器与 chunk server 交互（只要当所需 chunks 存在才交互） a. 元数据服务器在某些服务器上创建 chunks b. chunk server 告知元数据服务器 chunks 创建成功 元数据服务器告知客户端可写的 chunk server 上的指定 chunks 客户端向指定 chunk server 写数据 chunk server 之间进行同步 chunk server 互相通知同步成功 chunk server 告知客户端写入成功 客户端告知元数据服务器写入成功 MFS 简单部署实验环境： Master：192.168.60.134 MFS VIP：192.168.60.200 Backup：192.168.60.135 Chunk1：192.168.60.136 Chunk2：192.168.60.130 Client：192.168.60.131 先要配置官方源，官网配置，版本 3.0 对 Master：yum install moosefs-master moosefs-cgi moosefs-cgiserv moosefs-cli 对 chunkserver：yum install moosefs-chunkserver 对 metalogger：yum install moosefs-metalogger 对 Client：yum install moosefs-client 启动 masterMaster 的配置文件目录/etc/mfs/ mfsexports.cfg：被挂载目录及权限控制文件 mfsmaster.cfg：master 主配置文件 mfstopology.cfg： mfsmaster.cfgWORKING_USER = mfs # 运行master的用户WORKING_GROUP = mfs # 运行master的用户组SYSLOG_IDENT = mfsmaster # master在syslog中的标识LOCK_MEMORY = 0 # 执行mlockall()以避免mfsmaster进程溢出NICE_LEVEL = -19 # 运行优先级DATA_PATH = /var/lib/mfs # 数据存放路径EXPORTS_FILENAME = /etc/mfs/mfsexports.cfg # exports文件路径TOPOLOGY_FILENAME = /etc/mfs/mfstopology.cfg #BACK_LOGS = 50 # metalogger改变的log文件数量MATOML_LISTEN_HOST = * # metalogger监听IP地址MATOML_LISTEN_PORT = 9419 # metalogger监听端口MATOCS_LISTEN_HOST = * # chunkserver监听IPMATOCS_LISTEN_PORT = 9420 # chunkserver监听端口MATOCL_LISTEN_HOST = * # 客户端监听IPMATOCL_LISTEN_PORT = 9421 # 客户端监听端口REPLICATIONS_DELAY_INIT = 60 # 延迟复制的时间 mfsexports.cfg文件格式 客户端IP地址（或范围） 挂载点 参数 其中挂载点路径有两个注意： /表示以 MFS 根为根目录 .表示以 MFSMETA 文件系统为根目录 参数可设置以下访问权限，逗号分隔多个参数： ro：只读 rw：读写 alldirs：允许挂载任何指定的子目录 maproot：映射为 root password：指定客户端密码 启动操作先在 master 上配置 VIP。ifconfig ens32:0 192.168.60.200/24 up。 在 master 上使用命令mfsmaster start即可启动主服务器。会开启三个端口 master &lt;-&gt; metaloggers module: listen on *:9419master &lt;-&gt; chunkservers module: listen on *:9420main master server module: listen on *:9421 其中 9419 用于监听 metalogger，9420 监听 chunkserver，9421 监听 master mfsmaster -c 指定mfsmaster配置文件 -f 在前端运行 -i 忽略元数据结构错误 -a 自动从更改日志记录恢复元数据 -x 显示更多的日志信息，最高-xx start 启动mfsmaster stop 停止 reload 重载配置 restart 重启 info 打印mfsmaster信息 test 测试 kill 杀死 可以使用命令mfscgiserv start启动 MFS 的图形化 web 监控，通过 9425 端口访问，该图形化 web 是用 python 写的。 启动 metalogger配置文件/etc/mfs/mfsmetalogger.cfg MASTER_HOST = mfsmaster # 可配置master的主机名（需要在/etc/hosts中配置）、IP地址、域名等指定masterMASTER_PORT = 9419 # master监听端口MASTER_RECONNECTION_DELAY = 5 # 重连延迟MASTER_TIMEOUT = 10 # 连接超时时间 启动 metalogger，mfsmetalogger start，查看端口可看到与 master 建立了长连接 ESTAB 0 0 192.168.60.135:40476 192.168.60.200:9419 users:((&quot;mfsmetalogger&quot;,pid=7735,fd=8)) mfsmetalogger命令与mfsmaster类似。 启动 chunkserver准备磁盘，分区、制作文件系统并挂载。 mount /dev/sdb1 /var/mfsdatachown -R mfs:mfs /var/mfsdata # 一定要执行，否则启动失败 chunkserver 挂载点配置文件/etc/mfs/mfshdd.cfg，只要将挂载点写入该文件即可。 echo &quot;/var/mfsdata&quot; &gt;&gt; /etc/mfs/mfshdd.cfg 参数配置文件/etc/mfs/mfschunkserver.cfg MASTER_HOST = mfsmaster # 同metalogger，要与hosts中一致MASTER_PORT = 9420 # master的连接端口CSSERV_LISTEN_HOST = * # 监听客户端IP，即指允许指定的客户端使用CSSERV_LISTEN_PORT = 9422 # 监听客户端的端口 命令mfschunkserver start启动 chunkserver。 但通过 web 查看状态，发现已经使用了 260M 左右。 因为 master 向 data 服务器申请空间是按最少 256M 申请的，低于 256M 则不会再申请空间，因此用于 MFS 的磁盘空间一定要大于 256M，并且空闲的空间一定要大于 1G 才能参与分配，即用于 MFS 磁盘的空间大小至少大于 1G，应该从几个 G 开始。 启动 clientMFS 客户端的挂载依赖于 fuse 工具，需要先安装。默认系统已安装。但需要安装内核模块modprobe fuse。客户端上也要配置hosts文件添加 mfsmaster。 创建 mfs 组与 mfs 用户，创建挂载目录 groupadd mfsuseradd -g mfs mfs -s /sbin/nologinmkdir /mnt/mfsdata 使用mfsmount挂载 mfs。mfsmount /mnt/mfsdata -H mfsmaster 挂载完成后，查看df可发现挂载成功。 客户端的/bin/中有许多 mfs 的工具，但都指向mfstools 在挂载目录中创建文件 # touch 1.conf# mfsfileinfo 1.conf # 只创建空文件并不会创建chunks1.conf: no chunks - empty file# dd if=/dev/zero of=/mnt/mfsdata/aaa count=200000# mfsfileinfo aaa # 查看文件信息，已经成功由MFS分配存储aaa: chunk 0: 0000000000000002_00000001 / (id:2 ver:1) copy 1: 192.168.60.130:9422 (status:VALID) copy 2: 192.168.60.136:9422 (status:VALID) chunk 1: 0000000000000003_00000001 / (id:3 ver:1) copy 1: 192.168.60.130:9422 (status:VALID) copy 2: 192.168.60.136:9422 (status:VALID) 在客户端的/mnt/mfsdata/中有以下目录 # lsconf ini jpg md png 创建备份mfssetgoal -r 3 conf，备份三份 conf 目录。 # mfssetgoal -r 3 confconf: inodes with goal changed: 1001 inodes with goal not changed: 0 inodes with permission denied: 0 查看文件备份情况 # mfsgetgoal confconf: 3 # 有三份备份 MFS 数据存放在 chunk 中，类似 block，数据是会分为多个 chunk 的，每个 chunk 的大小为 64M，若一个文件大于 64M，则会分为多个 chunk。 # dd if=/dev/zero of=./aaa bs=1M count=63 # 63M的文件在一个chunk中# mfsfileinfo aaaaaa: chunk 0: 0000000000000008_00000001 / (id:8 ver:1) copy 1: 192.168.60.130:9422 (status:VALID) copy 2: 192.168.60.136:9422 (status:VALID)# dd if=/dev/zero of=./bbb bs=1M count=65 # 65M的文件分为了两个chunk存储# mfsfileinfo bbbbbb: chunk 0: 000000000000000B_00000001 / (id:11 ver:1) copy 1: 192.168.60.130:9422 (status:VALID) copy 2: 192.168.60.136:9422 (status:VALID) chunk 1: 000000000000000C_00000001 / (id:12 ver:1) copy 1: 192.168.60.130:9422 (status:VALID) copy 2: 192.168.60.136:9422 (status:VALID) 查看文件删除后会在回收站里的时间 # mfsgettrashtime conf/1.confconf/1.conf: 86400 # 86400s，即一天 设置文件删除后在回收站里的时间 # mfssettrashtime -r 864000 conf/ # -r递归设置conf/: inodes with trashtime changed: 1001 挂载 mfs 回收站 # mfsmount -H mfsmaster -m /mnt/mfs-trash# ls /mnt/mfs-trashsustained trash master 宕机切换在 master 上/var/lib/mfs/中存放修改记录changelog.X.mfs和元数据记录metadata.mfs.back 在 backup 的/var/lib/mfs/中也存放着修改记录changelog_ml.X.mfs和元数据记录metadata_ml.mfs.back和metadata.mfs 若要在 backup 上恢复数据并身份转为 master，可以通过命令mfsmaster -a直接恢复。 注：mfsmetarestore命令在 1.7 版本已被废除。 MFS 集群内各角色的启动与停止规范的启动顺序： 启动 Masters 启动所有 chunk servers 启动 metalogger 挂载客户端 规范的停止顺序： 客户端卸载挂载 停止所有 chunk servers 停止 metalogger 停止 master MFS 高可用有几个解决方法： 部署多个日志备份服务器（metalogger） 使用 heartbeat 或 keepalived+DRBD 实现 master 高可用 使用 keepalived+inotify 实现 master 高可用（不推荐） keepalived+DRBD 实现高可用实验环境： Master：192.168.60.134 Backup：192.168.60.135 Chunk1：192.168.60.136 Chunk2：192.168.60.130 Client：192.168.60.131 在 Master 和 Backup 上设置 DRBD 分区，sdc 大小 2G，sdc1 有 400M 做元数据存储，剩余 sdc2 做数据存储。 设备 Boot Start End Blocks Id System/dev/sdc1 2048 821247 409600 83 Linux/dev/sdc2 821248 4194303 1686528 83 Linux 在 Master 和 Backup 上都修改 DRBD 配置，创建mfs.res resource mfsdata &#123; protocol C; on host1 &#123; device /dev/drbd0; disk /dev/sdc2; address 192.168.80.128:7789; # 心跳线 meta-disk /dev/sdc1[0]; &#125; on host2 &#123; device /dev/drbd0; disk /dev/sdc2; address 192.168.80.129:7789; meta-disk /dev/sdc1[0]; &#125;&#125; 创建 DRBD 操作 drbdadm create-md alldrbdadm up all 配置 keepalived，在两台主机上配置 global_defs &#123; # 保持默认即可 notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.60.1 smtp_connect_timeout 30 router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state MASTER interface ens32 virtual_router_id 51 priority 120 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.60.200 &#125;&#125;","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"存储","slug":"存储","permalink":"https://coconutmilktaro.top/tags/%E5%AD%98%E5%82%A8/"},{"name":"MFS","slug":"MFS","permalink":"https://coconutmilktaro.top/tags/MFS/"},{"name":"文件系统","slug":"文件系统","permalink":"https://coconutmilktaro.top/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"},{"name":"分布式","slug":"分布式","permalink":"https://coconutmilktaro.top/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"HAProxy Starter Guide翻译","slug":"HAProxy-Starter-Guide翻译","date":"2018-12-02T15:45:17.000Z","updated":"2022-05-30T02:51:53.801Z","comments":true,"path":"2018/HAProxy-Starter-Guide翻译/","link":"","permalink":"https://coconutmilktaro.top/2018/HAProxy-Starter-Guide%E7%BF%BB%E8%AF%91/","excerpt":"译者：serchaofan 翻译主要借助翻译工具，并进行校对。 根据官方 18.14 与 18.15 文档 Starter Guide的翻译，并非完整翻译，且有的地方会有修改与删除。","text":"译者：serchaofan 翻译主要借助翻译工具，并进行校对。 根据官方 18.14 与 18.15 文档 Starter Guide的翻译，并非完整翻译，且有的地方会有修改与删除。 快速介绍负载均衡和负载均衡器 HAProxy 介绍 HAProxy 是什么与不是什么 HAProxy 如何工作 基础功能 代理 SSL 监控 高可用 负载均衡 粘性 采样与转换信息 ACLS 和条件 内容转换 绑定表 格式化字符串 HTTP 重写与重定向 服务器保护 日志 统计 高级功能 管理 系统特定功能 脚本 调优 配套产品及替代产品 Apache Nginx Varnish 替代产品 快速介绍负载均衡和负载均衡器负载均衡包括聚合多个组件，以便在每个组件的单独流量上实现总处理能力，而无需客户端用户的任何干预并且是可扩展的。这导致组件在仅执行一个操作所花费的时间内同时执行更多操作。 但是，单个操作仍然会一次在单个组件上执行，并且不会比没有负载均衡时更快。 它始终需要至少与可用组件一样多的操作和有效的负载均衡机制来充分利用所有组件并从负载均衡中充分受益。一个很好的例子是高速公路上的车道数量，不增加车辆速度而允许尽可能多的车辆在同一时间段内通过。 负载均衡的示例： 多处理器系统中的进程调度 链路负载均衡（例如 EtherChannel，Bonding） IP 地址负载均衡（例如 ECMP，DNS round-robin） 服务器负载均衡（通过负载均衡器） 执行负载均衡操作的机制或组件称为负载均衡器。 在 Web 环境中，这些组件称为“网络负载均衡器”，更常见的是“负载均衡器”，因为此说法是迄今为止负载均衡的最好示例。 负载均衡器可以起作用： 在链路级别：这称为链路负载均衡，它包括选择发送数据包的网络链路 在网络级别：这称为网络负载均衡，它包括选择一系列数据包将使用的路由 在服务器级别：这称为服务器负载均衡，它包括决定哪个服务器将处理连接或请求 存在两种不同的技术并满足不同的需求，但有一些重叠。在每种情况下，重要的是要记住，负载均衡包括将流量从其自然流转移，并且这样做总是需要最少的维护，以维持所有路由决策之间所需的一致性水平。 第一个技术作用于数据包级别并且或多或少地单独处理数据包。 输入和输出数据包之间存在一对一的关系，因此可以使用常规网络嗅探器跟踪负载均衡器两侧的流量。这项技术非常便宜且速度极快。 它通常以硬件（ASIC）实现，能够达到线速（line rate），例如执行 ECMP 的交换机。 通常是无状态的，它也可以是有状态的（考虑一个数据包所属的会话并称为 layer4-LB 或 L4），如果数据包没有被修改，可以支持 DSR（直接服务器返回，不再通过 LB），但几乎不提供内容感知（content awareness）。 该技术非常适合网络级负载均衡，但有时用于高速的基本服务器负载均衡。 第二个技术作用于会话内容。 它要求输入流作为一个整体进行重组和处理。 可以修改内容，并且将输出流分段为新分组。出于这个原因，它通常由代理执行，它们通常被称为第 7 层负载均衡器或 L7。这意味着每一侧有两个不同的连接，并且输入和输出数据包大小与计数之间没有关系。客户端和服务器不需要使用相同的协议（例如 IPv4 与 IPv6，不加密与 SSL）。 操作始终是有状态的，返回流量必须通过负载均衡器。 额外的处理带来了成本，因此并不总是能够实现线速，特别是对于小数据包。 另一方面，它提供了广泛的可能性，并且通常通过纯软件实现，即使嵌入到硬件设备中也是如此。 该技术非常适合服务器负载均衡。 基于数据包的负载均衡器通常以直通模式（cut-though mode）部署，因此它们安装在流量的正常路径上，并根据配置进行转移。 返回的流量不一定通过负载均衡器。 可以对网络目的地址进行一些修改，以便将流量引导到适当的目的地。 在这种情况下，返回流量必须通过负载均衡器。如果路由不能实现这一点，则负载均衡器也可以用自己的路由替换数据包的源地址，强制返回流量通过它。 基于代理的负载均衡器部署为具有自己的 IP 地址和端口的服务器，无需更改体系结构。有时，这需要对应用程序执行一些调整，以便客户端正确定向到负载均衡器的 IP 地址，而不是直接定向到服务器。某些负载均衡器可能必须调整某些服务器的响应才能实现这一点（例如 HTTP 重定向中使用的 HTTP Location 头字段）。某些基于代理的负载均衡器可能会拦截其不拥有的地址的流量，并在连接到服务器时欺骗客户端的地址。这使得它们可以像通常的路由器或防火墙一样进行部署，采用与基于数据包的负载均衡器非常相似的直通模式。对于结合了分组模式和代理模式的产品，这是特别受欢迎的。 在这种情况下，DSR 显然仍然不可能，并且返回流量仍然必须路由回负载均衡器。 一种非常可扩展的分层方法将包括具有从多个负载均衡链路接收流量的前端路由器，并使用 ECMP 将该流量分配到多个有状态分组的负载均衡器（L4）的第一层。这些 L4 负载均衡器又将流量传递给更大数量的基于代理的负载均衡器（L7），这些负载均衡器必须解析内容以确定哪个服务器最终将接收流量。 流量的组件数量和可能的路径增加了失败的风险; 在非常大的环境中，永久性地将一些故障部件修复或替换是正常的。 在不了解整个堆栈的运行状况的情况下完成负载均衡会显着降低可用性。 出于这个原因，任何严谨的负载均衡器都将验证它打算提供流量的组件是否仍然存活且可访问，并且它将停止向故障流量提供流量。 这可以使用各种方法实现。 最常见的一种是定期发送探测包以确保组件仍可运行。 这些探测包称为“健康检查”。 它们必须代表要解决的失败类型。 例如，基于 ping 的检查不会检测到 Web 服务器已经崩溃并且不再监听端口，而与端口的连接将验证这一点，而更高级的请求甚至可以验证服务器是否仍然有效并且它依赖的数据库仍然可以访问。 健康检查通常需要进行一些重试以防止偶尔的监测错误。检查之间的时间间隔必须足够小，以确保在发生错误后故障组件的使用时间不会太长。 其他方法包括对发送到目的地的生产流量进行抽样，以观察是否正确处理，并去除返回不适当响应的组件。 然而，这需要牺牲一部分生产流量，这并不总是可以接受的。 这两种机制的组合提供了两全其美的优势，两者都用于检测故障，只有健康检查才能检测到故障的结束。 最后一种方法涉及集中报告：中央监控代理定期更新所有负载均衡器的所有组件的状态。 这为所有组件提供了基础架构的全局视图，但有时精度或响应性较低。 它最适合具有许多负载均衡器和许多服务器的环境。 第 7 层负载均衡器还面临另一个被称为粘性或持久性的挑战。 原则是它们通常必须将来自同一来源（例如最终用户）的多个后续请求或连接指向同一目标。 最着名的例子是在线商店的购物车。 如果每次点击都会导致新连接，则必须始终将用户发送到保存其购物车的服务器。 内容感知使得更容易发现请求中的某些元素以识别要将其传递到的服务器，但这并不总是足够的。 例如，如果把源地址用作选择服务器的指标，则可以确定将使用基于散列的算法，并且将始终按可用服务器的数量基于地址划分将给定的 IP 地址发送到同一服务器 。 但是如果一台服务器出现故障，结果会发生变化，所有用户都会突然被发送到另一台服务器并丢失购物车。 针对此问题的解决方案在于记录所选目标，以便每次看到相同的访问者时，无论可用服务器的数量如何，都会将其定向到同一服务器。 信息可以存储在负载均衡器的内存中，在这种情况下，如果不是单独的话，可能必须将其复制到其他负载均衡器，或者可以使用各种方法存储在客户端的内存中，前提是客户端能够随每个请求呈现该信息（cookie 插入，重定向到子域等）。 这种机制提供了额外的好处，即不必依赖不稳定或分布不均匀的信息（例如源 IP 地址）。 事实上，这是采用第 7 层负载均衡器而不是第 4 层负载均衡器的最有力理由。 为了提取诸如 cookie，主机头字段，URL 或其他信息之类的信息，负载均衡器可能需要解密 SSL/TLS 流量，甚至可能在将其传递给服务器时对其进行重新加密。 这个复杂的任务解释了为什么在一些高流量的基础设施中，有时可能会有很多负载均衡器。由于第 7 层负载均衡器可以对流量执行许多复杂的操作（解密，解析，修改，匹配 cookie，决定要发送到哪个服务器等），它肯定会造成一些麻烦，并且通常会为显露出很多麻烦而被指责。 通常会发现服务器不稳定并且周期性地停止重启，或者对于 Web 服务器，它们会传递带有一些硬编码链接的页面，迫使客户端直接连接到一个特定的服务器而不通过负载均衡器，或者他们需要很长时间才能在高负荷下做出反应，导致超时，这就是为什么日志记录是第 7 层负载均衡的一个极其重要的方面。一旦报告故障，重要的是要确定负载均衡器是否做出了错误的决定，如果是这样，如何才能不再发生这种情况。 HAProxy 介绍HAProxy 写作“HAProxy”来指定产品，而“haproxy”被指定为可执行程序，软件包或进程。 然而，两者通常用于两种目的，并且发音为 H-A-Proxy。，很早以前，“haproxy”曾经代表“高可用性代理”，这个名字用两个单独的单词写成，但现在它只不过是“HAProxy”。 HAProxy 是什么与不是什么HAProxy 是： TCP 代理：它可以接受来自侦听套接字的 TCP 连接，连接到服务器并将这些套接字连接在一起，允许流量在两个方向上流动 HTTP 反向代理（在 HTTP 术语中称为“网关”）：它将自身表示为服务器，通过侦听 TCP 套接字上接受的连接接收 HTTP 请求，并使用不同的连接将请求从这些连接传递到服务器。 SSL 终端/发起者/卸载程序（offloader）：SSL / TLS 可用于来自客户端，到服务器的连接，甚至两个连接的连接。 TCP 规范化程序（normalizer）：由于操作系统本地终止了连接，双方之间没有关系，因此无效数据包、标志组合、窗口通告（window advertisement）、序列号、不完整连接（SYN 泛洪）等异常流量不会被传递到另一边。 这可以保护脆弱的 TCP 栈免受协议攻击，并且还允许与客户端优化连接参数，而无需修改服务器的 TCP 栈设置。 HTTP 规范化程序：配置为处理 HTTP 流量时，仅传递有效的完整请求。 这可以防止许多基于协议的攻击。 另外，规范中存在容差的协议偏差是固定的，因此它们不会在服务器上引起问题（例如，多行头）。 HTTP 修复工具：它可以修改/修复/添加/删除/重写 URL 或任何请求或响应头。 这有助于解决复杂环境中的互操作性（interoperability）问题。 基于内容的交换机：它可以根据请求中的任何元素来决定将请求或连接传递给哪个服务器。 因此，可以在同一端口上处理多个协议（例如，HTTP，HTTPS，SSH）。 服务器负载均衡器：它可以对 TCP 连接和 HTTP 请求进行负载均衡。 在 TCP 模式下，对整个连接采取负载均衡决策。 在 HTTP 模式下，根据请求做出决定。 流量调节器：它可以在不同点应用一些速率限制，保护服务器免受过载，根据内容调整流量优先级，甚至通过标记数据包将这些信息传递给较低层和外部网络组件。 防止 DDoS 和服务滥用（service abuse）：它可以维护每个 IP 地址，URL，cookie 等的大量统计信息，并检测何时发生滥用，然后采取措施（减慢违规行为，阻止它们，将它们发送到过时的内容 等）。 网络故障排除的观察点：由于日志中报告的信息的精确性，它通常用于缩小网络相关问题的范围。 HTTP 压缩卸载程序：它可以压缩未被服务器压缩的响应，从而减少连接不良或使用高延迟移动网络的客户端的页面加载时间。 HAProxy 不是： 显式 HTTP 代理，即浏览器用于访问互联网的代理。 有专门用于此任务的优秀开源软件，例如 Squid。 但是，HAProxy 可以部署在这样的代理之前，以提供负载均衡和高可用性。 缓存代理：它将按原样返回从服务器收到的内容，不会干扰任何缓存策略。 有很好的开源软件可以完成这项任务，比如 Varnish。 HAProxy 可以部署在这样的缓存之前，通过智能负载均衡提供 SSL 卸载和可扩展性。 数据清理程序：它不会修改请求体和响应体。 Web 服务器：在启动期间，它将自己隔离在 chroot jail 中并删除其权限，以便一旦启动它就不会执行任何单个文件系统访问。 因此，它无法转变为 Web 服务器。 有很好的开源软件，如 Apache 或 Nginx，HAProxy 可以部署在它们前端，以提供负载均衡和高可用性。 基于数据包的负载均衡器：它不会看到 IP 数据包也不会看到 UDP 数据包，也不会执行 NAT 甚至更少的 DSR（动态源路由协议）。 这些是较低层的任务。 一些基于内核的组件（如 IPVS（Linux 虚拟服务器））已经很好地完成了这项工作并与 HAProxy 完美匹配。 HAProxy 如何工作HAProxy 是一个单线程，事件驱动的非阻塞引擎，它将非常快的 I / O 层与基于优先级的调度程序相结合。 由于它的设计考虑了数据转发目标，因此其架构经过优化，可以尽可能快地以尽可能少的操作移动数据。 因此，它实现了一个分层模型，在每个级别提供旁路机制（bypass mechanisms），确保数据不会达到更高级别，除非需要。 大多数处理都是在内核中执行的，HAProxy 尽最大努力通过提供一些提示或者在猜测它们可以在以后分组时避免某些操作来尽可能快地帮助内核完成工作。 因此，典型数据显示，在 TCP 或 HTTP 关闭模式下，HAProxy 中花费的处理时间占 15％，而在内核占 85％，在 HTTP keep-alive 模式下，HAProxy 约占 30％，而内核占 70％。 单个进程可以运行许多代理实例。根据试验，单个进程中大到 300000 个不同代理的配置运行正常。 因此，通常不需要为所有实例启动多个进程。 可以使 HAProxy 在多个进程上运行，但它有一些限制。 一般来说，它在 HTTP 关闭或 TCP 模式下没有意义，因为内核端不能很好地扩展一些操作，如connect()。 它可以很好地扩展到 HTTP 的 keep-alive 模式，但是可以通过单个进程实现的性能通常比常见的需求高出一个数量级。 但是，当用作 SSL 卸载器（offloader）时，它确实有意义，并且在多进程模式中很好地支持此功能。 HAProxy 只需要运行 haproxy 可执行文件和配置文件。 对于日志记录，强烈建议使用正确配置的 syslog 守护程序并记录日志轮换。 在启动之前解析配置文件，然后 HAProxy 尝试绑定所有侦听套接字，并在任何失败时拒绝启动。做到这一点，它就不会运行失败了。 这意味着没有运行时故障，如果它接受启动，它将一直有效，直到它停止。 一旦 HAProxy 被启动，它会做三件事： 处理传入的连接 定期检查服务器状态(称为健康检查) 与其他 haproxy 节点交换信息 处理传入连接是迄今为止最复杂的任务，因为它依赖于许多配置可能性，但它可以概括为以下 9 个步骤： 接受来自属于称为frontend的配置实体的侦听套接字的传入连接 ，引用一个或多个侦听地址 将特定于 frontend 的处理规则应用于这些可能导致阻塞它们，修改某些头或拦截它们以执行某些内部小程序（例如统计页面或 CLI）的连接 将这些传入连接传递给另一个表示称为backend的服务器池的配置实体，该服务器场包含服务器列表和此服务器池的负载均衡策略 将特定于后端的处理规则应用于这些连接 根据负载均衡策略决定将连接转发到哪个服务器 将特定于后端的处理规则应用于响应数据 将特定于前端的处理规则应用于响应数据 发出日志详细报告发生的事情 在 HTTP 中，循环回第二步以等待新请求，否则关闭连接 前端和后端有时被认为是半代理，因为它们只看端到端连接的一侧。前端只关心客户端，而后端只关心服务器。 HAProxy 还支持完全代理，它们正是前端和后端的联合。 当需要 HTTP 处理时，配置通常会分为前端和后端，因为它们会打开很多可能性，因为任何前端都可以将连接传递给任何后端。 对于仅使用 TCP 的代理，使用前端和后端很少提供好处，并且使用完整代理可以使配置更具可读性。 基础功能本节将列举 HAProxy 实现的许多功能，其中一些功能通常可以从任何现代负载均衡器中获得，其中一些功能是 HAProxy 架构的直接优势。 更多高级功能将在下一节中详细介绍。 代理代理是通过两个独立连接在客户端和服务器之间传输数据的操作。通过代理和连接管理，HAProxy 具有以下基本特性： 为服务器提供干净的连接，以防止客户端出现任何客户端缺陷或攻击 监听多个 IP 地址或端口，甚至端口范围 透明：截取任何甚至不属于本地系统的任意 IP 地址的流量 服务器端口不需要与监听端口相关，甚至可以通过固定偏移量（对范围有用）进行转换 透明连接：连接服务器时，如果需要会欺骗客户端（或任何其他主机）的 IP 地址 为多站点 LB 中的服务器提供可靠的返回 IP 地址 借助缓冲区和可能短暂的连接来卸载服务器，以减少它们的并发连接数和内存占用量 优化 TCP 栈（例如 SACK），拥塞控制和减少 RTT 影响 支持双方不同的协议系列（例如 IPv4 / IPv6 / Unix） 超时强制执行：HAProxy 支持多级别的超时，具体取决于连接的阶段，因此断开的客户端或服务器或攻击者不能长时间获得资源 协议验证：检查 HTTP，SSL 或有效负载，拒绝无效的协议元素，除非指示无论如何接受它们 策略执行：确保只转发允许的内容 传入和传出连接都可能仅限于某些网络命名空间（仅限 Linux），因此可以轻松构建跨容器，多用户负载均衡器 PROXY 协议将客户端的 IP 地址呈现给服务器，即使对于非 HTTP 流量也是如此。这是一个 HAProxy 扩展，现在被许多第三方产品采用，在撰写本文时至少有以下产品： 客户端：haproxy，stud，stunnel，exaproxy，ELB，squid 服务器：haproxy，stud，postfix，exim，nginx，squid，node.js，varnish SSLGoogle 的工程师（http://istlsfastyet.com/）认为HAProxy的SSL栈是最具特色的功能之一。 使其相当完整的最常用特性是： 基于 SNI 的多主机，不限制站点数量并专注于性能。 至少有一个部署用于运行 50000 个域及其各自的证书 对通配符证书（wildcard certificates）的支持减少了对许多证书的需求 基于证书的客户端身份验证，如果无法提供有效证书，则使用可配置策略。例如，这允许不同的服务器池重新生成客户端证书 后端服务器的身份验证确保后端服务器是真正的后端服务器而不是中间人 使用后端服务器进行身份验证让后端服务器知道它实际上是连接到它的预期 haproxy 节点 TLS NPN 和 ALPN 扩展使得可以可靠地卸载 SPDY/HTTP2 连接并以明文形式将它们传递给后端服务器 当客户端请求证书状态请求时，通过提供内联 OCSP 响应，OCSP 装订（stapling）进一步减少了首页加载时间 动态记录大小调整可提供高性能和低延迟，并且当数据包仍处于运行状态时，允许浏览器开始获取新对象，从而显着缩短页面加载时间 永久访问所有相关的 SSL/TLS 层信息，用于日志记录、访问控制、报告等。这些元素可以嵌入到 HTTP 报头中，甚至可以作为代理协议扩展，这样卸载的服务器就可以获得如果它自己执行 SSL 终止时会有的所有信息 在易受攻击的 SSL 库上检测、记录和阻止某些已知攻击，例如影响 OpenSSL 某些版本的 Heartbleed 攻击 支持无状态会话恢复（RFC 5077 TLS 故障单扩展）。可以从 CLI 更新 TLS 票证，通过频繁翻转（rotate）票证为他们提供实现 Perfect Forward Secrecy 的方法。 监控HAProxy 非常关注可用性。 因此，它关心服务器状态，并将其自身状态报告给其他网络组件： 使用每台服务器的参数持续监控服务器的状态。 这确保了服务器的路径可用于常规流量 健康检查支持两个滞后（hysteresis）的上下转换，以防止状态振荡 可以将检查发送到不同的地址/端口/协议：这样可以轻松检查被视为代表多个服务的单个服务，例如 HTTP + HTTPS 服务器的 HTTPS 端口。 服务器可以跟踪其他服务器并同时关闭：这可确保托管多个服务的服务器可以原子方式失败，并且不会将任何人发送到部分故障的服务器 可以在服务器上部署代理以监视负载和运行状况：服务器可能有兴趣报告其负载，运行状态，管理状态，而不管运行状况检查可以看到什么。通过在服务器上运行一个简单的代理，除了验证整个路径的运行状况检查外，还可以考虑服务器对自身运行状况的看法 提供各种检查方法：TCP 连接，HTTP 请求，SMTP hello，SSL hello，LDAP，SQL，Redis，send /expect 脚本，所有有/无 SSL 状态更改在日志和统计信息页面中以失败原因通知（例如，在检测到故障时收到的 HTTP 响应）。在发生此类更改时，也可以将电子邮件发送到可配置的地址 服务器状态也在统计接口上报告，并且可用于做出路由决定，以便可以根据流量大小和/或健康状况（例如，丢失 DC 间链路）将流量发送到不同的服务器场。 HAProxy 可以使用运行状况检查请求将信息传递给服务器，例如其名称，重量，服务器场中其他服务器的数量等，以便服务器可以根据这些知识调整其响应和决策（例如，推迟备份以保持更多 CPU 可用） 服务器可以使用健康检查报告更详细的状态，而不仅仅是打开/关闭（例如，我想停止，请停止发送新访问者） HAProxy 本身可以将其状态报告给外部组件，例如路由器或其他负载均衡器，从而可以构建非常完整的多路径和多层基础架构。 高可用就像任何负载均衡器一样，HAProxy 非常重视可用性，以确保最佳的全局服务持续性： 仅使用有效的服务器，其他的被自动从负载均衡服务器群中剔除，在某些条件下，仍有可能强制使用它们 支持优雅关闭，以便可以在不影响任何连接的情况下将服务器从服务器群中剔除 当活跃服务器关闭时自动使用备份服务器并替换它们，以便在可能的情况下不会丢失会话。 这还允许构建多个路径以到达相同的服务器（例如，多个接口） 当服务器过多时，能够返回服务器群的全局故障状态。 这与监视功能相结合，使上游组件可以为给定的服务选择不同的 LB 节点 无状态设计使构建集群变得容易：通过设计，HAProxy 尽最大努力确保最高的服务持续性，而无需存储在发生故障时可能丢失的信息。 这确保了接管是最无缝的。 与标准 VRRP 守护程序保持良好集成：HAProxy 告知 keepalived 其状态，并与浮动虚拟 IP 地址很好地对应。 注意：仅使用基于集群的解决方案（Heartbeat，…）的 IP 冗余协议（VRRP/CARP），因为它们是提供最快，最无缝和最可靠切换的解决方案。 负载均衡HAProxy 提供了一套相当完整的负载均衡功能，其中大多数功能在许多其他负载均衡产品中是不支持的： 支持不少于 9 种负载均衡算法，其中一些适用于输入数据，以提供无限的可能性列表。 最常见的是 round-robin（用于短连接，依次选择每个服务器），leastconn（用于长连接，选择最近最少使用的具有最低连接数的服务器），source（用于 SSL 服务器群或终端服务器群，服务器直接依赖于客户端的源地址），uri（对于 HTTP 缓存，服务器直接依赖于 HTTP URI），hdr（服务器直接依赖于特定 HTTP 头字段的内容），first（对于短期虚拟机，所有连接都打包在最小的服务器子集上，以便可以关闭未使用的服务器） 以上所有算法都支持服务器权重，以便可以适应服务器群中不同的级别的服务器，或者将一小部分流量引导到特定服务器（调试模式，运行下一版本的软件等） 支持动态权重的轮询、最小控制和一致哈希，这允许从 CLI 动态修改服务器权重，甚至允许服务器上运行的代理修改服务器权重 只要支持动态权重，就支持慢启动，这允许服务器逐步获取流量。 这是脆弱的应用程序服务器的一个重要特性，它需要在运行时编译类以及需要在全速运行之前填满的冷缓存（cold caches） 散列可以应用于各种元素，如客户端的源地址，URL 组件，查询字符串元素，报文头字段值，POST 参数，RDP cookie 在服务器群中添加或删除服务器时，一致性哈希（consitent hashing）可保护服务器群免受大量重新分发的影响。 这在大型缓存群中非常重要，它允许使用慢启动来重新填充冷缓存 许多内部指标，例如每个服务器、每个后端的连接数，后端中可用连接插槽的数量等，可以构建非常先进的负载均衡策略。 粘性如果没有粘性（stickness），应用程序负载均衡将毫无用处。 HAProxy 提供了一套相当全面的可能性，可以将访问者维持在同一台服务器上，甚至可以跨越各种事件，例如服务器添加/删除，下线/上线周期，并且某些方法可以克服多个负载均衡节点之间的距离并不需要任何复制： 如果需要，粘性信息可以单独匹配并从不同的地方学习。 例如，JSESSIONID cookie 可以在 cookie 和 URL 中匹配。 可以同时学习多达 8 个并行源，每个源可以指向不同的绑定表（stick-table） 粘性信息可以来自请求或响应中可以看到的任何内容，包括源地址，TCP 有效负载偏移和长度，HTTP 查询字符串元素，报头字段值，cookie 等 以多主方式在所有节点之间复制绑定表 常用的元素，如 SSL-ID 或 RDP cookie（用于 TSE 群）可直接访问，以方便操作 所有粘性规则都可以由 ACL 动态调节 可以决定不绑定某些服务器，例如备份服务器。这样当名义上的（nominal）服务器返回集群时，它会自动恢复负载。 这通常用于多路径环境 在 HTTP 中，通常不会学习任何东西，而是操纵专用于粘性的 cookie。 为此，可以检测，重写，插入或添加这样的 cookie，让客户端记住分配了哪个服务器 服务器可以决定在注销时更改或清除粘性 cookie，以便离开的访问者自动从服务器解除绑定 使用基于 ACL 的规则，无论服务器的状态如何，都可以选择性地忽略或强制粘性，结合高级健康检查，帮助管理员验证他们正在安装的服务器是否正常运行，再对外服务 在 cookie 上设置最大空闲时间（maximum idle time）和持续时间（duration）的机制可确保在永不关闭的设备（智能手机，电视，家用电器）上顺利停止粘性，而无需将其存储在持久存储上 多个服务器条目可以共享相同的粘性键，以便在一个路径发生故障时多路径环境中不会丢失粘性 软停止（soft-stop）确保只有具有粘性信息的用户才能继续访问他们已被分配到的服务器，但新用户不能被分配到那些服务器 采样与转换信息HAProxy 支持使用大量“采样函数”进行信息采样。 原则是提取称为样本的信息，以便立即使用。 这用于粘性，构建条件，在日志中生成信息或丰富 HTTP 头。 可以从各种来源获取样本： 常量：整数，字符串，IP 地址，二进制块 进程：日期，环境变量，服务器/前端/后端/进程状态，字节/连接计数/速率，队列长度，随机生成器，… 变量：每个会话，每个请求，每个响应变量 客户端连接：源和目标地址和端口，以及所有相关的统计计数器 SSL 客户端会话：协议，版本，算法，密码，密钥大小，会话 ID，所有客户端和服务器证书字段，证书序列，SNI，ALPN，NPN，某些扩展的客户端支持 请求和响应缓冲区内容：偏移/长度的任意有效载荷，数据长度，RDP cookie，SSL hello 类型的解码，TLS SNI 的解码 HTTP（请求和响应）：方法，URI，路径，查询字符串参数，状态代码，报头值，位置报头值，cookie，捕获，身份验证，正文元素 然后，样本可以通过许多称为“转换器”的运算符来实现一些转换。 转换器消耗样本并生成新样本，可能是完全不同的类型。 例如，转换器可以用于仅返回输入字符串的整数长度，或者可以将字符串转换为大写。 在最终使用之前，可以将任意数量的转换器串联应用于样品。 在所有可用的样本转换器中，以下是最常用的： 算术和逻辑运算符：它们可以对输入数据执行高级计算，例如计算比率，百分比或简单地从一个单元转换为另一个单元 当某些地址需要通过较大的网络进行分组时，IP 地址掩码非常有用 数据表示：URL 解码，base64，十六进制，JSON 字符串，散列 字符串转换：提取固定位置的子串，固定长度，提取某些分隔符周围的特定字段，提取某些单词，更改大小写，应用基于正则表达式的替换 日期转换：转换为 HTTP 日期格式，将本地转换为 UTC，反之，添加或删除偏移量 查找绑定表（stick table）中的条目以查找统计信息或分配的服务器 通过文件（主要用于定位）的基于映射的键值转换 映射映射是一种强大的转换器类型，包括在引导时将两列文件加载到内存中，然后查找第一列中的每个输入样本，并在找到条目时返回第二列上的相应模式，或者返回默认值。 输出信息也是一个样本，它可以反过来进行其他转换，包括其他映射查找。 映射最常用于将客户端的 IP 地址转换为 AS 号或国家/地区代码，因为它们支持网络地址的最长匹配，但它们还可用于各种其他目的。它们的部分优势来自于可以通过 CLI 或使用其他样本的某些操作进行快速更新，使它们能够在后续访问到来前完成存储和检索信息。 另一个优势来自基于二叉树的索引，即使它们包含数十万个条目，它们也非常快，使得位置定位非常方便且易于设置。 ACLS 和条件HAProxy 中的大多数操作都可以是有条件的。 通过使用逻辑运算符（AND，OR，NOT）组合多个 ACL 来构建条件。 每个 ACL 都是基于以下元素的一系列测试： 用于检索要测试的元素的示例获取方法 一系列可选的转换元件 要匹配的模式列表 一种匹配方法，用于指示如何将模式与样本进行比较 例如，可以从 HTTP“主机”头部获取样本，然后可以将其转换为小写，然后使用正则表达式匹配方法与多个正则表达式模式进行匹配。 从技术上讲，ACL 与映射构建在同一个核心上，它们共享完全相同的内部结构，模式匹配方法和性能。 唯一真正的区别是，ACL 只返回“找到”或“未找到”，而不是返回样本。 在使用方面，ACL 模式可以在配置文件中内联声明，并且不需要自己的文件。 可以命名 ACL 以便于使用或使配置易于理解。 命名 ACL 可以多次声明，它将依次评估所有定义，直到匹配为止。 提供了大约 13 种不同的模式匹配方法，其中包括 IP 地址掩码，整数范围，子串，正则表达式。 它们像函数一样工作，就像使用任何编程语言一样，只评估所需的内容，因此当涉及 OR 的条件已经为真时，不会评估下一个条件，并且当涉及 AND 的条件已经为假时，其余的条件则无需评估。 声明的 ACL 的数量没有实际限制，并且提供了少数常用的 ACL。 但是经验表明，使用大量命名 ACL 的设置很难排除故障，并且有时使用内联匿名 ACL 更容易，因为它需要更少的分析范围之外的引用。 内容转换HAProxy 实现了一种称为基于内容的转换机制。 原则是连接或请求到达前端，然后处理此请求或连接携带的信息，此时可以编写基于 ACL 的条件，利用这些信息来决定哪些后端处理请求。 因此，根据请求的内容将流量引导到一个后端或其他后端。 最常见的示例包括使用路径中的 Host 头和 / 或元素（子目录或文件扩展名）来确定 HTTP 请求是针对静态对象还是应用程序，以及将静态对象流量路由到后端快速轻量的服务器，以及所有剩余流量路由到更复杂的应用服务器，从而构成了一个细粒度的虚拟主机解决方案。 这可以方便地使多种技术作为更全面的解决方案共存。 内容交换的另一个用例包括根据各种标准使用不同的负载均衡算法。 缓存可以使用 URI 哈希，而应用程序将使用循环法（round-robin）。 最后，它允许多个客户通过强制执行每个后端（因此按客户连接限制）来使用一小部分公共资源。 内容转换规则可以很好地扩展，但其性能可能取决于所使用的 ACL 的数量和复杂性。 但是，也可以编写动态内容转换规则，其中样本值直接变为后端名称，而根本不使用 ACL。 据报道，这种配置在生产中至少有 300000 个后端工作正常。 绑定表绑定表（stick table）通常用于存储粘性信息，即保持对某个访问者所指向的服务器的引用。 然后，密钥是与访问者关联的标识符（其源地址，连接的 SSL ID，HTTP 或 RDP cookie，从 URL 或有效负载中提取的客户编号，…），然后存储的值为服务器的标识符。 绑定表可以使用 3 种不同类型的样本作为其键：整数，字符串和地址。 代理中只能引用一个绑定表，并且在任何地方都使用代理名称指定它。 最多可以并行跟踪 8 个键。 一旦密钥和服务器都已知，就在请求或响应处理期间提交服务器标识符。 绑定表内容可以在主主（active-active）模式下与其他 HAProxy 节点（称为“对等节点（peer）”）以及重新加载操作期间的新进程一起复制，以便如果客户机的请求分布在多个节点上，则所有负载均衡节点共享相同的信息并做出相同的路由决策。 由于绑定表是基于允许识别客户端的索引，因此它们通常还用于存储额外信息，例如每个客户端的统计信息。 额外的统计信息需要一些额外的空间，需要明确声明。 可以存储的统计类型包括输入和输出带宽，并发连接数，一段时间内的连接速率和计数，错误的数量和频率，一些特定的标签和计数器等。为了支持保持这些信息在不被强制绑定到给定服务器，它实现了一个特殊的“跟踪”特性，允许同时跟踪来自不同表的 3 个键，而不考虑粘性规则。 可以从 CLI 搜索，转储和清除每个存储的统计信息，并添加到实时故障排除功能。 虽然这种机制可以用来代表返回的访问者或根据好的或坏的行为来调整提供的服务质量，但它主要用于对抗服务滥用（service abuse），更常见的是 DDoS，因为它允许构建复杂的模型，以高处理速度检测某些不良行为。 格式化字符串HAProxy 需要处理字符串的许多地方，例如日志，重定向，添加报头等。 为了提供最大的灵活性，引入了格式化字符串的概念，最初用于记录目的，这解释了为什么它仍称为“日志格式”。 这些字符串包含转义字符，允许将各种动态数据（包括变量和样本提取表达式）引入字符串，甚至在结果转换为字符串时调整编码（例如，添加引号）。 这提供了一种构建报头内容或自定义日志行的强大方法。 此外，为了保持构建大多数常见字符串的简单性，提供了大约 50 个特殊标记作为日志中常用信息的捷径。 HTTP 重写与重定向如果没有合适的工具，在从未为此设计的应用程序前面安装负载均衡器可能是一项具有挑战性的任务。 在这种情况下，最常请求的操作之一是调整请求和响应头，以使负载均衡器显示为源服务器并修复硬编码信息。 这需要更改请求中的路径（强烈建议不要这样做），修改主机头字段，修改重定向的位置响应头字段，修改 cookie 的路径和域属性等。 还有一些服务器有些冗长，往往会在响应中泄漏太多信息，使它们更容易受到针对性攻击。 虽然理论上讲负载均衡器并不能解决这个问题，但实际上它位于基础设施中最好的位置，以保证一切都被清理干净。 同样，有时负载均衡器必须拦截某些请求，并通过重定向到新的目标 URL 进行响应。 虽然有些人往往混淆重定向和重写，但这些是两个完全不同的概念，因为重写使客户端和服务器看到不同的东西（并且访问页面的位置不一致），而重定向则要求客户端访问新的 URL，以便它看到与服务器相同的位置。 为此，HAProxy 支持各种重写和重定向的可能性，其中包括： 请求和响应中基于正则表达式的 URL 和报头重写。 正则表达式是最常用的修改报头值的工具，因为它们易于操作和易于理解 也可以根据格式化字符串附加，删除或替换报头，以便传递信息（例如客户端 TLS 算法和密码） HTTP 重定向可以使任何 3xx 代码重定向到相对，绝对或完全动态（格式化的字符串）的 URI HTTP 重定向还支持一些额外的选项，例如设置或清除特定 cookie，删除查询字符串，如果缺少则附加斜杠等 所有操作都支持基于 ACL 的条件 服务器保护HAProxy 可以最大限度地提高服务可用性，为此，需要付出巨大努力来保护服务器免受过载和攻击。 第一个也是最重要的一点是，只有完整有效的请求才会被转发到服务器， 最初的原因是 HAProxy 需要找到它与字节流保持同步所需的协议元素，第二个原因是在请求完成之前，无法知道某些元素是否会改变其语义。 这样做的直接好处是服务器不会暴露于无效或不完整的请求。 这是一种非常有效的防止慢速逃逸攻击（slowloris attacks）的保护措施，对 HAProxy 几乎没有任何影响。 另一个重要的点是，HAProxy 包含用于存储请求和响应的缓冲区，并且仅在服务器完成时向服务器发送请求，并通过从本地网络快速读取整个响应，服务器端的连接只在短时间内被使用，这将尽可能地保留服务器资源。 对此的直接扩展是 HAProxy 可以人为地限制并发连接的数量或服务器未完成的请求，这保证了服务器永远不会过载，即使它在流量高峰期间不断以 100％的容量运行。当一个插槽被释放时，所有多余的请求将被排队等待处理。 最后，这种巨大的资源节省通常可以确保更好的服务器响应时间，最终实际上比通过重载服务器更快。排队的请求可能被重新分配到其他服务器，甚至在客户端中止时在队列中中止，这也保护服务器免受“重新加载效应（reload effect）”的影响，即访问者在缓慢加载的页面上每次点击“重新加载”通常会导致新请求，使服务器维持在过载状态。 慢启动机制还可以在服务器仍在完成启动或编译某些类时保护服务器不受高流量水平的影响。 关于协议级保护，可以放宽 HTTP 解析器以接受非标准兼容但无害的请求或响应，甚至修复它们。这允许在开发修复程序时访问伪造应用程序。 同时，使用详细报告完全捕获有问题的消息，该报告可帮助开发人员发现应用程序中的问题。最危险的协议违规会被正确检测和处理并修复。 例如，如果值完全相同，则具有两个 Content-length 头的格式错误的请求或响应将被修复，或者如果它们不同则被拒绝，因为它会成为安全问题。 协议检查不仅限于 HTTP，它也可用于其他协议，如 TLS 或 RDP。 当检测到协议违规或攻击时，有多种选项可以响应用户，例如返回常见的“HTTP 400 bad request”，使用 TCP 重置关闭连接，或者在长时间延迟后伪造错误（“tarpit”）迷惑攻击者。 所有这些都有助于通过阻止违规客户端进行维护成本非常高的攻击来保护服务器。 HAProxy 还提出了一些更高级的选项来防止意外数据泄漏和会话交叉（session crossing）。 它不仅可以记录可疑的服务器响应，还会记录并可选地阻止可能影响指定访问者机密性的响应。 一个这样的示例，可缓存的响应出现在可缓存的 cookie 中，可以导致中间缓存将其传递给另一个访问者，从而导致意外的会话共享。 日志对于负载均衡器来说，日志记录是一个非常重要的功能，首先是因为负载均衡器经常被错误地指责导致了它显现的问题，其次是因为它被放置在需要分析所有正常和异常活动的基础设施中的关键点并与其他组件相关联。 HAProxy 提供非常详细的日志，具有毫秒精度，和可在防火墙日志中搜索的确切连接接受时间（例如，用于 NAT 关联）。 默认情况下，TCP 和 HTTP 日志非常详细，包含故障排除所需的所有内容，例如源 IP 地址和端口，前端，后端，服务器，计时器（请求接收持续时间，队列持续时间，连接建立时间，响应报头时间，数据传输时间），全局进程状态，连接计数，队列状态，重试次数，详细的粘性操作和断开连接原因，带有安全输出编码的报头捕获（header captures）。然后可以扩展或替换此格式以包括任何采样数据，变量，捕获，从而产生非常详细的信息。 例如，可以记录客户端访问的累积请求数或不同 URL。 可以使用标准 ACL 根据请求调整日志级别，因此可以自动静默一些脏日志，不会在一小部分流量发生某些异常行为时引发警告（例如，过多的 URL 或一个源地址的 HTTP 错误）。 管理日志也以其自己的级别发出，以通知服务器的丢失或恢复。 每个前端和后端可以使用多个独立的日志输出，这可以简化多租户。 日志优选地通过 UDP 发送，可能是 JSON 编码的，并且在可配置的线长度（line length）之后被截断以便保证传送。 统计HAProxy 提供基于 Web 的统计报告界面，其中包含身份验证，安全级别和范围。 因此，可以为每个托管客户（hosted customer）提供他自己的页面，仅显示他自己的实例。 此页面可以位于常规网站的隐藏 URL 部分，因此不需要打开新端口。 此页面还可以报告其他 HAProxy 节点的可用性，以便一眼就能看出是否一切正常。 视图是合成的，可以访问许多详细信息（例如错误原因，上次访问和上次更改持续时间等），这些也可以作为 CSV 表访问，其他工具可以导入以绘制图形。 该页面可以自刷新以用作大显示器上的监视页面。 在管理模式下，该页面还允许更改服务器状态以简化维护操作。 高级功能管理HAProxy 旨在在常规生产环境中保持极其稳定和安全的管理。它是作为一个单独的可执行文件提供的，不需要任何安装过程。 多个版本可以轻松共存，这意味着可以（并推荐）按重要性顺序逐步升级实例，而不是一次性迁移所有实例。配置文件很容易进行版本控制。配置检查是离线完成的，因此不需要重新启动可能失败的服务。在配置检查期间，可以检测到许多高级错误（例如隐藏另一个错误的规则，或者不起作用的粘性），并且提出详细的警告和配置提示来修复它们。向后配置文件的兼容性非常及时，版本 1.5 仍然完全支持 13 年前编写的 1.1 版本的配置，而 1.6 仅删除对几乎未使用的过时关键字的支持，这些关键字可以采用不同的方式。配置和软件升级机制平稳且无中断，因为它允许新旧进程在系统上共存，每个进程都处理自己的连接。 启动时会报告系统状态，构建选项和库兼容性。 一些高级功能允许应用程序管理员顺利停止服务器，检测服务器上何时没有活动，然后将其脱机，停止，升级并确保在升级时不会占用任何流量，然后再次通过正常路径对其进行测试并且不向外部开放服务，所有这一切都没有触及 HAProxy。 这确保了在开放时间内可以利用所有可用的技术资源进行复杂的生产操作。 进程试图尽可能地节约资源，使用内存池来节省分配时间并限制内存碎片，一旦发送内容就释放有效负载缓冲区，并支持强制执行强内存限制，超过该限制，连接必须等待缓冲区变为可用，而不是分配更多内存。 该系统有助于保证在某些严格的环境中使用内存。 命令行界面（CLI）可用作 UNIX 或 TCP 套接字，以执行许多操作并检索故障排除信息。 在此套接字上完成的所有操作都不需要更改配置，因此它主要用于临时更改。 使用此接口可以更改服务器的地址、权重和状态，查询统计信息和清除计数器，转储和清除粘性表，可能有选择的关键标准，转储和终止客户端和服务器端连接，转储捕获的错误，详细分析错误的确切原因和位置，转储、添加和删除 ACL 和映射中的条目，更新 TLS 共享密钥，立即将连接限制和速率限制应用于任意前端（在共享托管环境中很有用），并禁用特定前端以释放监听端口（在禁止白天操作且仍需要修复时非常有用）。 对于必须使用 SNMP 的环境，至少存在两个代理，一个代理随 HAProxy 源提供，并依赖于 Net-SNMP Perl 模块。 另一个是商业包提供的，不需要 Perl。 两者在覆盖范围方面大致相同。 通常建议在部署 HAProxy 的机器上安装 4 个实用程序： socat（为了连接到 CLI，虽然 netcat 的某些分支也可以在一定程度上做到这一点） 来自最新 HAProxy 版本的 halog：这是日志分析工具，它可以非常快速地解析本机 TCP 和 HTTP 日志（每秒 1 到 2GB）并提取有用的信息和统计信息，例如每个 URL 的请求，每个源地址，已根据响应时间或错误率排序的 URL ，终止代码等。它是为了在生产服务器上部署帮助解决问题，所以必须准备使用 tcpdump：强烈建议您使用所需的网络跟踪来解决日志中可见的问题。 有一段时间，应用程序和 haproxy 的分析会发生分歧，网络跟踪是判断谁对谁错的唯一方法。 由于 tcpdump，在网络堆栈和管理程序中检测 bug 也相当常见。 strace：这是 tcpdump 的附件。 它将报告 HAProxy 真正看到的内容，并将帮助从 HAProxy 负责的问题中找出操作系统负责的问题。 当怀疑 HAProxy 中存在错误时，通常会要求 Strace 系统特定功能根据部署的 HAProxy 操作系统，可能会提供或需要某些额外功能。 虽然它在许多平台上都受支持，但 HAProxy 主要是在 Linux 上开发的，这解释了为什么某些功能仅在此平台上可用。 透明绑定和连接功能，对绑定连接到特定网络接口的支持，以及将多个进程绑定到相同 IP 地址和端口的功能仅在 Linux 和 BSD 系统上可用，虽然只有 Linux 对可用进程之间的传入请求执行内核端负载均衡。 在 Linux 上，还有许多额外的功能和优化，包括支持网络命名空间（也称为“容器”），允许 HAProxy 成为所有容器之间的网关，能够在客户端连接上设置 MSS，Netfilter 标记和 IP TOS 字段，在监听端支持 TCP FastOpen，TCP 用户超时（user timeouts）让内核在检测到客户端在配置的超时时间之前消失时快速终止连接，TCP 拼接（splicing）让内核在连接的两端之间转发数据，从而避免多个内存副本，启用“defer-accept”绑定选项的能力只有在内核缓冲区中数据可用时才会收到传入连接的通知，并且能够通过 ACK 确认连接发送请求（有时称为“背驮式（piggy-back）”），通过使用“tcp-smart-connect”选项启用。 在 Linux 上，HAProxy 还非常注意操纵 TCP 延迟的 ACK，以便在网络上保存尽可能多的数据包。 有些系统有一个不可靠的时钟，它在过去和将来会来回跳跃。这种情况曾经发生在一些 NUMA 系统中，其中多个处理器没有观察到完全相同的时间，并且最近它在虚拟化环境中变得更加普遍，其中虚拟时钟与真实时钟无关，导致巨大的时间跳跃（有时已观察到长达 30 秒）。这通常会导致很多关于超时执行的麻烦。 由于这些系统的缺陷，HAProxy 保持其自己的单调时钟，该时钟基于系统的时钟，但是会测量和补偿时钟漂移。这确保即使系统时钟非常糟糕，定时器仍然保持相当准确，并且超时也可以继续工作。 请注意，此问题会影响在此类系统上运行的所有软件，并非特定于 HAProxy。常见的影响是虚假超时（spurious timeouts）或应用冻结（application freezes）。 因此，如果在系统上检测到此行为，则必须修复此行为，而不管 HAProxy 是否保护自己不受其影响。 脚本HAProxy 可以构建为支持 Lua 嵌入式语言，以实现请求或响应的复杂操作，路由决策，统计处理等相关的广泛的新功能。 使用 Lua 甚至可以与其他服务器建立并行连接以交换信息。 这样，例如使开发认证系统变得可能（尽管很复杂）。 有关如何使用 Lua 的更多信息，请参阅文档“doc/lua-api/index.rst”。 调优典型的 CPU 使用率数据显示，在 TCP 或 HTTP 关闭模式下，HAProxy 中花费的处理时间占 15％，而在内核占 85％，在 HTTP keep-alive 模式下，HAProxy 约占 30％，而内核占 70％。 这意味着操作系统及其调优对全局性能有很大影响。 用户之间的用途差异很大，一些用于带宽，另一些用于请求率，其他用于连接并发或用于 SSL 性能。 本节旨在提供一些帮助完成此任务的要素。 重要的是要记住，每个操作都带有开销，因此每个单独的操作都会增加其他操作的开销，这在某些情况下可以忽略不计，并且在其他情况下可能占主导地位。 在处理来自连接的请求时，我们可以这样说： 转发数据的开销低于解析请求或响应报头 解析请求或响应头的成本低于建立然后关闭与服务器的连接 建立关闭连接开销低于 TLS 恢复操作 TLS 恢复操作的成本低于完整的需要密钥计算的 TLS 握手 空闲连接比缓冲区保存数据的连接消耗更少的 CPU 资源 TLS 上下文比与数据连接的内存开销更高 因此在实践中，处理有效负载字节比头字节开销更少，因此使用大对象（每个卷单元的请求很少）比使用小对象（每个卷单元的请求很多）更容易实现高网络带宽。 这解释了为什么始终使用大对象测量最大带宽，而使用小对象测量请求率或连接速率。 某些操作可以在分布在多个 CPU 上的多个进程中很好地扩展，而其他操作不能扩展。网络带宽不会扩展到很远，因为 CPU 资源不足是大对象的瓶颈，到达网络接口的主要是网络带宽和数据总线。由于在处理本地端口表时系统中有一些锁，所以连接速率在多个处理器上不能很好地扩展。持久连接上的请求速率伸缩性非常好，因为它不涉及太多内存和网络带宽，也不需要访问锁定结构。TLS 密钥计算非常好，因为它完全受 CPU 限制。TLS 恢复规模适度，但在大约 4 个进程时达到其极限，其中访问共享表的开销抵消了从更大的功耗中获得的小收益。 人们可以从一个非常好的调优系统中得到的性能数字在以下范围内。重要的是将它们作为数量级，并根据处理器、IRQ 设置、内存类型、网络接口类型、操作系统调优等预期任何方向都会发生显着变化。 在运行 3.7 GHz 的 Core i7 上发现了以下数字，配备了运行 Linux 内核 3.10，HAProxy 1.6 和 OpenSSL 1.0.2 的双端口 10 Gbps 网卡。 HAProxy 在单个专用 CPU 内核上作为单个进程运行，另外两个内核专用于网络中断： 对于 256kB 或更高的对象，明文的 20Gbps 最大网络带宽；对于 41kB 或更高的对象，为 10Gbps 使用带有大型对象的 AES256-GCM 密码的 4.6 Gbps TLS 流量 从客户端到服务器每秒 83000 个 TCP 连接 从客户端到服务器每秒 82000 个 HTTP 连接 服务器关闭（server-close）模式下每秒 97000 个 HTTP 请求（与客户端保持活跃状态，与服务器关闭） 端到端保持（end-to-end keep-alive）模式下每秒 243000 个 HTTP 请求 每秒 300000 个过滤的 TCP 连接（反 DDoS） 在持久 TLS 连接上的 keep-alive 模式下每秒 160000 个 HTTPS 请求 使用 TLS 恢复连接，每秒 13100 个 HTTPS 请求 使用与 RSA2048 重新协商的 TLS 连接，每秒 1300 个 HTTPS 连接 每 GB 内存 20000 个并发饱和连接，包括系统缓冲区所需的内存，通过仔细调整可以做得更好，但这很容易实现。 每 GB 内存大约 8000 个并发 TLS 连接（仅客户端），包括系统缓冲区所需的内存 每 GB 内存大约 5000 个并发端到端 TLS 连接（双方），包括系统缓冲区所需的内存 因此，要记住的一个好的经验法则是请求率是 TLS 保持活动和 TLS 恢复之间数除以 10，或 TLS 恢复和 TLS 重新协商之间的数除以 10，或 HTTP keepalive 和 HTTP close 之间的值除以 3。 另一个是，带有 AES 指令的高频核心可以为每个核心提供大约 5 Gbps 的 AES-GCM。 拥有更多内核并不会有用（TLS 除外），并且由于频率较低而甚至会适得其反。 通常，少量但高频的核心更好。 另一个好的经验法则是考虑在同一台服务器上，HAProxy 将能够饱和： 大约 5-10 个静态文件服务器或缓存代理 约 100 个反病毒代理 大约 100-1000 个应用服务器，取决于所使用的技术 配套产品及替代产品HAProxy 与下面列出的某些产品集成得相当好，这就是为什么在这里提到它们，即使它们与 HAProxy 没有直接关系。 ApacheApache 是一个实际的标准 HTTP 服务器。 这是一个非常完整的模块化项目，支持文件服务和动态内容。 它可以作为某些应用程序服务器的前端， 甚至可以代理请求和缓存响应。 在所有这些使用案例中，通常需要前负载均衡器。 Apache 可以在各种模式下工作，有些模式比其他模式更繁重。 某些模块仍然需要较繁重的预分叉（pre-forked）模型，并且会阻止 Apache 通过大量连接进行良好的扩展。 在这种情况下，HAProxy 可以通过将每个服务器的连接限制强制设置为一个安全值来提供极大的帮助，并且可以显著提高服务器的速度，并保存应用程序可以更好地使用的资源。 Apache 可以使用“mod_rpaf”扩展名从 X-Forwarded-For 报头中提取客户端的地址。 当在其配置中指定“option forwardfor”时，HAProxy 将自动提供此报头。 当暴露于互联网时，HAProxy 也可以为 Apache 提供良好的保护，它可以更好地抵御各种类型的 DoS 攻击。 NginxNGINX 是第二个实际的标准 HTTP 服务器。 就像 Apache 一样，它涵盖了广泛的功能。 NGINX 建立在与 HAProxy 类似的模型上，因此处理数以万计的并发连接没有问题。 当用作某些应用程序的网关时（例如使用内含的 PHP FPM），设置一些前端连接限制以减少 PHP 应用程序的负载通常是有益的。 HAProxy 作为常规负载均衡器和流量调节器显然都非常有用，可以通过解除拥塞来加速 PHP。 此外，由于它们的事件驱动架构，两种产品都使用非常少的 CPU，因此通常很容易在同一系统上安装它们。 NGINX 实现了 HAProxy 的 PROXY 协议，因此 HAProxy 能很容易地将客户端的连接信息传递给 NGINX，以便应用程序获取所有相关信息。 一些基准测试还表明，对于大型静态文件服务，在 NGINX 前面的 HAProxy 上实现一致的哈希可以通过优化操作系统的缓存命中率(基本上是乘服务器节点的数量)来实现。 VarnishVarnish 是一种智能缓存反向代理，可能最好的描述是 Web 应用程序加速器。 Varnish 没有实现 SSL/TLS，并希望将其所有 CPU 周期专用于最佳功能。 Varnish 还实现了 HAProxy 的 PROXY 协议，因此 HAProxy 可以很容易地作为 SSL 卸载程序和负载均衡器部署在 Varnish 前面，并将所有相关的客户端信息传递给它。此外，当服务器提供压缩对象时，Varnish 自然支持从缓存中解压缩，但是不会压缩。 然后，当后端服务器不实现压缩时，可以使用 HAProxy 压缩传出数据，除非流量较小，否则在负载均衡器上压缩并不是一个好主意。 在跨多个节点构建大型缓存集群时，HAProxy 可以使用一致的 URL 哈希来智能地将负载分配给缓存节点，避免缓存重复，从而得到一个总缓存大小，即所有缓存节点的总和。 替代产品Linux 虚拟服务器（LVS 或 IPVS）是 Linux 内核中包含的第 4 层负载均衡器。 它在数据包级别工作并处理 TCP 和 UDP。 在大多数情况下，它更多的是补充而不是替代，因为它根本没有第 7 层知识。 Pound 是另一个著名的负载均衡器。它比 HAProxy 简单得多，功能也少得多，但对于许多非常基本的设置，都可以使用这两种方法。它的作者总是首先关注代码的可审计性，并希望保持少量的特性。它的基于线程的体系结构在连接计数高的情况下伸缩性较差，但它是一个很好的产品。 Pen 是一款非常轻的负载均衡器。它支持 SSL，使用客户机 IP 地址的固定大小表维护持久性。它支持面向包的模式，允许它在一定程度上支持直接服务器返回和 UDP。它适用于小负载（持久性表只有 2048 个条目）。 NGINX 可以在某种程度上进行一些负载均衡，尽管它显然不是它的主要功能。 生产流量用于检测服务器故障，负载均衡算法更受限制，并且粘性非常有限。 但是在它已经存在的一些简单部署场景中它是有意义的。 好处是，由于它与 HAProxy 的集成非常好，因此在达到其限制后添加 HAProxy 没有任何问题。 Varnish 还对其后台服务器进行了一些负载均衡，并支持真正的健康检查。但是它并没有实现粘性，所以就像 NGINX 一样，只要不需要粘性，这就足够了。同样，由于 HAProxy 和 Varnish 集成得非常好，所以很容易在以后将其混用以补充功能集。","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"HAProxy","slug":"HAProxy","permalink":"https://coconutmilktaro.top/tags/HAProxy/"},{"name":"翻译","slug":"翻译","permalink":"https://coconutmilktaro.top/tags/%E7%BF%BB%E8%AF%91/"}]},{"title":"NFS高可用笔记","slug":"NFS高可用笔记","date":"2018-11-27T00:27:27.000Z","updated":"2022-05-30T02:51:53.849Z","comments":true,"path":"2018/NFS高可用笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/NFS%E9%AB%98%E5%8F%AF%E7%94%A8%E7%AC%94%E8%AE%B0/","excerpt":"NFS 高可用概述 NFS+Keepalived+DRBD 搭建","text":"NFS 高可用概述 NFS+Keepalived+DRBD 搭建 NFS 高可用概述NFS 的物理磁盘往往做 RAID10 或 RAID0，再通过 DRBD 同步 NFS，以及设置 VIP 进行主从热备。若主 NFS 宕机后，默认情况在几秒内，新的主 NFS 就可以启动同步程序自动把数据同步到所有从 NFS 中。 NFS+Keepalived+DRBD 搭建实验环境： host1：NFS-Master ens32：192.168.60.134 ens34：192.168.70.128（Heartbeat 线网卡） ens35：192.168.80.128（DRBD 线网卡） sdb：2G（NFS 存数据） host2：NFS-Backup ens32：192.168.60.135 ens34：192.168.70.129（Heartbeat 线网卡） ens35：192.168.80.129（DRBD 线网卡） sdb：2G（NFS 存数据） host3：NFS-Slave ens32：192.168.60.136 首先在 host1 和 host2 上安装 Cluster Glue，cluster-glue 下载 解压后，进入目录，执行autogen.sh，生成configure文件 安装可能需要的依赖： yum install -y glib2-devel libxml2-devel bzip2-devel flex-devel bison-devel OpenIPMI-devel net-snmp-devel ipmitool ipmiutil ipmiutil-devel asciidoc 编译安装 ./configure --prefix=/usr/local/glue \\ LIBS=&#x27;/lib64/libuuid.so.1&#x27; # 一定要指定否则会报错 然后make &amp;&amp; make install 下载 heartbeat 源码包，下载地址，解压进入目录执行./bootstrap生成configure文件，开始构建 ./configure --prefix=/usr/local/heartbeat \\ CFLAGS=-l/usr/local/heartbeat/include -L/usr/local/heartbeat/lib 编译安装 keepalived。先检查依赖libnl-devel与libnl3-devel ./configure --prefix=/usr/local/keepalived \\ --bindir=/usr/bin \\ --sbindir=/usr/sbin \\ --sysconfdir=/etc 首先配置分区一个是 400M，用于存放元数据，一个是 1.6G，用于存放业务数据。 安装 DRBD，可通过 yum 安装，但要先安装 elrepo 源，下载地址，安装完成后，查看该 repo 文件，将enabled设为 1。 安装 DRBD，先yum search 查看要安装的版本，安装 9 的版本需要安装drbd90-utils和kmod-drbd90。 导入 drbd 内核modprobe drbd，再查看lsmod | grep drbd # lsmod | grep drbddrbd 541356 0libcrc32c 12644 2 xfs,drbd drbd 模块导入成功。 编辑/etc/drbd.d/nfs.res配置，两台主机上要一致。 resource data &#123; protocol C; on host1 &#123; device /dev/drbd1; disk /dev/sdb2; address 192.168.80.128:7789; # 心跳线 meta-disk /dev/sdb1[0]; &#125; on host2 &#123; device /dev/drbd1; disk /dev/sdb2; address 192.168.80.129:7789; meta-disk /dev/sdb1[0]; &#125;&#125; 创建 drbd 磁盘drbdadm create-md data并启动drbdadm up data，两台都要执行。 查看是否正常启动，以下为 drbd9 的显示，与 8 版本不同。 # cat /proc/drbdversion: 9.0.14-1 (api:2/proto:86-113)GIT-hash: 62f906cf44ef02a30ce0c148fec223b40c51c533 build by mockbuild@, 2018-05-04 03:32:42Transports (api:16): tcp (9.0.14-1) 使 host1 变为角色变为 primary。drbdadm primary data --force 在 host1 上挂载mount /dev/drbd1 /var/drbd1，并在其中创建文件。 一定要在设置了 primary 角色后才挂载，否则挂载不上。 然后 host1 卸载挂载，并角色转为 secondary umount /var/drbd1drbdadm secondary data 在 host2 上转变角色为 primary，并挂载 drbdadm primary datamount /dev/drbd1 /var/drbd1 进入目录后查看，能看到在 host1 上创建的文件，同步实现。实验完后切换回来，仍然是 host1 为 primary，host2 为 secondary。 安装 NFS 服务，在两台主机上yum install nfs-utils rpcbind systemctl start nfs-server.service rpcbind.service应该是先启动 rpcbind、再启动 nfs-server。 在两台 host 上修改 nfs 配置文件/etc/exports /var/drbd1 192.168.60.*(rw,sync) 命令exportfs -r重新导入/etc/exports中的目录。 使用showmount -e 192.168.60.134查看是否配置成功 # showmount -e 192.168.60.134Export list for 192.168.60.134:/var/drbd1 192.168.60.* 开启一台同网段客户端验证。先安装nfs-utils和rpcbind 执行mount -t nfs 192.168.60.134:/var/drbd1 /var/drbd1 查看该目录，里面有创建的文件，NFS 获取成功。 错误解决参考文章： http://doc.okbase.net/zhangjie830621/archive/79242.html https://www.linuxidc.com/Linux/2012-11/73620.htm http://www.bubuko.com/infodetail-1848309.html","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"高可用","slug":"高可用","permalink":"https://coconutmilktaro.top/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"},{"name":"nfs","slug":"nfs","permalink":"https://coconutmilktaro.top/tags/nfs/"}]},{"title":"LDAP部署与维护","slug":"LDAP部署与维护","date":"2018-11-24T01:22:47.000Z","updated":"2022-06-21T15:16:21.207Z","comments":false,"path":"2018/LDAP部署与维护/","link":"","permalink":"https://coconutmilktaro.top/2018/LDAP%E9%83%A8%E7%BD%B2%E4%B8%8E%E7%BB%B4%E6%8A%A4/","excerpt":"","text":"目录服务目录是一类为了浏览和搜索数据而设计的特殊数据库，按照树状形式存储信息，目的包含基于属性的描述性信息，支持高级过滤功能。 目录不支持大多数事务型数据库支持的高吞吐量和复杂更新操作。目录服务适合的业务应用于大量查询与搜索操作，而不是大量写入操作。目录服务器还能提供主从服务器同步目录数据功能。 DNS 就是一个典型的大范围分布式目录服务 目录服务器功能： 按照网络管理员指令，强制实施安全策略，保证目录信息安全 目录数据库可分布在一个网络的多台计算机上，提高响应速度 复制目录，使更多用户可使用目录，提高可靠性和稳定性 将目录划分为多个数据源（存储区），以存储大量对象 X.500X.500 是构成全球分布式的名录服务系统的协议，描述了用户信息的存储和访问方式，X.500 不是一个协议，而是一个协议族，包括从 X.501 到 X.525 等一系列完整的目录服务协议。X.500 采用层次结构（类似 DNS）。 X.500 相关协议或机制： DAP 目录访问协议：控制服务器和客户端之间的通信 DSA 目录系统代理：用于存储目录信息的数据库，采用分层格式，提供快速高效的搜索功能，与目录信息树 DIT 连接。DSA 之间通过链操作实现分布式访问。 DUA 目录用户代理：用于访问一个或多个 DSA 的用户接口程序 DSP 目录系统协议：控制两个或多个 DSA 间、DUA 与 DSA 间的交互 DISP 目录信息映像协议：定义了如何将选定的信息在服务器间进行复制 DOP 目录操作绑定协议：定义了服务器间自动协商连接配置的机制 X.501：模型定义，定义目录服务的基本模型和概念 X.509：认证框架，定义如何处理目录服务中客户和服务器的认证 X.511：抽象服务定义，定义 X.500 提供的服务原语 X.518：分布式操作过程定义，定义如何跨平台处理目录服务 X.520：定义属性类型和数据元素 X.521：定义了对象类 X.525：定义在多个服务器间的复制操作 X.530：定义目录管理系统的使用 X.500 的 OSI 模型： 独立的目录信息树 DIT，采用层级化节点结构，每个节点都有一个唯一名称标识 DN（也称唯一辨别名），唯一名称由相对唯一名称 RDN（也称相对辨别名）标识和父节点标识组成。 X.500 特征： 分散维护：运行 X.500 的每个站点只负责本地目录，可立刻进行更新和维护 搜索性能：X.500 有强大的搜索功能，支持用户建立复杂查询 单一全局命名空间：为用户提供单一同性命名空间（Single Homogeneous Namespace），类似 DNS，但比 DNS 灵活且可扩展。 结构化信息结构：目录中定义了信息结构，允许本地扩展 基于标准的目录服务：请求应用目录信息的应用（邮件、资源分配器等）能访问重要且有价值的信息 基于 OSI 协议，需要在会话层和表示层进行许多连接的建立和包处理 LDAPLDAP 就是活动目录在 Linux 上的一个实现。 LDAP（Lightweight Directory Access Protocol）轻量目录访问协议，基于 X.500 标准，支持 TCP/IP。 LDAP 支持一主多从、多主多从以及分布式。 DAP 是一个重量级的协议，在整个 OSI 协议栈上操作，需要占用大量计算资源，而 LDAP 设计在 TCP/IP 上，以小得多的资源消耗实现了大多数 DAP 功能。LDAP 服务器可当做网关访问 X.500 服务器，但基本都是在 X.500 服务器上直接实现 LDAP。 单独的 LDAP 守护程序 slapd，可看做是轻量级 X.500 服务器。LDAP 就是轻量级的 DAP。 LDAP 与 X.500 的相同点： 都实现了通用的平台结构 信息模型上，都使用了项、对象类、属性等概念描述信息 命名空间上，都使用了目录信息树结构和层次命名模型 功能模型上，都使用了相似操作命令 认证框架上，都实现了用户名密码、安全加密认证 灵活性上，目录规模都可大可小 分布性上，目录信息都可分布在多个目录服务器上，服务器由各组织管理，保证目录信息总体结构一致 LDAP 常用名词： dc：Domain Component，域名部分，会将完整域名分成几部分。如 example.com 会分为 dc=example,dc=com uid：User id，用户 ID。最常见的 uid 是从/etc/passwd中转来的条目。 ou：Organization Unit，组织单位，类似文件系统的子目录，是一个容器对象，可包含其他各种对象。 cn：Common Name，公共名称。最常见的 cn 是从/etc/group中转来的条目 sn：Surname，姓 dn：Distinguished Name，唯一辨别名，类似文件系统的绝对路径，每个对象都有一个唯一名称。 rdn：Relative dn：相对辨别名，类似文件系统的相对路径，是与目录树结构无关的部分。 c：Country，国家。如 CN、US o：Organization，组织。如 Inc LDAP 目录结构与信息： 目录数据库以目录信息树 DIT 为存储方式的。 DIT 由条目（Entry）构成，条目相当于关系数据库中的表的记录。 条目又由 DN 的键值对（Attribute-Value）组成 LDAP 允许通过 objectClass 来控制哪些属性是条目必须的，objectClass 的值是条目必须遵从的方案 schema 定义的。 LDAP 目录的根称为 BaseDN ou 下是真正的用户条目 LDAP 数据导入导出的格式是 LDIF，LDIF 是 LDAP 数据库信息的一种文本格式。 LDAP 特点总结： 跨平台 树型结构，无需 SQL 语句维护 静态数据快速查询 使用基于推拉的复制信息技术 支持多种安全协议（SASL、SSL、TLS）和多种访问控制 支持异类数据存储（存储文件可以是文本或图片） C/S 模型 LDAP 常见应用： 数字证书管理、授权管理、单点登录 分布式计算环境 DCE、统一描述发现与集成协议 UDDI MAIL、DNS、网络用户管理、电话号码簿 内网组织信息服务、电子政务目录、人口基础库 LDAP 目录数据文件： LDIF（LDAP Data Interchange Format，轻量级目录交换格式）是一种 ASCII 文件格式，能够向目录导入与修改信息。 LDIF 文件注意点： 空行分割一个条目或定义 #注释 属性: 属性值 属性可被重复赋值 每行结尾不能有空格 每条记录必须有至少一个 objectClass 属性 LDAP 的配置模式 基本的目录查询服务：slapd 仅为本地域提供目录服务，不会以任何方式与别的目录服务器交互。 目录查询代理服务：带有指针（Refferals），类似 DNS，若本地的 LDAP 无法处理，则会返回一个指针，指向更高级的服务器地址。 异机复制数据，即主从同步：LDAP 的 slurpd 守护进程是用于将 slapd 上的改变传播到一个或多个从的 slapd 上。可以通过 inotify+rsync 方案实现简单的同步。 LDAP Docker部署使用的镜像为：osixia/openldap 官方Github仓库：https://github.com/osixia/docker-openldap 启动命令： docker run -d --name ldap-service \\ --hostname ldap-service \\ -p 389:389 -p 689:689 \\ -v /data/openldap/database:/var/lib/ldap \\ -v /data/openldap/config:/etc/ldap/slapd.d \\ --env LDAP_ORGANISATION=&quot;公司名&quot; \\ --env LDAP_DOMAIN=&quot;公司域名&quot; \\ --env LDAP_ADMIN_PASSWORD=&quot;admin账号的密码&quot; \\ --env LDAP_TLS=false \\ --detach osixia/openldap:stable 注：启动前需要先建目录/data/openldap，这个是ldap数据目录，需要定期备份。LDAP_ORGANISATION 指定的是公司名，若不填就是默认值Example Inc.LDAP_DOMAIN 指定的是公司域名，若不填则是默认值example.org。该值会转化为namingContexts的值dc=xxx,dc=xxx，例如example.org会转为dc=example,dc=org，也就是DN中的后缀。 该容器启动后开放了389和689两个端口，即OpenLDAP监听的端口： 389：默认监听端口，是传输明文数据的 636：加密监听端口，默认启动时不开启，是传输密文数据的 常用的ldap管理界面，是由php编写的，同样可通过容器启动。 官方Github仓库：https://github.com/osixia/docker-phpLDAPadmin docker run --name phpldapadmin-service \\ -p 6443:443 -p 6680:80 \\ --hostname phpldapadmin-service \\ --link ldap-service \\ --env PHPLDAPADMIN_LDAP_HOSTS=ldap-service \\ --env PHPLDAPADMIN_HTTPS=false \\ --detach osixia/phpldapadmin:stable 注：–link 指定的是上面启动的ldap的容器名，将两个容器连接PHPLDAPADMIN_LDAP_HOSTS环境指定的也是ldap的容器名，这个环境变量指定的就是LDAP服务器的域名或IP，docker容器间网络便能使用容器名访问。 启动完成后就能通过6680端口访问，admin用户登录名就是cn=admin,dc=xxx,dc=xxx 一般的目录层级为： ou=group|- ou=某某大部门 |- ou=某某子部门ou=people|- cn=用户 或者只留一个ou=people下面直接建用户就行，连group都不用。 配套的还有个小工具，自助密码修改服务，也是用Docker部署。 官方Github仓库：https://github.com/ltb-project/self-service-password 启动前创建/data/self-service-password目录，并需要在该目录中创建一个php文件，填写ldap服务的连接配置。 docker run -p 6681:80 \\ -v /data/self-service-password/config.inc.local.php:/var/www/conf/config.inc.local.php \\ -itd ltbproject/self-service-password:latest 以下为 &lt;?php$ldap_url = &quot;ldap://LDAP服务器地址&quot;;$ldap_starttls = false;$ldap_binddn = &quot;cn=admin,dc=xxx,dc=xxx&quot;;$ldap_bindpw = &#x27;admin密码&#x27;;$ldap_base = &quot;dc=xxx,dc=xxx&quot;;$ldap_login_attribute = &quot;cn&quot;;$ldap_fullname_attribute = &quot;cn&quot;;$keyphrase = &quot;xxxxx&quot;;$use_tokens = false;?&gt; keyphrase可以是随机字符串，用于加密，不过不要随意变动 LDAP 对接其他系统Gitlab公司的gitlab版本为14.9，支持以下写法 gitlab_rails[&#x27;ldap_enabled&#x27;] = truegitlab_rails[&#x27;prevent_ldap_sign_in&#x27;] = false###! **remember to close this block with &#x27;EOS&#x27; below**gitlab_rails[&#x27;ldap_servers&#x27;] = YAML.load &lt;&lt;-&#x27;EOS&#x27; main: # &#x27;main&#x27; is the GitLab &#x27;provider ID&#x27; of this LDAP server label: &#x27;LDAP&#x27; host: &#x27;填写LDAP服务器IP&#x27; port: 389 uid: &#x27;uid&#x27; bind_dn: &#x27;cn=admin,dc=xxx,dc=xxx&#x27; password: &#x27;填写admin密码&#x27; encryption: &#x27;plain&#x27; # &quot;start_tls&quot; or &quot;simple_tls&quot; or &quot;plain&quot; verify_certificates: true# smartcard_auth: false active_directory: false allow_username_or_email_login: false# lowercase_usernames: false# block_auto_created_users: false base: &#x27;dc=xxx,dc=xxx&#x27; attributes: username: [&#x27;uid&#x27;, &#x27;mail&#x27;] email: [&#x27;mail&#x27;]EOS gitlab重启后再访问就多了LDAP登录的方式 Jenkins需要先确保LDAP插件安装完成在全局安全配置中的安全域指定LDAP，并填入LDAP连接信息 YearningYearning SQL审计平台也支持LDAP登录，在设置页面，填入LDAP信息，并重启yearning服务（一定要重启，不然不生效）","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"LDAP","slug":"LDAP","permalink":"https://coconutmilktaro.top/tags/LDAP/"},{"name":"验证","slug":"验证","permalink":"https://coconutmilktaro.top/tags/%E9%AA%8C%E8%AF%81/"}]},{"title":"SaltStack学习笔记","slug":"SaltStack学习笔记","date":"2018-11-16T08:09:09.000Z","updated":"2022-06-21T15:16:21.219Z","comments":true,"path":"2018/SaltStack学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/SaltStack%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"Salt 概述 SaltStack 架构 SaltStack 安装部署 Master 迁移 returner syslog mysql event 分组 常用模块 archive cmd cp cron dnsutil file network pkg service status saltutil state user group partition system pillar nginx test 配置管理 使用模板 grains pillar syndic Job Schedule Salt SSH Salt 概述Salt 是一个配置管理系统，能够维护预定义状态的远程节点，是一个分布式远程执行系统，用来在远程节点上执行命令和查询数据。Salt 基于 Python 开发，提供大量 python 接口。底层使用 ZeroMQ 消息队列 pub/sub 方式通信。采用 RSA key 认证身份。 Salt 的核心功能 使命令发送到远程系统是并行的而不是串行的 使用安全加密的协议 使用最小最快的网络载荷 提供简单的编程接口 Salt 同样引入了更加细致化的领域控制系统来远程执行，使得系统成为目标不止可以通过主机名，还可以通过系统属性。 SaltStack 是围绕 Salt 开发的一系列技术栈，具有三大功能： 远程管理（Remote Execution） 配置管理（Config Management） 云管理（Cloud Management） SaltStack 架构SaltStack 基于 C/S 架构，服务器端称为 Master，客户端称为 Minion。中间件使用的是 ZeroMQ。 Salt 的三种运行方式：1. Local 本地 2. Master/Minion（C/S 架构） 3. Salt SSH（无客户端） SaltStack 安装部署先去saltstack 官网下载源，选系统以及 python 版本对应的源，安装后再安装 salt 的所有部件 salt # salt主程序salt-api # salt的REST APIsalt-cloud # salt云配置器salt-minion # salt的客户端部件salt-master # salt的管理部件salt-ssh # salt的无客户端版本，使用的是ssh通信salt-syndic # salt的master-of-master组件 安装完成后，使用systemctl start salt-master启动 salt 服务器端，同时会自动开启两个端口4505和4506。 4505：salt 的消息发布专用端口 4506：服务器与客户端通信的端口 可以通过salt-master命令管理 salt-master。 -c # 指定配置文件--saltfile # 指定saltfile路径，若不指定，会在当前目录查找-u # 指定运行用户-d # 后台运行--pid-file # 指定pid文件--log-level # 指定日志等级（控制台）--log-file # 指定日志文件--log-file-level # 指定日志等级（日志文件） 在客户端上，只要安装salt-minion即可。安装完成后启动systemctl start salt-minion即可。 启动进程后，无论是 salt 的哪个部件，都会在/var/log/salt/中创建一个日志文件，名字就是组件名，如master、minion。 日志的默认级别为 warning，可通过 master 配置文件的log_level参数配置 在客户端配置文件中，找到参数master，配置的是服务器端的主机标识，默认叫salt，于是修改/etc/hosts，添加服务器端 IP 和标识 salt。 注意：客户端和服务器的防火墙一定要方形 4505 和 4506 端口，否则公钥无法传递。 在服务端查看日志，说明已经开始进行认证了，但此时认证没有通过。 [salt.transport.mixins.auth][INFO ][32699] Authentication request from s4[salt.transport.mixins.auth][INFO ][32699] New public key for s4 placed in pending[salt.transport.mixins.auth][INFO ][32708] Authentication request from s3[salt.transport.mixins.auth][INFO ][32708] New public key for s3 placed in pending 在服务端上salt-key -L查看公钥 # salt-key -LAccepted Keys:Denied Keys:Unaccepted Keys:s3s4Rejected Keys: 添加这两个密钥即可salt-key -a s3和salt-key -a s4即可 在添加前，两个未认证的密钥是存放在/etc/salt/pki/master/minions_pre目录下的，在认证后，就会转移到minions目录下，并且客户端的/etc/salt/pki/minion目录中会生成 master 公钥minion_master.pub。 salt-key命令 -L # 列出所有公钥-a # 允许指定的公钥-A # 允许所有公钥-r # 拒绝指定公钥-R # 拒绝所有公钥-d # 删除指定公钥-D # 删除所有公钥 注：当客户端启动 salt-minion 时，会自动将主机名写入到/etc/salt/minion_id中，并且还会生成/etc/salt/pki/中的密钥等参数，如果修改了主机名，一定要将该minion_id和pki目录都删除，然后重启 salt-minion。而且还要在 master 上删除该主机原来的 key，重新接受新的 key，再重启服务。一旦服务端与客户端的 key 不一致，客户端会自动停止 minion 进程。 测试是否与客户端正常通信 # salt &quot;*&quot; test.ping # &#x27;*&#x27;是正则表达式，匹配所有通过认证的客户端s4: Trues3: True 可以通过salt &#39;*&#39; sys.doc查看可用函数，或通过网页查看 salt 命令匹配主机的常用参数 -E # 正则匹配-L # 以主机id为列表匹配-G # 根据主机的grains信息匹配-I # 根据主机的pillar信息匹配-N # 根据master的配置文件中分组匹配-C # 根据条件运算符or\\not\\and匹配-S # 根据主机IP或子网匹配-b # 设置操作的minion的个数，可设置数字，或百分比（对于所有minion） master 配置文件常用配置参数 interface: 0.0.0.0 # 网卡绑定的地址，一般保持默认即可publish_port: 4505 # 发布端口，默认4505user: root # 启动用户ret_port: 4506 # 接收消息的端口，默认4506max_open_files: 100000 # 最大同时打开文件数，尽量设大些worker_threads: 5 # 启动的线程数，不得小于3cachedir: /var/cache/salt/master # 存储任务和缓存的目录keep_jobs: 24 # 执行命令结果的缓存时间timeout: 5 # salt命令或API的连接超时时间job_cache: True # master是否缓存任务执行结果，若管理主机超过5000台，最好换其他方式存储。 minion 配置文件常用配置参数 master: salt # 设置master，可以是fqdn，或IP地址retry_dns: 30 # 使用dns解析域名前等待的时间master_port: 4506 # 向master发送结果信息的端口cache_jobs: False # 是否在本地缓存执行结果，默认不缓存，因为结果都发往masterbackup_mode: minion # 当文件改变时会对该文件备份 Master 迁移首先在原先的 Master 上将/etc/salt/pki目录打包tar -cf pki.tar /etc/salt/pki 将该 tar 包传到新的 Master 上的/etc/salt/中，然后解压。 在原 Master 上执行操作，更改 minion 客户端上的 hosts 文件。先查看一下客户端上原配置，然后修改为新的 MasterIP 地址。 # salt &quot;*&quot; cmd.run &#x27;grep salt /etc/hosts&#x27;s3: 172.16.246.131 salts4: 172.16.246.131 salt# salt &#x27;*&#x27; cmd.run &quot;sed -i &#x27;s/172.16.246.131/172.16.246.133/g&#x27; /etc/hosts&quot;s4:s3:# salt &#x27;*&#x27; cmd.run &quot;grep salt /etc/hosts&quot;s4: 172.16.246.133 salts3: 172.16.246.133 salt 然后仍在原 master 上执行命令，重启客户端的 salt-minion # salt &#x27;*&#x27; service.restart salt-minions3: Trues4: True 执行完成后，原 Master 已经无法对客户端操作了，在新的 Master 上测试。确保新 Master 上将两台主机的密钥接受了。使用salt &#39;*&#39; test.ping，操作成功，Master 迁移完成。 returnersalt 客户端通过 returner 接口，向服务器端返回数据。在服务器端的 salt 命令可以添加参数--return决定将返回的数据存储在哪。returner 列表 syslog将数据返回到主机操作系统的 syslog 工具。必需的 python 模块：syslog，json syslog returner 只是重用操作系统的 syslog 工具来记录返回数据。 salt &#39;*&#39; network.interfaces --return syslog 在客户端上查看/var/log/messages，可看到信息返回为 json 格式。 s4 /salt-minion: &#123;&quot;return&quot;: &#123;&quot;ens33&quot;: &#123;&quot;up&quot;: true, &quot;hwaddr&quot;: &quot;00:0c:29:15:5f:47&quot;, &quot;inet6&quot;: [&#123;&quot;scope&quot;: &quot;link&quot;, &quot;address&quot;: &quot;fe80::905f:45b9:8486:c538&quot;, &quot;prefixlen&quot;: &quot;64&quot;&#125;], &quot;inet&quot;: [&#123;&quot;label&quot;: &quot;ens33&quot;, &quot;netmask&quot;: &quot;255.255.255.0&quot;, &quot;address&quot;: &quot;172.16.246.136&quot;, &quot;broadcast&quot;: &quot;172.16.246.255&quot;&#125;]&#125;, &quot;lo&quot;: &#123;&quot;up&quot;: true, &quot;hwaddr&quot;: &quot;00:00:00:00:00:00&quot;, &quot;inet6&quot;: [&#123;&quot;scope&quot;: &quot;host&quot;, &quot;address&quot;: &quot;::1&quot;, &quot;prefixlen&quot;: &quot;128&quot;&#125;], &quot;inet&quot;: [&#123;&quot;label&quot;: &quot;lo&quot;, &quot;netmask&quot;: &quot;255.0.0.0&quot;, &quot;address&quot;: &quot;127.0.0.1&quot;, &quot;broadcast&quot;: null&#125;]&#125;&#125;, &quot;jid&quot;: &quot;20181117111540048278&quot;, &quot;success&quot;: true, &quot;id&quot;: &quot;s4&quot;, &quot;fun&quot;: &quot;network.interfaces&quot;, &quot;retcode&quot;: 0, &quot;fun_args&quot;: []&#125; mysql将数据返回到 mysql 中。需要服务器端有pymysql模块，客户端有 python 的 mysql 客户端模块。 master 和 minion 中有关于 returner 的配置，默认包含 mysql，但要启用仍然要配置。在 minion 的配置文件取消return: mysql注释，并添加以下参数 mysql.host: &#x27;172.16.246.133&#x27;mysql.user: &#x27;salt&#x27;mysql.pass: &#x27;salt&#x27;mysql.db: &#x27;salt&#x27;mysql.port: &#x27;3306&#x27; 由于每台服务器都要和 mysql 连接，会使得 mysql 服务器的压力很大，在实际环境中不会这样调用。 eventSalt Event System 是一个本地的 ZeroMQ pub interface，用于触发事件，使第三方应用程序或外部进程能够对 Salt 内的行为做出反应，发送信息通知 salt 或其他操作系统。 事件系统由两个主要组件组成： 发布事件的事件套接字（event sockets）。 事件库（event library）可以监听事件并将事件发送到 salt 系统。 每个 event 都有一个标签，事件标签允许快速置顶过滤事件，且每个 event 都有一个数据结构，是一个 dict 类型，包含事件的信息。 分组master 配置文件中nodegroups块用于设置分组。也可以在master.d/中创建独立的 nodegroups 配置文件。 #nodegroups:# group1: &#x27;L@foo.domain.com,bar.domain.com,baz.domain.com or bl*.domain.com&#x27;# group2: &#x27;G@os:Debian and foo.domain.com&#x27;# group3: &#x27;G@os:Debian and N@group1&#x27;# group4: # 列表写法，等同于 G@foo:bar or G@foo: baz# - &#x27;G@foo:bar&#x27;# - &#x27;or&#x27;# - &#x27;G@foo:baz&#x27; 分组可设置的匹配规则 Letter 含义 例 G Grains glob 匹配 G@os: Ubuntu E PCRE minion id 匹配 `E@web\\d+.(dev P Grains PCRE 匹配 `P@os: (RedHat L minions 列表 L@minion1, minion2 I Pillar glob 匹配 I@pdata: foobar S 子网/IP 匹配 S@192.168.1.0/24 or S@192.168.1.100 R Range Cluster 匹配 R@foo.bar D Minion Data 匹配 D@key: value C Compound 匹配（可匹配多种上面的匹配规则，称为混搭匹配） G@os: Ubuntu and I@pdata: foobar or web* 常用模块可使用sys.list_modules列出所有可用模块，可使用sys.doc查看指定模块的用法 archive gunzip：解压 gzip gzip：gzip 压缩 rar：rar 压缩 unrar：rar 解压 unzip：zip 解压 zip：zip 压缩 tar：打包 salt &#x27;*&#x27; archive.gzip /tmp/file.gz /root/a.ymlsalt &#x27;*&#x27; archive.gunzip /tmp/file.gz /root/ cmd run：运行命令 salt &#x27;*&#x27; cmd.run &#x27;free -m&#x27; script：执行脚本 cp get_dir： cache_file： cache_files： cache_local_file： cache_master： get_file： get_file_str： get_url： crondnsutilfilenetworkpkg主机程序安装管理，能根据主机的系统使用不同的包管理工具。 install：安装软件 remove：卸载软件 upgrade：升级软件 refresh_db：检查 repos service主机服务管理 enable：开机自启 disable：开机不自启 reload：重载配置 restart：重启 start：启动 stop：停止 status：状态 statussaltutilstateuser `` grouppartitionsystempillarnginxtest配置管理配置管理（Configuration Management），也称组态管理，是一个建立系统工程的过程，用来建立和维持一个产品，使该产品的效能、功能及物理特性在生命周期中都保持稳定和一致性。Salt 的配置描述文件称为 sls 文件（Salt State）。 State 结构： Top 文件，配置管理的入口文件，默认为top.sls。 sls 的模块使用点分割。如salt://apache/install.sls或salt://apache/install/init.sls都可用apache.install表示。 在 top.sls 中若指定了 apache，则在执行时会查找 state 根目录下 apache 目录中的init.sls，若找不到则找根目录下的apache.sls sls 文件间可用include或extend引用或扩展。 sls 中 ID 必须唯一，ID 为 state 的名称。 states 模块列表 在 master 上查看配置文件/etc/salt/master中file_roots参数配置 file_roots: base: # 基础版，一般只要base版就行 - /srv/salt/ # 指定salt文件的根目录，需要先手动创建 dev: # 开发版 - /srv/salt/dev/services - /srv/salt/dev/states prod: # 生产版 - /srv/salt/prod/services - /srv/salt/prod/states 在配置中还有一个state_top参数，Salt 在执行自定义 sls 配置时会根据该参数指定的 sls 文件（默认为top.sls）中的定义查找要执行的文件 首先在/srv/salt/中创建一个 sls 文件top.sls base: # 使用base版 &#x27;*&#x27;: # 目标所有主机 - apache # 执行的sls文件名 然后创建apache.sls文件 apache-service: # 功能块ID service.running: # states模块 - name: httpd # 指定服务 - enable: True # 设置服务开机自启 - reload: True # 设置服务重载 执行salt -L &#39;s3&#39; state.highstate或salt -L &#39;s3&#39; state.highstate salt://apache或salt -L &#39;s3&#39; state.highstate apache # salt -L &#x27;s3&#x27; state.highstates3:---------- ID: apache-service Function: service.running Name: httpd Result: True Comment: Service httpd is already enabled, and is running Started: 09:36:45.596459 Duration: 143.647 ms Changes: ---------- httpd: TrueSummary for s3------------Succeeded: 1 (changed=1)Failed: 0------------Total states run: 1Total run time: 143.647 ms 如果 sls 文件中的操作有依赖或先后关系，还可以在 sls 文件中指定以下参数： require：本 state 执行前需要先执行哪些 state require_in： watch：除了 require 外，也监测依赖的 state 状态，若状态发生变化，则做出反应 watch_in： prereq：通过test=True检查所依赖的 state 状态，若状态发生变化，则执行 prereq_in： apache: # statesID pkg.installed: - name: httpd file.managed: - name: /etc/httpd/conf/httpd.conf # 目标文件 - source: salt://httpd.conf # 源文件 - require: - pkg: apache # 需要httpd已安装，apache为statesID service.running: - name: httpd - enable: True - reload: True - watch: # 需要file和pkg同时满足要求 - pkg: apache - file: apache 使用模板修改/srv/salt/的配置文件httpd.conf。 Listen &#123;&#123; http_port &#125;&#125;ServerName &#123;&#123; server_name &#125;&#125; 修改apache.sls的file.managed。 file.managed: - name: /etc/httpd/conf/httpd.conf - source: salt://httpd.conf - require: - pkg: apache - template: jinja # 指定模板的格式 - context: http_port: 8080 server_name: s3.example.com 然后执行，可看到修改信息 Changes: ---------- diff: --- +++ @@ -1,10 +1,10 @@ ServerRoot &quot;/etc/httpd&quot; -Listen 81 +Listen 8080 # 替换了端口号 Include conf.modules.d/*.conf User apache Group apache ServerAdmin root@localhost -ServerName www.example.com:80 +ServerName s3.example.com # 替换了主机域名 &lt;Directory /&gt; AllowOverride none Require all denied 若有多台主机需要配置，则可以使用 Jinja 的 if 判断结合 grains - context: &#123;% if grains[&#x27;id&#x27;] == &#x27;s3&#x27; %&#125; http_port: 81 server_name: s3.example.com &#123;% elif grains[&#x27;id&#x27;] == &#x27;s4&#x27; %&#125; http_port: 82 server_name: s4.example.com &#123;% endif %&#125; 或者结合 pillar。先在/srv/pillar/httpd.sls中配置 apache: &#123;% if grains.id == &#x27;s3&#x27; %&#125; http_port: 81 server_name: s3.example.com &#123;% elif grains.id == &#x27;s4&#x27; %&#125; http_port: 82 server_name: s4.example.com &#123;% endif %&#125; 然后刷新salt &#39;*&#39; saltutil.refresh_pillar，并查看是否能获取 # salt &#x27;*&#x27; pillar.get apaches4: ---------- http_port: 82 server_name: s4.example.coms3: ---------- http_port: 81 server_name: s3.example.com 然后修改/srv/salt/apache.sls - context: http_port: &#123;&#123; salt[&#x27;pillar.get&#x27;](&#x27;apache:http_port&#x27;, 80) &#125;&#125; # 若该项存在就使用该项的值，否则就用括号中另一个值 server_name: &#123;&#123; salt[&#x27;pillar.get&#x27;](&#x27;apache:server_name&#x27;, &#x27;www.example.com&#x27;) &#125;&#125; grainsgrains 是 Salt 的重要组件之一，用于收集客户端的信息，包括 CPU、内核、系统等。在 minion 上配置 Grains。 可在 master 上通过 grains 获取 minion 的信息。可用salt &#39;*&#39; grains.ls查看可选项 grains.items查看所有项与对应值，grains.item ITEM查看指定项的值 # salt -L &#x27;s3&#x27; grains.item oss3: ---------- os: CentOS 客户端自定义项与值，可以在 minion 的/etc/salt/minion配置文件中添加，也可以在/etc/salt/minion.d/中创建独立文件。修改完需要重启 minion 服务。 grains: roles: # 自定义项 - web # 项的值 - proxy 然后在 master 上查看roles项的值 # salt -L &#x27;s3&#x27; grains.item roless3: ---------- roles: - web - proxy 还可以通过grains.get直接获取指定项的值 # salt -L &#x27;s3&#x27; grains.get roless3: - web - proxy pillarPillar 在 Master 上定义，功能类似 Grains，但比 Grains 更加灵活，能给特定的 minion 定义需要的数据。在 master 配置文件中的pillar_roots块。 #pillar_roots: # pillar的根目录# base:# - /srv/pillar # 需要手动创建 在/srv/pillar中创建top.sls base: &#x27;*&#x27;: - httpd 然后创建httpd.sls httpd: function: state.sls args: - &#x27;httpd&#x27; 使用命令salt &#39;*&#39; saltutil.refresh_pillar刷新 pillar，无须重启服务。 # salt -L &#x27;s3&#x27; pillar.datas3: ---------- httpd: ---------- args: - httpd function: state.sls Grains 和 Pillar 的区别： 用途不同：Grains 用于存储 Minion 的基本数据信息，Pillar 用于存储 Master 分配给 Minion 的数据信息 存储区域不同：Grains 元数据存储在 Minion 端，Pillar 元数据存储在 Master 端 更新方式不同：Grains 在 Minion 启动时更新或通过saltutil.sync_grains刷新，Pillar 元数据存储在 Master 端，可用saltutil.refresh_pillar刷新，更加灵活。 syndicsyndic 是一个允许建立 salt 命令拓扑结构的工具，当两台 master 上都运行了 syndic，则高一级的 master 可以管理到另一台下的所有 minion，Master 的 Master 也称为Master of Master，syndic 常用于代理 proxy。 加入一台新的 salt 主机，IP 地址为172.16.246.134，安装salt-syndic，然后在现 master（172.16.246.158）的 master 配置文件中找到syndic_master参数并修改。 syndic_master: 172.16.246.134 # 更高一级的Master的IP地址syndic_log_file: /var/log/salt/syndic # 日志文件路径，可不改order_masters: True # 更高一级的master能管理低等级的master的syndic接口，默认为False master 和更高级别的 master 都要开启salt-syndic服务。在新 salt 主机上添加 master 的密钥，重启服务，然后测试。 # salt &#x27;*&#x27; service.restart httpd # &#x27;*&#x27;能包含master所管理的minion和master本身。sys1.example.com: Trues4: Trues3: True JobSalt 的任务管理 job。当 Master 下发指令时，会附带产生的 jid（job id，格式%Y%m%d%H%M%S%f），Minion 在接收到指令后开始执行时，会在本地 cachedir（默认/var/cache/salt/minion下的proc目录）产生以该 jid 命名的文件，用于在执行完毕将结果传给 Master，并删除该临时文件。Master 会将结果存放在/var/cache/salt/master/jobs目录，默认缓存 24 小时，可通过 master 配置的keep_jobs修改。 可在salt命令后添加-v显示当前命令的 jid。在 master 上通过命令salt-run jobs.list_jobs查看已缓存的 job saltutil中 job 的管理方法 running：查看 minion 正在运行的 Jobs find_job &lt;jid&gt;：查看指定 jid 的 job signal_job &lt;jid&gt; &lt;signal&gt;：给指定 jid 进程发送信号 term_job &lt;jid&gt;：终止指定 jid 进程，信号为 15 kill_job &lt;jid&gt;：终止指定 jid 进程，信号为 9 也可通过命令salt-run查看 job salt-run jobs.active：查看所有 Minion 当前正在运行的 jobs，即在所有 Minion 上运行saltutil.running salt-run jobs.lookup_jid &lt;jid&gt;：查看指定 jid 进程的运行结果 salt-run jobs.list_jobs：列出当前 master 的 jobs cache 中的所有 jobs Schedule用于在 Master 或 Minion 定期执行 Schedule 中配置的任务。Master 配置 Schedule 运行 runner，Minion 端配置 Schedule 为远程执行。可以在配置文件中或 pillar 中配置 Schedule。 在/srv/pillar/中创建 schedule 文件schedule.sls，并在top.sls中添加该文件，然后编写 Schedule。然后刷新 pillar。 schedule: job1: function: cmd.run args: - &quot;date &gt;&gt; /tmp/test.log&quot; minutes: 1 可通过salt-run jobs.list_jobs查看所有 jobs。 Salt SSHSalt ssh 基于 ssh，无需 Zeromq 和 agent。salt 也为 ssh 构建了一个系统结构 Roster，为 salt ssh 提供需要连接的主机及权限信息。 Roster 的配置文件：/etc/salt/roster salt ID: # 配置target的ID host: # 目标主机IP地址或域名 user: # 登录用户 passwd: # 用户密码 sudo: # 是否通过sudo执行，可选 port: # 连接目标的ssh端口 priv: # ssh私钥 timeout: # 等待回应的超时时间 参考文章 Saltstack 自动化运维工具详细介绍 SaltStack 学习 saltstack 快速入门 Saltstack-部署","categories":[],"tags":[{"name":"运维","slug":"运维","permalink":"https://coconutmilktaro.top/tags/%E8%BF%90%E7%BB%B4/"},{"name":"自动化","slug":"自动化","permalink":"https://coconutmilktaro.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"},{"name":"SaltStack","slug":"SaltStack","permalink":"https://coconutmilktaro.top/tags/SaltStack/"}]},{"title":"Memcached笔记","slug":"Memcached笔记","date":"2018-11-05T08:21:08.000Z","updated":"2022-06-21T15:16:21.212Z","comments":true,"path":"2018/Memcached笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Memcached%E7%AC%94%E8%AE%B0/","excerpt":"","text":"Memcached 概述Memcached 是一个高性能、支持高并发的分布式内存缓存系统，由 C 语言编写，是基于存储键值对的 hashmap，用于动态 web 应用，减轻后端服务器和数据库的负载。采用 C/S 架构，服务器端的程序叫 Memcached，客户端程序叫 Memcache。 Memcached 通过事先规划好的系统内存空间中临时缓存数据库中的各类数据，达到减少前端业务服务对数据库的直接高并发访问。原理与缓存服务器一致。 Memcached 特点： 协议简单：采用基于文本行的协议，可通过 telnet 等命令直接操作数据 支持 epoll/kqueue 异步 I/O 模型，使用 libevent 作为事件处理通知机制 采用 key/value 键值对存储 全内存缓存，效率高。无持久化存储设计。当重启系统或 Memcached 服务时，数据会全部丢失。若要持久化保存，需要使用 Redis。 支持分布式集群，但不同服务器之间互不通信，每个节点独立存取数据。 多线程处理时采用 pthread 线程模式。若要激活多线程，需在编译时指定./configure --enable-threads。 应用场景： 作为数据库的查询数据缓存 完全数据缓存：例如商品分类等一般不会改变的数据。若将这类缓存做成静态文件，然后在 web 前端缓存或用 CDN 加速效果更好。 热点数据缓存：例如用户的商品等一直会变化的数据。用于缓存经常被访问的数据。可通过数据库插件或相关软件实现缓存与数据库同步。 作为集群节点的 session 会话共享存储将用户请求产生的 session 信息统一存到缓存，提高速度。 网站更新 Memcached 数据时的工作流程 程序更新或删除数据时，首先处理后端数据库 处理数据库的同时，会通知 Memcached，告知对应的数据失效，保证缓存数据与数据库一致。 若是在高并发读写场合，除了以上操作，还需通过相关机制，例如在数据库上部署相关程序（如数据库中设置触发器使用 UDFs），实现当数据库有更新时就把数据更新到缓存中，实现数据预热，减少第一次查询对数据库的压力。甚至可以把缓存作为数据库的从库，实现主从复制。 Memcached 在企业常见架构中的位置： Memcached 内存管理机制： 采用 slab 内存分配机制：全称 Slab Allocation。会提前将大内存分为若干 1M 的 slab，每个小对象称为 chunk，把相同尺寸的内存块分为组 chunks slab class，可重复利用。当新增数据对象时，会根据空闲 chunk 的表进行分配，避免了大量重复的初始化和清理，减轻内存管理器负担。存储在 chunk 中的数据项称为 item。slab 分配器是基于对象（内核中的数据结构）进行管理的，每当要申请这样一个对象时，slab 分配器就从一个 slab 列表中分配一个 chunk 出去，而且是选择最适合数据大小的 slab 分配一个能存下这个数据的最小 chunk，而当要释放时，将其重新保存在该列表中，从而避免内部碎片。slab 分配器并不丢弃已经分配的对象，而是释放并把它们保存在内存中。slab 分配对象时，会使用最近释放的对象的内存块，因此其驻留在 cpu 高速缓存中的概率会大大提高。 早期使用的是 malloc，但会产生内存碎片，导致性能下降。 采用 LRU 对象清除机制：Least Recent Used，淘汰最不常使用的，即缓存中最长时间没有被访问的会被淘汰。 采用 hash 机制快速检索 item：对 item 做 hash，建立 hash 表 Slab 机制的缺点： 在 chunk 分配时也会产生浪费，因为分配的是能存下这个数据的最小 chunk，所以几乎一定会有空间浪费。避免浪费内存的方法： 预先计算应用存入的数据大小，或把同一业务类型的数据存入一个 Memcached，确保存入数据大小相对均匀，以减少内存浪费。 在启动缓存时指定 Growth Factor 因子，通过-f选项，可以在某种程度上控制每组 slab 之间的差异，默认为 1.25。 Memcached 预热： 当需要大规模重启 Memcached 时，要先在前端控制网站入口的访问流量，然后重启 Memcached 集群进行数据预热，再逐步放开前端的流量控制。在启动服务器集群前，一定要从网站的后端依次往前端启动，Memcached 要提前预热。 Memcached 检测过期与删除机制：Memcached 不会主动检测 item 对象是否过期，而是在进行 get 操作时检查时间戳 ，这种策略称为懒惰检测对象过期策略，这种策略不会在过期数据上浪费 CPU 资源。在删除 item 时，不会自动释放内存空间，而是做删除标记 ，将指针放入 slot 回收槽，下次分配时直接使用。在分配内存时，会优先使用已过期的键值对空间，并采用 LRU 算法进行淘汰。若不希望系统使用 LRU 清除数据，可使用-M启动 Memcached。 Memcached 简单部署安装与简单操作安装 Memcached 前确保libevent和libevent-devel已安装。然后可通过 yum 或源码包安装。推荐使用 yum 安装，虽然版本会老一点。yum install -y memcached或apt install -y memcached 源码包安装，进入解压目录。 ./configure --prefix=/usr/local/memcached \\ --bindir=/usr/bin \\ --sbindir=/usr/sbin \\ --sysconfdir=/etc 然后make &amp;&amp; make install 服务器端memcached命令参数： memcached -p # TCP端口，默认为11211 -U # UDP端口，默认0（未开启） -d # 后台运行 -u # 指定验证用户 -m # 内存限制，默认64MB -c # 连接限制，默认1024 -v # 显示详细信息 -t # 使用的线程数，默认4 -R # 每个event最大请求数，默认20 -C # 禁用CAS -M # 不使用LRU清除数据 -L # 启用大内存页，可降低内存浪费 启动 Memcached 实例 memcached -m 128m -d -u root -c 8192# 如果要使用root用户启动，必须要加-u 可使用 telnet 进入 Memcached 进行操作。telnet 127.0.0.1 11211 memcached 命令格式 &lt;command&gt; &lt;key&gt; &lt;flags&gt; &lt;exptime&gt; &lt;bytes&gt; # 命令&lt;datablock&gt; # 填数据&lt;status&gt; # 返回的状态 命令 说明 command set\\get\\add\\replace\\append\\prepend\\cas key 键名 flags 客户端用来标识数据格式的数值，如 json、xml 等 exptime 存活时间，0 为永远，单位秒（小于 30 天），unix 时间戳（大于 30 天） bytes 字符个数，不包含\\r\\n datablock 文本行，以\\r\\n 结尾 status 命令返回状态，STORED\\NOT_STORED\\NOT_FOUND\\EXISTS\\ERROR\\CLIENT_ERROR\\SERVER_ERROR 常用操作 插入数据 set key1 0 0 5abcdeSTORED // 表示成功添加 设置的多少字节，就必须是多少字节，不能多也不能少 ，否则就会报错 CLIENT_ERROR bad data chunkERROR 查询数据 get key1VALUE key1 0 5abcdeEND 删除数据 delete key1DELETED 关闭 Memcached：若启动了多个实例，使用killall或pkill会同时关闭这些实例，所以最好在启动时添加-P指定 pid 文件，便于管理不同实例。 memcached -d -u root -p 11211 -P /var/run/memcached-11211.pidmemcached -d -u root -p 11212 -P /var/run/memcached-11212.pid 然后通过 pid 停止指定 memcached kill `cat /var/run/memcached-11211.pid` 也可以通过nc命令连接 memcached # printf &quot;stats\\r\\n&quot; | nc 127.0.0.1 11211STAT pid 5235STAT uptime 1283STAT time 1578635117STAT version 1.5.6 Ubuntu...... 部署在 LNMP 环境中需要获取 php 的 memcached 插件 memcache，在php memcache 插件下载解压后进入目录。先确保php-devel（centos）或php-dev（ubuntu）已安装。在该目录下运行phpize，生成configure文件。然后执行./configure --enable-memcache --with-php-config=/usr/bin/php-config。若是 ubuntu，则php-config变为php-config7.2。然后make &amp;&amp; make install。会提示安装到的路径，如Installing shared extensions: /usr/lib/php/20170718/，在该路径下可找到memcache.so，说明插件安装完成。 Memcached 架构优化参考书目 跟老男孩学 Linux 运维 WEB 集群实战","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Memcached","slug":"Memcached","permalink":"https://coconutmilktaro.top/tags/Memcached/"},{"name":"缓存","slug":"缓存","permalink":"https://coconutmilktaro.top/tags/%E7%BC%93%E5%AD%98/"}]},{"title":"MongoDB基础笔记","slug":"MongoDB基础笔记","date":"2018-10-29T03:41:54.000Z","updated":"2022-05-30T02:51:53.848Z","comments":true,"path":"2018/MongoDB基础笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/MongoDB%E5%9F%BA%E7%A1%80%E7%AC%94%E8%AE%B0/","excerpt":"基于 MongoDB 4.0.3 MongoDB 概述 CRUD 索引 权限","text":"基于 MongoDB 4.0.3 MongoDB 概述 CRUD 索引 权限 MongoDB 概述MongoDB 是一个跨平台的基于分布式文件存储的面向文档的数据库，提供高性能，高可用性和易于扩展。由 C++ 语言编写，旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。 MongoDB 中的记录是一个文档，由键值对组成。 MongoDB 文档类似于 JSON 对象。字段的值可以包括其他文档，数组和文档数组。 MongoDB 特点： 提供高性能数据持久性：对嵌入式数据模型的支持减少了数据库系统的 I / O 活动。索引支持更快的查询，并且可以包含来自嵌入式文档和数组的键。 丰富的查询语言 高可用性：MongoDB 的复制工具称为副本集（replica set），它提供自动故障转移和数据冗余。 副本集是一组 MongoDB 服务器，它们维护相同的数据集，提供冗余并提高数据可用性。 水平扩展（Horizontal Scalability）：分片将数据分配到集群的不同机器上。支持基于分片键（shard key）创建数据区域（zones of data） 支持主从复制。主服务器可以执行读写操作，从服务器从主机复制数据，只能用于读取或备份 MongoDB 能够实现负载均衡 支持多个存储引擎（WiredTiger、In-Memory、MMAPv1） 提供可插拔存储引擎 API，允许第三方为 MongoDB 开发存储引擎 可以将不同结构的文档存储在同一个数据库中 可对任何属性创建索引 支持二进制数据和大型对象 MongoDB 结构： database：数据库 collection：集合，对应了 SQL 的 table document：文档，对应了 SQL 的 row，即值 field：域，对应 SQL 的 column 字段 index：索引 primary key：主键，MongoDB 自动将**_id**字段设置为主键 安装 MongoDB 需要安装mongodb-org、mongodb-org-mongos、mongodb-server、mongodb-org-shell、mongodb-org-tools MongoDB 默认端口 27017，配置文件/etc/mongod.conf。 常见命令： db：显示当前数据库名 show dbs：显示所有数据库，默认有四个，admin、config、local、test use 数据库名：切换数据库，若不存在就会自动创建 db.dropDatabase()：删除数据库，删除前一定要切换到该数据库 show collections：显示当前库中的所有集合 db.collection.drop()：删除指定集合 show users：显示当前数据库的所有用户 MongoDB 的数据类型： ObjectID：文档 ID，12 字节，十六进制数。由以下信息构成： 前 4 个字节为创建时的时间戳 接下来 3 个字节为机器 ID 接下来 2 个字节为 MongoDB 的服务进程 ID 最后 3 个字节为简单的增量值 例：5bd6b780 ba51d7 f829 d135e9 String：字符串，必须是有效的 UTF-8 Boolean：布尔值 Integer：整型 Double：浮点型 Arrays：数组或列表 Object：用于嵌入式文档，一个值就是一个文档 Null：Null 值 Timestamp：时间戳 Date：当前日期或时间，unix 格式 CRUD db.createCollection(name, options)：创建集合。options 是一个文档，用于指定集合配置。 db.collection.insertOne()：插入单个文档，若集合不存在，会自动创建 db.collection.insertMany()：插入多个文档 db.collection.insert()：既可以插入单个文档，也可插入多个文档 db.users.insertOne( &#123; name: &quot;zhangsan&quot;, age: 22, hobby: [&quot;climbing&quot;, &quot;swimming&quot;, &quot;game&quot;] &#125;)db.users.insertMany([ &#123; name: &quot;lisi&quot;, age: 23, hobby: [&quot;swimming&quot;, &quot;game&quot;] &#125;, &#123; name: &quot;wangwu&quot;, age: 25, hobby: [&quot;tennis&quot;, &quot;swimming&quot;] &#125;, &#123; name: &quot;zhaoliu&quot;, age: 21, hobby: [&quot;game&quot;] &#125;]) 在 MongoDB 中，存储在集合中的每个文档都需要一个唯一的_id字段作为主键。如果插入的文档省略了_id字段，MongoDB 驱动程序会自动为_id字段生成ObjectId。 MongoDB 中的所有写入操作都是单个文档级别的原子操作。 db.collection.find()：从集合中检索文档 db.users.find( &#123;&#125; ) # 显示所有记录&#123; &lt;field1&gt;: &lt;value1&gt;, ... &#125; 直接指定键值查询db.users.find( &#123; name: &quot;zhaoliu&quot; &#125; # name为zhaoliu)&#123; &lt;field1&gt;: &#123; &lt;operator1&gt;: &lt;value1&gt; &#125;, ... &#125; 使用查询运算符指定条件db.users.find( &#123; age: &#123;$gt: 23&#125;, # age大于23 hobby: &quot;climbing&quot; # 且hobby中有climbing &#125;)db.users.find( &#123; $or: [ # or关系 &#123; age: &#123;$gt: 23&#125; &#125;, # 要么age大于23 &#123; hobby: &quot;climbing&quot; &#125; # 要么hobby中有climbing ] &#125;)db.users.find( &#123; age: &#123;$gt: 23&#125;, # age大于23 $or: [ # 下面的为或关系 &#123; hobby: &quot;climbing&quot; &#125;, # 要么hobby中有climbing &#123; hobby: &quot;game&quot; &#125; # 要么hobby中有game ] &#125;)相当于SQL：select * from users where age &gt; 23 and (hobby=&quot;climbing&quot; or hobby=&quot;game&quot;)db.users.find( &#123; age: &#123; $in: [23, 24] &#125; &#125; # 查询age在[23, 24]中的文档)db.users.find( &#123; hobby: [ &quot;swimming&quot;, &quot;game&quot; ] &#125; # hobby必须包含该列表（包括列表中顺序也要一致）)db.users.find( &#123; hobby: &#123; $all: [ &quot;game&quot;, &quot;swimming&quot; ] &#125;&#125; # hobby必须包含该列表（顺序不用一致）)$elemMatch运算符匹配包含数组字段的文档，其中至少有一个元素匹配所有指定的查询条件。例：&#123; _id: 1, results: [ 82, 85, 88 ] &#125;,&#123; _id: 2, results: [ 75, 88, 89 ] &#125;db.scores.find( &#123; results: &#123; $elemMatch: &#123; $gte: 80, $lt: 85 &#125; &#125; &#125;)返回结果为_id为1的文档若包含数组（列表），可通过指定列表的下标进行匹配db.users.find( &#123; &quot;hobby.1&quot;: &quot;climbing&quot; # 查找hobby列表的下标为1的值为climbing的文档，一定要加引号 &#125;)在匹配数组中的文档时，该字典的字段顺序必须完全一致匹配列表中的文章，只要有一项满足就会被匹配例：&#123;&quot;item&quot; : &quot;paper&quot;, &quot;instock&quot; : [ &#123; &quot;warehouse&quot; : &quot;A&quot;, &quot;qty&quot; : 60 &#125;, &#123; &quot;warehouse&quot; : &quot;B&quot;, &quot;qty&quot; : 15 &#125; ] &#125;则db.inventory.find( &#123; &#x27;instock.qty&#x27;: &#123; $lte: 20 &#125; &#125; )也能匹配到该条文档 db.collection.updateOne()：更新单个文档 db.collection.updateMany()：更新多个文档 db.collection.update()：更新一个或多个文档 db.collection.replaceOne()：替换除_id字段之外的文档的整个内容 db.collection.save()：更新现有文档或插入新文档，用法与update()或insert()一致 db.users.update( &#123; name: &quot;zhangsan&quot; &#125;, # 指定要改的满足条件的文档 &#123; $set: # 指定$set进行修改 &#123; &quot;hobby.1&quot;: &quot;tennis&quot; &#125; &#125;)db.users.replaceOne( &#123; name: &quot;zhangsan&quot; &#125;, &#123; name: &quot;zhangsan&quot;, age: 24, hobby: [&quot;swimming&quot;, &quot;basketball&quot;, &quot;football&quot;] &#125;)替换文档必须仅包含字段/值对，即不包括更新运算符表达式。_id字段始终是文档中的第一个字段。字段名称的更新（包括重命名）可能会导致文档中字段的重新排序。 db.collection.deleteOne()：删除一个文档 db.collection.deleteMany()：删除多个文档 db.collection.remove()：删除单个文档或与指定过滤器匹配的所有文档。 db.users.deleteOne( &#123; name: &quot;zhangsan&quot; &#125;) 索引先创建 200000 条数据做测试 for(i = 0; i &lt; 200000;i++)&#123; db.test.insert( &#123; name: &quot;test&quot;+i &#125; )&#125; 查询指定数据 db.test.find( &#123; name: &quot;test10000&quot; &#125;).explain(&#x27;executionStats&#x27;)# explain(&#x27;executionStats&#x27;)用于显示查询过程信息 返回结果 &quot;executionStats&quot; : &#123; &quot;executionSuccess&quot; : true, &quot;nReturned&quot; : 1, &quot;executionTimeMillis&quot; : 93, # 花费93毫秒 &quot;totalKeysExamined&quot; : 0, &quot;totalDocsExamined&quot; : 210000,... 创建索引：db.collection.ensureIndex() db.test.ensureIndex(&#123; name: 1 &#125;)对name创建索引，1表示升序，-1表示降序 再次执行查询语句，得到以下结果 &quot;executionStats&quot; : &#123; &quot;executionSuccess&quot; : true, &quot;nReturned&quot; : 1, &quot;executionTimeMillis&quot; : 0, # 创建索引后，基本不消耗时间 &quot;totalKeysExamined&quot; : 1, &quot;totalDocsExamined&quot; : 1,... 唯一索引：db.collection.ensureIndex(&#123; name: 1 &#125;, &#123; unique: true &#125;) 联合索引：db.collection.ensureIndex(&#123; name: 1, XXX: 1 &#125;)设置多个字段 查看当前集合的所有索引：db.collection.getIndexes() [ &#123; &quot;v&quot; : 2, &quot;key&quot; : &#123; &quot;_id&quot; : 1 # _id为主键，也是默认的索引 &#125;, &quot;name&quot; : &quot;_id_&quot;, &quot;ns&quot; : &quot;test.test&quot; &#125;, &#123; &quot;v&quot; : 2, &quot;key&quot; : &#123; &quot;name&quot; : 1 # 添加的索引name &#125;, &quot;name&quot; : &quot;name_1&quot;, &quot;ns&quot; : &quot;test.test&quot; &#125;] 删除指定索引：db.collection.dropIndexes(&#39;索引名&#39;) 权限MongoDB 采用角色-用户-数据库的管理模式 常见的系统角色： root：只能在 admin 数据库中可用，是超级用户，具有超级权限 read：允许用户读取指定数据库 readWrite：允许用户读写指定数据库 创建超级管理员 先切换到 admin 数据库，然后创建 db.createUser( &#123; user: &#x27;admin&#x27;, pwd: &#x27;123123&#x27;, roles: [&#123; role: &#x27;root&#x27;, db: &#x27;admin&#x27; &#125;] &#125;) 然后修改 MongoDB 的配置文件/etc/mongod.conf 找到security配置，删除注释，并添加内容 security: authorization: enabled 然后重启 MongoDB，systemctl restart mongod，再重进 mongo 执行show dbs，出现报错，说明授权起作用了 [js] Error: listDatabases failed:&#123; &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;command listDatabases requires authentication&quot;, &quot;code&quot; : 13, &quot;codeName&quot; : &quot;Unauthorized&quot;&#125; :...... 需要指定用户名和密码以及参数--authenticationDatabase admin mongo -u username -p password --authenticationDatabase admin，登录后可以正常操作数据库 为单独的应用创建用户，专门用于该数据库的读写。 首先切换到测试数据库 test，创建用户 testuser 退出 MongoDB，重新使用该用户登录 mongo -u testuser -p 123123 --authenticationDatabase test 能查看到的数据库就只有test了，切换到别的数据库也无法查看任何数据","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://coconutmilktaro.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://coconutmilktaro.top/tags/MongoDB/"}]},{"title":"重学Docker笔记","slug":"重学Docker笔记","date":"2018-10-12T04:21:24.000Z","updated":"2022-06-21T15:30:09.002Z","comments":true,"path":"2018/重学Docker笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/%E9%87%8D%E5%AD%A6Docker%E7%AC%94%E8%AE%B0/","excerpt":"重新学习 Docker 整理，docker 版本 18.06","text":"重新学习 Docker 整理，docker 版本 18.06 Docker 安装注意事项官方安装说明页面 若要使普通用户也能有直接操作 Docker 命令的权限（不需要 sudo），可将该用户添加到 docker 组内。先要确保/var/run/docker.sock所属组为 docker。 DaoCloud 加速 之后systemctl daemon-reload以及重启 docker Docker 基础命令集环境信息： docker info：显示 docker 配置信息 docker version：显示 docker 版本号 容器生命周期管理： docker create：创建容器 docker exec：对运行的容器执行一条命令 docker kill：杀死容器 docker pause：停止指定容器的所有进程 docker restart：重启容器 docker rm：删除容器 docker run：创建并运行容器。run 就是 create 和 start 的组合 docker start：启动一个或多个停止的容器 docker stop：停止一个或多个容器 docker unpause：恢复指定容器的所有进程继续运行 镜像仓库管理： docker login：登录到 Docker 仓库 docker logout：登出 Docker 仓库 docker pull：拉取镜像 docker push：上传镜像 docker search：搜索镜像 镜像管理： docker build：用 Dockerfile 构建一个镜像 docker images：查看镜像 docker import： docker load： docker rmi： docker save： docker tag： docker commit：因为容器的变化而构建一个新的镜像 容器运维管理： docker attach：连接进入一个容器 docker export： docker inspect： docker port： docker ps：列出容器 docker rename：重命名一个容器 docker stats： docker top： docker wait： docker cp： docker diff： docker update： 容器资源管理： docker volume：管理数据卷 docker network：管理网络 系统日志信息： docker events：获取一个容器的实时信息 docker history：显示一个镜像的构建历史信息 docker logs：获取一个容器的日志 常用命令调用过程： Docker 核心原理namespaceLinux 内核虚拟化容器技术（LXC Kernel Namespace），将某个特定的全局系统资源通过抽象方法使得 namespace 中的进程看起来拥有自己的隔离的全局系统资源实例。LXC 提供以下六种隔离的系统调用。 namespace 隔离的资源 UTS 主机名和域名 IPC 进程间通信资源（信号量、消息队列、共享内存） PID 进程编号 Network 网络相关资源（网络设备、网络栈、端口等） Mount 挂载点（文件系统） User 用户与用户组 cgroupscgroups 全称control groups，是 Linux 内核的一种机制，可根据需求把一系列系统任务及子任务整合到按资源划分等级的不同组内，从而为系统资源管理提供一个统一的框架。即 cgroups 可以限制、记录任务组所使用的物理资源。 本质上，cgroups 是内核附加在程序上的一系列钩子 hook，通过程序运行时对资源的调度，触发相应的 hook 函数，以达到资源追踪和限制的目的。 cgroups 特点： cgroups 的 API 以一个伪文件系统实现，用户态的程序可以通过文件操作实现 cgroups 的组织管理 cgroups 的组织操作单元的细粒度可达到线程级别，用户可创建销毁 cgroups，实现资源再分配和管理 所有资源管理功能都以子系统方式实现，接口统一 子任务创建之初与父任务处于同一个 cgroups 控制组 cgroups 功能： 资源限制：对任务使用的资源总额进行限制。一旦超出配额就发出OOM（out of memory）的警告 优先级分配：通过分配的CPU 时间片数量及磁盘 IO 带宽大小，实际上就相当于控制了任务运行的优先级 资源统计：统计系统的资源使用量，适合于计费 任务控制：可对任务执行挂起、恢复等操作 cgroups 术语： task：任务，表示系统的一个进程或线程 cgroup：控制组，cgroups 中资源控制都是以 cgroup 为单位，表示按某种资源控制标准划分而成的任务组，包含一个或多个子系统。任务可在 cgroup 间迁移 subsystem：子系统，是一个资源调度控制器 hierarchy：层级，由一系列 cgroup 以一个树状结构排列，每个层级通过绑定对应的子系统进行资源控制，并且子节点能继承父节点挂载的子系统。 层级规则 规则 1：同一个层级可附加一个或多个子系统。事例如图，CPU 和 Memory 子系统附加到一个层级上 规则 2：一个子系统可附加到多个层级上（仅当目标层级只有唯一一个子系统时），一个已经附加在某个层级上的子系统不能附加到其他含有别的子系统的层级上。 规则 3：每新建一个层级时，该系统上所有任务默认加入这个新建层级的初始化 cgroup，称为 root cgroup。一个任务不能存在于同一个层级的不同 cgroup，但可以存在于不同层级的多个 cgroup 中。若将一个任务添加到同一层级的另一个 cgroup，则会自动将其从第一个 cgroup 中移除。 规则 4：任务在 fork/clone 自身时创建的子任务默认与原任务在同一个 cgroup，但完成后，父子任务间在 cgroup 方面互不影响。 子系统子系统是 cgroups 的资源控制系统，每种子系统独立控制一种资源。Docker 有以下子系统： blkio：限制块设备输入输出 cpu：控制对 CPU 的使用 cpuacct：对 CPU 资源使用情况的报告 cpuset：分配独立的 CPU 和内存 devices：开启/关闭任务对设备的访问 freezer：挂起/恢复任务 memory：限定任务的内存使用量，并生成内存资源使用报告 perf_event：可进行统一的性能测试 net_cls：控制网络流量，识别数据包 Linux 中 cgroup 表现为一个文件系统，需要 mount 才能使用。 # mount -t cgroupcgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu) 子系统文件都存放在/sys/fs/cgroup目录中。Docker daemon 会在每个子系统的控制组目录中创建一个 docker 控制组，并在其中为每个容器创建一个以容器 ID（长 ID）为名称的容器控制组，该容器中所有任务的 TID（进程或线程的 ID）都会写入该控制组的 tasks 文件。 # tree /sys/fs/cgroup/cpu/docker//sys/fs/cgroup/cpu/docker/├── c146b9740725896dca61d96788acecd80c3bb7f80aedd22359cd78adbfda4fdc│ ├── cgroup.clone_children│ ├── cgroup.procs│ ├── cpuacct.stat| |......│ └── tasks├── cgroup.clone_children├── cgroup.procs├── cpuacct.stat......└── tasks cgroups 实现原理cgroups 如何判断资源超限并做出措施对于不同系统资源，cgroups 提供了统一的接口对资源进行控制和统计。会有描述子系统资源状态的结构体记录所属 cgroup，当进程申请更多资源时，会触发 cgroup 用量检测，若超出限额，则拒绝，否则就给予相应资源并记录在统计信息中，不仅要考虑资源的分配和回收，还要考虑不同类型的资源等。 在超出限额后，会根据信号（如果设置的话）（如内存的 OOM 信号）决定进程是否挂起或继续执行。 cgroup 与任务之间的关联关系cgroup 与任务间是多对多关系，并不直接关联，而是通过一个中间结构将双向的关联信息记录。每个任务结构体能通过指针查询对应 group 的情况，也能查询各个子系统的情况，结构体把子系统状态指针包含进来，并由内核通过 container_of 宏定义获取对应结构体，关联到任务，实现资源限制。 使用注意Docker 需要挂载 cgroup 文件系统新建一个层级结构，挂载时指定要绑定的系统。除 cgroup 文件系统外，内核没有为 cgroups 的访问和操作添加任何系统调用。 无法将一个新的子系统绑定到一个已激活的层级，或从一个层级解除某个子系统的绑定。 只有递归卸载层级中的所有 cgroup，该层级才会被真正删除，否则即使上层的层级删除了，后代的 cgroup 中的配置也会依然生效。 在容器目录下，会有以下固定文件，描述 cgroup 相应信息。 tasks：在该 cgroup 中任务的进程或线程 ID（无序），意味着把这个任务加入这个 cgroup。 cgroup.procs：记录所有在 cgroup 的 TGID（线程组 ID，是线程组中第一个进程的 ID），意味着将与其相关的线程都加到这个 cgroup 中。 notify_on_release：是否在 cgroup 中最后一个任务退出时通知运行 release agent，默认为 0，不运行。若为 1 则表示运行 Docker 架构采用 C/S 架构，用户通过 Docker client 与 Docker daemon 建立通信。 docker daemon 是 docker 的核心后台进程，响应 client 的请求，然后调度给容器操作。 docker client 向 daemon 发送请求，可以是命令docker，也可以是使用了 docker API 的应用。 image management：docker 通过 distribution、registry、layer、image、reference 等模块实现 docker 镜像管理。 distribution：与 docker registry 交互，上传下载镜像及存储 registry 的元数据 registry：与 docker registry 有关的身份验证、镜像查找、镜像验证等操作 image：与镜像元数据相关的存储、查找、镜像层索引、镜像 tar 包导入导出的交互操作 reference：存储本地所有镜像的 repository 和 tag 名，维护与镜像 ID 间的映射关系 layer：与镜像层、容器层元数据有关的增删改查，将镜像层操作映射到实际存储镜像层文件系统的 graphdriver 模块 docker daemon 包含的三个主要模块：execdriver（容器执行驱动）、volumedriver（volume 存储驱动）、graphdriver（镜像存储驱动） execdriver：是对 namespaces、cgroups、selinux 等系统操作的二次封装，比 LXC 功能更全面。默认实现是官方的 libcontainer 库 volumedriver：是 volume 存储操作的执行者，负责 volume 增删改查。默认实现是 local，默认将文件存放在 docker 根目录下 volume 目录 graphdriver：是所有与容器镜像相关操作的执行者，会在 docker 工作目录下维护一组与镜像层对应的目录，存放镜像层关系和元数据。主要支持的 graphdriver：aufs、btrfs、zfs、devicemapper、overlay、vfs libnetwork：抽象出了容器网络模型（Container Network Model），提供统一接口。抽象除了 sandbox、endpoint、network 对象，由具体网络驱动操作对象，通过网络控制器的统一接口供调用者管理网络。主要实现：创建网络、创建 network namespace、虚拟网卡和所有网络相关配置等 镜像管理Docker 镜像的文件内容以及一些运行 Docker 容器的配置文件组成了 Docker 容器的静态文件系统运行环境——rootfs。 rootfs 是 Docker 容器在启动时内部进程可见的文件系统，即 Docker 容器的根目录。docker daemon 为 Docker 容器挂载 rootfs 时，先将 rootfs 设为只读（read-only），在挂载完毕后，利用联合挂载（union mount）在 rootfs 上再挂载一个读写层，使得读写层位于 Docker 容器文件系统的最顶层，下面是多个只读层。 Docker 镜像的特点： 分层 写时复制（copy-on-write）：在多个容器间共享镜像，只有 Docker 容器运行时文件系统变化时，才会把变化的文件写到可读写层。写时复制减少了镜像对磁盘空间的占用和容器启动时间。 内容寻址存储（content-addressable storage）：该机制根据文件内容索引镜像和镜像层。会对镜像层生成内容哈希值，作为镜像层唯一标识。提高了镜像安全性，并能检测数据完整性。 联合挂载：可以在一个挂载点同时挂载多个文件系统，将挂载点的原目录与被挂载内容进行整合。实现联合挂载的文件系统称为联合文件系统（union filesystem）。 存储管理数据卷网络管理容器安全参考资料 《Docker 开发指南》 《Docker 容器与容器云》 Docker 官方文档 版本 18.03","categories":[],"tags":[{"name":"docker","slug":"docker","permalink":"https://coconutmilktaro.top/tags/docker/"},{"name":"云计算","slug":"云计算","permalink":"https://coconutmilktaro.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"}]},{"title":"KVM学习笔记","slug":"KVM学习笔记","date":"2018-10-03T00:33:46.000Z","updated":"2022-06-28T18:53:00.642Z","comments":true,"path":"2018/KVM学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/KVM%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"虚拟化概述 KVM 概述 KVM 环境 KVM 操作","text":"虚拟化概述 KVM 概述 KVM 环境 KVM 操作 虚拟化概述KVM 概述KVM（Kernel-based Virtual Machine）基于内核的虚拟机，是一个 Linux 内核模块，使得 Linux 变成一个 Hypervisor（一个虚拟化的管理程序）。 KVM 环境首先要查看本机是否支持 KVM，KVM 基于 x86 虚拟化扩展技术（Intel VT或AMD-V）。 可通过cat /proc/cpuinfo | egrep &quot;vmx|svm&quot;查看，若有返回，则说明支持。其中vmx是对应 Intel 的处理器，svm是对应 AMD 的处理器，根据本机情况查找关键字。 或者可通过lsmod | grep kvm查看，若有该模块也可说明支持。 若用 VMware 虚拟机，需要在虚拟机的 settings 中开启虚拟化选项 确保已关闭 selinux。 安装 kvm 相关包（并非都要装） qemu-kvm 主要的 KVM 程序包 virt-manager GUI 虚拟机管理工具（需要桌面环境） virt-top 虚拟机统计命令 virt-viewer GUI 连接程序，连接到已配置好的虚拟机 libvirt C 语言工具包，提供 libvirt 服务 libvirt-client 为虚拟客户机提供的 C 语言工具包 virt-install 基于 libvirt 服务的虚拟机创建命令 bridge-utils 创建和管理桥接设备的工具 libvirt 是 KVM 的管理工具，包含守护进程程序 libvirtd，API 库和命令行工具 virsh dnf install qemu-kvm libvirt virt-install bridge-utils，这是命令行操作 kvm 需要的软件。开启 KVM 服务systemctl start libvirtd，并设置开机启动。 在启动以后，会自动创建一张虚拟网卡 virbr0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 192.168.122.1 netmask 255.255.255.0 broadcast 192.168.122.255 ether 52:54:00:b0:61:8a txqueuelen 1000 (Ethernet) 并且还会启动 dnsmasq 服务 nobody 6320 1 0 18:39 ? 00:00:00 /sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/libexec/libvirt_leaseshelperroot 6323 6320 0 18:39 ? 00:00:00 /sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/libexec/libvirt_leaseshelper 网卡的配置存放在/etc/libvirt/qemu/networks/default.xml中。 KVM 操作创建一个硬盘qemu-img create -f raw /opt/kvm.raw 10G，这里创建的硬盘虽然还没使用，但会直接占用硬盘空间，需要有一定的大小，因为要装系统。 # ll -h /opt/kvm.raw-rw-r--r-- 1 root root 1.0G 11月 12 18:49 /opt/kvm.raw 可通过qemu-img info /opt/kvm.raw查看该硬盘的信息 image: /opt/kvm.rawfile format: rawvirtual size: 1.0G (1073741824 bytes)disk size: 0` 若硬盘空间不够，就添加虚拟硬盘。注意，需要在虚拟机里有安装的镜像，所以要在虚拟机设置中把镜像设置连接。 然后创建镜像，把/dev/cdrom复制到一个容量足够的目录中。dd if=/dev/cdrom of=/disk/sdb1/fedora25.iso 创建一个虚拟机 virt-install --virt-type kvm \\ --name kvm-fedora \\ --ram 512 \\ --cdrom=/disk/sdb1/fedora27.iso \\ --network network=default \\ --noautoconsole \\ --os-type=linux \\ --os-variant=fedora25 \\ --graphics vnc,listen=0.0.0.0 \\ --disk path=/opt/kvm.raw提示开始安装，使用ss -anpt查看端口变化State Recv-Q Send-Q Local Address:Port Peer Address:PortLISTEN 0 1 *:5900 *:*开始监听5900端口 此时查看创建的 KVM 虚拟机，如果没有，可能是没有启动，可通过加上--all显示所有虚拟机 # virsh list Id 名称 状态---------------------------------------------------- 1 kvm-fedora running 也可查看本机信息 # virsh nodeinfoCPU 型号： x86_64CPU： 4CPU 频率： 2399 MHz.....内存大小： 4027656 KiB 并且，会在/etc/libvirt/qemu/下创建了一个kvm-fedora.xml配置文件。 若没有启动，可通过virsh start kvm-fedora启动虚拟机。 virsh destroy停止指定虚拟机，还能通过virsh list --all查看到 virsh undefine删除标记，即彻底删除该虚拟机 参考资料 KVM –介绍 KVM 虚拟化 CentOS7 安装 KVM 虚拟机详解 KVM Cloudman-KVM","categories":[{"name":"虚拟化","slug":"虚拟化","permalink":"https://coconutmilktaro.top/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"https://coconutmilktaro.top/tags/KVM/"},{"name":"虚拟化","slug":"虚拟化","permalink":"https://coconutmilktaro.top/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"}]},{"title":"Puppet自动化工具笔记","slug":"Puppet自动化工具笔记","date":"2018-10-01T02:48:31.000Z","updated":"2022-05-30T02:51:53.865Z","comments":true,"path":"2018/Puppet自动化工具笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Puppet%E8%87%AA%E5%8A%A8%E5%8C%96%E5%B7%A5%E5%85%B7%E7%AC%94%E8%AE%B0/","excerpt":"Puppet 概述 Puppet 工作流程 Puppet 安装部署","text":"Puppet 概述 Puppet 工作流程 Puppet 安装部署 Puppet 概述Puppet 是一个基于 Ruby 开发的主机配置管理工具，采用 C/S 结构，服务器端称为 Puppet Master 或 Puppet Server，客户端称为 Puppet Agent。 Puppet 有自己的语言，可管理配置文件、用户、cron 任务、软件包、服务等，这些紫铜实体称为资源。 Puppet 工作流程 客户端向 Master 发送认证请求 Master 通过认证返回确认信息 客户端调用 facter 探测主机的变量，如主机名、内存大小、IP 地址，然后通过 SSL 发送给 Master Master 检测主机名，找到对应 node 配置，解析内容，只解析 facter 发来信息与 node 有关的代码。解析分为：语法检查、生成伪代码 catalog，然后发送给客户端 客户端接收伪代码并执行 客户端在执行时判断是否有 file 文件，若有就向文件服务器发送请求 客户端判断是否有配置 report，若有配置，则将结果发送给 Master。Master 将结果写入日志。 Puppet 采用的拉取模式：Agent 定期（默认 30 分钟）向 Master 发送自身状态。 Puppet 服务器和客户端之间通信采用的协议是XMLRPC over HTTPS。 Puppet 安装部署首先确保时间同步。然后搭建 ruby 环境，或者直接安装 puppet，也会自动解决依赖。 yum install ruby。ruby 版本 2.4。puppet 到http://yum.puppetlabs.com/ 下载安装官方源。rpm -ivh http://yum.puppetlabs.com/puppetlabs-release-el-7.noarch.rpm以及http://yum.puppetlabs.com/puppet6/puppet-release-el-7.noarch.rpm。 然后用过 yum 安装 puppet-agent 和 puppet-server，版本为 6.0.2。 安装完后，会自动创建用户与用户组 puppet。设置/etc/hosts文件，添加客户端。 puppet 的配置目录/etc/puppet puppet/├── auth.conf # 认证配置├── fileserver.conf # 文件服务器配置├── hiera.yaml #├── manifests # 存放init.pp和其他配置的目录├── modules # 存放模块的目录├── puppet.conf # 主配置文件└── ssl # 存放SSL认证相关的文件，如密钥、证书等 启动 puppetmaster 服务systemctl start puppetmaster，也可通过命令puppet master --verbose --no-daemonize启动，会显示详细的启动过程。 puppetmaster 会开启一个端口 8140 在客户端只要安装ruby和 puppet。","categories":[],"tags":[{"name":"运维","slug":"运维","permalink":"https://coconutmilktaro.top/tags/%E8%BF%90%E7%BB%B4/"},{"name":"自动化","slug":"自动化","permalink":"https://coconutmilktaro.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"},{"name":"Puppet","slug":"Puppet","permalink":"https://coconutmilktaro.top/tags/Puppet/"}]},{"title":"MySQL高可用笔记","slug":"MySQL高可用笔记","date":"2018-10-01T02:45:25.000Z","updated":"2022-06-21T15:16:21.213Z","comments":true,"path":"2018/MySQL高可用笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E7%AC%94%E8%AE%B0/","excerpt":"MySQL 高可用概述 MySQL 多实例 MySQL 主从复制 MySQL 常用调优策略 硬件层优化 磁盘 I/O 优化 文件系统层优化 内核参数优化","text":"MySQL 高可用概述 MySQL 多实例 MySQL 主从复制 MySQL 常用调优策略 硬件层优化 磁盘 I/O 优化 文件系统层优化 内核参数优化 MySQL 高可用概述MySQL 多实例在一台服务器上，配置多个 MySQL 使用不同端口同时运行。为了便于管理 mysql 多实例，最好将配置文件、数据文件等单独存放在一个目录进行统一管理。 mkdir -p /mysql/&#123;3306,3307&#125;/&#123;data,conf,log&#125; 可以下载 mysql 的二进制包，解压后放在/usr/local/mysql中 /usr/local/mysql/├── LICENSE├── README├── bin├── docs├── include├── lib├── man├── share└── support-files 创建用户 mysql，并赋权chown -R mysql:mysql /usr/local/mysql 创建配置文件my.cnf，以下为配置模板 [client]port = 3306socket = /mysql/3306/mysql.sock[mysql]no-auto-rehash[mysqld]user = mysqlport = 3306socket = /mysql/3306/mysql.sockbasedir = /usr/local/mysqldatadir = /mysql/3306/datapid-file = /mysql/3306/mysql.pidserver-id = 1 # 不同实例的id,不同的实例配置中务必要用不同的idlog-bin = /mysql/3306/mysql-binlog-error = /mysql/3306/log/error.loglog-slow-queries = /mysql/3306/log/slow.logopen_files_limit = 1024back_log = 600max_connections = 800max_connect_errors = 3000table_cache = 614external-locking = FALSEmax_allowed_packet =8Msort_buffer_size = 1Mjoin_buffer_size = 1Mthread_cache_size = 100thread_concurrency = 2query_cache_size = 2Mquery_cache_limit = 1Mquery_cache_min_res_unit = 2k#default_table_type = InnoDBthread_stack = 192K#transaction_isolation = READ-COMMITTEDtmp_table_size = 2Mmax_heap_table_size = 2Mlong_query_time = 1#log_long_formatrelay-log = /mysql/3306/log/relay-binrelay-log-info-file = /mysql/3306/log/relay-log.infobinlog_cache_size = 1Mmax_binlog_cache_size = 1Mmax_binlog_size = 2Mexpire_logs_days = 7key_buffer_size = 16Mread_buffer_size = 1Mread_rnd_buffer_size = 1Mbulk_insert_buffer_size = 1M#myisam_sort_buffer_size = 1M#myisam_max_sort_file_size = 10G#myisam_max_extra_sort_file_size = 10G#myisam_repair_threads = 1#myisam_recoverlower_case_table_names = 1skip-name-resolveslave-skip-errors = 1032,1062replicate-ignore-db=mysqlinnodb_additional_mem_pool_size = 4Minnodb_buffer_pool_size = 32Minnodb_data_file_path = ibdata1:128M:autoextendinnodb_file_io_threads = 4innodb_thread_concurrency = 8innodb_flush_log_at_trx_commit = 2innodb_log_buffer_size = 2Minnodb_log_file_size = 4Minnodb_log_files_in_group = 3innodb_max_dirty_pages_pct = 90innodb_lock_wait_timeout = 120innodb_file_per_table = 0[mysqldump]quickmax_allowed_packet = 2M[mysqld_safe] # 服务启动配置log-error = /mysql/3306/log/mysql_3306.errpid-file = /mysql/3306/mysqld.pid 创建 mysql 启动脚本 #!/bin/shport=3306mysql_user=&quot;mysql&quot;mysql_pwd=&quot;123456&quot;CmdPath=&quot;/usr/local/mysql/bin/&quot;mysql_sock=&quot;/mysql/$&#123;port&#125;/mysql.sock&quot;#start Mysql Servicesfunction_start_mysql()&#123; if [ ! -e &quot;$mysql_sock&quot; ];then printf &quot;Starting MySQL...\\n&quot; /bin/sh $&#123;CmdPath&#125;/mysqld_safe --defaults-file=/mysql/$&#123;port&#125;/conf/my.cnf 2&gt;&amp;1 &gt; /dev/null &amp; else printf &quot;MySQL is running...\\n&quot; exit fi&#125;#stop Mysql Servicesfunction_stop_mysql()&#123; if [ ! -e &quot;$mysql_sock&quot; ];then printf &quot;MySQL is stopped...\\n&quot; exit else printf &quot;Stoping MySQL...\\n&quot; $&#123;CmdPath&#125;/mysqladmin -u $&#123;mysql_user&#125; -p$&#123;mysql_pwd&#125; -S /mysql/$&#123;port&#125;/mysql.sock shutdown fi&#125;#restart Mysql Servicesfunction_restart_mysql()&#123; printf &quot;Restarting MySQL...\\n&quot; function_stop_mysql sleep 2 function_start_mysql&#125;case $1 instart) function_start_mysql;;stop) function_stop_mysql;;restart) function_restart_mysql;;*) printf &quot;Usage: /mysql/$&#123;port&#125;/mysql &#123;start|stop|restart&#125;\\n&quot;esac MySQL 主从复制MySQL 提供了灵活的主从复制机制，可实现一主一从、一主多从、多主一从、主主复制、联级复制。 MySQL 复制原理：MySQL 使用二进制日志（默认未启用），二进制日志会记录所有修改数据库的 SQL 语句。 主服务器上执行了 SQL 操作后，会记录到二进制日志 binlog 从服务器生成两个线程，一个是 I/O 线程，一个是 SQL 线程，I/O 线程请求主数据库的 binlog，将得到的 binlog 写入中继日志 relay log。同时，从库的I/O线程会将主库的信息（即从库上配置的MASTER_HOST、MASTER_LOG_FILE、MASTER_LOG_POS等信息）存储在master.info文件，并实时更新 从数据库的 SQL 进程读取 relay log，并开启事务执行最新的一条 SQL 指令，实现主从一致 MySQL 常用调优策略硬件层优化修改服务器 BIOS 设置 选择 DAPC 模式，发挥 CPU 的最大性能 Memory Frequency（内存频率）选择 Maximum Performance 内存设置中启用 Node Interleaving，避免 NUMA 问题 磁盘 I/O 优化 使用 SSD 使用磁盘阵列，并使用阵列卡，同时配备 CACHE 及 BBU 模块 RAID 尽量选择 RAID10，而不是 RAID5 文件系统层优化 使用 deadline 或 noop 这两种 I/O 调度器，不要用 cfq 使用 xfs 文件系统，不要用 ext3，尽量不用 ext4 文件系统挂载 mount 参数中添加noatime，nobarrier选项 内核参数优化 修改vm.swappiness参数，降低 swap 使用率，尽量设为 5 到 10，最好不要设为 0，防止 OOM 故障 调整vm.dirty_background_ratio和vm.dirty_ratio参数，确保能持续将脏数据刷新到磁盘，避免瞬时 I/O 写 调整net.ipv4.tcp_tw_recycle和net.ipv4.tcp_tw_reuse都为 1，减少 TIME_WAIT，调高 TCP 效率","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"高可用","slug":"高可用","permalink":"https://coconutmilktaro.top/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"},{"name":"MySQL","slug":"MySQL","permalink":"https://coconutmilktaro.top/tags/MySQL/"}]},{"title":"DRBD数据同步笔记","slug":"drbd数据同步笔记","date":"2018-09-30T12:50:41.000Z","updated":"2022-05-30T02:51:53.878Z","comments":true,"path":"2018/drbd数据同步笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/drbd%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%AC%94%E8%AE%B0/","excerpt":"DRBD 概述 DRBD 功能 DRBD 基础搭建 准备 搭建","text":"DRBD 概述 DRBD 功能 DRBD 基础搭建 准备 搭建 DRBD 概述Distributed Replicated Block Device 分布式复制块设备是一种基于软件的，无共享，复制的存储解决方案。可以实现在网络中的两台服务器之间基于块设备级别的实时或异步镜像或同步复制，类似于 rsync+inotify，是基于 TCP/IP 网络的 RAID1。DRBD 是基于文件系统底层的，即 block 层级同步，效率较 rsync 更快。块设备可以是磁盘分区、LVM 逻辑卷、磁盘等。 DRBD 工作在文件系统的层级以下，比文件系统更加靠近操作系统内核以及 I/O。当 DRBD 使用在两台 HA 服务器上，一旦数据写入其中一台的本地磁盘，则该数据会被实时发送到另一台主机，以相同形式记录在文件系统中，实现主备数据同步。 DRBD 镜像的特点： 实时：应用程序修改设备上的数据时，会不断进行复制。 透明：应用程序无需知道数据存储在多个主机上。 同步或异步。使用同步镜像，在所有主机上执行写入后，应用程序会收到写入完成的通知。使用异步镜像，当写入在本地完成时，应用程序会收到写入完成的通知，这通常是在数据传播到其他主机之前。 DRBD 的核心功能是通过 Linux 内核模块实现的。具体而言，DRBD 构成虚拟块设备的驱动程序，因此 DRBD 位于系统 I / O 堆栈的底部附近。因为 DRBD 的定义以及 Linux 内核体系结构强制要求，DRBD 不依赖于它上面的层，也就无法管理上一层（即文件系统层），例如 DRBD 无法自动检测文件系统损坏。 其中，File system为 API 向外输出的接口，Buffer cache用来缓存数据与元数据的，Disk Scheduler是用来排序内存中即将要写入磁盘的数据合并读请求。 DRBD 的两种同步模式： 实时同步：当数据写入到本地磁盘和远端所有服务器磁盘都成功后才返回成功写入信息，可以防止本地和远端数据丢失或不一致，在生产环境中最常用。 异步同步：当数据写入到本地服务器成功后就直接返回成功写入信息，不管远端服务器是否写入成功。 DRBD 功能两种工作模式： 单主模式（Single-primary mode）：资源在任何给定时间仅在一个集群节点上的主要角色（primary role）中。由于保证只有一个集群节点可以随时处理数据，因此该模式可以与任何传统文件系统（ext3，ext4，XFS 等）一起使用。在单主模式下部署 DRBD 是高可用性（具有故障切换功能）集群的规范方法。 双主模式（Dual-primary mode）：资源在任何给定时间都在集群节点上的主要角色中。由于可以同时访问数据，因此该模式需要使用利用分布式锁管理器的共享集群文件系统，如 GFS 和 OCFS2。在双主模式下部署 DRBD 是负载均衡集群的首选方法，这些集群需要从两个节点进行并发数据访问。默认禁用此模式，必须在 DRBD 的配置文件中显式启用。 复制模型（Replication modes）：DRBD 支持三种不同的复制模式，允许三种复制同步性。 Protocol A：异步复制协议。一旦本地磁盘写入完成，则认为主节点上的本地写入操作已完成，并且复制数据包已放置在本地 TCP 发送缓冲区中。在强制故障转移的情况下，可能会发生数据丢失。故障转移后备用节点上的数据是一致的，但是崩溃之前执行的最新更新可能会丢失。协议 A 最常用于远程复制方案。与 DRBD Proxy 结合使用时，它可以提供有效的灾难恢复解决方案。 Protocol B：内存同步（半同步）复制协议。 一旦发生本地磁盘写入，并且复制数据包已到达对端节点，则认为主节点上的本地写入操作已完成。通常，在强制故障转移的情况下不会丢失写入。但是，如果两个节点同时出现电源故障并且主要数据存储的并发，出现不可逆转的破坏，则在主节点上完成的最近写入可能会丢失。 Protocol C：同步复制协议。只有在确认了本地和远程磁盘写入后，才认为主节点上的本地写操作已完成。因此，单个节点的宕机不会导致任何数据丢失。但是如果两个节点（或其存储子系统）同时被不可逆转地销毁，数据丢失也是不可避免的。 生产环境最常用的就是协议 C 复制协议的选择会影响部署的两个要素：保护（protection）和延迟（latency）。吞吐量（thoughput）很大程度上与所选的复制协议无关。 脑裂与自动修复 DRBD 脑裂：节点间由于网络故障或集群软件错误导致 DRBD 两个节点都切换为主节点而断开连接。在 8 版本后，DRBD 实现了脑裂的自动修复以及脑裂通知。DRBD 提供以下的修复策略： 丢弃较新的主节点的修改，即因脑裂升为主节点的节点。 丢弃老的主节点的修改 丢弃修改较少的主节点的修改。会先比较两个节点的数据，再丢弃修改较少的节点的数据 一个节点数据没有发生变化的情况下完美修复脑裂 DRBD 基础搭建准备实验继续沿用 heartbeat 的实验环境： Ubuntu18.04，heartbeat3.0 ubuntu-s1：172.16.246.155（ens33） heartbeat（ens37）：192.168.60.100 ubuntu-s2：172.16.246.156（ens33） heartbeat（ens37）：192.168.60.101 heartbeat 配置沿用实验的。 # ha.cf的配置node ubuntu-s1node ubuntu-s2auto_failback onmcast ens33 225.0.0.1 694 1 0bcast ens37debugfile /var/log/ha-debuglogfile /var/log/heartbeat.loglogfacility local2keepalive 2deadtime 30warntime 10initdead 120# authkeys的配置auth 11 sha1 3c767c41afb12ada140190ed82db3fd930e2efa3# haresources的配置ubuntu-s1 IPaddr::172.16.246.200/24/ens33ubuntu-s2 IPaddr::172.16.246.201/24/ens33 重启 heartbeat 服务，查看两台主机的 IP 地址 # ubuntu-s1inet 172.16.246.155/24 brd 172.16.246.255 scope global dynamic ens33inet 172.16.246.200/24 brd 172.16.246.255 scope global secondary ens33:6# ubuntu-s2inet 172.16.246.156/24 brd 172.16.246.255 scope global dynamic ens33inet 172.16.246.201/24 brd 172.16.246.255 scope global secondary ens33:6 并且一定要做到时间同步。两台虚拟机都加上一块硬盘，大小设为 1G。并且对新加磁盘sdb分区。分两个主分区，一个约 800M，一个约 200M。800M 的作为存储实际业务数据的分区，200M 的作为 meta data 分区，存储 drbd 同步的状态信息。 Device Boot Start End Sectors Size Id Type/dev/sdb1 2048 1640447 1638400 800M 83 Linux/dev/sdb2 1640448 2097151 456704 223M 83 Linux 在生产环境中，meta data 分区一般给 1-2G 大小。 分区完后，meta data 分区不要格式化建文件系统，且暂时都不要挂载。在sdb1上建文件系统。mkfs.ext4 /dev/sdb1 注：如果数据大小超过 2T，则需要用parted命令分区，不能用fdisk了。 安装 DRBD，在 ubuntu18.04 的库中软件叫做drbd-utils，安装即可，版本为 8.9，两台都要装。 安装完成后，查看 drbd 模块是否加载 # lsmod | grep drbddrbd 360448 0lru_cache 16384 1 drbdlibcrc32c 16384 4 nf_conntrack,nf_nat,drbd,raid456 源码编译安装 drbd 驱动，版本为 9.0.16。下载源码包，进入解压目录。 直接make &amp;&amp; make install，然后要modprobe drbd。在查看是否加载成功 # lsmod | grep drbddrbd 550058 0libcrc32c 12644 4 xfs,drbd,nf_nat,nf_conntrack 还是该网站下载 drbd-utils，版本需要看编译驱动后的说明。 Again: to manage DRBD 9 kernel modules and above,you want drbd-utils &gt;= 9.3 from above url. 这里下载了 drbd-utils-9.6 的包，进入解压目录 ./configure --prefix=/usr/local/drbd9 \\ --with-heartbeat \\ 允许heartbeat的haresources脚本 --sysconfdir=/etc/ 设置配置文件目录 然后make &amp;&amp; make install即可。 可能会报错make[1]: *** [drbdsetup.8] 错误 4，需要安装依赖docbook-style-xsl 搭建drbd 的配置文件为/etc/drbd.conf和/etc/drbd.d/中的global_common.conf以及所有.res结尾文件，.res结尾的文件是专门用来配置资源的。用于配置 drbd 参数的文件是global_common.conf，有模板文件/usr/share/doc/drbd-utils/examples/drbd.conf.example.gz参考。 global &#123; # 全局配置 usage-count no; # 不允许网站统计开源软件的安装量&#125;common &#123; handlers &#123; &#125; startup &#123; &#125; options &#123; &#125; disk &#123; &#125; net &#123; protocol C; &#125;&#125; 创建资源配置文件heartbeat.res resource data &#123; # 资源名，随便起 protocol C; # 采用协议C disk &#123; # 磁盘配置，可不配 on-io-error detach; &#125; on ubuntu-s1 &#123; # 服务器配置 device /dev/drbd1; # drbd会用专门的设备写数据 disk /dev/sdb1; # 数据存放磁盘 address 192.168.60.200:7789; # 主机的心跳线IP地址，后面的端口不用改 meta-disk /dev/sdb2[0]; # 存放meta data的分区 &#125; on ubuntu-s2 &#123; device /dev/drbd1; disk /dev/sdb1; address 192.168.60.201:7789; meta-disk /dev/sdb2[0]; &#125;&#125; 初始化 DRBD 的 meta data，创建 DRBD 记录信息的 meta data 分区元数据。 drbdadm create-md data，其中 data 就是在 res 文件中配置的资源名 初始化启动 DRBDdrbdadm up data，没报错说明启动成功。查看进程 # ps -ef | grep drbdroot 507 2 0 17:25 ? 00:00:00 [drbd-reissue]root 2682 2 0 20:15 ? 00:00:00 [drbd1_submit]root 2689 2 0 20:15 ? 00:00:00 [drbd_w_data]root 2877 2 0 21:06 ? 00:00:00 [drbd_r_data]root 2902 2 0 21:11 ? 00:00:00 [drbd_a_data]root 2903 2 0 21:11 ? 00:00:00 [drbd_as_data] 如果报错，可能是磁盘分区的问题，或者 drbd 的 meta data 创建的问题。 可以通过cat /proc/drbd查看 DRBD 的状态。以下是初始时的状态。 version: 8.4.10 (api:1/proto:86-101)srcversion: 17A0C3A0AF9492ED4B9A418 1: cs:WFConnection ro:Secondary/Unknown ds:Inconsistent/DUnknown C r----- ns:0 nr:0 dw:0 dr:0 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:818944 如果 ro 为Secondary/Unknown，说明 DRBD 没有连通，是网络的问题，检查网卡、路由。修复后再查看。 1: cs:Connected ro:Secondary/Secondary ds:Inconsistent/Inconsistent C r----- ns:0 nr:0 dw:0 dr:0 al:8 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:818944 两台 drbd 都是Secondary/Secondary则成功。ds为Inconsistent表示两端的数据还不一致。 在 ubuntu-s1 上使自身成为 primary 节点，drbdadm primary data，如果报错，可以加--force强制执行。 1: State change failed: (-2) Need access to UpToDate data 再查看两台主机的/proc/drbd，两台主机的 ro 是相反的。并且已完成了同步 # ubuntu-s1 1: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r----- ns:16976 nr:0 dw:0 dr:17616 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:802224 [&gt;....................] sync&#x27;ed: 3.0% (802224/819200)K finish: 0:04:39 speed: 2,828 (2,828) K/sec# ubuntu-s21: cs:SyncTarget ro:Secondary/Primary ds:Inconsistent/UpToDate C r----- ns:0 nr:41984 dw:41984 dr:0 al:0 bm:0 lo:1 pe:1 ua:0 ap:0 ep:1 wo:f oos:777216 [&gt;...................] sync&#x27;ed: 6.0% (777216/819200)K finish: 0:03:05 speed: 4,196 (4,196) want: 8,120 K/sec 如果状态为Secondary/Unknown，还有可能是出现了脑裂。可以在备节点上操作 drbdadm secondary datadrbdadm connect --discard-my-data data 然后在主节点操作drbdadm connect data，再查看是否解决。 在主节点挂载 DRBD 设备mount /dev/drbd1 /data，备节点是不能挂载的。 在主节点上测试dd if=/dev/zero of=/data/zero count=1000000，然后在备节点上查看/proc/drbd 1: cs:Connected ro:Secondary/Primary ds:UpToDate/UpToDate C r----- ns:0 nr:575700 dw:575700 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0 容量也同步为 500 多 M，说明数据同步成功。 参考文章 DRBD 工作原理及安装配置详解 DRBD 原理知识 DRBD 官方文档 【高可用 HA】HA 之 DRBD 详解（基于 CentOS7.0）","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"高可用","slug":"高可用","permalink":"https://coconutmilktaro.top/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"},{"name":"集群","slug":"集群","permalink":"https://coconutmilktaro.top/tags/%E9%9B%86%E7%BE%A4/"},{"name":"同步","slug":"同步","permalink":"https://coconutmilktaro.top/tags/%E5%90%8C%E6%AD%A5/"},{"name":"DRBD","slug":"DRBD","permalink":"https://coconutmilktaro.top/tags/DRBD/"}]},{"title":"Heartbeat笔记","slug":"Heartbeat笔记","date":"2018-09-30T10:27:26.000Z","updated":"2022-05-30T02:51:53.802Z","comments":true,"path":"2018/Heartbeat笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Heartbeat%E7%AC%94%E8%AE%B0/","excerpt":"Heartbeat 概念 脑裂 heartbeat 消息类型 heartbeat IP 地址接管及故障转移 Heartbeat 配置 配置文件参数 ha.cf authkeys haresources 实际配置 将 apache 交给 heartbeat 管理 故障排查 参考文章","text":"Heartbeat 概念 脑裂 heartbeat 消息类型 heartbeat IP 地址接管及故障转移 Heartbeat 配置 配置文件参数 ha.cf authkeys haresources 实际配置 将 apache 交给 heartbeat 管理 故障排查 参考文章 Heartbeat 概念Heartbeat 项目是 Linux-HA 工程的一个组成部分，它实现了一个高可用集群系统。可以将资源（IP 及程序服务等资源）从一台故障计算机快速转移到另一台运转正常的机器继续提供服务。 通过修改 heartbeat 的配置文件，可以指定一台 heartbeat 服务器作为主服务器，另一台自动成为热备服务器。在热备服务器上面配置 heartbeat 守护程序来监听来自主服务器的心跳信息。如果在规定时间内，无法监听到心跳信息，那么就启动故障转移，取得主服务器上的相关资源的所有权，接替主服务器继续不间断的提供服务，从而达到资源以及服务高可用的目的。而 heartbeat 还支持主主模式，即两台服务器互为主备，互相监听，发送心跳报文。 注：heartbeat 的业务切换时间大概在 5 到 20 秒，所谓的业务不间断其实是保障业务一致，不会造成数据错误。heartbeat 的高可用是服务器级别的，而不是服务级别的。 业务切换的常见条件为： 服务器宕机 心跳线故障 heartheat 服务本身故障 heartbeat 服务器间通信的方法： 串口线缆。线缆专门进行心跳通信，稳定，且不用配 IP 地址。缺点：服务器距离不能远。 两台服务器网卡通过以太网线直连。推荐使用 两台服务器网卡通过以太网设备连接。不稳定 heartbeat 应用场景： 主要用于双机场景，如：web 服务器、数据库、文件服务器、负载均衡器、代理服务器 而如果负载均衡采用了 LVS，则最好不要使用 heartbeat，而是使用 keepalived，因为 heartbeat 没有对下面节点 RS 的健康状态检查。 heartbeat 适合更多机器的连接，keepalived 在多机（超过两台）上可能会出问题 需要数据同步的业务最好使用 heartbeat，如：mysql 双主多从、NFS/MFS 等。也可配合 drbd 服务同步。如果使用 inotify+rsync 解决了同步问题，也可以用 keepalived。 脑裂两台正常运行的高可用服务器在心跳超时内无法监听到对方心跳报文，于是各自启动了故障转移，获取了资源的所有权，两台服务器都拥有同一个 VIP 地址，数据会出现不一致或丢失，这种情况称为脑裂（split brain）。 发生脑裂的原因： 高可用服务器对之间的心跳链路故障，导致无法正常通信 心跳线故障 网卡或相关驱动故障 IP 配置冲突 心跳线间连接的设备故障，如交换机 仲裁机器故障 高可用服务器上开启了防火墙，过滤掉了心跳报文 心跳配置不一致，如心跳方式、心跳广播冲突，以及软件 BUG 等 防止脑裂的方法： 同时使用串口线缆和以太网线缆，组成两条心跳线 检测到脑裂时强行关闭一个节点，若备节点认为主节点故障，则会自动向主节点发送关机命令（此功能需要特殊设备支持，如 STONITH、fencing） 对脑裂的告警，及时采取措施 启用磁盘锁，正在提供服务的一方锁住共享磁盘，即使发生脑裂也不会出现数据不一致或丢失情况。 增加仲裁机制。例如设置参考 IP 地址，若能 ping 通的服务器则接管服务，ping 不通的服务器主动放弃竞争。 STONITH：Shoot-The-Other-Node-In-The-Head，是 heartbeat 的一个组件，能够保护数据使其不会因为节点异常或者同时访问而遭到损坏。用于集群服务无法停下的情况，在这种情况下，集群可以使用 STONITH 来强制整个节点离线，并让服务在其它节点上安全启用。 heartbeat 消息类型三种 heartbeat 消息类型： 心跳消息：控制心跳频率和出现故障后进行故障转换的等待时间。可以单播广播和组播，约 150 字节。 集群转换消息：ip-request和ip-request-resp 当主服务器恢复后，使用ip-request消息要求备机将服务的提供权交还给主服务器。备服务器将服务提供权释放后，通过ip-request-resp通知主服务器，主节点收到后开始正常提供服务 重传消息：rexmit-request控制重传心跳请求 heartbeat IP 地址接管及故障转移heartbeat 通过 IP 地址接管和 ARP 广播进行故障转移。为防止 ARP 老化时间内，客户端仍请求已故障的服务器，备服务器会进行强制所有客户端进行 ARP 表项刷新。 VIP 为对外提供服务的 IP 地址，因此需要在 DNS 上配置将网站的域名解析到这个 VIP。有两种手工配置 VIP 的方法： ifconfig eth0:1 [IP地址] netmask [掩码] up ip addr add [IP地址/掩码] broadcast [该网段广播地址] dev eth1 注：ip addr能看到网卡别名和 VIP，而ifconfig无法看到。 Heartbeat 配置在红帽系的库（包括 epel）中已经没有 heartbeat 了，在 ubuntu 的库中还有，所以用 ubuntu 做实验。ubuntu 版本 18.04，heartbeat 版本 3.0.6。 heartbeat 的默认配置文件目录为/etc/ha.d，主要的配置文件存放在/usr/share/doc/heartbeat中。 ha.cf.gz，是被 gz 压缩的，解压后得到ha.cf。是 heartbeat 的参数配置文件 authkeys，是 heartbeat 认证文件 haresources.gz，也是 gz 压缩，解压得到haresources，是 heartbeat 资源配置文件 将这三个文件复制到/etc/ha.d中 heartbeat 的资源目录为/etc/ha.d/resources.d/，可以将开发的程序直接放在该目录中，然后在haresources中调用。 实验环境： 两台 ubuntu 作为负载均衡器，要有双网卡，一个提供服务，一个做心跳线 负载均衡器 1（master）：172.16.246.155（网卡 ens33） heartbeat 网卡：192.168.60.100（网卡 ens37） 负载均衡器 2（backup）：172.16.246.156（网卡 ens33） heartbeat 网卡：192.168.60.101（网卡 ens37） 设置好/etc/hosts，使能通过主机名访问，主机名要和uname -n的结果一致。 配置文件参数列举的是常用参数，并没有修改为实验用的值 ha.cfdebugfile /var/log/ha-debug # 调试日志位置logfile /var/log/ha-log # 日志位置logfacility local0 # 日志设备keepalive 2 # 心跳间隔deadtime 30 # 认为主节点宕机的超时时间warntime 10 # 心跳延迟时间，备份节点无法接收主节点的心跳时就会往日志写入一个警告日志initdead 120 # heartbeat在首次运行后，需等待120秒才能启动主服务器的资源。取值至少为deadtime的两倍udpport 694 # UDP端口，默认为694mcast eth0 225.0.0.1 694 1 0 # 多播端口auto_failback on # 是否开启自动故障恢复。因故障而切换的资源是否要在主节点恢复后再切回主节点bcast eth0 # 广播端口node XXX # 节点名，先定义的是主节点，后定义的都是备用节点 authkeys# 可用的加密算法：crc、sha1、md5。crc不需要密码。官方推荐sha1和md5。crc没有安全性auth 11 crc2 sha1 HI!3 md5 Hello! 注：authkeys文件的权限必须是600，否则无法启动 heartbeat haresources只要添加 heartbeat 的两台主机即可，指定 VIP，无须手动创建 ubuntu-s1 IPaddr::172.16.246.200/24/ens33ubuntu-s2 IPaddr::172.16.246.201/24/ens33# 其中的IPaddr是/etc/ha.d/resources.d/中的IPaddr脚本 实际配置ha.cf配置，两台都要配 debugfile /var/log/ha-debuglogfile /var/log/heartbeat.loglogfacility local2keepalive 2deadtime 30warntime 10initdead 120mcast ens33 225.0.0.1 694 1 0bcast ens37auto_failback onnode ubuntu-s1node ubuntu-s2 authkeys配置，先用命令sha1sum -t输入密码生成密钥。然后复制到文件中，只保留 sha1。 auth 11 sha1 3c767c41afb12ada140190ed82db3fd930e2efa3 修改haresources，配置的是 VIP，不需要手动创建 ubuntu-s1 IPaddr::172.16.246.200/24/ens33ubuntu-s2 IPaddr::172.16.246.201/24/ens33 这三个文件都要做到两端一致（除了日志的设定可以不一致）。然后启动 heartbeat，systemctl start hearthbeat 等待initdead的时间后，查看网卡，可以发现 VIP 的网卡已自动创建 ens33:1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.16.246.200 netmask 255.255.255.0 broadcast 172.16.246.255 ether 00:0c:29:93:da:9b txqueuelen 1000 (Ethernet) 模拟一台宕机，关闭ubuntu-s1的 heartbeat，查看 ubuntu-s2。发现成功迁移。 ens33:1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.16.246.201 netmask 255.255.255.0 broadcast 172.16.246.255 ether 00:0c:29:e5:d5:55 txqueuelen 1000 (Ethernet)ens33:2: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.16.246.200 netmask 255.255.255.0 broadcast 172.16.246.255 ether 00:0c:29:e5:d5:55 txqueuelen 1000 (Ethernet) 重启 ubuntu-s1 的 heartbeat，再查看 ubuntu-s2 的网卡，已经迁移回去，因为开启了auto_failback ens33:1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.16.246.201 netmask 255.255.255.0 broadcast 172.16.246.255 ether 00:0c:29:e5:d5:55 txqueuelen 1000 (Ethernet) 将 apache 交给 heartbeat 管理将命令apachectl复制到/etc/ha.d/resource.d/中，然后修改haresources文件，在一个主机的最后加上apachectl ubuntu-s1 IPaddr::172.16.246.200/24/ens33 apachectl 注意： 脚本要放在/etc/ha.d/resource.d/中 脚本的执行要有 start 和 stop 两个参数 脚本要有可执行权限 haresources文件中的脚本名一定是resource.d中的指定脚本 关闭 apache 和 heartbeat，然后只启动 heartbeat，但是 apache 也被启动了。可看到日志中 ResourceManager(default)[8373]: info: Running /etc/ha.d/resource.d/apachectl start heartbeat 两种方法实现高可用： 仅控制 VIP 资源转移，而不负责资源的启动与关闭。一般用于 web 服务 既控制 VIP 资源转移，又负责资源的启动与关闭。一般用于数据库、存储服务，为了控制数据一致性，防止两台都在写。 故障排查若主节点出现故障，可以使用命令hb_standby将业务推到备节点，再对主节点配置进行检查。该脚本存放在/usr/share/heartbeat/中。 将故障排除后，还可执行脚本/usr/share/heartbeat/hb_takeover再次接管业务。 参考文章 Heartbeat 介绍 Heartbeat 高可用解决方案 heartbeat 单独提供高可用服务","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"heartbeat","slug":"heartbeat","permalink":"https://coconutmilktaro.top/tags/heartbeat/"},{"name":"高可用","slug":"高可用","permalink":"https://coconutmilktaro.top/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"},{"name":"集群","slug":"集群","permalink":"https://coconutmilktaro.top/tags/%E9%9B%86%E7%BE%A4/"}]},{"title":"Sed、Awk与Shell编程","slug":"Sed与Awk与Shell编程","date":"2018-09-29T04:08:18.000Z","updated":"2022-05-30T02:51:53.870Z","comments":true,"path":"2018/Sed与Awk与Shell编程/","link":"","permalink":"https://coconutmilktaro.top/2018/Sed%E4%B8%8EAwk%E4%B8%8EShell%E7%BC%96%E7%A8%8B/","excerpt":"Sed Awk Shell 编程","text":"Sed Awk Shell 编程 Sedsed 是一个面向字符流的编辑器，对文本进行过滤和替换操作，sed 一次仅读取一行进行操作，适合处理大数据文件。可在一个或多个文件上自动实现编辑，简化对多个文件执行相同的编辑处理工作。 sed 默认不修改源文件，仅仅修改输出信息，sed 先将从文件读入的内容放入缓冲区，称为模式空间，在模式空间中对文件的副本操作，再输出到屏幕。 sed [选项]... [输入文件]... -n, --quiet, --silent 静默模式，结果不显示到屏幕 -e 脚本 添加脚本指令，可添加多个 -f 脚本文件 添加脚本文件 --follow-symlinks 直接修改文件时跟随软链接 -i[SUFFIX] 直接修改源文件，若指定SUFFIX前缀，则进行对源文件的备份 -l N 指定l命令（输出非打印字符）可输出的行长度 --posix 关闭所有 GNU 扩展 -s, --separate 默认sed将输入的多个文件名当做一个长的输入流，而GNU sed允许看做单独的文件 -u, --unbuffered 从输入文件读取最少的数据，即最低限度的缓存输入和输出 sed 的指令：[地址]指令 内容，在 sed 中/称为定界符，也可用其他的符号作为定界符，如:或|等。 a：append 追加，若不指定行数，则会在每一行后都添加一行内容 例：sed &#x27;2a XXXXX&#x27; 文件 在文件的第二行后添加一行内容XXXXX sed &#x27;/XXX/a XXXXX&#x27; 文件 在所有包含XXX的行后添加一行XXXXX sed &#x27;1,4a XXXXX&#x27; 在第1到4行后添加一行XXXXX sed &#x27;$a XXXXX&#x27; 在最后一行后添加一行，$表示最后一行 i：insert 插入，是在行前添加一行内容，若不指定行数，则在每一行前添加 例：sed &#x27;2i XXXXX&#x27; 文件 在第二行前添加一行XXXXX sed &#x27;/XXX/i XXXXX&#x27; 在包含XXX的行前添加一行XXXXX d：delete 删除 例：sed &#x27;2d&#x27; 删除第二行 sed &#x27;/^$/d&#x27; 删除空白行 sed &#x27;1~2d&#x27; ~用于指定从第几行开始的指定步长行的内容 1~2用于指定第1行开始的两行，即第1,2行 s：substitution 替换 例：sed &#x27;s/XXX/XXXX/&#x27; 将XXX替换为XXXX，会替换第一个匹配的 sed &#x27;s/XXX/XXXX/n&#x27; 只替换第n个匹配的XXX，n的范围是1-512 sed &#x27;s/XXX/XXXX/g&#x27; 对模式空间的所有匹配都更改 c：替换 例：sed &#x27;/XXX/c XXXXX&#x27; 将包含XXX的一行替换为XXXXX p：打印 例：sed &#x27;s/XXX/XXXX/p&#x27; 替换后，打印替换后的句子（会重复打印）以及其他未替换的内容 若和-n一起使用，则只打印进行处理的行 n：一遇到匹配的行就立刻移动到下一行 若要执行多个指令，则指令间用逗号分隔，或通过-e 指令1 -e 指令2...指定，最好通过文件添加指令，然后通过-f指定指令文件。 还可使用/XXX/ &#123;指令/内容&#125;替换：匹配的语句支持正则表达式 例：&lt;body&gt;hello&lt;body&gt; sed &#x27;s/body/\\/body/2&#x27; 将第二个body换为/body，还可用&#123;&#125;实现 sed &#x27;/body/ &#123;s//\\/body/2&#125;&#x27; 就是s/后的要替换的内容提前到前面 还可以用&amp;替换要替换的部分 sed &#x27;/body/ &#123;s//\\/&amp;/2&#125;&#x27; 正则表达式\\w\\+匹配每一个单词，例：将每个单词都添加一个[] sed &#39;s/\\w\\+/[&amp;]&#39; \\n匹配子串，n 表示第 n 个子串，用\\(XXX\\) 匹配子串，会将 XXX 作为主串，将 XXX 后的字符串作为子串，例：将 abcdefg 中的 efg 替换为 fff：sed &#39;s/\\(abcd\\)efg/\\1fff&#39; 可通过在匹配的行间添加逗号选定行范围：sed &#39;/efg/,/abc/&#39; 将指定的内容添加到匹配的行下面：使用a\\指令，sed &#39;/abc/a\\test&#39;，将 test 字符串插入到匹配包含 abc 的行的下面。同理，i\\将指定内容添加到匹配的行上面 打印奇数行：sed -n &#39;p;n&#39;或sed -n &#39;1~2p&#39;打印偶数行：sed -n &#39;n;p&#39;或sed -n &#39;2~2p&#39; AwkAwk 是一种模式匹配的程序设计语言，用于对文本和数据进行扫描和处理，常用操作是将数据转换为格式化的报表。常见的 awk 编译器版本有 awk，gawk，gawk 与 awk 一致。 awk 先逐行扫描文件，寻找匹配特定模式的行，并进行操作。因此，awk 基本结构就是由模式匹配和处理动作组成。 Shell 编程三种命令行下运行 shell 脚本的方式： 直接使用bash命令执行，此时 shell 脚本不需要可执行权限，也不需要再第一行指定 shell 添加可执行权限，然后./XXX.sh执行 直接source命令执行 常见系统变量： $HOME：当前用户家目录 $IFS：内部字段分隔符 $LANG：默认语言 $PATH：默认可执行程序路径 $PWD：当前目录 $UID：当前用户 ID $USER：当前用户 $RANDOM：随机生成一个 0-32767 的整数 $HOSTNAME：主机名 变量： 普通变量：VAR=value 临时环境变量：export VAR=value 作用域： shell 进程的环境变量作用域为 shell 进程 当 export 导入到系统变量时，作用域变为 shell 进程及其子进程 ps -axjf | grep pts 938 1443 1443 1443 ? -1 Ss 0 0:00 \\_ sshd: root@pts/0 1443 1447 1447 1447 pts/0 1497 Ss 0 0:00 | \\_ -bash 1447 1497 1497 1447 pts/0 1497 R+ 0 0:00 | \\_ ps -axjf 1447 1498 1497 1447 pts/0 1497 S+ 0 0:00 | \\_ grep --color=auto pts第一列：PPID父进程ID第二列：PID子进程ID 当 ssh 连接 shell 时，当前终端 PPID（-bash）是 sshd 的 PID（root@pts/0），所以当前终端下的所有进程 PPID 都为-bash 的 PID在-bash 下设置的变量，只在-bash 进程下有效，而在-bash 的子进程中无效，只有export 后才能生效。所以在当前 shell 定义的变量一定要 export，否则在写脚本时，会引用不到。退出终端后，所有用户定义的变量都会清除。 位置变量指的是函数或脚本后跟的第 n 个参数。$1-$n，需要注意的是从第 10 个开始要用花括号调用，例如$&#123;10&#125; shift 可对位置变量控制，每执行一次 shift 命令，位置变量个数就会减一，而变量值则提前一位。shift n，可设置向前移动 n 位。 #!/bin/bashecho &quot;1: $1&quot;shiftecho &quot;2: $2&quot;shiftecho &quot;3: $3&quot;bash test.sh a b c1: a2: c3: 特殊变量： $0 脚本自身名字 $? 返回上一条命令是否执行成功，0 为执行成功，非 0 则为执行失败 $# 位置参数总数 $* 所有的位置参数被看做一个字符串 $@ 每个位置参数被看做独立的字符串 $$ 当前进程 PID $! 上一条运行后台进程的 PID 变量引用： Shell 中所有变量引用使用$符，后跟变量名。有时个别特殊字符会影响正常引用，那么需要使用$&#123;VAR&#125;，如$VAR_ 若变量名与其他字符串紧碍着，也会误认为是整个变量 将命令结果作为变量值，如 VAR=echo 123及VAR=$(echo 123) 在变量赋值时，如果值有空格，Shell 会把空格后面的字符串解释为命令 VAR=1 2 3-bash: 2: command not found 单引号是告诉 Shell 忽略特殊字符，而双引号则解释特殊符号原有的意义，比如$、!。 参考资料 Shell 从入门到精通","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://coconutmilktaro.top/tags/Linux/"},{"name":"sed","slug":"sed","permalink":"https://coconutmilktaro.top/tags/sed/"},{"name":"awk","slug":"awk","permalink":"https://coconutmilktaro.top/tags/awk/"}]},{"title":"Zabbix搭建笔记","slug":"zabbix搭建笔记","date":"2018-09-28T10:23:06.000Z","updated":"2022-06-29T15:39:30.260Z","comments":true,"path":"2018/zabbix搭建笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/zabbix%E6%90%AD%E5%BB%BA%E7%AC%94%E8%AE%B0/","excerpt":"zabbix 概述 zabbix 搭建 LNMP/LAMP 环境搭建 Zabbix zabbix 操作 监控一台主机 配置用户 配置主机 添加监控项 新建触发器 设置通知 使用服务器本地邮箱发送报警邮件 新建模板 新建图表 详细配置操作 主机资产管理 批量更新 Zabbix 事件 触发器 action 自动发现与自动注册 低级别发现 自定义监控项 zabbix 主动与被动模式 zabbix 实战 监控 MySQL 监控 Apache 监控 Nginx 监控 PHP-FTPM 监控 Tomcat 监控 Redis zabbix 与微信整合 zabbix 与 Logstash 整合","text":"zabbix 概述 zabbix 搭建 LNMP/LAMP 环境搭建 Zabbix zabbix 操作 监控一台主机 配置用户 配置主机 添加监控项 新建触发器 设置通知 使用服务器本地邮箱发送报警邮件 新建模板 新建图表 详细配置操作 主机资产管理 批量更新 Zabbix 事件 触发器 action 自动发现与自动注册 低级别发现 自定义监控项 zabbix 主动与被动模式 zabbix 实战 监控 MySQL 监控 Apache 监控 Nginx 监控 PHP-FTPM 监控 Tomcat 监控 Redis zabbix 与微信整合 zabbix 与 Logstash 整合 zabbix 概述Zabbix 是一个企业级开源的分布式监控套件，可以监控网络和服务的监控状况。 zabbix 组成：zabbix server和zabbix agent Zabbix Server 可通过 SNMP、Zabbix_agent、ping、端口扫描等方法提供对远程服务器的监视 Zabbix Agent 安装在需要被监控的目录服务器上收集信息。监听端口 10050 zabbix 核心组件： zabbix server：收集 agent 的监控信息，对数据统计操作，设置配置。zabbix server 可单独监控，也可与 agent 结合。可轮询 agent 主动接收监控数据，也可被动接收。监听端口 10051 zabbix databases：存储所有配置信息，以及监控数据。一般可以是：mysql，oracle，sqlite zabbix web GUI：通常与 server 运行在同一主机上（可在不同主机），用于可视化操作 zabbix 可选组件： proxy：代理服务器，用于分布式监控环境，代理 server 接收 agent 的监控数据，汇总后统一发往 server agent：被监控主机，收集本地数据 zabbix 也可用于监控 java 应用，可基于 JMX 组件监控 JVM zabbix 服务进程： zabbix_agentd：zabbix agent 的守护进程 zabbix_server：zabbix server 的守护进程 zabbix_get：zabbix 的一个工具，用于拉取远端客户端的信息，通常用于排错。需要安装 zabbix-get zabbix_sender：zabbix 的一个工具，用于主动推送数据给 server 或 proxy，通常用于耗时较长的检查或大量主机监控的场景。需要安装 zabbix-sender zabbix_proxy：zabbix proxy 的守护进程。需要安装 zabbix-proxy-mysql|pgsql|sqlite3 zabbix_java_gateway：java 网关，用于监控 java 应用环境，类似 agentd。只能主动推送数据。 常用术语： 监控项 item：一个特定的监控指标的数据，监控项是 zabbix 数据收集的核心 触发器 trigger：一个表达式，用于评估某监控对象的某特定 item 内所接收的数据是否在合理范围内，即阈值。当数据量大于阈值时，触发器状态从 ok 变为 problem 事件 event：发生的事情，如触发器状态的变化，新的 agent 或 agent 重新注册 动作 action：指对特定事件事先定义的处理方法，包含操作与条件 报警升级 escalation：发送警报或执行远程命令的自定义方案 媒介 media：发送通知的手段或通道，如 Email，jabber，SMS 通知 notification：通过选定的媒介向用户发送的有关某事件的信息 远程命令：预定义的命令，可在被监控主机处于某特定条件下自动执行 模板 template：用于快速定义被监控主机的预设条目集合，包含 item，trigger，graph，screen（多个 graph），application，low-level discovery rule。模板可以直接链接到单个主机 应用程序 application：一组 item 的集合 web 场景 web scennaria：用于检测 web 站点可用性的一个或多个 http 请求 Zabbix 特点： 配置简单：可使用模板，直接添加监控设备、可配置组监控、可对模板继承，进行精细设定 实时绘图，自定义监控图表（面板），支持网络拓扑图 灵活的告警机制：可自定义告警升级（escalation）、接受者和告警方式，还可通过远程命令实现自动化动作 action 可进行不同类型数据的收集：性能、SNMP、IPMI、JMX，可自定义收集数据的间隔 数据存储：可将数据存放在数据库中，并内置数据清理机制 网络自动发现机制：自动发现网络设备、文件系统、网卡等，agent 自动注册 zabbix 由 C 开发，高性能，内存消耗低。web 前段由 php 编写 提供丰富的 API 可进行权限认证，并进行访问控制 zabbix 搭建搭建 zabbix 监控服务器端 zabbix 需要 LAMP 或 LNMP 的环境，先安装以下环境gcc gcc-c++ autoconf automake zlib zlib-devel openssl openssl-devel pcre-devel 安装 php 环境：yum install php php-fpm 安装 mysql/mariadb 环境：yum install mariadb* LNMP/LAMP 环境搭建 Zabbix可通过 yum 安装 nginx，但版本不是最新的。通过源码安装 nginx 版本为 1.14。 首先创建 nginx 用户及用户组。然后下载源码包并解压，进入目录 ./configure --prefix=/usr/local/nginx \\ --sbin-path=/usr/sbin/nginx \\ --conf-path=/etc/nginx/nginx.conf \\ --error-log-path=/var/log/nginx/error.log \\ --pid-path=/var/run/nginx/nginx.pid \\ --lock-path=/var/lock/nginx.lock \\ --user=nginx \\ --group=nginx \\ --http-log-path=/var/log/nginx/access.log \\ --http-client-body-temp-path=/var/tmp/nginx/client \\ --with-http_ssl_module \\ --with-http_stub_status_module \\ --with-http_gzip_static_module \\ --with-http_dav_module \\ --with-http_stub_status_module \\ --with-http_addition_module \\ --with-http_flv_module \\ --with-http_mp4_module \\ --with-http_sub_module \\ --with-debug 进入/etc/nginx/nginx.conf添加一行user nginx nginx 安装 zabbix，首先去官网选择主机环境版本下载页，安装 zabbix 的 repo 源。 rpm -i https://repo.zabbix.com/zabbix/4.0/rhel/7/x86_64/zabbix-release-4.0-1.el7.noarch.rpm 若官方的源下载慢，则可使用 aliyun 的源，要自己配 repo [zabbix]name=Zabbix Official Repository - $basearchbaseurl=https://mirrors.aliyun.com/zabbix/zabbix/4.4/rhel/7/$basearch/enabled=1gpgcheck=0 然后安装zabbix-server-mysql zabbix-web-mysql zabbix-agent zabbix-web 若是客户端，不需要搭建 LAMP 或 LNMP 环境，只需要安装 repo 源和zabbix-agent和zabbix-sender，并且zabbix-sender也不是必须安装，若要主动向 zabbix 服务器发送监控数据时才需要安装。 zabbix 的几个目录： /etc/zabbix：zabbix 配置目录 /var/log/zabbix：zabbix 日志目录 /var/run/zabbix：zabbix 运行目录 /usr/lib/zabbix：zabbix 库文件目录 /usr/share/zabbix：zabbix 的 web 文件目录 修改 nginx 配置文件，找到下面配置，修改fastcgi_param后的路径为/usr/share/zabbix location ~ \\.php$ &#123; root html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /usr/share/zabbix$fastcgi_script_name; include fastcgi_params;&#125; 在 mysql 创建 zabbix 库，和管理数据库的用户 zabbix create database zabbixdb;grant all on zabbixdb.* to zabbix@127.0.0.1 identified by &#x27;zabbix&#x27;;flush privileges; 导入 zabbix 的 sql 文件，sql 文件存放在/usr/share/doc/zabbix-server-mysql-3.4.14/create.sql.gz中，用gunzip create.sql.gz解压，然后导入mysql -u root -p zabbixdb &lt; create.sql 修改/etc/zabbix/zabbix_server.conf DBHost=localhostDBName=zabbixdbDBUser=zabbixDBPassword=zabbix 安装 zabbix 后，会自动创建系统用户 zabbix，但这个用户是设置了无法登录，而 zabbix 不允许。需要重新创建 zabbix命令 zabbix_server -c 指定配置文件，默认/etc/zabbix/zabbix_server.conf -f 在前台运行zabbix_server -R 执行运行时管理功能，功能如下 config_cache_reload 重新读取配置缓存 housekeeper_execute 执行housekeeper log_level_increase=target 提升日志等级，若不指定target则影响zabbix所有进程 log_level_decrease=target 降低日志等级，同上 #target可以是PID，进程类型zabbix_agentd 与zabbix_server参数一致，并多了下面的配置 -p 显示已知的items -t 测试指定的item 启动 zabbix_server 服务systemctl start zabbix-server.service或zabbix_server启动 在 apache 或 nginx 配置文件中创建虚拟主机后，通过浏览器访问 # apache虚拟主机配置&lt;VirtualHost *:80&gt; ServerName &quot;zabbix.monitor1.com&quot; DocumentRoot &quot;/usr/share/zabbix&quot; &lt;Directory &quot;/usr/share/zabbix&quot;&gt; Require all granted AllowOverride None &lt;/Directory&gt;&lt;/VirtualHost&gt;# nginx虚拟主机配置server &#123; listen 80; server_name zabbix.monitor1.com; location / &#123; root /usr/share/zabbix; index index.php index.html; &#125; location ~ \\.php$ &#123; root /usr/share/zabbix; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /usr/share/zabbix$fastcgi_script_name; include fastcgi_params; &#125;&#125; 根据网页提示，修改 php 配置文件/etc/php.ini # 时区错误date.timezone = Asia/Shanghai 全部修改完成后重启 php-fpm 和 httpd。再次访问安装界面，完成安装。默认登录用户为admin，默认登录密码zabbix 在监控主机上需要修改配置文件/etc/zabbix/zabbix_agentd.conf 最基本就只需要修改一项Server = 监控服务器的IP地址 并且默认不能以 root 身份运行 zabbix_agentd，可以修改配置文件 AllowRoot=1 # 是否允许root运行agentd，1为允许，0为不允许或修改User=zabbix # 运行agentd的用户，需要取消注释 使用 zabbix_get 工具检查是否能获取数据 # zabbix_get -s 192.168.80.128 -p 10050 -k &quot;system.uptime&quot;198526 zabbix 操作 监控一台主机 详细配置操作 监控一台主机配置用户 Administration –&gt; Users 创建一个用户 设置用户媒介（如何通知） 官方文档详细配置说明 配置主机 Configuration –&gt; Hosts 默认已存在一个主机 Zabbix server，监控本机。在 Create host 添加新主机。 注：如果是虚拟机主机，则需要在同一个网段 添加监控项 Configuration –&gt; Hosts –&gt; Items –&gt; create items 有几个需要填写的项： Name：监控项名 Key：监控项技术上的名称，即要获取的信息 Type of information：信息类型，即数据格式，有 Numeric（无符号/浮点）、character、log、text 其他选项详情 第一次获得的监控项值最多需要 60 秒才能到达。然后，默认 30 秒更新一次，可通过 Update interval 修改 然后在 Monitoring 的 Lateset data 中添加显示的主机或主机组。然后在下面添加项的右侧 Graph 查看图像。 新建触发器 Configuration –&gt; Hosts –&gt; Triggers –&gt; Create trigger 触发器表达式可直接 Add 选择，也可手动编写，触发器表达式语法 可在 Monitroing 的 Problems 中添加问题报告的主机和触发器。 触发器表达式格式： &#123;&lt;server&gt;:&lt;key&gt;.&lt;function&gt;(&lt;parameter&gt;)&#125;&lt;operator&gt;&lt;constant&gt;# server：主机名# key：监控项的键# function：触发器函数# parameter：触发器函数的参数（如果有的话）# operator：判断符号。有：&gt;、&lt;、&lt;&gt;、&gt;=、&lt;=、=# constant：常数值，即判断的数值 常见触发器函数 diff：返回值若为 1 表示最近的值与之前不同，0 为无变化 last：获取最近的值。需要指定参数#num，为最近的第 num 个值。例：last(#2) avg：返回一段时间的平均值。例：avg(5)为最近 5 秒的平均值，avg(#5)为最近五次的平均值，avg(3600,86400)为一天前的一个小时的平均值 change：返回最近获得值与之前获得值的差值，返回字符串 0 表示相等，1 表示不等。 nodata：是否能接收到数据，返回 1 表示指定的间隔内未收到数据，0 表示正常接收数据 count：返回指定时间间隔内数值的统计 sum：返回指定时间间隔中收集的值的总和。例：sum(600)表示 600s 内接收到所有值的和，sum(#5)表示最后 5 个值的和 设置通知 Administration –&gt; Media Types zabbix 中提供的几种媒介（Media）类型： Email：电子邮件 SMS：手机短信，通过连接至 zabbix 服务器 GSM Modem 发送通知 Jabber：jabber 消息。Jabber 是一个开放的基于 XML 的协议，能实现基于 Internet 的即时通讯服务 自定义脚本通知：调用位于配置文件的AlertScriptsPath变量定义的脚本目录中的脚本 使用服务器本地邮箱发送报警邮件首先安装 mailx 软件，直接 yum 安装即可。然后测试 echo &quot;test&quot; | mail -s &quot;test&quot; xxxx@qq.com 然后进入 zabbix web 的 Administration 中 Media types 新建一个媒介 一个媒体类型必须通过发送地址来关联用户，否则它将无法生效。 发送通知是 Zabbix 中动作（actions）执行的操作之一，因此为了建立一个通知，需要创建动作。 Configuration –&gt; Actions –&gt; Create action 新建模板 Configuration –&gt; Templates –&gt; Create template 在 Configuration 的 Hosts 中选择一个主机的 item，并点击 Copy 进行复制，在复制界面选择目的模板 通过此法向模板中添加监控项。 在 Host 的主机配置表中，选择 Templates，然后添加模板，先点 select 选模板，然后 add 添加。 新建图表 Configuration –&gt; Hosts –&gt; Graphs –&gt; Create graph show legend：是否显示图例 percentile line：是否显示百分位线，用作参考 Graph type：有四种图表 Normal：普通线图 Stacked：堆图 Pie：饼图 Exploded：爆炸图（分裂的饼图） 详细配置操作主机资产管理 Configuration –&gt; Hosts –&gt; Host inventory 有三种设置模式：disabled（关闭）、manual（手动）、automatic（自动） 手动模式需要输入设备类型、序列号等信息。自动模式会自动填充，需要在监控项中添加一些项才能实现。 system.hw.chassis[full|type|vendor|model|serial] - 默认是 [full], 需要root权限system.hw.cpu[all|cpunum,full|maxfreq|vendor|model|curfreq] - 默认是[all,full]system.hw.devices[pci|usb] - 默认是 [pci]system.hw.macaddr[interface,short|full] - 默认是 [all,full], interface支持正则表达式system.sw.archsystem.sw.os[name|short|full] - 默认是 [name]system.sw.packages[package,manager,short|full] - 默认是 [all,all,full], package支持正则表达式 可在 Inventory 中的 Hosts 查看配置的主机现有资产数据。 批量更新一次更改多个主机的某些属性。 Configuration –&gt; Hosts 选中多个主机，点下方的 Mass update。 Host 选项卡： Replace host groups：从任何现有主机组中删除主机，并替换为该字段中指定的主机 Add new or existing host groups：从现有主机组指定其他主机组 Remove host groups：从主机中删除特定主机组 Templates 选项卡： Link Templates：指定模板，可选择替换或添加。Clear when unlinking选项将不仅可以取消链接任何以前链接的模板，还可以删除所有继承自它们的元素（监控项、触发器等）。 Zabbix 事件事件是基于时间戳进行标记的，是采取动作的基础，来源于三个途径： 触发器事件：每次触发器状态改变就会生成相应事件 发现（discovery）事件：zabbix 会周期性扫描网络发现规则中的指定 IP 范围，一旦发现主机或服务，就会生成发现事件 有 8 类发现事件：Service Up，Service Down，Host Up，Host Down，Service Discovered，Service Lost，Host Discovered，Host Lost 主动 agent 自动发现事件：也称自动注册事件，当一个此前状态未知的主动 agent 发起检测请求时会生成该类事件 因此，Zabbix 的通知机制也称为基于事件的通知机制。 触发器当每次采集的数据超出了设置的触发器阈值，则触发器状态会变为Problem，若数据在范围之内，则触发器状态变为OK。 事件成功迭代（OK event generation）设置，用于控制如何生成正常事件（OK event） 表达式（Expression）：当表达式结果为 FALSE，Problem 会生成一个 OK 事件 恢复表达式（Recovery expression）：当表达式结果为 FALSE，且恢复表达式结果为 TRUE，Problem 状态会变为 OK 事件。如果触发器的恢复条件和问题标准不同，则可以使用此设置。 无（None）：正常事件从来不生成。可以和多重问题事件生成一起结合使用，以便在某事件发生时可以更简单的发送通知。 事件成功关闭（OK event closes）设置，用来控制哪些问题事件（Problem events）被关闭 所有问题（All problems）：正常事件（OK event）将关闭触发器创建的所有打开的问题 所有问题如果标记的值匹配（All problems if tag values match）：正常事件（OK event）将关闭触发器创建的打开的问题，并且至少有一个匹配的标记值。 触发器的严重性： 未分类（Not classified）：未知严重性（灰） 信息（Information）：提示（浅蓝） 警告（Warning）：警告（黄） 一般严重（Average）：一般问题（橙） 严重（High）：发生重要的事（浅红） 灾难（Disaster）：灾难，财务损失（红） 触发器提示颜色可在 Adminstration –&gt; General –&gt; Trigger severities 中修改 事件关联是一种设置自定义事件关闭（导致正常事件生成）的规则，该规则定义了新的问题事件如何与现有的问题事件配对，并通过生成相应的正常事件来关闭新的事件或匹配事件。 action自动发现与自动注册zabbix 发现包括三种： 自动网络发现（network discovery） 主动客户端自动注册（active agent auto-registration） 低级别发现（low-level discovery） zabbix 网络发现基于的信息种类： IP 段自动发现 可用外部信息（FTP、SSH、WEB、POP3 等） 从 zabbix 客户端收到的信息 从 SNMP 客户端收到的信息 网络发现由两个步骤组成：发现（discovery）和动作（action） zabbix 会周期性扫描网络发现规则中的 IP 段，动作是对发现的主机进行设置的过程。 配置自动发现需在 Coufiguration 的 Discovery 配置。修改 IP Range 和 Update interval，并添加 Checks 中选项，指定类型为 zabbix agent，并指定键值，zabbix server 会尝试去指定网段内的所有主机获取该值，若能获取则自动发现成功。 配置自动发现动作在 Configuration 的 Actions，选择右上角的事件源为 discovery，然后创建 action。 进入配置后可修改计算方式、触发条件，或创建新的触发条件 可进入 Operations 修改或添加操作 自动注册用于 Agent 主动向 Server 注册，且主要适用于条件未知情况（agent 的 IP 地址段或 agent 的操作系统信息等）。 配置客户端自动注册的步骤： 在客户端配置文件中设置参数 修改 zabbix_agentd.conf，修改后重启 zabbix-agentd 服务 Server=192.168.1.134 # 本机IP地址ServerActive=192.168.1.133 # 主动模式下，Zabbix Server的IPHostname=KubeServer2 # 主机名，仅用于显示，不用和主机名一致HostMetadata=linux zabbix.kube2 # 元信息，用于标识识别 在 zabbix web 中配置一个动作 在 Configuration 中 actions 的选项 auto-registration 并创建 然后直接配置 condition，选择条件包含的内容 继续配置 operations，添加几个操作 低级别发现Low-Level discovery（LLD）：当例如要对网卡进行监控时，由于网卡名可能以 eth 开头或 enps 开头，若分别针对不同网卡名设置会很繁琐，而使用 LLD 就可解决问题。 zabbix 中支持的数据项发现： 文件系统发现 网络接口发现 SNMP OID 发现 CPU 核以及状态 zabbix 自带的 LLD key： vfs.fs.discovery：适用于 zabbix agent 监控方式 snmp.discovery：适用于 SNMP agent 监控方式 net.if.discovery：适用于 zabbix agent 监控 system.cpu.discovery：适用于 zabbix agent 监控 可通过zabbix_get获取 agent 的数据，但不支持 SNMP agent。 zabbix_get [options] -s 指定主机名或IP地址 -p 指定agent上获取数据的端口，默认为10050 -k 指定键 自定义监控项agent 的配置文件中User parameters用于设置定义项，可设置多个。 首先将UnsafeUserParameters设为 1，启动自定义参数。然后设置自定义项 UserParameter=&lt;key&gt;, &lt;shell command&gt;例：UserParameter=ping, echo 1 可以在/etc/zabbix/zabbix_agentd.d/中创建配置文件专门配置自定义项。 若要让键能接收参数，只需要在键后添加[*]。例：UserParameter=ping[*], echo $1 zabbix 主动与被动模式默认 zabbix server 会去每个 agent 上抓取数据，即 Agentd 被动模式。但当监控主机数量过大时，可能会导致 web 页面卡顿、监控告警不及时、图标显示终端等问题。 可通过两个方面优化： 部署多个 zabbix proxy，做分布式监控 调整 zabbix agent 为主动模式 Agentd 主动模式指：客户端收集本端监控信息后主动发给 server。 修改客户端配置 StartAgents=3 # 指定agentd收集的数据往哪发。默认值为3。# 若要开启主动模式，需要将该值设为0。# 关闭被动模式后，agent的10050端口也会关闭 同时需要在 server 端修改配置，保证性能 StartPollers=5 # 减少主动收集数据的进程。默认为5，也可不改StartTrappers=200 # 负责处理agentd推送数据的进程调大 然后需要在网页端配置，将监控类型从zabbix agent改为zabbix agent(active) zabbix 实战监控 MySQL编写一个监控 mysql 的脚本 check_mysql #!/bin/bash# 主机IP地址MYSQL_HOST=&#x27;127.0.0.1&#x27;# 端口MYSQL_PORT=&#x27;3306&#x27;# 数据连接MYSQL_CONN=&quot;/usr/bin/mysqladmin -h$&#123;MYSQL_HOST&#125; -P$&#123;MYSQL_PORT&#125;&quot;# 检查参数是否正确if [ $# -ne &quot;1&quot; ]; then echo &quot;arg error&quot;fi# 获取数据case $1 in Uptime) result=$($&#123;MYSQL_CONN&#125; status | cut -f2 -d&#x27;:&#x27; | cut -f1 -d&#x27;T&#x27;) echo $result ;; Com_update) result=$($&#123;MYSQL_CONN&#125; extended-status | grep -w &quot;Com_update&quot; | cut -d&#x27;|&#x27; -f3) echo $result ;; Slow_queries) result=$($&#123;MYSQL_CONN&#125; status | cut -f5 -d&#x27;:&#x27; | cut -f1 -d&#x27;O&#x27;) echo $result ;; Com_select) result=$($&#123;MYSQL_CONN&#125; extended-status | grep -w &quot;Com_select&quot; | cut -d&#x27;|&#x27; -f3) echo $result ;; Com_rollback) result=$($&#123;MYSQL_CONN&#125; extended-status | grep -w &quot;Com_rollback&quot; | cut -d&#x27;|&#x27; -f3) echo $result ;; Questions) result=$($&#123;MYSQL_CONN&#125; status | cut -f4 -d&#x27;:&#x27; | cut -f1 -d&#x27;S&#x27;) echo $result ;; Com_insert) result=$($&#123;MYSQL_CONN&#125; extended-status | grep -w &quot;Com_insert&quot; | cut -d&#x27;|&#x27; -f3) echo $result ;; Com_delete) result=$($&#123;MYSQL_CONN&#125; extended-status | grep -w &quot;Com_delete&quot; | cut -d&#x27;|&#x27; -f3) echo $result ;; Com_commit) result=$($&#123;MYSQL_CONN&#125; extended-status | grep -w &quot;Com_commit&quot; | cut -d&#x27;|&#x27; -f3) echo $result ;; Bytes_sent) result=$($&#123;MYSQL_CONN&#125; extended-status | grep -w &quot;Bytes_sent&quot; | cut -d&#x27;|&#x27; -f3) echo $result ;; Bytes_received) result=$($&#123;MYSQL_CONN&#125; extended-status | grep -w &quot;Bytes_received&quot; | cut -d&#x27;|&#x27; -f3) echo $result ;; Com_begin) result=$($&#123;MYSQL_CONN&#125; extended-status | grep -w &quot;Com_begin&quot; | cut -d&#x27;|&#x27; -f3) echo $result ;; *) echo &quot;Usage: $0(Uptime|Com_update|Slow_queries|Com_select|Com_rollback|Questions|Com_insert|Com_delete|Com_commit|Bytes_sent|Bytes_received|Com_begin)&quot; ;;esac 需要先在/etc/my.cnf中配置登录用户名和密码 [mysqladmin]user=rootpassword=redhat 且mysqladmin status的执行结果为 # mysqladmin statusUptime: 4707 Threads: 1 Questions: 88227 Slow queries: 0 Opens: 91 Flush tables: 2 Open tables: 117 Queries per second avg: 18.743 然后将该脚本存放在/etc/zabbix/shell中，并修改执行权限以及用户权限为 zabbix 修改/etc/zabbix/zabbix_agentd.d/userparameter_mysql.conf，删除没有用或错误的 UserParameter=mysql.ping,HOME=/etc /usr/bin/mysqladmin ping 2&gt;/dev/null | grep -c aliveUserParameter=mysql.status[*],/etc/zabbix/shell/check_mysql $1UserParameter=mysql.version,/usr/bin/mysql -V 重启 agentd 后，在网页端进行配置 添加主机，配置 host name 和 agent interfaces 在主机配置中设置模板为Template DB MySQL 然后在 hosts 的 item 中查看是否全部启用 并在 Monitoring 的 Latest Data 查看数据 监控 Apache在 zabbix agent 服务器上修改 httpd 的配置文件，添加以下内容，开启检查 httpd 的扩展功能 ExtendedStatus On # 开启扩展的status查看功能&lt;location /server-status&gt; # 后面的脚本里就是获取这里的数据，若要改为别的，则还要修改脚本 SetHandler server-status Require ip 127.0.0.1 192.168.80.132 # 允许本地和zabbix server查看&lt;/location&gt; 下载 zabbix-apache 的监控脚本 www.ixdba.net/zabbix/zabbix-apache.zip 解压后有两个文件，一个是监控 apache 数据脚本 zapache，一个是监控模板 zapache-template.xml 赋予脚本执行权限chmod 755 zapache，并存放在/etc/zabbix/shell中，并非强制，只是便于管理。与 mysql 监控类似，需要在 agent 端配置文件/etc/zabbix/zabbix-agentd.d/userparameter_apache.conf，名字可任起。 UserParameter=zapache[*],/etc/zabbix/shell/zapache $1 然后重启 zabbix-agentd 在 zabbix web 上添加 zapache 的模板配置 能在 LatestData 里查看配置的 item 监控 Nginx在被监控主机上操作。配置文件添加 location /nginx-status &#123; stub_status on; access_log off; allow 127.0.0.1; allow 192.168.80.132; deny all;&#125; 编写 nginx 的监控脚本，可通过http://www.ixdba.net/zabbix/zabbix-nginx.zip下载 #!/bin/bashHOST=127.0.0.1 # 要监控的apache主机PORT=80 # 该主机的nginx端口if [ $# -eq &quot;0&quot; ];then echo &quot;Usage:$0(active|reading|writing|waiting|accepts|handled|requests|ping)&quot;fifunction active &#123; /usr/bin/curl &quot;http://$HOST:$PORT/nginx-status&quot; 2&gt;/dev/null | grep &#x27;Active&#x27; | awk &#x27;&#123;print $NF&#125;&#x27;&#125;function reading &#123; /usr/bin/curl &quot;http://$HOST:$PORT/nginx-status&quot; 2&gt;/dev/null | grep &#x27;Reading&#x27; | awk &#x27;&#123;print $2&#125;&#x27;&#125;function writing &#123; /usr/bin/curl &quot;http://$HOST:$PORT/nginx-status&quot; 2&gt;/dev/null | grep &#x27;Writing&#x27; | awk &#x27;&#123;print $4&#125;&#x27;&#125;function waiting &#123; /usr/bin/curl &quot;http://$HOST:$PORT/nginx-status&quot; 2&gt;/dev/null | grep &#x27;Waiting&#x27; | awk &#x27;&#123;print $6&#125;&#x27;&#125;function accepts &#123; /usr/bin/curl &quot;http://$HOST:$PORT/nginx-status&quot; 2&gt;/dev/null | awk NR==1 | awk &#x27;&#123;print $1&#125;&#x27;&#125;function handled &#123; /usr/bin/curl &quot;http://$HOST:$PORT/nginx-status&quot; 2&gt;/dev/null | awk NR==2 | awk &#x27;&#123;print $2&#125;&#x27;&#125;function requests &#123; /usr/bin/curl &quot;http://$HOST:$PORT/nginx-status&quot; 2&gt;/dev/null | awk NR==3 | awk &#x27;&#123;print $3&#125;&#x27;&#125;function ping &#123; /sbin/pidof nginx | wc -l&#125;$1 同样将该脚本放在/etc/zabbix/shell中，并赋予执行权限，修改所属用户为 zabbix 然后创建 zabbix agent 配置/etc/zabbix/zabbix_agentd.d/userparameter_nginx.conf UserParameter=nginx.status[*],/etc/zabbix/shell/nginx-status.sh $1 并重启 agent 服务 下载模板配置 xml 文件http://www.ixdba.net/zabbix/zabbix-nginx.zip 然后同理导入模板 同理在主机上添加模板 去 Latest Data 中查看数据是否获取成功 监控 PHP-FTPM修改 php 主机上的配置文件/etc/php-fpm.d/www.conf，找到;pm.status_path = /status取消注释，开启状态页 在 www 块下的一些内容 pm = dynamic # php-fpm开启进程的方式，有static和dynamic若为dynamic，则将初始进程数设为pm.start_servers的值，会动态增加删除，最大不超过pm.max_spare_servers的值，最小不超过pm.min_spare_servers 的值若为static，则进程数始终为pm.max_children的值pm.max_children = 50pm.start_servers = 5pm.min_spare_servers = 5pm.max_spare_servers = 35若服务器的内存大于16G，则推荐静态，否则推荐动态 在 nginx 配置文件中添加 location ~ ^/status$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_param SCRIPT_FILENAME $fastcgi_script_name; include fastcgi_params;&#125; 然后重启 nginx 和 php-fpm，再通过localhost/status访问查看 \\# curl 127.0.0.1/statuspool: www # fpm池子名process manager: dynamic # 进程管理方式start time: 26/Apr/2019:20:54:52 +0800start since: 2 # 运行时长accepted conn: 1 # 当前池子接收的请求数listen queue: 0 # 请求等待队列max listen queue: 0 #请求等待队列最高数量listen queue len: 128 # socket等待队列长度idle processes: 4 # 空闲进程数量active processes: 1 # 活跃进程数量total processes: 5 # 总进程数量max active processes: 1 # 最大活跃进程数量（从fpm启动开始的）max children reached: 0 # 达到进程最大数量限制的次数，若不为0，说明上一条可能有点小slow requests: 0 若访问/status?xml 则会以 xml 格式输出 若访问/status?json 则会以 json 格式输出 可通过curl -s &quot;192.168.80.136/status?xml&quot; | grep &quot;accepted-conn&quot; | awk -F &#39;&gt;|&lt;&#39; &#39;&#123;print $3&#125;&#39; 获取监控值 因此在 /etc/zabbix/zabbix_agentd.d/userparameter_phpfpm.conf配置 UserParameter=php-fpm.status[*],/usr/bin/curl -s &quot;http://localhost/status?xml&quot; | grep &quot;&lt;$1&gt;&quot; | awk -F&#x27;&lt;|&gt;&#x27; &#x27;&#123;print $$3&#125;&#x27; 然后在https://www.ixdba.net/zabbix/zbx_php-fpm_templates.zip下载模板文件。同理在 web 中导入 监控 Tomcat需要在 zabbix_server 上启动 java poller 和 zabbix_java，zabbix_java 相当于一个 java gateway，端口号 10052，还需要在 java 服务器上开启 12345 端口。 zabbix 监控 java 的数据获取顺序：java poller——&gt;java gateway:10052——&gt;tomcat:12345 java 主机配置 java 环境 centos7 安装 openjdk yum install java-1.8.0-openjdk 安装 jre yum install java-1.8.0-openjdk-devel 安装 jdk # java -versionopenjdk version &quot;1.8.0_212&quot;OpenJDK Runtime Environment (build 1.8.0_212-b04)OpenJDK 64-Bit Server VM (build 25.212-b04, mixed mode)# javac -versionjavac 1.8.0_212 配置环境变量 JAVA_HOME=&quot;/usr/lib/jvm/jre-1.8.0-openjdk-1.8.0.212.b04-0.el7_6.x86_64&quot;CLASSPATH=&quot;$JAVA_HOME/lib&quot;PATH=&quot;$JAVA_HOME/bin:​$PATH&quot; 下载 tomcat9 https://tomcat.apache.org/download-90.cgi#9.0.19 赋予目录下 bin/中 sh 脚本执行权限，并执行catalina.sh start启动 tomcat，确认环境配置无问题，然后停止catalina.sh stop 配置 Tomcat JMX JMX：JMX（Java Management Extensions，即 Java 管理扩展）是一个为应用程序、设备、系统等植入管理功能的框架。JMX 可以跨越一系列异构操作系统平台、系统体系结构和网络传输协议，灵活的开发无缝集成的系统、网络和服务管理应用。 修改 catalina.sh 脚本，添加 CATALINA_OPTS=”-server -Xms256m -Xmx512m -XX:PermSize=64M -XX:MaxPermSize=128m -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=192.168.80.132 -Dcom.sun.management.jmxremote.port=12345” 将 Djava.rmi.server.hostname 设为 zabbix server 的 IP 地址 默认 zabbix server 是没有 java 支持的，应该下载 zabbix-java-gateway 如果使用的 zabbix 的官方源，可直接yum install zabbix-java-gateway 安装后，会生成一个/usr/sbin/zabbix_java_gateway脚本，执行该脚本，开启 zabbix_java_gateway 服务，查看端口 10052 是否开启 修改 zabbix_server 的配置文件，取消以下参数注释，并重启 zabbix_server JavaGateway=127.0.0.1JavaGatewayPort=10052StartJavaPollers=5 zabbix 默认有 tomcat 的模板，但有问题，先删掉，再重新导入https://www.ixdba.net/zabbix/zbx_tomcat_templates.zip 监控 RedisRedis-cli 的获取信息命令 redis-cli info [参数]若不填参数，则返回所有信息可选参数：server： redis的通用信息clients：客户端连接信息memory：内存消耗信息persistence：持久化信息stats：通用统计数据replication：主从复制信息cpu：CPU消耗数据commandstats：redis命令统计信息cluster：Redis集群信息keyspace：数据库统计信息 在https://www.ixdba.net/zabbix/zbx-redis-template.zip下载脚本和模板 先检查脚本，确认配置参数正确。如果 redis 没有配置密码，则要修改脚本将$PASS 删掉，否则会无法获取信息 #!/bin/bashREDISCLI=&quot;/usr/local/bin/redis-cli&quot;HOST=&quot;127.0.0.1&quot;PORT=6379PASS=&quot;&quot;if [[ $# == 1 ]];then case $1 in version) result=`$REDISCLI -h $HOST -p $PORT info server | grep -w &quot;redis_version&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27;` echo $result ;; uptime) result=`$REDISCLI -h $HOST -p $PORT info server | grep -w &quot;uptime_in_seconds&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27;` echo $result ;; connected_clients) result=`$REDISCLI -h $HOST -p $PORT info clients | grep -w &quot;connected_clients&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27;` echo $result ;; blocked_clients) result=`$REDISCLI -h $HOST -p $PORT info clients | grep -w &quot;blocked_clients&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27;` echo $result ;; used_memory) result=`$REDISCLI -h $HOST -p $PORT info memory | grep -w &quot;used_memory&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27;` echo $result ;; used_memory_rss) result=`$REDISCLI -h $HOST -p $PORT info memory | grep -w &quot;used_memory_rss&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27;` echo $result ;; used_memory_peak) result=`$REDISCLI -h $HOST -p $PORT info memory | grep -w &quot;used_memory_peak&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27;` echo $result ;; used_memory_lua) result=`$REDISCLI -h $HOST -p $PORT info memory | grep -w &quot;used_memory_lua&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27;` echo $result ;; used_cpu_sys) result=`$REDISCLI -h $HOST -p $PORT info cpu | grep -w &quot;used_cpu_sys&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27;` echo $result ;; used_cpu_user) result=`$REDISCLI -h $HOST -p $PORT info cpu | grep -w &quot;used_cpu_user&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27;` echo $result ;; used_cpu_sys_children) result=`$REDISCLI -h $HOST -p $PORT info cpu | grep -w &quot;used_cpu_sys_children&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27;` echo $result ;; used_cpu_user_children) result=`$REDISCLI -h $HOST -p $PORT info cpu | grep -w &quot;used_cpu_user_children&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27;` echo $result ;; rdb_last_bgsave_status) result=`$REDISCLI -h $HOST -p $PORT info Persistence | grep -w &quot;rdb_last_bgsave_status&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27; | grep -c ok` echo $result ;; aof_last_bgrewrite_status) result=`$REDISCLI -h $HOST -p $PORT info Persistence | grep -w &quot;aof_last_bgrewrite_status&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27; | grep -c ok` echo $result ;; aof_last_write_status) result=`$REDISCLI -h $HOST -p $PORT info Persistence | grep -w &quot;aof_last_write_status&quot; | awk -F&#x27;:&#x27; &#x27;&#123;print $2&#125;&#x27; | grep -c ok` echo $result ;; *) echo -e &quot;\\033[33mUsage: $0 &#123;connected_clients|blocked_clients|used_memory|used_memory_rss|used_memory_peak|used_memory_lua|used_cpu_sys|used_cpu_user|used_cpu_sys_children|used_cpu_user_children|rdb_last_bgsave_status|aof_last_bgrewrite_status|aof_last_write_status&#125;\\033[0m&quot; ;; esacelif [[ $# == 2 ]];then case $2 in keys) result=`$REDISCLI -h $HOST -p $PORT info | grep -w &quot;$1&quot; | grep -w &quot;keys&quot; | awk -F&#x27;=|,&#x27; &#x27;&#123;print $2&#125;&#x27;` echo $result ;; expires) result=`$REDISCLI -h $HOST -p $PORT info | grep -w &quot;$1&quot; | grep -w &quot;keys&quot; | awk -F&#x27;=|,&#x27; &#x27;&#123;print $4&#125;&#x27;` echo $result ;; avg_ttl) result=`$REDISCLI -h $HOST -p $PORT info | grep -w &quot;$1&quot; | grep -w &quot;avg_ttl&quot; | awk -F&#x27;=|,&#x27; &#x27;&#123;print $6&#125;&#x27;` echo $result ;; *) echo -e &quot;\\033[33mUsage: $0 &#123;db0 keys|db0 expires|db0 avg_ttl&#125;\\033[0m&quot; ;; esacfi 创建配置文件/etc/zabbix/zabbix_agentd.d/userparameter_redis.conf UserParameter=Redis.Info[*],/etc/zabbix/shell/redis_status $1 $2UserParameter=Redis.Status,/usr/bin/redis-cli -h 127.0.0.1 -p 6379 ping|grep -c PONG 同理在 web 上导入模板文件，并添加到主机 zabbix 与微信整合进入企业微信网页 按要求填写，并扫码 邀请成员 进入我的企业-&gt;微工作台，可复制下面的二维码，使成员通过微信关注微工作台，即可在微信中接收企业通知和使用企业应用，成员无需下载企业微信客户端。 当成员关注了企业的微工作台，成员信息详情中的”微工作台“状态变为“已关注”。 进入应用管理，创建应用 创建完成后，应用主界面如下，需要注意 AgentID 和 Secret，需要配置到 zabbix 中 且“我的企业”中“企业信息”的“企业 ID” 也要注意，也要配置到 zabbix 中的。 下载微信告警脚本下载地址，并添加执行权限，将脚本放在目录/usr/lib/zabbix/alertscripts，专门存放告警脚本的目录，将脚本改名成 weixin。先测试脚本是否能用： # 脚本参数：Usage of /usr/lib/zabbix/alertscripts/weixin: -agentid string 应用AgentID -author string http://www.oneoaas.com -corpid string 企业ID -corpsecret string 应用的Secret -msg string 消息内容 -user string 要发给的用户账号，在用户详情页面的# 测试：将网页上的参数带入# /usr/lib/zabbix/alertscripts/weixin -agentid=1000002 -corpid=ww534be49f1c1ded73 -corpsecret=sCtXyz28Nfgaglf51qz9HUZ9MaHuB1TBQJLqEknKNzw -user=GuTianYi -msg=&quot;微信告警测试消息&quot;&#123;&quot;errcode&quot;:0,&quot;errmsg&quot;:&quot;ok&quot;,&quot;invaliduser&quot;:&quot;&quot;&#125;# 查看手机微信，已获取 在 zabbix web 上配置微信告警。进入管理-&gt;媒介类型-&gt;创建媒介 若是 zabbix 是英语环境，则需要媒介名字为英文，否则添加不了。 进入 Users，选择一个用于发监控的用户，配置媒介。SendTo 填微信用户账号。 进入 Actions，创建动作 添加操作 可修改消息内容 示例：Problem started at &#123;EVENT.TIME&#125; on &#123;EVENT.DATE&#125;Problem name: &#123;EVENT.NAME&#125;Host: &#123;HOST.NAME&#125;Host IP: &#123;HOST.IP&#125;Item Name: &#123;ITEM.NAME&#125;Item Value: &#123;ITEM.LASTVALUE&#125;Severity: &#123;EVENT.SEVERITY&#125;Trigger Name: &#123;TRIGGER.NAME&#125;Current Status: &#123;TRIGGER.STATUS&#125;Original problem ID: &#123;EVENT.ID&#125;&#123;TRIGGER.URL&#125; 还可以通过 Recovery Operations 修改回复以后的通知 修改系统文件即可触发 zabbix 与 Logstash 整合首先安装 LogStash，版本为 7.6在/usr/share/logstash/bin/logstash-plugin，官方提供了 logstash 的插件管理器 logstash-plugin list install update remove 安装 logstash 的 zabbix 插件 /usr/share/logstash/bin/logstash-plugin install logstash-output-zabbix 创建配置文件/etc/logstash/conf.d/zabbix.conf # input从/var/log/secure 读数据input &#123; file &#123; path =&gt; [&quot;/var/log/secure&quot;] type =&gt; &quot;system&quot; start_position =&gt; &quot;beginning&quot; # 从头开始读数据 &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;SYSLOGTIMESTAMP:message_timestamp&#125; %&#123;SYSLOGHOST:hostname&#125; %&#123;DATA:message_program&#125;(?:\\[%&#123;POSINT:message_pid&#125;\\])?: %&#123;GREEDYDATA:message_content&#125;&quot; &#125; #这里通过grok对message字段的数据进行字段划分，这里将message字段划分了5个子字段。其中，message_content字段会在output中用到。 &#125; mutate &#123; # 新增的字段，字段名是zabbix_key，值为oslogs。 add_field =&gt; [ &quot;[zabbix_key]&quot;, &quot;oslogs&quot; ] # 新增的字段，字段名是zabbix_host，值可以在这里直接定义，也可以引用字段变量来获取。 # 这里的%&#123;host&#125;获取的就是日志数据的主机名，这个主机名与zabbix web中“主机名称”需要保持一致。 add_field =&gt; [ &quot;[zabbix_host]&quot;, &quot;%&#123;host&#125;&quot; ] &#125; mutate &#123; # 这里是删除不需要的字段 remove_field =&gt; &quot;@version&quot; remove_field =&gt; &quot;message&quot; &#125; date &#123; # 这里是对日志输出中的日期字段进行转换，其中message_timestamp字段是默认输出的时间日期字段，将这个字段的值传给@timestamp字段。 match =&gt; [ &quot;message_timestamp&quot;,&quot;MMM d HH:mm:ss&quot;,&quot;MMM dd HH:mm:ss&quot;, &quot;ISO8601&quot;] &#125;&#125;output &#123; if [message_content] =~ /(ERR|error|ERROR|Failed)/ &#123; # 定义在message_content字段中，需要过滤的关键字信息 # 也就是在message_content字段中出现给出的这些关键字，那么就将这些信息发送给zabbix。 zabbix &#123; #这个zabbix_host将获取上面filter部分定义的字段变量%&#123;host&#125;的值 zabbix_host =&gt; &quot;[zabbix_host]&quot; zabbix_key =&gt; &quot;[zabbix_key]&quot; #这个zabbix_key将获取上面filter部分中给出的值 zabbix_server_host =&gt; &quot;172.16.213.140&quot; #这是指定zabbix server的IP地址 zabbix_server_port =&gt; &quot;10051&quot; #这是指定zabbix server的监听端口 zabbix_value =&gt; &quot;message_content&quot; # 这个很重要，指定要传给zabbix监控项item（oslogs）的值，zabbix_value默认的值是&quot;message&quot;字段 # 因为上面已经删除了&quot;message&quot;字段，因此，这里需要重新指定，根据上面filter部分对&quot;message&quot;字段的内容划分 # 这里指定为&quot;message_content&quot;字段，其实，&quot;message_content&quot;字段输出的就是服务器上具体的日志内容。 &#125; &#125; # stdout &#123; codec =&gt; rubydebug &#125; # 这里是开启调试模式，当第一次配置的时候，建议开启 # 这样过滤后的日志信息直接输出的屏幕，方便进行调试，调试成功后，即可关闭。&#125; 参考文章 zabbix 官方中文手册 51cto 专栏——无监控 不运维 朱双印个人日志-zabbix","categories":[],"tags":[{"name":"运维","slug":"运维","permalink":"https://coconutmilktaro.top/tags/%E8%BF%90%E7%BB%B4/"},{"name":"监控","slug":"监控","permalink":"https://coconutmilktaro.top/tags/%E7%9B%91%E6%8E%A7/"},{"name":"zabbix","slug":"zabbix","permalink":"https://coconutmilktaro.top/tags/zabbix/"}]},{"title":"ELK日志架构笔记","slug":"ELK日志架构笔记","date":"2018-09-24T08:28:34.000Z","updated":"2022-06-28T18:46:18.572Z","comments":true,"path":"2018/ELK日志架构笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/ELK%E6%97%A5%E5%BF%97%E6%9E%B6%E6%9E%84%E7%AC%94%E8%AE%B0/","excerpt":"本篇文章将完整描述基于ELK的日志系统的设计与部署，以及所有涉及的知识点。","text":"本篇文章将完整描述基于ELK的日志系统的设计与部署，以及所有涉及的知识点。 ElasticsearchElasticsearch 是基于 Lucene 的搜索框架，使用 Java 编写，它提供了一个分布式多用户能力的全文搜索引擎，基于 RESTful web 接口，上手容易，拓展节点方便，可用于存储和检索海量数据，接近实时搜索，海量数据量增加，搜索响应性能几乎不受影响。 Apache Lucene：目前存在的拥有最先进，高性能和全功能搜索引擎功能的库。但仅仅是一个库，Elasticsearch 则是提供了 Lucene 库的 RESTful API 接口，将所有的功能打包成一个单独的服务，做到”开箱即用“。 Elasticsearch 主要特点： 全文检索，结构化检索 数据统计、分析，接近实时处理 分布式搜索（可部署数百台服务器） 自动发现节点 副本机制 处理 PB 级别的结构化或者非结构化数据 保障可用性 搜索纠错，自动完成 与各种语言基础，与 Hadoop、Spark 等大数据分析平台集成 使用场景：日志搜索，数据聚合，数据监控，报表统计分析 使用 Elasticsearch 的大企业：维基百科、卫报、StackOverflow、Github、ebay Elasticsearch 安装 因为 Lucene 和 Elasticsearch 都是 Java 写的，所以首先搭建 JDK 环境，下载 JDK1.8，设置环境变量，并source /etc/profile应用 echo &quot;export JAVA_HOME=/usr/local/jdk1.8&quot; &gt;&gt; /etc/profileecho &quot;export CLASSPATH=\\$JAVA_HOME/lib&quot; &gt;&gt; /etc/profileecho &quot;export PATH=\\$JAVA_HOME/bin:\\$PATH&quot; &gt;&gt; /etc/profilesource /etc/profile 下载 Elasticsearch，目前版本为 6.4.1，解压到/usr/local/elasticsearch-6.4 不能使用 root 运行elasticsearch，需要创建一个用户 useradd elasticchown -R elastic:elastic /usr/local/elasticsearch-6.4 切换到该用户，并执行elasticsearch。等待一段时间启动，然后执行curl localhost:9200查看。若看到 json 对象信息则说明成功。 curl localhost:9200&#123; &quot;name&quot; : &quot;3dcAoxl&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;GNdmHFuXTsK-6B-swVNQag&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;6.4.1&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;tar&quot;,...... &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 运行报错 bootstrap checks failed #bootstrap检查失败max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] #最大虚拟内存太低 解决： 临时解决： sysctl -w vm.max_map_count=262144 永久解决：修改/etc/sysctl.conf文件，添加 vm.max_map_count设置，并执行sysctl -p max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536] 解决：修改 unix 最大同时打开文件数，ulimit -n 65536 elasticsearch -E &lt;键值对&gt; 设置参数 -d, --daemonize 后台启动 -p, --pidfile &lt;Path&gt; 设置PID文件 -q, --quiet 静默启动 -s, --silent 显示最少的输出 -v, --verbose 显示详细输出 Elasticsearch 配置文件ES 核心配置文件config/elasticsearch.yml cluster.name: my-application #集群名称，若相同，且是同一网段会自动加入node.name: node-1 #当前节点名称node.attr.rack: r1 #network.host: 127.0.0.1 #默认情况下，Elastic只允许本机访问 #若要远程访问，取消注释并修改值为0.0.0.0 JVM 配置文件jvm.option，最好不要调。 -Xms1g #最小堆内存-Xmx1g #最大堆内存#两个值最好一致，且为机器物理内存的一半到2/3，也不能太小-XX:+UseConcMarkSweepGC-XX:CMSInitiatingOccupancyFraction=75-XX:+UseCMSInitiatingOccupancyOnly#垃圾回收算法，官方已经优化过了，不用调整 Elasticsearch 概念Elasticsearch 是 面向文档 的，意味着它存储整个对象或文档。且 Elasticsearch 不仅存储文档，而且索引每个文档的内容使之可以被检索。在 Elasticsearch 中，是对文档进行索引、检索、排序和过滤，而不是对行列数据。这就是 ES 能支持复杂的全文搜索的原因。 Elasticsearch 使用 JSON 作为文档的序列化格式。存储数据到 Elasticsearch 的行为叫做索引（动词，索引一个文档就是存储一个文档到索引），一个 Elasticsearch 集群可以包含多个索引 ，相应的每个索引可以包含多个类型 。这些不同的类型存储着多个文档 ，每个文档又有多个字段 。Elasticsearch 和 Lucene 使用倒排索引（也称反向索引）结构达到较高的检索速度。倒排索引就是关系型数据库通过增加一个索引到指定列上以提高搜索速度。 RESTRepresentational State Transfer 表述性状态传递，是一种软件架构风格，提供的是一组设计原则和约束条件，主要用于客户端与服务器交互的软件，使得软件更简洁、有层次，更利于实现缓存等机制。 REST 提供的与资源交互的方法：类似于 HTTP，但 REST 的方法仅仅面向资源，无法对 web 应用操作。 GET：列出 URI 以及资源中详细信息 PUT：将给定的一组资源替换当前资源 POST：在指定资源中创建、追加一个新资源 DELETE：删除资源 HEAD：获取头信息 PUT /megacorp/employee/1&#123; &quot;first_name&quot; : &quot;John&quot;, &quot;last_name&quot; : &quot;Smith&quot;, &quot;age&quot; : 25, &quot;about&quot; : &quot;I love to go rock climbing&quot;, &quot;interests&quot;: [ &quot;sports&quot;, &quot;music&quot; ]&#125; megacorp为索引名，employee为类型名，1为雇员 ID。Elasticsearch 仅需要找到雇员 ID 文件，就能知道该雇员的所有信息。 若要与关系型数据库对照，索引（indice 或 index）对应库，类型（type）对应表，文档（document）对应行，字段（field）对应列 Elasticsearch 通过将数据分片（shards）存储以解决数据量大时不能直接存储在一块硬盘中，且无法一次性搜索超大的数据量的情况。创建索引时，只需定义所需的分片数即可。每个分片本身都是一个功能齐全且独立的“索引”，可以托管在集群中的任何节点上。 每个 Elasticsearch 分片都是 Lucene 索引。每个 Lucene 可包含的最大文件数量为 Integer.MAX_VALUE - 128 个文件可以使用/_cat/shards监视分片大小。 并通过创建分片的副本（replicas），当主分片不可用时，副本就充当主分片使用。Elasticsearch 为每个索引分配 5 个主分片和 1 个副本，若集群中有两个节点，该索引的分片数会翻倍，即 10 个分片。 集群原理一个运行的 Elasticsearch 实例为一个节点，集群是由一个或多个拥有相同cluster.name的节点构成的，当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。 cluster.name默认为elasticsearch 主节点：负责管理集群范围内的所有变更（增删索引和节点等），任何节点都可以成为主节点。主节点并不需要涉及到文档级别的变更和搜索等操作，因此流量的增加它也不会成为瓶颈。 每个节点都知道任意文档所处的位置，并且能够将请求直接转发到存储客户所需文档的节点。 集群健康可通过curl localhost:9200/_cluster/health或使用telnet 127.0.0.1 9200并输入GET /_cluster/health HTTP/1.1获取集群的健康状况，或通过curl 127.0.0.1 9200/_cat/health?v查看。 &#123; &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;status&quot;: &quot;green&quot;, &quot;timed_out&quot;: false,......&#125; 其中健康状况就是字段status，有三个可能值： green：所有的主分片和副本分片都正常运行 yellow：所有的主分片都正常运行，但不是所有的副本分片都正常运行 red：有主分片没能正常运行，数据可能丢失，需要紧急修复 水平扩容读操作、搜索和返回数据都可以同时被主分片或副本分片所处理，所以拥有越多的副本分片时，也将拥有越高的吞吐量。在运行中的集群上是可以动态调整副本分片数目的 ，可以按需伸缩集群。 curl -H &quot;Content-Type: application/json&quot; -X PUT localhost:9200/blogs/_settings -d &#x27;&#123; &quot;number_of_replicas&quot;: 2&#125;&#x27; 如果只是在相同节点数目的集群上增加更多的副本分片并不能提高性能，因为每个分片从节点上获得的资源会变少，需要增加更多的硬件资源来提升吞吐量，但是更多的副本分片数提高了数据冗余量。 添加索引可通过curl添加索引 curl -H &quot;Content-Type: application/json&quot; -X PUT localhost:9200/blogs/article/1 -d &#x27;&#123; &quot;title&quot;: &quot;article1&quot;, &quot;content&quot;: &quot;article1&quot;&#125;&#x27;# -H设置内容类型，要设为JSON格式# -X设置请求类型，设为PUT# -d设置请求数据 若添加成功就会返回以下信息： &#123; &quot;_index&quot;: &quot;blogs&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;:&#123; &quot;total&quot;: 2, #目前总共的分片数 &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 0, &quot;_primary_term&quot;: 1&#125; 对已存在的记录再进行 PUT 操作就会更新该记录，同时，该字段的_version和_result都会改变，_version会+1，_result会变为updated。 再通过curl localhost:9200/blogs/article/1，获取该文章的元数据，以及_source属性，存储的就是文章中定义的内容。 &#123; &quot;_index&quot;: &quot;blogs&quot;, &quot;_type&quot;: &quot;article&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;title&quot;: &quot;article1&quot;, &quot;content&quot;: &quot;article1&quot; &#125;&#125; 可通过curl localhost:9200/_cat/indices?v获取当前节点的索引信息 若要删除某个索引或类型或文档，都可通过curl -X DELETE localhost:9200/要删的资源删除。 简单搜索curl localhost:9200/_search?pretty获取本节点的所有文档信息，并且返回结果不仅告知匹配了哪些文档，还包含了整个文档本身：显示搜索结果给最终用户所需的全部信息。?pretty会将 json 重新排版显示。 &#123; ...... &quot;hits&quot; : &#123; &quot;total&quot; : 2, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;blogs&quot;, &quot;_type&quot; : &quot;article&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;article2&quot;, &quot;content&quot; : &quot;article2&quot; &#125; &#125;, ...... ] &#125;&#125; 可通过_all字段进行指定文档或类型中的搜索，例如/_all/employee/_search?进行指定类型中搜索（所有索引的中的employee（如果存在）） ?q=字段:值进行查询字符串（Query-string）搜索 curl localhost:9200/_search?q=title:article2&#123; ...... &quot;hits&quot;:&#123;&quot;total&quot;:1,&quot;max_score&quot;:0.2876821,&quot;hits&quot;:[&#123;&quot;_index&quot;:&quot;blogs&quot;,&quot;_type&quot;:&quot;article&quot;,&quot;_id&quot;:&quot;2&quot;,&quot;_score&quot;:0.2876821,&quot;_source&quot;:&#123; &quot;title&quot;: &quot;article2&quot;, &quot;content&quot;: &quot;article2&quot; &#125;&#125;]&#125;&#125; 查询表达式搜索使用的是 Elasticsearch 开发的 DSL（领域特定语言），基于 JSON 定义查询，能够构造复杂的查询语句。 不使用 Query-string 查询，而是通过请求体查询，请求会通过 Json 构造。 curl localhost:9200/_search -X GET -H &quot;Content-Type: application/json&quot; -d &#x27;&#123; &quot;query&quot;:&#123; &quot;match&quot;:&#123; #使用了match类型查询 &quot;title&quot;: &quot;article1&quot; &#125; &#125;&#125;&#x27; 常用请求体搜索规则： &quot;query&quot;:&#123;&#125;表示开始查询，其中定义许多查询规则，会计算评分数量（相关度）_score &quot;bool&quot;：进行布尔匹配 &quot;must&quot;：包含 &quot;must_not&quot;：不包含 &quot;match&quot;：普通匹配，若用空格隔开多个关键字，则 es 认为是或的关系，如果要同时满足多个关键词，即与关系，必须用bool查询 &quot;match_phrase&quot;：短语精确匹配 &quot;filter&quot;：过滤器，不会计算评分数量 &quot;range&quot;：匹配范围，例如：&quot;range&quot;:&#123;age&quot;:&#123;&quot;gt&quot;:30&#125;&#125;匹配 age 大于 30 &quot;size&quot;：设置一次返回的结果数量，默认为 10 条。 &quot;from&quot;：设置移位，默认从位置 0 开始 # 已经alias curl_lo_g=&#x27;curl -X GET -H &#x27;Content-Type:application/json&#x27;&#x27;# export LO_ES=&#x27;localhost:9200&#x27;curl_lo_g $&#123;LO_ES&#125;/_all/employee/_search -d &#x27;&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gt&quot;: &quot;30&quot; #要加上双引号 &#125; &#125; &#125;, #这里有逗号 &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &quot;swimming&quot; &#125; &#125; &#125; &#125;&#125;&#x27; 对于 filter 和 query 的区别： 大部分 filter 的速度快于query 的速度 filter 不会计算相关度得分，且结果会有缓存，效率高 全文搜索、评分排序，使用 query 是非过滤，精确匹配，使用 filter 高亮搜索能将搜索结果的要搜索的字符串高亮显示， curl_lo_g $&#123;LO_ES&#125;/_all/employee/_search -d &#x27;&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;hobby&quot;: &quot;climbing&quot; &#125; &#125;, &quot;highlight&quot;: &#123; #只需要添加highlight搜索即可 &quot;fields&quot;: &#123; #fields指定高亮的字段 &quot;hobby&quot;: &#123;&#125; #需要高亮搜索的字段 &#125; &#125;&#125;&#x27;......&#123;&quot;_index&quot;:&quot;tech&quot;,&quot;_type&quot;:&quot;employee&quot;,&quot;_id&quot;:&quot;3&quot;,&quot;_score&quot;:0.2876821,&quot;_source&quot;:&#123; &quot;name&quot;: &quot;wangwu&quot;, &quot;age&quot;: &quot;26&quot;, &quot;address&quot;: &quot;yangzhou&quot;, &quot;hobby&quot;: [&quot;swimming&quot;, &quot;climbing&quot;]&#125;,&quot;highlight&quot;:&#123; #标出高亮部分 &quot;hobby&quot;:[ &quot;&lt;em&gt;climbing&lt;/em&gt;&quot; #高亮部分由HTML标签&lt;em&gt;封装 #告诉浏览器把其中的文本表示为强调的内容 #通常为斜体 ]&#125;&#125;]&#125;&#125; 聚合聚合 aggregations 用于生成基于数据的精细分析结果，类似 SQL 的group by。 LogstashLogstash 是一个开源的服务器端数据处理管道（Pipeline），它可以同时从多个源中提取数据，对其进行转换，然后将其发送到数据存储（如 Elasticsearch）。支持丰富的 Input 和 Output 类型，能够处理各种应用的日志。 Logstash 对于每一行数据（称为 event）按流水线三个部分进行操作： input：负责产生事件（即数据），即数据源，如 syslog、数据库日志、web 日志、文件系统日志、java 的 log4j、网络日志、防火墙等各类日志，kafka、RabbitMQ 等消息队列，移动设备、智能家居、传感器、联网汽车等 IoT 数据，以及 Beats 能获取的数据。是必须配置 filter：负责数据处理与转换，包括过滤，分类等操作。不是必须配置。 output：负责数据的输出，可输出到数据分析或存储的软件，如 Elasticsearch，nagios，kibana 等数据处理软件。是必须配置 Logstash 开箱即用，包含许多聚合（aggregation）和突变（mutation），以及模式匹配（pattern matching），地理映射（geo mapping）和动态查找（dynamic lookup）功能。 Logstash 安装 下载 Logstash 包，版本为 6.4.1，解压到/usr/local/logstash6.4 进入 logstash 的bin目录执行./logstash -e &#39;input&#123;stdin&#123;&#125;&#125; output&#123;stdout&#123;codec=&gt;rubydebug&#125;&#125;&#39;，需要等待一段时间，期间会有信息，直到出现Successfully started Logstash API endpoint &#123;:port=&gt;9600&#125;，然后输入hello world即可看到以下信息。若要退出，按Ctrl+D。 hello world&#123; &quot;message&quot; =&gt; &quot;hello world&quot;, &quot;host&quot; =&gt; &quot;VM_0_7_centos&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;@timestamp&quot; =&gt; 2018-09-26T11:09:10.781Z&#125; docker 下载 Logstash：直接docker pull logstash即可。 docker 下启动 Logstash：首先要确保本地存放 pipeline 配置文件的目录存在。通过在该目录添加配置文件。或者直接-v ~/config:/usr/share/logstash/config可直接修改所有配置 docker run -it -v ~/pipeline:/usr/share/logstash/pipeline logstash Logstash 配置文件： logstash.yml：主配置文件 pipelines.yml：管道的配置，包括 input，filter，output jvm.options：JVM 配置文件 log4j2.properties：log4j2 的配置 startup.options：启动脚本选项文件，包含 Logstash 的变量。若要让 Logstash 按修改后的配置运行，需要重新用 root 运行bin/system-install导入参数。 自定义的 Logstash 配置文件，一般以.conf结尾，同样存放在配置文件目录中。 logstash 命令 logstash -n NAME 指定logstash的node.name，若不指定默认是当前的主机名 -f CONFIG_PATH 从特定文件或目录加载logstash配置 -e CONFIG_STRING 使用给定的字符串作为配置数据（与配置文件的语法相同） 默认输入：“input &#123;stdin &#123;type =&gt; stdin&#125;&#125;” 默认输出：“output &#123;stdout &#123;codec =&gt; rubydebug&#125;&#125;“ 若直接使用默认，则-e &quot;&quot; 即可（不能什么都不加） --field-reference-parser MODE 在解析字段引用时使用的模式 字段引用解析器用于扩展管道配置中的字段引用，能更好地处理非法和模糊输入 可用的MODE值：1. LEGACY：LEGACY解析器，不发出警告 2. COMPAT：COMPAT解析器，对每个不同的模糊或非法语法输入警告一次（默认使用） 3.STRICT：STRICT解析器，模糊或非法语法输入会引发插件运行时异常 --modules MODULES 加载logstash模块，不能与&#x27;-e&#x27;或&#x27;-f&#x27;一起使用 因为会覆盖在logstash.yml中加载的模块 两种写法：--modules module1 --modules module2 ... --modules=module1,module2 -M MODULES_VARIABLE Load variables for module template. Multiple instances of &#x27;-M&#x27; or &#x27;--modules.variable&#x27; are supported. Ignored if &#x27;--modules&#x27; flag is not used. Should be in the format of &#x27;-M &quot;MODULE_NAME.var.PLUGIN_TYPE.PLUGIN_NAME.VARIABLE_NAME=VALUE&quot;&#x27; as in &#x27;-M &quot;example.var.filter.mutate.fieldname=fieldvalue&quot;&#x27; --setup Load index template into Elasticsearch, and saved searches, index-pattern, visualizations, and dashboards into Kibana when running modules. (default: false) --cloud.id CLOUD_ID Sets the elasticsearch and kibana host settings for module connections in Elastic Cloud. Your Elastic Cloud User interface or the Cloud support team should provide this. Add an optional label prefix &#x27;&lt;label&gt;:&#x27; to help you identify multiple cloud.ids. e.g. &#x27;staging:dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy&#x27; --cloud.auth CLOUD_AUTH Sets the elasticsearch and kibana username and password for module connections in Elastic Cloud e.g. &#x27;username:&lt;password&gt;&#x27; --pipeline.id ID Sets the ID of the pipeline. (default: &quot;main&quot;) -w COUNT 指定pipeline worker数量（即线程数），默认1 --experimental-java-execution (Experimental) Use new Java execution engine. (default: false) -b, --pipeline.batch.size SIZE Size of batches the pipeline is to work in. (default: 125) -u, --pipeline.batch.delay DELAY_IN_MS When creating pipeline batches, how long to wait while polling for the next event. (default: 50) --pipeline.unsafe_shutdown Force logstash to exit during shutdown even if there are still inflight events in memory. By default, logstash will refuse to quit until all received events have been pushed to the outputs. (default: false) --path.data PATH 数据存储目录。插件需要能访问该目录，默认安装目录下的data/ -p, --path.plugins PATH 插件目录，可指定多个Plugins are expected to be in a specific directory hierarchy: &#x27;PATH/logstash/TYPE/NAME.rb&#x27; where TYPE is &#x27;inputs&#x27; &#x27;filters&#x27;, &#x27;outputs&#x27; or &#x27;codecs&#x27; and NAME is the name of the plugin. (default: []) -l PATH 指定Logstash的日志目录，默认安装目录下的logs/ --log.level LEVEL 设置日志等级（fatal/error/warn/info/debug/trace），默认info --config.debug Print the compiled config ruby code out as a debug log (you must also have --log.level=debug enabled). WARNING: This will include any &#x27;password&#x27; options passed to plugin configs as plaintext, and may result in plaintext passwords appearing in your logs! (default: false) -i, --interactive SHELL Drop to shell instead of running as normal. Valid shells are &quot;irb&quot; and &quot;pry&quot; -t 检查logstash配置文件语法是否正常 -r 自动检测配置文件是否变动，若变动自动重载 --config.reload.interval RELOAD_INTERVAL How frequently to poll the configuration location for changes, in seconds. (default: 3000000000) --http.host HTTP_HOST Web API binding host (default: &quot;127.0.0.1&quot;) --http.port HTTP_PORT Web API http port (default: 9600..9700) --log.format FORMAT Specify if Logstash should write its own logs in JSON form (one event per line) or in plain text (using Ruby&#x27;s Object#inspect) (default: &quot;plain&quot;) --path.settings SETTINGS_DIR 包含logstash.yml的目录，可通过LS_SETTINGS_DIR环境变量配置 默认&quot;/usr/local/logstash6.4/config&quot; --verbose 相当于设置日志等级为info --debug 相当于设置日志等级为debug --quiet 相当于设置日志等级为info Logstash 如何工作关闭 Logstash可通过systemctl stop logstash或直接kill关闭。Logstash 有自己关闭过程，以达到安全地关闭： 首先停止所有的input、filter、output插件 处理完所有管道中的事件 最后关闭 Logstash 进程 在处理过程中，以下的状况会影响关闭过程： input插件以很慢的速度接收数据 速度慢的filter，如执行sleep(10000)的Ruby filter或执行非常繁重的查询的 Elasticsearch 过滤器。 一个断开连接的output插件，等待重新连接以刷新正在进行的事件。 Logstash 有一个停顿检测机制（stall detection），可以在关闭过程中分析管道和插件的行为。此机制会对内部队列中的飞行事件（in-flight events）数量和繁忙的 worker 线程列表定期生成信息报告。 若要在 Logstash 关闭阶段直接强行关闭，可在主配置文件中设置pipeline.unsafe_shutdown值为true，但这样可能造成数据丢失，不安全。 Logstash 配置文件logstash.yml因为配置文件的语法是 YAML，所以有两种写法： pipeline: batch: size: 125 delay: 50#等同于pipeline.batch.size: 125pipeline.batch.delay: 50 配置文件也支持$&#123;&#125;引用变量 如果使用命令的--modules指定模块，则配置文件中所有配置的模块都会被忽略。 模块的配置： modules: - name: 模块名 var.插件类型.插件名.键: 值 所有配置参数： node.name #节点名，默认为主机名#path参数path.data #数据存放目录，默认为安装目录的data/path.config #logstash.yml路径path.plugins #插件的路径#插件应该位于特定的目录层次结构中：PATH/logstash/TYPE/NAME.rb#其中TYPE的值可以是inputs，filters，outputs或codecs，NAME是插件的名称#http参数http.port #监听的http主机端口，默认为9600http.host #监听的http主机IP，默认为127.0.0.1#日志log参数log.level #日志等级（fatal/error/warn/info/debug/trace），默认infolog.format #日志格式，默认为plain格式path.logs #日志的路径，默认安装目录的logs/#pipeline参数（关于pipeline的配置可专门存放在一个配置文件中，如pipeline.yml）pipeline.id #pipeline的ID，默认为mainpipeline.workers #管道的worker数量，默认为CPU的核数pipeline.batch.size #单个工作线程将从输入收集的最大事件数，默认125。增大此值会加大内存开销，还需调整JVM堆内存参数pipeline.batch.delay #event被调度到worker前等待的时间，单位毫秒，默认50pipeline.unsafe_shutdown #在关闭时立刻退出，会导致数据丢失。 #默认为false，完全退出前会将数据处理好并输出到屏幕再退出#config参数config.string #pipeline配置config.test_and_exit #检查配置是否有效，然后退出。默认为false不检查config.reload.automatic #定期检查配置是否已更改，并在配置发生更改时重新加载配置。默认false不检查config.reload.interval #检查配置是否变动的时间间隔，需要上一条开启。默认3sconfig.debug #是否开启调试日志消息。若开启还需要将log.level的值设为debug。默认false #注：日志消息可能会包含明文密码config.support_escapes #是否开启转义字符，即启用\\转义。默认falsemodules #设置模块#queue参数queue.type #用于事件缓冲的队列模型（model to use for event buffering），有以下两种，默认memory。 memory：传统基于内存的队列 persisted：基于磁盘的响应队列（disk-based ACKed queueing）path.queue #启用持久队列（即上一项值为persisted）时存储数据文件的目录。默认data/queuequeue.pagt_capacity #启用持久队列时使用的页面数据文件的大小。默认64MBqueue.max_events #启用持久队列时队列中未读事件的最大数量。默认0，不限制queue.max_bytes #队列的总容量，以字节数表示。确保磁盘容量大于此值。默认1G#若与上一项同时配置，则Logstash会加载先配置的一项queue.checkpoint.acks #启用持久队列时强制检查点之前的最大ACK响应事件数。默认1024，若设为0则不限制queue.checkpoint.writes #在启用持久队列时强制检查点之前写入事件的最大数量。默认1024，若设为0则不限制queue.drain #启用后，Logstash将等待直到持久队列耗尽，然后才能关闭。默认false#dead_letter_queue参数dead_letter_queue.enable #是否启用dead_letter_queue，简称DLQ功能。默认false不启用dead_letter_queue.max_bytes #每个DLQ的最大大小。超出就会删除条目，默认1Gpath.dead_letter_queue #DLQ目录的位置，默认为安装目录的data/dead_letter_queue 自定义 Logstash 配置文件自定义的配置文件主要用于指定input、filter、output插件等管道参数。 配置文件支持的值类型： 列表 Lists：[ ]中包含多个值。如path =&gt; [&#39;XXX&#39;,&#39;XXX&#39;] 布尔值 Boolean：指定true或false 字节 Bytes：是字符串字段，表示有效的字节单位。支持SI（k M G T P E Z Y）和二进制（binary）（Ki Mi Gi Ti Pi Ei Zi Yi）单位。二进制单位基数为 1024，SI 单位基数为 1000。此字段不区分大小写，并接受值和单位之间的空格。 编解码器 codec：表示数据的 Logstash 编解码器的名称。编解码器可用于输入和输出。例：codec =&gt; json 输入编解码器提供了一种在数据进入输入之前对其进行解码的便捷方式。 输出编解码器提供了一种在数据离开输出之前对数据进行编码的便捷方式。 使用输入或输出编解码器无需在 Logstash 管道中使用单独的过滤器。 哈希 Hash：键值对集合，多条键值对间使用空格间隔，而不是逗号 数字 Number：数字必须为浮点型或整型 密码 Password：密码必须是一个字符串，且该字符串应未被记录或打印 URI：可以是完整的 URL，也可以是类似邮件地址，如user:pass@XXX.net，如果 URI 包含密码，则不会记录或打印 URI 的密码部分 路径 Path：表示有效操作系统路径的字符串 字符串 String：必须用引号括住，可以是单引号或双引号 转义序列 Escape Sequences：默认不启用转义序列。如果要在字符串中使用转义字符，需要在logstash.yml中设置config.support_escapes：true。 Logstash 日志Logstash 的日志存放在LS_HOME/logs中，默认日志等级为INFO， Logstash 的日志框架基于Log4j 2框架，其大部分功能直接暴露给用户。 在调试问题时，尤其是插件问题时，一般将日志记录级别增加到DEBUG以获取更详细的消息。从 5.0 版本开始，可以在 Logstash 中配置特定日志子系统的日志记录。 Logstash 提供一个带有开箱即用设置的log4j2.properties文件，可以更改轮换策略，类型和其他 log4j2 配置。需要重启 Logstash 以应用该配置。 慢日志（Slowlog）用于报告在通过管道（pipeline）时花费不正常时间的事件的日志消息。慢日志同样存放在LS_HOME/logs中。可在主配置文件中添加 slowlog 的配置，如下： slowlog.threshold.warn: 2sslowlog.threshold.info: 1sslowlog.threshold.debug: 500msslowlog.threshold.trace: 100ms 以上配置指定了触发慢日志的条件。在过滤器中处理超过 100ms 的事件会在慢日志中记录为 trace 等级的事件，超过 2 秒的事件会记录为等级为 warn 的事件 可通过curl -X GET localhost:9600/_node/logging?pretty获取关于日志的信息 curl localhost:9600/_node/logging?pretty&#123; &quot;host&quot; : &quot;VM_0_7_centos&quot;, &quot;version&quot; : &quot;6.4.1&quot;, &quot;http_address&quot; : &quot;127.0.0.1:9600&quot;, &quot;id&quot; : &quot;07b4b966-d732-4263-bc16-1efc6e927e1c&quot;, &quot;name&quot; : &quot;VM_0_7_centos&quot;, &quot;loggers&quot; : &#123; #显示的是日志子系统以及日志等级 &quot;logstash.agent&quot; : &quot;INFO&quot;, &quot;logstash.api.service&quot; : &quot;INFO&quot;, &quot;logstash.codecs.line&quot; : &quot;INFO&quot;, &quot;logstash.codecs.rubydebug&quot; : &quot;INFO&quot;,...... 可通过curl -X PUT localhost:9600/_node/logging?pretty -H &#39;Content-Type: application/json&#39; -d &#39;&#123;...&#125;&#39;动态设置指定日志子系统的日志等级。例如： curl -XPUT &#x27;localhost:9600/_node/logging?pretty&#x27; -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123; &quot;logger.logstash.outputs.elasticsearch&quot; : &quot;DEBUG&quot;&#125;&#x27; 则会在log4j2.properties配置中自动添加上该指定配置。若要重置已通过日志记录 API 动态更改的任何日志记录级别，需要通过将 PUT 请求发送到_node/logging/reset将所有日志记录级别都恢复为log4j2.properties文件中指定的值 curl -X PUT localhost:9600/_node/logging/reset?pretty 将其他任意日志导入 Logstash 的操作：编写一个 pipeline 配置文件test.conf，或直接在pipeline.yml添加 input &#123; #设置input参数 file &#123; #通过列表添加两个日志 path =&gt; [&#x27;/var/log/httpd/access_log&#x27;,&#x27;/var/log/squid/access.log&#x27;] &#125;&#125;output &#123; #标准输出，一定要加，否则无法输出到屏幕 stdout &#123;&#125;&#125; 启动 Logstash，bin/logstash -f config/test.conf。会不断获取 httpd 和 squid 的日志消息 &#123; &quot;path&quot; =&gt; &quot;/var/log/httpd/access_log&quot;, &quot;message&quot; =&gt; &quot;127.0.0.1 - - [02/Oct/2018:16:11:16 +0800] \\&quot;GET / HTTP/1.0\\&quot; 200 10 \\&quot;-\\&quot; \\&quot;ApacheBench/2.3\\&quot;&quot;, &quot;@timestamp&quot; =&gt; 2018-10-02T08:11:20.344Z, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;host&quot; =&gt; &quot;VM_0_7_centos&quot;&#125;&#123; &quot;path&quot; =&gt; &quot;/var/log/squid/access.log&quot;, &quot;message&quot; =&gt; &quot;1538467887.306 656 180.126.242.119 TCP_TUNNEL/200 33103 CONNECT xui.ptlogin2.qq.com:443 - HIER_DIRECT/xui.ptlogin2.qq.com -&quot;, &quot;@timestamp&quot; =&gt; 2018-10-02T08:11:27.355Z, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;host&quot; =&gt; &quot;VM_0_7_centos&quot;&#125; 若要将 elasticsearch 的日志都再导入 elasticsearch，可进行以下配置： input &#123; file &#123; path =&gt; &#x27;/usr/local/es-6.4/logs/elasticsearch.log&#x27; type =&gt; &#x27;elasticsearch&#x27; start_position =&gt; &#x27;beginning&#x27; #从日志的头开始读 &#125;&#125;output &#123; elasticsearch &#123; #使用elasticsearch插件 hosts =&gt; &#x27;127.0.0.1:9200&#x27; #指定elasticsearch源 index =&gt; &#x27;es_message-%&#123;+YYYY.MM.dd&#125;&#x27; #指定index &#125; stdout&#123; codec =&gt; rubydebug &#125;&#125; 启动 Logstash 就会显示已导入 elasticsearch 的日志 &#123; &quot;type&quot; =&gt; &quot;elasticsearch&quot;, &quot;message&quot; =&gt; &quot;[2018-10-03T16:06:50,392][INFO ][o.e.c.m.MetaDataMappingService] [system135] [.kibana/PnIK501cQUydUEIVp0icjw] update_mapping [doc]&quot;, &quot;@timestamp&quot; =&gt; 2018-10-03T08:06:50.560Z, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;host&quot; =&gt; &quot;system5.example.com&quot;, &quot;path&quot; =&gt; &quot;/usr/local/es-6.4/logs/elasticsearch.log&quot;&#125; Logstash 常用插件默认的 Logstash 安装包括 Beats 输入插件。 Beats 输入插件使 Logstash 能够从 Elastic Beats 框架接收事件，任何与 Beats 框架一起使用的 Beat（如 Packetbeat 和 Metricbeat），也可以将事件数据发送到 Logstash。 Grok：是 Logstash 过滤器的基础，用于从非结构化数据中获取结构，具有丰富的集成模式，能快速处理 Web，系统，网络和其他类型的事件格式。 Codecs：通常用于简化对 JSON 和多行事件等常见事件结构的处理。 KibanaKibana 是一个开源分析和可视化平台，旨在与 Elasticsearch 协同工作。可使用 Kibana 搜索，查看以及与存储在 Elasticsearch 索引中的数据进行交互，可以轻松地执行高级数据分析，并在各种图表（charts），表格（tables）和地图（maps）中可视化数据。 Kibana 是基于 JS 的 WEB 界面，在 Node.js 上运行，而官方在 Kibana 包中包含了必要的 Node.js 二进制文件，并且不支持针对单独维护的 Node.js 版本运行 Kibana，因此不需要单独搭建 Nodejs 环境。 应将 Kibana 配置为针对相同版本的 Elasticsearch 节点运行，即版本要一致。 注：从 V6.0.0 开始，Kibana 仅支持 64 位操作系统。 Kibana 安装下载 Kibana 包，版本为 6.4.1，解压到/usr/local/kibana6.4 kibana 需要 elasticsearch 的开启才能正常使用，否则启动 kibana 会不断报错，进入 Kibana 后也会提示 status 为 red，无法正常使用，因此需要先启动 elasticsearch。开启后，进入 kibana 目录下bin执行kibana命令。需要等待一段时间直到出现信息[info][listening][server][http] Server running at http://localhost:5601。通过浏览器localhost:5601访问 kibana。 注：内存或 CPU 不足会将 Elasticsearch 杀死，Kibana 也就无法启动 Kibana 的文件结构：除了bin、config、data、plugins，kibana 还有以下目录： node： node_modules optimize：存放透明的源代码。某些管理操作（例如，插件安装）导致源代码在运行中被重新传输。 src webpackShims 可在浏览器访问localhost:5601/status查看 kibana 是否启动正常，插件是否加载正常，以及 kibana 的当前信息。 Kibana 配置Kibana 只有一个配置文件KIBANA_HOME/config/kibana.yml。默认运行在 localhost 的 5601 端口。 常见配置： server.port: 5601 #Kibana服务端口server.host: &quot;localhost&quot; #向哪些主机开放端口，若要所有主机都能访问，即客户端能远程访问，需要设为0.0.0.0server.name: &quot;your-hostname&quot; #Kibana实例名，一般为主机名server.basePath: &quot;&quot; #server.maxPayloadBytes: 1048576server.rewriteBasePath: falseelasticsearch.url: &quot;http://localhost:9200&quot; #Elasticsearch的地址，需要设置正确elasticsearch.preserveHost: truekibana.index: &quot;.kibana&quot;kibana.defaultAppId: &quot;home&quot;#elasticsearch.username: &quot;user&quot; #设置es授权用户名#elasticsearch.password: &quot;pass&quot; #设置es授权用户密码#server.ssl.enabled: false #是否开启ssl#server.ssl.certificate: /path/to/your/server.crt#server.ssl.key: /path/to/your/server.key#elasticsearch.ssl.certificate: /path/to/your/client.crt#elasticsearch.ssl.key: /path/to/your/client.key Kibana 基本功能添加 index 的管理首先要在 elasticsearch 添加 index 数据 curl -X PUT -H &quot;Content-Type: application/json&quot; localhost:9200/tech/employee/1 -d &#x27;&#123; &quot;name&quot;: &quot;zhangsan&quot;, &quot;age&quot;: &quot;25&quot;, &quot;address&quot;: &quot;nanjing&quot;, &quot;hobby&quot;: [ &quot;football&quot;, &quot;tennis&quot;, &quot;game&quot; ]&#125;&#x27; 然后刷新 kibana，进入 Management 中的 Kibana，选 Index pattern，并创建。 创建完成后，进入 Discover 菜单，可查看插入的数据 使用 kibana 提供的数据进行分析从kibana 文档中下载数据，可选择银行账户数据 account.json，下载后通过以下操作导入 elasticsearch。开启 kibana，进入 Management 添加 index pattern，然后进入 Discover 菜单，选择 bank，添加要看的字段。 curl -H &#x27;Content-Type: application/x-ndjson&#x27; -XPOST &#x27;localhost:9200/bank/account/_bulk?pretty&#x27; --data-binary @accounts.json 为数据创建报表，进入 Visualize 菜单，可根据需要选择报表形式，此处选 Pie 饼图，然后再选择 bank 即可进入定制界面。 选择 split slices，然后在聚合（aggregation）中选择 range，然后进行自定义数据范围 ELK 架构若环境的内存少，就在 es 配置文件添加以下配置 bootstrap.memory_lock: false 为避免内存与磁盘间的 swap，会损耗大量性能 bootstrap.system_call_filter: false 参考文章 全文搜索引擎 Elasticsearch 入门教程 每天 5 分中玩转 docker 容器技术 Elasticsearch: 权威指南 Elasticsearch 官方文档 Logstash 简单介绍 ELK 之 Logstash Logstash 官方文档 ES 之五：ElasticSearch 聚合","categories":[{"name":"大数据","slug":"大数据","permalink":"https://coconutmilktaro.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"日志","slug":"大数据/日志","permalink":"https://coconutmilktaro.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%97%A5%E5%BF%97/"},{"name":"架构","slug":"大数据/日志/架构","permalink":"https://coconutmilktaro.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%97%A5%E5%BF%97/%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"运维","slug":"运维","permalink":"https://coconutmilktaro.top/tags/%E8%BF%90%E7%BB%B4/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://coconutmilktaro.top/tags/Elasticsearch/"},{"name":"ELK","slug":"ELK","permalink":"https://coconutmilktaro.top/tags/ELK/"},{"name":"Kafka","slug":"Kafka","permalink":"https://coconutmilktaro.top/tags/Kafka/"},{"name":"Kibana","slug":"Kibana","permalink":"https://coconutmilktaro.top/tags/Kibana/"},{"name":"Logstash","slug":"Logstash","permalink":"https://coconutmilktaro.top/tags/Logstash/"},{"name":"日志","slug":"日志","permalink":"https://coconutmilktaro.top/tags/%E6%97%A5%E5%BF%97/"},{"name":"Filebeat","slug":"Filebeat","permalink":"https://coconutmilktaro.top/tags/Filebeat/"}]},{"title":"重学数据结构与算法笔记","slug":"重学数据结构与算法笔记","date":"2018-09-15T03:42:06.000Z","updated":"2022-05-30T02:51:54.010Z","comments":true,"path":"2018/重学数据结构与算法笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/%E9%87%8D%E5%AD%A6%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/","excerpt":"使用 C 与 python 实现。 学习目录： 数据结构与算法分析概念 线性表 顺序表 链式表","text":"使用 C 与 python 实现。 学习目录： 数据结构与算法分析概念 线性表 顺序表 链式表 数据结构与算法分析概念数据：是对客观事物的符号表示，是可被计算机识别并加工处理的对象。 数据对象：是性质相同的数据元素的集合。 数据元素：是数据的基本单位，在程序中通常作为一个整体进行处理。 数据项：是组成数据元素的不可分割的最小单位。 以上四个术语的关系：数据（包含）数据对象（包含）数据元素（包含）数据项 数据结构是相互之间存在一种或多种特定关系的数据元素的集合。 数据元素相互之间的关系称为结构，有以下几种结构：集合、线性结构（一对一）、树形结构（一对多）、图状结构/网状结构（多对多），其中除线性结构外的其他结构，也属于非线性结构。 数据结构是一个二元组：Data_Structure = (D, S)，其中 D 是数据元素的有限集，S 是 D 上关系的有限集。 数据结构由四部分组成：数据元素、数据元素之间的逻辑关系、逻辑关系在计算机中的存储表示、所规定的操作 数据的存储表示和运算算法的描述构成了数据结构的实现。 数据结构在计算机中的表示称为存储结构。有两种存储结构： 顺序存储结构：将逻辑上相关的数据元素一次存储在地址连续的存储空间内。特点：借助元素在存储器中的相对位置来表示数据元素间的逻辑关系。 链式存储结构：数据元素可以存储在任意的存储空间中，可以是连续的存储空间，可以是不连续的存储空间。元素的存储位置不能体现逻辑关系，而是需要通过指示元素存储地址的指针表示数据元素之间的逻辑关系。 数据类型是性质相同的值的集合以及定义在该值集上的运算操作的集合。 抽象数据类型（ADT）是指一个数学模型以及定义在该模型上的一组操作。不论内部结构如何变化，只要数学特性不变，就不影响其外部使用。ADT 有两个特征： 数据封装：把数据和操纵数据的运算组合在一起的机制 信息隐蔽：数据的使用者只需知道运算的定义便可访问数据，无需了解数据的存储和实现细节 抽象数据类型可以用三元组表示：(D, S, P)。其中 D 表示数据对象、S 表示 D 上的关系集、P 表示对 D 的基本操作集 算法是对特定问题求解步骤的描述，有 5 个特性：输入、输出、可行性、确定性、有穷性 好的算法需要具备的特性：正确性、可读性、健壮性、高效性 算法复杂性是算法运行所需要的计算机资源的量，其中时间资源的量称为时间复杂度，空间资源的量称为空间复杂度。 复杂度与问题的规模、算法的输入、算法本身的函数有关，其中最主要因素是问题规模。 一个算法的时间花销与算法中语句的执行次数成正比。一个算法中语句执行次数为语句频度，记为T(n)，其中 n 为问题规模大小。若有一个函数f(n)，则算法渐进时间复杂度记为T(n) = O(f(n)) 渐进表示法： 渐进上界记号： $O(g(n)) = { f(n)|存在正常数c和n_0，使得对所有n \\geq n_0有：0 \\leq f(n) \\leq cg(n) }$ 渐进下界记号： $\\Omega(g(n)) = { f(n)|存在正常数c和n_0，使得对所有n \\geq n_0有：0 \\leq cg(n) \\leq f(n) }$ 紧渐近界记号： $\\Theta(g(n)) = { f(n)|存在正常数c_1,c_2和n_0，使得对所有n \\geq n_0有：c_1g(n) \\leq f(n) \\leq c_2g(n) }$ 常见的渐进时间复杂度（从小到大）: ​ $O(1)&lt;O(\\log_2n)&lt;O(n)&lt;O(n\\log_2n)&lt;O(n^2)&lt;O(n^3)&lt;O(2^n)$ 线性表线性表是零个或多个有限数据元素构成的线性序列。除第一个和最后一个元素，其他元素都有唯一一个直接前驱元素和直接后继元素。 抽象数据类型线性表定义： 数据对象：$D={a_i|a_i \\in ElemSet, i = 1,2,…,n, n \\geq 0 }​$ 数据关系：$R1={ &lt;a_{i-1},a_i&gt;|a_{i-1}, a_i \\in D, i=2,…,n }​$ 线性存储分为顺序存储与链式存储。 顺序表顺序表包含三个部分： 存储空间的起始位置 顺序表最大存储容量 顺序表当前长度 顺序表有两种创建方式：静态表、动态表 静态表： #define MaxSize 50 // 最大长度typedef int Elemtypetypedef struct &#123; Elemtype data[MaxSize]; // 顺序表元素 int length; // 顺序表当前长度&#125;SqList; 动态表： typedef int Elemtypetypedef struct &#123; Elemtype *data; // 数组指针 int MaxSize, length;&#125;SqList; 动态表创建初始化： #define InitSize 100SqList L;L.data = (Elemtype *)malloc(sizeof(Elemtype) * InitSize) 插入操作： 判断i是否正确 判断表长是否超过数组长度 从后向前到第i个位置，分别将这些元素都向后移动一位 将该元素插入位置i并表长加 1 bool ListInsert(SqList &amp;L, int i, Elemtype e)&#123; if(i &lt; 1 || i &gt; L.length + 1) return false; if(L.length &gt;= MaxSize) return false; for(int j = L.length; j &gt;= i; j--) L.data[j] = L.data[j-1]; L.data[i-1] = e; L.length++;&#125; 最好情况：在表尾插入，不用移元素。复杂度$O(1)$ 最坏情况：在表头插入，要移 n 次元素。复杂度$O(n)$ 平均情况：在表中间插入，平均移动元素次数为$\\frac{n}{2}​$ 删除操作： 判断i是否正确 取删除元素 将删除元素后的所有元素都向前移动一位 修改表长 bool ListDelete(SqList &amp;L, int i, Elemtype &amp;e)&#123; if(i &lt; 1 || i &gt; L.length) return false; e = L.data[i-1]; for(int j = i; j &lt; L.length; j++) L.data[j-1] = L.data[i]; L.length--; return true;&#125; 最好情况：删除表尾元素，复杂度$O(1)​$ 最坏情况：删除表头元素，复杂度$O(n)​$ 平均情况：删除表中间元素，平均移动元素次数$\\frac{n-1}{2}​$ 链式表链式表分为单链表、静态链表、循环链表、双链表。 链式存储的存储单元可连续也可不连续，每个结点包含数据域和指针域，数据域存储数据信息，指针域存储下一个结点的位置信息。 单链表定义： typedef struct LNode &#123; // 单链表结点类型 Elemtype data; // 数据域 struct LNode *next; // 指针域&#125;LNode, *LinkList; // 定义一个节点和链表 通常会用头指针标识一个单链表，如LinkList L; 头结点和头指针的区别： 不管带不带头结点，头指针始终指向链表的第一个结点 而头结点是带头结点链表中的第一个结点，通常不存信息。 设置头结点的作用： 统一第一个结点和其余结点的操作 统一空链表和非空链表的操作 头插法建立单链表：将新结点插到当前链表的表头 LinkList CreateList(LinkList &amp;L) &#123; LNode *s; int x; L = (LinkList)malloc(sizeof(LNode)); L.next = NULL; scanf(&quot;%d&quot;, &amp;x); while(x != 9999)&#123; s = (LNode)malloc(sizeof(LNode)); s.data = L.next; L.next = s; scanf(&quot;%d&quot;, &amp;x); &#125; return L;&#125; 尾插法建立单链表：将新结点插到当前链表的表尾 LinkList CreateList(LinkList &amp;L) &#123; LNode *s, *r = L; // r为表尾指针，指向表尾结点 int x; L = (LinkList)malloc(sizeof(LNode)); scanf(&quot;%d&quot;, &amp;x); while(x != 9999) &#123; s = (LNode *)malloc(sizeof(LNode)); s.data = x; r.next = s; r = s; // r指向新的表尾结点 scanf(&quot;%d&quot;, &amp;x); &#125; r.next = NULL; return L;&#125; 按序号查找：从第一个结点，顺着 next 搜索到指定序号结点 LNode *GetElem(LinkList L, int i) &#123; LNode *p = L.next; int j = 1; if(i == 0) return L; // i为0时返回头结点 if(i &lt; 1) return NULL; while(p != NULL &amp;&amp; j &lt; i) &#123; p = p.next; j++; &#125; return p;&#125; 按值查找：从第一个结点，顺着 next 搜索到指定值的结点 LNode *LocateElem(LinkList L, Elemtype e) &#123; LNode *p = L.next; while(p != NULL &amp;&amp; p.data != e) &#123; p = p.next; &#125; return p;&#125;","categories":[],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://coconutmilktaro.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"算法","slug":"算法","permalink":"https://coconutmilktaro.top/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"Cacti监控学习","slug":"Cacti监控学习","date":"2018-09-15T03:27:44.000Z","updated":"2022-06-28T18:40:30.778Z","comments":true,"path":"2018/Cacti监控学习/","link":"","permalink":"https://coconutmilktaro.top/2018/Cacti%E7%9B%91%E6%8E%A7%E5%AD%A6%E4%B9%A0/","excerpt":"本篇笔记包含以下内容： Cacti 原理与安装 Cacti 安装 参考文章","text":"本篇笔记包含以下内容： Cacti 原理与安装 Cacti 安装 参考文章 Cacti 原理与安装Cacti 是一套基于 PHP，MySQL，SNMP 及 RRDTool 开发的网络流量监测图形分析工具。使用 SNMP 服务获取数据，用 rrdtool 存储和更新数据，并可以使用 rrdtool 生成图表。因此 SNMP 和 RRDtool 是 Cacti 的关键。注意，Cacti 仅仅是一个展示工具，是一个 PHP 网页，真正实现数据收集以及绘图的是 SNMP 和 RRDtool。 MySQL 与 PHP 用来存储一些变量数据并对变量进行调用，如主机名、主机 IP、Snmp 团体名、端口号、模板信息等，Snmp 抓取的数据并不存放在 MySQL 中，而是存放在 rrdtool 生成的 RRD 文件中，rrdtool 对数据的更新和存储就是对 RRD 文件的处理，RRD 文件是大小固定的档案文件，能存储的数据量在创建时就被定义好了。 Cacti 特点： 提供图形化页面操作实现rrdtool create命令 周期性执行能取得数据的命令，并将取回的数据存储在 rrd 文件中 通过 rrdtool 绘图并展示 强大的用户管理机制 丰富的插件库，如 thold，并提供插件框架允许自定义模板 Cacti 模板分为三类： 图形模板：定义图形的绘制 数据模板：定义如何获得数据，如何保存数据 主机模板：就是分好类的图形模板和数据模板，可直接应用于一个或一类主机 SNMP 简介 Simple Network Management Protocol 简单网络管理协议，由一组网络管理的标准组成，包含一个应用层协议、数据库模型（database schema）和一组资源对象。该协议能够支持网络管理系统，用以监测连接到网络上的设备是否有任何引起管理上关注的情况。 SNMP 管理的网络主要由三部分组成： 被管理的设备 SNMP 代理（Agent） 网络管理系统（NMS） 三部分之间的关系： 网络中被管理的每一个设备都存在一个管理信息库（MIB）用于收集并储存管理信息。通过 SNMP 协议，NMS 能获取这些信息。被管理设备，又称为网络单元或网络节点，可以是支持 SNMP 协议的路由器、交换机、服务器或者主机等等。 SNMP 代理是被管理设备上的一个网络管理软件模块，拥有本地设备的相关管理信息，并用于将它们转换成与 SNMP 兼容的格式，传递给 NMS。 NMS 运行应用程序来实现监控被管理设备的功能。另外，NMS 还为网络管理提供大量的处理程序及必须的储存资源。 上述资料引用自百度百科 snmp RRDtool 简介 Round Robin Database Tool 轮询式数据库工具，是一个强大的绘图的引擎。其中，Round Robin是一种存储数据的方式，使用固定大小的空间来存储数据，并有一个指针指向最新的数据的位置。RRDtool 针对处理的是时序型数据(time-series data)，比如网络带宽，温度，CPU 负载等等这些和时间相关联的数据或者说指标。 上述资料引用自百度百科 rrdtool 和RRDtool 入门详解 Cacti 安装首先需要搭建 LAMP 环境yum install httpd php php-devel php-gd gd gd-devel gcc glibc openssl* mariadb* zlib* php-xml libxml libjpeg libpng freetype cairo-devel pango-devel cairo 是一个 2D 图形库 Pango 是一个用于布局和呈现文本的库 gd 也是一个图形库，用于动态生成图片 yum install net-snmp \\ net-snmp-devel \\ net-snmp-utils \\ lm_sensors \\ rrdtool* 安装 snmp 主程序及相关监控工具。net-snmp会提供两个命令snmpwalk和snmpget lm_sensors：是一款基于 linux 系统的硬件监控的软件。可以监控主板，CPU 的工作电压，温度等数据。 开启snmpd和snmptrapd服务systemctl start snmpd snmptrapd 修改 snmp 配置文件/etc/snmp/snmpd.conf，找到com2sec notConfigUser default public一行，复制到下一行并修改 com2sec myuser 127.0.0.1 mycommunity# 127.0.0.1可配置为要监控的主机或网段 找到下面的 group 配置，同样复制一行并修改 group mygroup v2c myuser 再下面，找到 view 配置，添加一行 view all included .1 保存并重启 snmpd 服务。执行snmpwalk -c mycommunity 127.0.0.1 -v2c可看到大量信息。 注：如果是被监控主机，只需要安装net-snmp和lm_sensors即可。 cacti 安装完成后，会在/etc/httpd/conf.d/中生成一个cacti.conf配置文件，可以不用改动，文件中指定的网页存储位置为/usr/share/cacti/，该目录中存放着所有 php 网页。 cacti 的 sql 数据存放在/usr/share/doc/cacti/cacti.sql需要导入数据库。首先要进入 mariadb，创建数据库cactidb，退出后，mysql -u root -p cactidb &lt; /usr/share/doc/cacti/cacti.sql导入数据库。 在数据库中创建用户管理cactidb，进入数据库grant all on cactidb.* to cactiadmin@localhost identified by &quot;cactiadmin&quot;;并flush privileges; 设置 httpd 虚拟主机，使用户通过cacti.example.com直接访问 &lt;VirtualHost *:80&gt; ServerName cacti.example.com DocumentRoot &quot;/usr/share/cacti&quot; ErrorLog &quot;log/cacti-access.log&quot; CustomLog &quot;log/cacti-error.log&quot; common&lt;/VirtualHost&gt;&lt;Directory &quot;/usr/share/cacti&quot;&gt; Require all granted Options Indexes AllowOverride None&lt;/Directory&gt; 修改管理 Cacti 的配置文件/usr/share/cacti/include/config.php，修改以下内容： $database_default = &#x27;cactidb&#x27;; 设置数据库名$database_username = &#x27;cactiadmin&#x27;; 设置数据库中cacti用户名$database_password = &#x27;cactiadmin&#x27;; 设置数据库中cacti用户密码$url_path = &#x27;/&#x27;; 网页访问的路径，可改可不改，若不改就是通过http://localhost/cacti访问 创建普通用户用于周期性执行获取数据的 php 脚本，因为为了安全性，不能让管理员执行。useradd cactiuser，并且将cacti目录中log和rra目录的所属人和所属组都改为cactiuser，chown -R cactiuser:cactiuser /usr/share/cacti/log /usr/share/cacti/rra 至此，安装配置完毕，重启 Apache，浏览器输入cacti.example.com访问，开始网页配置。 若遇到以下报错： 说明 cacti 数据库管理员cactiadmin没有对mysql.time_zone_name表的select权限，需要授权。 grant select on mysql.time_zone_name to cactiadmin@localhost;flush privileges; 并且要修改/etc/my.cnf配置，在[mysqld]下添加： default-time-zone = &#x27;+8:00&#x27; 重启并进入 mysql，使用命令验证 show variables like &#x27;%time_zone%&#x27;;+------------------+--------+| Variable_name | Value |+------------------+--------+| system_time_zone | CST || time_zone | +08:00 |+------------------+--------+ 退出 MySQL，使用命令mysql_tzinfo_to_sql tz_file tz_name | mysql -u root -p mysql tz_file指 timezone 文件，存放在/usr/share/zoneinfo中 执行mysql_tzinfo_to_sql /usr/share/zoneinfo/Asia/Shanghai Shanghai | mysql -u root -p mysql 说明 php 的 timezone 没设置，修改/etc/php.ini，把;date.timezone =注释去除，设置为date.timezone = Asia/Shanghai。支持的时区表 网页下拉还有类似的问题，需要修改 mysql 表中相应参数。修改/etc/my.cnf文件，在[mysqld]下添加报错项，只要满足即可。 max_heap_table_size=2048Mtmp_table_size=2048Mjoin_buffer_size=2048Minnodb_buffer_pool_size=2048Minnodb_doublewrite=offinnodb_flush_log_at_timeout=10innodb_read_io_threads=32innodb_write_io_threads=16 修改完后重启 php 和 mysql systemctl restart mariadb.servicesystemctl restart php-fpm.service 重新访问cacti.example.com。进入安装选项页面： 有两种选项： New Primary Server：若是主节点就选这项New Remote Poller：若是用于收集主节点无法访问的服务器的信息，就选这项 Cacti 的各个路径已自动设置好。由于 Spine 还没有安装，所以会提示错误，但不影响安装。 安装模板，若为 Linux 或 unix 主机，必选Local Linux Machine，若为 Windows 主机，必选Windows Device。 用户登录界面，初始的管理员用户名和密码都是admin，登陆后会强制要求更改。 密码设置有几个条件必须满足： 大于 8 位 含有字母大小写 至少包含一个数字 至少包含一个特殊字符 若想绕过这些规则，可直接进入 mysql 的 cactidb 库，执行update user_auth set password = md5(&quot;密码&quot;) where username=&quot;admin&quot;; 然后就进入了 cacti 主界面。 查看 Graph 页面，出现以下报错： 是因为没有运行/usr/share/cacti/poller.php，这个是 cacti 自带的脚本，用于收集数据，并生成图表。默认 cacti 每 5 分钟收集一次信息，所以要设置定时，每五分钟运行该脚本。而 cacti 安装后已生成一个文件/etc/cron.d/cacti，内容如下：若带有注释，就将注释去除，并要修改用户名。需要确定crond服务是否启动。 */5 * * * * cactiuser /usr/bin/php /usr/share/cacti/poller.php &gt; /dev/null 2&gt;&amp;1 最好通过crontab -e -u cactiuser输入*/5 * * * * /usr/bin/php /usr/share/cacti/poller.php &gt; /dev/null 2&gt;&amp;1设置 cron。 先手动执行一次php /usr/share/cacti/poller.php &gt; /dev/null 2&gt;&amp;1，可通过查看/var/log/cacti/cacti.log确认是否能获取数据。然后查看/usr/share/cacti/rra/是否有 rrd 文件。然后重启 httpd，访问 cacti 的 Graph。 有可能没有启动的原因是系统时间和 BIOS 时间不符，通过hwclock -s同步。 参考文章 Cacti 实战 Linux 运维之道（第二版） 高性能网站构建实战 Cacti 完全使用手册 ( 让你快速个性化使用 Cacti ) 服务器监控系统 cacti cacti 安装与配置 使用 SNMP 和 Cacti 监控 Linux 服务器","categories":[{"name":"系统运维","slug":"系统运维","permalink":"https://coconutmilktaro.top/categories/%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"运维","slug":"运维","permalink":"https://coconutmilktaro.top/tags/%E8%BF%90%E7%BB%B4/"},{"name":"监控","slug":"监控","permalink":"https://coconutmilktaro.top/tags/%E7%9B%91%E6%8E%A7/"},{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"Cacti","slug":"Cacti","permalink":"https://coconutmilktaro.top/tags/Cacti/"}]},{"title":"SQLite学习笔记","slug":"SQLite学习笔记","date":"2018-09-07T12:39:39.000Z","updated":"2022-05-30T02:51:53.868Z","comments":true,"path":"2018/SQLite学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/SQLite%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://coconutmilktaro.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"SQLite","slug":"SQLite","permalink":"https://coconutmilktaro.top/tags/SQLite/"}]},{"title":"IS-IS学习笔记","slug":"IS-IS学习笔记","date":"2018-08-05T07:18:10.000Z","updated":"2022-05-30T02:51:53.803Z","comments":true,"path":"2018/IS-IS学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/IS-IS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"基于华三网络学习笔记 本篇主要包含以下内容： OSI 地址 IS-IS 概述 IS-IS 常见术语： IS-IS 协议报文 IS-IS 网络类型 IS-IS 实现 邻接关系 LSDB 同步 参考资料","text":"基于华三网络学习笔记 本篇主要包含以下内容： OSI 地址 IS-IS 概述 IS-IS 常见术语： IS-IS 协议报文 IS-IS 网络类型 IS-IS 实现 邻接关系 LSDB 同步 参考资料 OSI 地址在 OSI 协议体系中，OSI 地址标识了一台支持 OSI 协议的设备。IS-IS 的报文封装在数据链路层，采用 OSI 报文格式，包含 OSI 地址。IS-IS 协议将 ISO 网络层地址称 NSAP。IS-IS 用 OSI 地址标识不同 IS，并构建网络拓扑数据库，计算到达各节点的最短路径树。 OSI 地址使用的是 NASP（Network Service Access Point 网络服务接入点）地址格式，是 IP 地址和上层协议号的组合，用于标识设备和设备启用的服务。 NASP 由 IDP（Initial Domian Part 初始域部分）和 DSP（Domain Specific Part 域指定部分），IDP 表示 IP 地址的主网络号，DSP 表示 IP 地址的子网号和主机地址。IDP 和 DSP 长度是可变的，但 NASP 的总长最多为 20 字节，最少 8 字节。 在 IS-IS 中，NASP 地址被分为 3 部分：可变长区域地址，System ID，NSEL。 System ID 用于在区域中唯一表示主机或服务器，一般会由 Router ID 转换得出。 转换方法：Router ID 的每部分都扩展为 3 位数字，不足则在前补零，将扩展后的地址重新划分为 3 部分，每部分 4 个数字，得到 System ID NSEL 类似于协议标识符，当协议为 IP 时，NSEL 均为 00。 路由器只需配置一个区域地址，但最多可以配置 3 个，同一区域中所有节点的区域地址都相同。 NET（Network Entity Title 网络实体名称）指示的是 IS 本身的网络层信息，不包括传输层信息，可看做 NSEL 为 0 的特殊的 NASP。一台路由器只需配置一个 NET，最多 3 个，若配置多个 NET，则必须保证 System ID 相同。NET 除了可以通过 Router ID 转换变得，也可通过 MAC 地址转换变得，但 MAC 地址由于具有全局性，一个区域内的路由器的 MAC 没有规律，管理不方便，所以一般还是用 Router ID 映射。 IS-IS 概述IS-IS（Intermediate System-to-Intermediate System，中间系统到中间系统）是 ISO 为 CLNP（Connection Less Network Protocol，无连接网络协议）设计的一种动态路由协议。IS-IS 能够同时应用在 TCP/IP 和 OSI 环境中，形成了集成化 IS-IS。采用 TLV 架构，易于扩展。 IS-IS 属于内部网关路由协议，用于自治系统内部。IS-IS 是一种链路状态协议，与 TCP/IP 网络中的 OSPF 协议非常相似，使用最短路径优先算法 SPF 进行路由计算。 IS-IS 常见术语：区域（Area）：路由域的细分单元，IS-IS 允许将整个路由域分为多个区域 路由域（Routing Domain）：较大的区域，可包含多个区域 中间系统 Intermediate System（IS）：即路由器 终端系统 End System（ES）：即主机 ES-IS：主机和路由器之间运行的协议 IS-IS：路由器与路由器之间运行的协议，就是用来提供路由域内或一个区域内的路由 IS-IS 路由器有三种角色： Level-1：负责区域内的路由，只与属于同一区域的 Level-1 和 Level-1-2 路由器形成邻居关系，维护一个 Level-1 的链路状态数据库，该链路状态数据库包含本区域的路由信息，到区域外的报文转发给最近的 Level-1-2 路由器。 Level-2：负责区域间的路由，可以与同一区域或者其它区域的 Level-2 和 Level-1-2 路由器形成邻居关系，维护一个 Level-2 的链路状态数据库，该链路状态数据库包含区域间的路由信息。所有Level-2路由器和Level-1-2路由器组成路由域的骨干网，负责在不同区域间通信，路由域中的 Level-2 路由器必须是物理连续的，以保证骨干网的连续性。 Level-1-2：同时属于 Level-1 和 Level-2 的路由器，可以与同一区域的 Level-1 和 Level-1-2 路由器形成 Level-1 邻居关系，也可以与同一区域或者其他区域的 Level-2 和 Level-1-2 路由器形成 Level-2 的邻居关系。Level-1 路由器必须通过 Level-1-2 路由器才能连接至其他区域。Level-1-2 路由器维护两个链路状态数据库，Level-1 的链路状态数据库用于区域内路由，Level-2 的链路状态数据库用于区域间路由。 每台路由器只能属于一个区域，区域边界在链路上。 IS-IS 协议报文IS-IS 使用协议数据单元 PDU 进行通讯。PDU 有以下类型： IS-IS Hello PDU：简称 IIH，负责路由间的邻居关系建立和维护 链路状态 PDU：简称 LSP，描述路由器中的所有链路状态信息 时序报文 SNP：用于确认邻居间最新接收的 LSP，类似于确认报文。包括两种报文：CSNP 和 PSNP 全时序报文 CSNP：包含网络中每个 LSP 的摘要信息。当路由器收到一个 CSNP 时，它会将该 CSNP 与其链路状态数据库 LSDB 进行比较，如果该路由器丢失了一个在 CSNP 中存在的 LSP 时， 它会发送一个组播 PSNP，向网络中其它路由器索要其需要的 LSP。 部分时序报文 PSNP：在点对点链路中用于确认接收的 LSP 和请求最新或者丢失的 LSP；在广播链路中仅用于请求最新或者丢失的 LSP。 IS-IS 报文直接封装在链路层数据中。报头包含通用报头 Common Header 和专用报头 Specific Header。 IS-IS 网络类型点对点：主要用于 PPP、HDLC 广播：主要用于以太网 IS-IS 实现邻接关系 邻居关系建立 若在点对点网络，只要 IS 能接收到对端的 P2P IIH 报文，则邻居能建立，状态变为 UP 若在广播网络，邻居建立需要三次握手。 邻接关系建立 若在点对点网络： 若在同一区域 Area，L1 间只建立 L1 邻接关系，L1 和 L1/2 只建立 L1 邻接关系，L1/2 间建立 L1 和 L2 邻接关系。 若在不同区域，L1 间不建立邻接关系（邻居关系都不是），L2 间建立 L2 邻接关系，L1/2 间建立 L2 邻接关系。 若在广播网络：会选举 DIS（Desginated IS，指定 IS），类似 DR，相同角色的 IS 间会选举一个，例如 L1 的路由器间选出一个，与 L2 间选出的并不冲突。 DIS 的作用： 一旦一个设备选举为 DIS 以后，DIS 发送 HELLO 数据包的时间间隔是普通路由器的 1/3，这样可以保证 DIS 失效的时候可以被快速检测到。 DIS 的选举是抢占的, 不能不参加选举，IS-IS 中不存在备份 DIS,当一个 DIS 不能工作的时候，直接选举另外一个。 在广播子网中创建并向所有的路由器通告伪节点 LSP(Link State Protocol Data unit 链路状态数据单元). 在 LAN 中通过每 10s 周期性发送 CSNP（完全数据库描述）来泛洪 LSP(Link State Protocol Data unit 链路状态数据单元). DIS 的选举过程： 比较接口优先级，高的优 具有最大的(SNPA 子网接入点)的路由器将当选 DIS。广播网络中 SNPA 是指 MAC 地址 点到点 广播 Hello 报文 P2P IIH Level-1/2 LAN IIH Hello 报文形式 单播 组播 Hello 定时器 10s 10s，DIS 为 3.3s 邻接关系数量 1 多个 LSDB 同步同步相关报文： LSP 报文：用于描述链路状态信息 Level-1 LSP 仅在区域内传播，Level-2 LSP 在骨干网传播 SNP 报文：用于描述 LSDB 中 LSP 摘要，并对邻居之间最新接收的 LSP 进行确认 CSNP 报文：包含所有 LSP 的摘要信息，在广播网络中周期发送，在点对点网络中只在第一次发送 PSNP 报文：列举最近收到的一个或多个 LSP 序号，用于 LSP 确认 在广播网络中： 所有同类路由器向 DIS 发送自己的所有 LSP DIS 周期发送 LSP 摘要信息 IS 向 DIS 发送 PSNP 响应 DIS 回复 LSP_K 参考资料 百度百科 IS-IS","categories":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"IS-IS","slug":"IS-IS","permalink":"https://coconutmilktaro.top/tags/IS-IS/"}]},{"title":"Postfix邮件服务器笔记","slug":"Postfix邮件服务器学习笔记","date":"2018-08-01T13:04:56.000Z","updated":"2022-05-30T02:51:53.864Z","comments":true,"path":"2018/Postfix邮件服务器学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Postfix%E9%82%AE%E4%BB%B6%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"邮件服务概述 Mail 与 DNS 的相关性 邮件传送流程 常见邮件服务或协议 SMTP 使用 SMTP 协议发送邮件 POP IMAP 邮件服务器搭建 参考文章","text":"邮件服务概述 Mail 与 DNS 的相关性 邮件传送流程 常见邮件服务或协议 SMTP 使用 SMTP 协议发送邮件 POP IMAP 邮件服务器搭建 参考文章 邮件服务概述Mail 与 DNS 的相关性mail server 需要有一个合法的域名存在于 DNS 服务器上（不是一定要自己架设 DNS 服务器），可以找 ISP 或 DNS 服务商等注册。 只要有主机名称对应到 IP ，即是有 A ( Address )这个 DNS 的标志后，那么就可以架设 mail server 了。 MX（Mail Exchanger）这个 DNS 设定中的标志，主要就是要给 mail server 用的，可以让 Internet 上面的信件马上找寻到 Mail 主机的位置，并且 MX 后面可以接数字，一个 domain 或者是一个主机可以有多个 MX 标志。当主要的 mail server 挂点时，由于有 MX 标号，信件不会直接退回，而是跑到下一个 MX 设定的主机去，并且暂存在该处，等到主要的 mail server 起来之后，这个 MX 设定的主机就会将信件传送到目的地。当有了 MX 标志之后，要传送 mail 的时候，可以直接依据 DNS 的 MX 标志直接将信件传送到该设定的 MX 邮件主机，而不需要去寻问到底邮件要寄到哪里去，实现邮件路由。 邮件传送流程 MUA（Mail User Agent）：邮件用户代理，即邮件客户端，用于收发邮件 MTA（Mail Transfer Agent）：邮件传送代理，即邮件服务，具有收受邮件和转递邮件的功能 MDA（Mail Delivery Agent）：邮件递送代理，是 MTA 下的一个程序。用于分析 MTA 收到的新建表头或内容等资料，决定这封邮件的去向，即 MTA 的转递功能是由 MDA 完成的。且 MDA 能过滤垃圾邮件、自动回复 Mailbox：邮件信箱，即某个用户专用的邮件存放点，Linux 默认的信箱目录为/var/spool/mail MRA（Mail Receive Agent）：邮件接收代理。当用户端收受信件时，使用的是 MRA 的 POP3, IMAP 等通讯协定，并非 MTA 的 SMTP。 步骤： 获取本地 MTA 的使用权限，即向 MTA 注册邮箱账号密码 编写邮件，传送到 MTA。邮件包含：标头（收寄件人邮箱，标题）、内容 若该邮件的目的是本地端 MTA 账号，会通过 MDA 发往本地的 mailbox若该邮件的目的是远端 MTA，则开始转递，向下一跳 MTA 的 25 端口发送 对方 MTA 接收邮件，等待远端用户读取 常见邮件服务或协议SMTPSMTP（Send Mail Transfer Protocol）：是一种发送邮件的协议，使用 TCP 的 25 端口。在建立一个 TCP 连接后，在这个连接上进行控制、应答与数据发送。客户端以文本发送请求，服务器端回复 3 个数字的应答。 客户端常见指令 指令 说明 HELO \\&lt;domain\\&gt; 开始通信 EHLO \\&lt;domain\\&gt; 开始通信（扩展 HELO） MAIL FROM: \\&lt;reverse-path\\&gt; 发送人 RCPT TO: \\&lt;forward-path\\&gt; 接收人 DATA 发送电子邮件的正文 RSET 初始化 VERY \\&lt;string\\&gt; 确认用户名 EXPN \\&lt;string\\&gt; 将邮件组扩展为邮件地址列表 NOOP 请求应答 QUIT 关闭 每个指令和应答最后必须追加换行指令 CR、LF 由于 SMTP 不具备验证发送者的功能，因此无法避免垃圾邮件，但也可通过 POP before SMTP 或 SMTP 认证对发送者认证。并且，邮件收发双方必须同时在线，否则邮件会发送失败。 使用 SMTP 协议发送邮件首先检查本机的 25 号端口是否开启。在 root 用户上向本地用户 tom 发送邮件。 [root@s1 ~]# telnet 127.0.0.1 25 # 使用telnet连接本地25端口Trying 127.0.0.1...Connected to 127.0.0.1.Escape character is &#x27;^]&#x27;.220 s1.localdomain ESMTP Postfixhelo s1 # 开始通信，跟上主机名250 s1.localdomainmail from:root # 发送方250 2.1.0 Okrcpt to:tom # 接收方250 2.1.5 Okdata # 开始正文354 End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt;hellohellohello. # 单独换一行，并打. ，再换行结束正文250 2.0.0 Ok: queued as E4B5D20E4560quit # 退出221 2.0.0 ByeConnection closed by foreign host. 切换到 tom 用户 [tom@s1 ~]$ mailHeirloom Mail version 12.5 7/5/10. Type ? for help.&quot;/var/spool/mail/tom&quot;: 1 message 1 new&gt;N 1 root@s1.localdomain Thu Jan 24 05:18 15/432&amp; 1 # 查看第一封邮件Message 1:From root@s1.localdomain Thu Jan 24 05:18:01 2019Return-Path: &lt;root@s1.localdomain&gt;X-Original-To: tomDelivered-To: tom@s1.localdomainDate: Thu, 24 Jan 2019 05:17:05 +0800 (CST)From: root@s1.localdomainStatus: Rhellohellohello POP为了解决 SMTP 的弊端，引入了 POP（Post Office Protocol）协议，这是一种用于接收邮件的协议，TCP 端口 110，发送端的邮件根据 SMTP 协议被转发到一直处于插电状态的 POP 服务器，客户端再根据 POP 协议从 POP 服务器接收对方发来的邮件，该过程中支持身份验证，邮件客户端会从邮件服务器上获取所有发给自己的新邮件，然后关闭连接，在关闭连接后，邮件服务器会删除所有被标记为已接收的邮件。当前 POP 的版本为 3，写作 POP3。 客户端的应答只有两种：正常的”+OK”，错误的”-ERR”。 POP3 的收信方式： MUA 透过 POP3 连接到 MRA 的 110 端口， 并且输入帐号与密码来取得正确的认证与授权 MRA 确认该使用者帐号/密码没有问题后，会前往该使用者的 Mailbox (/var/spool/mail/使用者帐号) 取得使用者的信件并传送给使用者的 MUA 上 当所有的信件传送完毕后，使用者的 mailbox 内的资料将会被删除 POP3 与 SSL 结合称为 POP3S，实现邮件加密， IMAPIMAP（Internet Message Access Protocol）因特网消息访问协议，与 POP 类似，是接收邮件的协议。在 POP 中，邮件有客户端进行管理，IMAP 中邮件由服务器进行管理。不必从邮件服务器上下载所有邮件也可以阅读，并且会对已读和未读进行分类管理。当前版本为 4，写作 IMAP4。 可以将 mailbox 的资料转存到主机上的家目录，不但可以建立邮件档案，也可以针对信件分类管理。 邮件服务器搭建直接 yum 安装 postfix。若安装了 postfix，就要停止 sendmail 的所有相关服务。 postfix 的相关配置文件： /etc/postfix/main.cf：主配置文件 /etc/postfix/master.cf：postfix 运行参数，一般不用修改 /etc/postfix/access：访问控制 /etc/aliases：邮件别名，或设置邮件群组 postfix 命令 postfix check #检查配置文件或权限等 start #启动postfix stop #关闭postfix flush #强制将目前邮件队列中的邮件寄出 reload #重载配置文件 postfix 服务的配置文件/etc/postfix/main.cf inet_interfaces = localhost # 接收本地的请求。# all为所有请求，localhost-only为仅接收本地请求myhostname = host.domain.tld # 系统主机名（FQDN）mydomain = domain.tld # 系统域名myorigin = $myhostname # 发信源mydestination = $myhostname # 指定发给本地邮件的域名relayhost = $mydomain # 中继服务器域名mynetworks = 127.0.0.0/8 # 信任的客户端 可以直接使用命令postconf -e修改配置项。 postconf -e &quot;inet_interfaces=all&quot;postconf -e &quot;myhostname=s1.example.com&quot;postconf -e &quot;mydomain=example.com&quot;postconf -e &quot;myorigin=example.com&quot;postconf -e &quot;inet_protocols=ipv4&quot; 参考文章 鸟哥的 Linux 私房菜 图解 TCP/IP","categories":[],"tags":[{"name":"Postfix","slug":"Postfix","permalink":"https://coconutmilktaro.top/tags/Postfix/"},{"name":"邮件服务","slug":"邮件服务","permalink":"https://coconutmilktaro.top/tags/%E9%82%AE%E4%BB%B6%E6%9C%8D%E5%8A%A1/"},{"name":"sendmail","slug":"sendmail","permalink":"https://coconutmilktaro.top/tags/sendmail/"}]},{"title":"Tomcat学习笔记","slug":"Tomcat学习笔记","date":"2018-08-01T13:04:02.000Z","updated":"2022-05-30T02:51:53.876Z","comments":true,"path":"2018/Tomcat学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Tomcat%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"tomcat9.0.13，jdk1.8 Tomcat 概述 Tomcat 结构 Tomcat 与 HTTP 的通信方式 Connector Container Tomcat 连接器 Tomcat 环境搭建 Tomcat 目录结构 server.xml webapp 结构 配置 WEB 应用","text":"tomcat9.0.13，jdk1.8 Tomcat 概述 Tomcat 结构 Tomcat 与 HTTP 的通信方式 Connector Container Tomcat 连接器 Tomcat 环境搭建 Tomcat 目录结构 server.xml webapp 结构 配置 WEB 应用 Tomcat 概述Tomcat 是 Apache 基金会下的一个核心项目，是一个轻量级的 web 应用服务器，主要用于作为 JSP 应用与 Servlet 的后端服务器。最早项目名叫 catalina，后改名为 tomcat。 Tomcat 注重于 servlet 引擎，对静态页面的加载不如 apache，因此常常 tomcat 与 apache 组网，apache 用于静态页面生成，动态请求会转发给 tomcat 处理。 Tomcat 结构 Server 为 Tomcat 的一个实例，一个 JVM 只能包含一个 Tomcat 实例。Server 的端口号为 8005，监听的是 shutdown 命令，若接收到 shutdown 信息，则关闭 tomcat。可在server.xml中看到 &lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt; Service 包含两个部分： Connector：处理连接相关事务，提供 socket 与（request 和 response）的转化 Container：封装和管理 servlet，具体处理 request 请求 一个 Service 只有一个 Container，可有多个 Connector。 有两个默认的 service： 一个端口为 8080，负责建立 http 连接 一个端口为 8009，负责与其他 http 服务器建立连接 Tomcat 与 HTTP 的通信方式有两种方式： Tomcat 提供专门的插件 JK，负责 Tomcat 与 HTTP 服务通信，把 JK 插件安装在对方 HTTP 服务器上，当 http 服务器收到请求会先通过 JK 过滤 URL，决定是否把请求发送到 Tomcat。 AJP 协议（Apache JServ Protocol）。Tomcat 为实现与 HTTP 通信而定制的协议，有很高效率。过程如下： 请求到达 Tomcat，经过 Service 交给 Connector，将请求封装为 Request 和 Response 处理 封装后交给 Container 处理 再返回给 Container，Connector 通过 Socket 将结果返回给客户端 Connector Connector 使用 ProtocolHandler 处理请求，不同 ProtocolHandler 代表不同连接类型 ProtocolHandler 包含三个部件： Endpoint：处理底层 socket 连接，用来实现 TCP/IP。Endpoint 的抽象实现 AbstractEndpoint 定义了两个内部类 Acceptor 和 AsyncTimeout，以及一个接口 Handler Acceptor 用于监听请求 AsyncTimeout 用于检查异步 Request 超时 Handler 用于处理接收到的 socket，交给 Processor Processor：将 Endpoint 收到的 socket 封装成 Request，用来实现 HTTP Adapter：将 Request 交给 Container（Servlet 容器）进行具体处理 ContainerContainer 用于封装 Servlet 以及具体处理 Request 请求 Container 包含四个子容器： Engine：用于管理多个站点，一个 Service 最多一个 Engine Host：站点，每个 Host 代表一个虚拟主机，可通过配置 Host 添加站点 Context：应用，对应程序或一个 （WEB-INF 目录以及下面的 web.xml 文件） Wrapper：Servlet webapps|-- ROOT|-- docs|-- examples|-- host-manager`-- manager webapps 目录下每个文件夹都是一个 context，ROOT 存放主应用，其他目录存放子应用，整个 webaapps 就是一个 Host 站点。ROOT 下可直接通过域名访问，其他子应用通过域名+路径访问 Container 使用的 pipeline-valve 处理请求。Pipeline-Valve 是责任链模式 责任链模式：在一个请求处理过程中有很多处理者（handler）依次对请求处理，每个处理者负责自己相应的处理，处理完后将处理后的请求返回，依次让下一个处理者处理。 Pipeline-valve 与普通责任链不同的地方： 每个 pipeline 都有特定 valve，而且在管道的最后一个执行，称为 Basevalve，该 valve 不可删除 上层容器的管道 Basevalve 会调用下层容器的管道 四个子容器的 Basevalve 分别在： StandardEngineValve StandardHostValve StandardContextValve StandardWrapperValve Connector 接收的请求会传给最上层的 EnginePipeline，依次执行最终到 StandardWrapperValve。此时 StandardWrapperValve 会创建 Filterchain，调用其 doFilter 方法处理请求，Filterchain 包含配置的与请求匹配的 Filter 和 Servlet，doFilter 会依次调用所有的 Filter 的 doFilter 和 Servlet 的 service 方法，请求即被处理。所有的 PipelineValve 执行完成后，将结果返回给 Connector，Connector 再通过 socket 将结果返回给客户端。 Tomcat 连接器Tomcat 连接器分为两类： HTTP 连接器 Web 服务器连接器 HTTP 连接器有三种： 基于 java 的 HTTP/1.1 连接器：Tomcat 默认使用的连接器，即 Coyote；它是 Tomcat 作为 standalone 模式工作时所用到的连接器，可直接响应来自用户浏览器的关于 JSP、servlet 和 HTML 的请求；此连接器是一个 Java 类，定义在 server.xml 当中，默认使用 8080 端口 高性能 NIO HTTP/1.1 连接器（java 开发）：支持非阻塞式 IO 和 Comnet，在基于库向 tomcat 发起请求时，此连接器表现不俗 native APR HTTP/1.1 连接器（C++开发）：在负载较大的场景中，此连接器可以提供非常好的性能 Tomcat 环境搭建搭建 Tomcat，首先要配置 JAVA 环境，下载 jdk，解压/usr/local/jdk8。或者直接安装 openjdk，选择版本 1.8 debian/ubuntu：apt-get install openjdk-8-jre openjdk-8-jdk设置JAVA_HOME为/usr/lib/jvm/java-8-openjdk-amd64/centos/redhat/fedora：yum install java-1.8.0-openjdk java-1.8.0-openjdk-devel设置JAVA_HOME为/usr/lib/jvm/java-1.8.0-openjdk/ 然后在/etc/profile配置环境变量 export JAVA_HOME=/usr/local/jdk8export CLASSPATH=$JAVA_HOME/libexport PATH=$JAVA_HOME/bin:$PATH 使用java -version和javac -version检查是否安装及配置成功 Tomcat 目录结构# tree -d -L 1.├── bin # 存放启动和关闭Tomcat的脚本文件├── conf # 存放配置文件├── lib # 存放服务器支撑jar包├── logs # 存放日志文件├── temp # 存放临时文件├── webapps # web应用目录└── work # Tomcat工作目录 bin目录 ├── catalina.sh # Tomcat的核心脚本文件，用于启动、停止tomcat等操作├── ciphers.sh├── configtest.sh├── daemon.sh├── digest.sh├── makebase.sh├── setclasspath.sh├── shutdown.sh # Tomcat停止脚本├── startup.sh # Tomcat启动脚本，相当于catalina.sh run，而且是后台运行├── tool-wrapper.sh└── version.sh # 显示Tomcat版本信息 conf目录 conf/|-- catalina.policy|-- catalina.properties|-- context.xml|-- jaspic-providers.xml|-- jaspic-providers.xsd|-- logging.properties|-- server.xml|-- tomcat-users.xml|-- tomcat-users.xsd`-- web.xml server.xml：Tomcat 的主配置文件，包含 Service, Connector, Engine, Realm, Valve, Hosts 主组件的相关配置信息； web.xml：遵循 Servlet 规范标准的配置文件，用于配置 servlet，并为所有的 Web 应用程序提供包括 MIME 映射等默认配置信息； context.xml：所有 host 的默认配置信息； tomcat-users.xml：Realm 认证时用到的相关角色、用户和密码等信息；Tomcat 自带的 manager 默认情况下会用到此文件；在 Tomcat 中添加/删除用户，为用户指定角色等将通过编辑此文件实现； catalina.policy：Java 相关的安全策略配置文件，在系统资源级别上提供访问控制的能力； catalina.properties：Tomcat 内部 package 的定义及访问相关的控制，也包括对通过类装载器装载的内容的控制；Tomcat6 在启动时会事先读取此文件的相关设置； Tomcat 以面向对象的方式运行，它可以在运行时动态加载配置文件中定义的对象结构，这有点类似于 apache 的 httpd 模块的调用方式。server.xml 中定义的每个主元素都会被创建为对象，并以某特定的层次结构将这些对象组织在一起。 server.xml配置层次： &lt;server&gt; &lt;service&gt; &lt;connector /&gt; &lt;engine&gt; &lt;host&gt; &lt;context&gt; &lt;/context&gt; &lt;/host&gt; &lt;host&gt; ..... &lt;/host&gt; &lt;/engine&gt; &lt;/service&gt;&lt;/server&gt; 组件类别： 顶级组件：位于配置顶层 server 容器类组件：可包含其他组件 service： engine： host： context： webapp： 连接器组件：连接用户请求到 tomcat connector 嵌套类组件：位于容器中，不包含其他组件 valve： access log valve： remote address filter valve： logger： Server示例：&lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt; &lt;Listener className=&quot;org.apache.catalina.startup.VersionLoggerListener&quot; /&gt; ......&lt;/Server&gt; 相关属性：className: 用于实现此 Server 容器的完全限定类的名称；port: 接收 shutdown 指令的端口，默认仅允许通过本机访问，默认为 8005；shutdown: 发往此 Server 用于实现关闭 tomcat 实例的命令字符串，默认为 SHUTDOWN； Service示例：&lt;Service name=&quot;Catalina&quot;&gt; webapp 结构webapp 有特定的组织格式，是一种层次型目录结构；通常包含了 servlet 代码文件、jsp 页面文件、类文件、部署描述符文件等等，一般会打包成归档格式 示例： examples/|-- WEB-INF| |-- classes| |-- jsp| |-- jsp2| |-- lib| |-- tags| `-- web.xml|-- index.html|-- jsp| |--......| |-- images| |-- include| |-- index.html...... /：web 应用的根目录，ROOT 为根目录内容 /WEB-INF：包含当前 webapp 的 deploy 描述符，如所有的 servlets 和 JSP 等动态文件的详细信息，会话超时时间和数据源等；通常用于定义当前 webapp 特有的资源，通常 web.xml 和 context.xml 均放置于此目录 /WEB-INF/classes：包含所有服务器端类及当前应用程序相关的其它第三方类等 /WEB-INF/lib：包含 JSP 所用到的 JAR 文件，此 webapp 自有能够被打包为 jar 格式的类 配置 WEB 应用有五种方法配置 web 应用 在conf目录中context.xml中配置，配置会被所有 web 应用加载 在conf目录中引擎目录(catalina)的主机目录(localhost)下新建server.xml.default配置，会被主机的所有 web 应用加载 在conf目录中catalina的localhost目录中创建任意一个.xml文件，在文件中添加上下文，而文件名会被用作 web 应用的名称（虚拟站点名）。一个 web 应用可以映射多个虚拟目录。 在主机目录下META-INF/中context.xml 在conf目录下server.xml中在&lt;Host&gt;中添加上下文（就是 web 应用路径） 参考文章四张图带你了解 Tomcat 系统架构 &gt; Tomcat 安装及配置详解","categories":[],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"Tomcat","slug":"Tomcat","permalink":"https://coconutmilktaro.top/tags/Tomcat/"},{"name":"web","slug":"web","permalink":"https://coconutmilktaro.top/tags/web/"},{"name":"java","slug":"java","permalink":"https://coconutmilktaro.top/tags/java/"}]},{"title":"SSH与SSL协议学习笔记","slug":"SSH与SSL协议学习笔记","date":"2018-08-01T13:02:56.000Z","updated":"2022-05-30T02:51:53.868Z","comments":true,"path":"2018/SSH与SSL协议学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/SSH%E4%B8%8ESSL%E5%8D%8F%E8%AE%AE%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"SSL/TLS 概念 SSL 协议架构 OpenSSL 概念 SSH 协议 sshd 服务 sshd_config 配置 密钥分发命令 参考文章","text":"SSL/TLS 概念 SSL 协议架构 OpenSSL 概念 SSH 协议 sshd 服务 sshd_config 配置 密钥分发命令 参考文章 SSL/TLS 概念SSL（Secure Socket Layer，安全套接字层）是一种标准安全协议，由美国网景（Netscape）开发，用于在在线通信中建立 Web 服务器和浏览器之间的加密链接。SSL 技术的使用确保了 Web 服务器和浏览器之间传输的所有数据都保持加密状态。TLS（transport Layer Security，安全传输层协议）是 SSL 标准化后的版本，与 SSL 基本没有区别。 SSL/TLS 的主要功能： 认证用户与服务器，确保数据发送到正确的客户端和服务器，即可靠性 加密数据，即机密性 维护数据的完整性 SSL 协议架构SSL 基于 TCP，且分为两个子层：握手层和记录层，其中握手层负责建立 SSL 连接，记录层负责对报文的加解密。 握手层： 协商加密能力 协商密钥参数 验证对方身份 建立并维护 SSL 会话 握手层协议报文格式： 消息类型 Type 消息长度 Length 消息相关参数 Content SSL 提供三种握手过程，分别为： 无客户端身份认证的全握手 客户端向服务器发送以下信息：支持的 SSL 最高版本、加密套件列表、压缩算法列表、客户端随机数（32 位）、会话 ID 服务器端回应客户端以下信息：服务器同意的 SSL 版本、加密套件、压缩算法、会话 ID、服务器端随机数。并且还会发送服务器的证书、服务器端密钥交换的信息，最后通知对端握手信息已发完 客户端再向服务器端发送密钥参数和握手过程的验证报文，并通知对端开始启用加密参数 服务器再向客户端发送自己的握手过程验证报文，并通知对端开始启用加密参数 有客户端身份认证的全握手 与上面类似，但在第 2 步后，服务器端还会向客户端请求客户端的证书，然后客户端回应自己的证书，并会附加上数字签名 会话恢复 当 SSL 连接因某些原因不正常断开后，可在超时时间内进行会话恢复。 客户端向服务器发送的消息与第 1 条一致，其中会话 ID 为上一次 SSL 连接的会话 ID。其余过程也基本一致。 记录层： 保护传输数据的机密性，对数据进行加密和解密 验证传输数据的完整性，计算报文摘要 对报文压缩 保证数据传输的可靠有序 记录层对数据包的三个操作：分片、压缩、加密 记录层协议报文格式： 报文类型：1 个字节，密钥改变协议（20）、告警协议（21）、握手协议（22）、应用层数据（23） 版本：2 字节，TLS1.0（3,1）、SSL3.0（3,0） 长度：2 字节记录层报文的长度，包括加密数据和 MAC 值 MAC：消息验证码 SSL 会话与连接 SSL 会话是指客户端与服务器间的关联关系，通过握手协议创建。而 SSL 连接是用于点对点数据的传输，连接的维持时间比较短暂，且一定与一个会话关联。 一次会话过程通常会发起多个 SSL 连接来完成任务，这些连接共享会话定义的安全参数，这样可以避免为每个 SSL 连接单独进行安全参数的协商，而只需在会话建立时进行一次协商，提高了效率。 HTTPS 与 HTTP 连接的建立耗时也因为 SSL 层而出现 3 倍的差距。可通过curl -w &quot;TCP handshake: %&#123;time_connect&#125;, SSL handshake: %&#123;time_appconnect&#125;&quot; -so /dev/null 网址测试。 OpenSSL 概念OpenSSL 是一个 SSL 的密码库，是对 SSL 协议的实现，包含了主要的密码算法，常用的密钥和证书封装管理功能。 OpenSSL 提供八种对称加密算法（DES、AES、Blowfish、CAST、IDEA、RC2、RC5），支持四种非对称加密算法（DH、RSA、DSA、椭圆曲线 EC），实现五种信息摘要算法（MD2、MD5、MDC2、SHA（SHA+SHA1）、RIPEMD） Heartblood 漏洞简介 心脏出血漏洞，于 2014 年被公开。受害者的内存内容就会以每次 64KB 的速度进行泄露，通过读取网络服务器内存，攻击者可以访问敏感数据，从而危及服务器及用户的安全。 SSH 协议Secure Shell 安全壳协议，是建立在 TCP 上的安全协议，端口号 22。可以防止中间人攻击、DNS 和 IP 欺骗，并可加快数据的传输速度，且通过 ssh 传输的数据都是经过压缩的。 目前 SSH 有两个版本 SSH1 和 SSH2，这两个版本互不兼容。SSH 有以下特点： 支持 DES、3DES 加密 支持公钥（密钥）验证方式、密码（口令）验证方式、不验证 支持 RSA 认证 SSH 连接建立过程： 版本号协商：客户端与服务器协商出双方使用的 SSH 版本 密钥与算法协商：客户端与服务器交换算法协商报文，协商出使用的算法，并且生成会话密钥和 ID 认证：客户端向服务器发送认证请求，服务器端对客户端认证 会话请求：客户端向服务器发送会话请求，服务器等待并处理客户端请求 交互会话：数据加密传输 sshd 服务通过 openssh 软件实现 sshd 服务，sshd 正是使用 ssh 协议进行远程访问或传输文件的服务。 sshd 主要要有三个软件： openssh：包含 openssh 服务器与客户端需要的核心文件 openssh-clients：openssh 客户端软件 openssh-server：openssh 服务器软件 Openssh 的配置文件 /etc/ssh/ssh_config：客户端配置文件 /etc/ssh/sshd_config：服务器端配置文件 ssh 命令常见选项： ssh [username@]host [options] [command] -p 指定连接的远程主机端口，默认22 -v 显示详细信息，一般用于拍错 -C 压缩所有数据 可直接通过ssh在远端执行命令 -l 指定登录用户名 sshd_config 配置Port 22 #端口号#为安全起见，在实际生产环境中，最好将端口改为非22，减小ssh暴露的危险Protocol 2 #SSH版本，默认2，SSH1已淘汰AddressFamily #ListenAddress 0.0.0.0 #设置sshd服务器监听的本地IP地址。0.0.0.0表示监听本地所有IP地址（如果有多个）HostKey /etc/ssh/ssh_host_rsa_key #服务器秘钥文件的路径（还有dsa等密钥）Compression yes #是否可使用压缩指令KeyRegenerationInterval 1h #服务器重新生成密钥的周期ServerKeyBits 1024 #服务器密钥的长度LogLevel INFO #日志等级LoginGraceTime 2m #输入密码后，若2分钟内未连接成功，则断开PermitRootLogin yes #是否允许使用root登录远程主机，若为生产环境需要设为noStrictModes yes #ssh在接收登录请求之前是否检查用户根目录和rhosts文件的权限和所有权，默认开启SyslogFacility AUTHPRIV #日志类型PubkeyAuthentication yes #是否开启公钥验证，如果使用公钥验证的方式登录时，则设置为yesAuthorizedKeysFile .ssh/authorized_keys #公钥验证文件的路径PasswordAuthentication yes #是否开启密码验证PermitEmptyPasswords no #是否允许空密码登录PrintMotd yes #登录后是否打印信息（上次登录时间和地点等），信息内容可在/etc/motd中编辑PrintLastLog yes #显示上次登录的信息，默认允许UsePrivilegeSeparation sandbox #是否允许权限较低的程序一共用户操作，会让sshd在远程用户登入后产生一个属于该用户的sshd程序，使系统较安全UseDNS yes #为了判断客户端是否合法，会使用DNS反查客户端主机名。 #若是内网，则no可以让连接更快。MaxAuthTries 6 #最多密码尝试次数MaxSessions 10 #最多终端数ClientAliveInterval 0 #向客户端发送keepalive报文的间隔ClientAliveCountMax 3 #若三次收不到keepalive消息，则认为连接断开TCPKeepAlive #是否持续连接，设置yes可以防止死连接#SSH Server会传送KeepAlive的讯息给Client端，以确保两者的联机正常 最好将 ssh 的日志文件/var/log/secure的路径改掉，减小入侵后 ssh 日志文件被删除的风险。可修改/etc/rsyslog.conf的authpriv参数，包括特权信息如用户名在内的认证活动。 默认：authpriv.* /var/log/secure ，修改此项即可改变 ssh 日志路径 密钥分发命令ssh-keygen用于生成密钥对。 参考文章 百度百科-ssl 百度百科-ssh","categories":[],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"OpenSSL","slug":"OpenSSL","permalink":"https://coconutmilktaro.top/tags/OpenSSL/"},{"name":"SSL","slug":"SSL","permalink":"https://coconutmilktaro.top/tags/SSL/"},{"name":"SSH","slug":"SSH","permalink":"https://coconutmilktaro.top/tags/SSH/"}]},{"title":"无人值守学习笔记","slug":"无人值守学习笔记","date":"2018-08-01T12:59:39.000Z","updated":"2022-06-28T19:02:24.237Z","comments":true,"path":"2018/无人值守学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/%E6%97%A0%E4%BA%BA%E5%80%BC%E5%AE%88%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"本篇主要包含以下内容： PXE 概述 Kickstart Kickstart 概述 Kickstart 准备 DHCP 配置 TFTP 配置 使用 VSFTP 搭建镜像源 使用 HTTP 搭建镜像源 Syslinux 配置 网络安装试验 Kickstart 配置 手动配置 system-config-kickstart 配置 Cobbler Cobbler 安装 使用 cobbler-web 图形化配置 参考文章","text":"本篇主要包含以下内容： PXE 概述 Kickstart Kickstart 概述 Kickstart 准备 DHCP 配置 TFTP 配置 使用 VSFTP 搭建镜像源 使用 HTTP 搭建镜像源 Syslinux 配置 网络安装试验 Kickstart 配置 手动配置 system-config-kickstart 配置 Cobbler Cobbler 安装 使用 cobbler-web 图形化配置 参考文章 PXE 概述Preboot Execution Environment 远程预启动执行环境，就是使计算机通过网络启动。 要达成 PXE 必须要有两个环节： 客户端的网卡必须要支持 PXE 用户端功能，并且开机时选择从网卡启动，这样系统才会以网卡进入 PXE 客户端的程序 PXE 服务器必须要提供至少含有 DHCP 以及 TFTP 的服务 DHCP 服务必须要能够提供客户端的网络参数，还要告知客户端 TFTP 所在的位置； TFTP 则提供客户端的boot loader及kernel file下载路径。 可选的其他服务：NFS、FTP、HTTP 等 完整的 PXE 交互过程： Client 向 DHCP 发送 IP 地址请求消息，DHCP 检测 Client 是否合法（主要是检测 Client 的网卡 MAC 地址），如果合法则返回 Client 的 IP 地址，同时将启动文件 pxelinux.0 的位置信息一并传送给 Client Client 向 TFTP 发送获取 pxelinux.0 请求消息，TFTP 接收到消息之后再向 Client 发送 pxelinux.0 大小信息，试探 Client 是否满意，当 TFTP 收到 Client 发回的同意大小信息之后，正式向 Client 发送 pxelinux.0，Client 接收并执行 pxelinux.0 文件 Client 向 TFTP Server 发送获取针对本机的配置信息文件的请求（在 TFTP 服务的 pxelinux.cfg 目录下，这是系统菜单文件，格式和 isolinux.cfg 格式一样，功能也是类似），TFTP 将配置文件发回 Client，继而 Client 根据配置文件执行后续操作。 Client 向 TFTP 发送 Linux 内核请求信息，TFTP 接收到消息之后将内核文件发送给 Client Client 向 TFTP 发送根文件请求信息，TFTP 接收到消息之后返回 Linux 根文件系统 Client 启动 Linux 内核 Client 从 FTP 或 HTTP 下载安装源文件，读取自动化安装脚本 引用自Cobbler 原理解析 KickstartKickstart 概述Kickstart 是通过自动应答文件，将安装系统过程中手动设置的语言、密码、网络等参数自动设置。 Kickstart 文件有三种生成方式： 手动书写 system-config-kickstart图形化配置 红帽系系统自带的Anaconda生成 Kickstart 准备服务分工介绍： DHCP：为安装的新主机分配 IP 地址 TFTP：仅仅提供引导文件 VSFTP|HTTP|NFS：提供系统镜像中所有文件，然后会根据 Kickstart 文件自动选择要安装的软件，并配置 防止访问出现错误，先将 selinux 设为 Permissive。setenforce 0 DHCP 配置首先配置 DHCP 服务。yum install dhcp修改配置文件/etc/dhcp/dhcpd.conf # 日志级别log-facility local7;# DNS服务器域名option domain-name-servers system1.example.com;# 网关option routers 192.168.10.2;# 默认分配时间default-lease-time 600;# 最大分配时间max-lease-time 7200;subnet 192.168.10.0 netmask 255.255.255.0 &#123; # 地址分配范围 range 192.168.10.101 192.168.10.110; # TFTP服务器（重要） next-server 192.168.10.100; # TFTP服务器上的共享启动文件名（重要） filename &quot;pxelinux.0&quot;;&#125; 重新加载并设置开机自启systemctl restart dhcpdsystemctl enable dhcpd若开启了防火墙应该放行服务 firewall-cmd --permanent --add-service=dhcpfirewall-cmd --permanent --add-port=67/tcp --add-port=67/udpfirewall-cmd --reload TFTP 配置配置 TFTP 服务，首先需要安装xinetd服务，因为 TFTP 是被 Xinetd 动态管理的服务。yum install xinetd tftp-server修改配置文件/etc/xinetd.d/tftp service tftp&#123; socket_type = dgram protocol = udp wait = yes user = root server = /usr/sbin/in.tftpd # server_args指定共享目录路径 server_args = -s /tftpboot disable = no per_source = 11 cps = 100 2 flags = IPv4&#125;disable的值默认为yes，表示禁用tftp，因此要改为no，开启tftp 重启 Xinted 服务systemctl restart xinetd.service 通过查看服务是否开启# ss -aupt | grep xinetdudp UNCONN 0 0 *:tftp *:* users:((&quot;xinetd&quot;,pid=3490,fd=5)) 若开启了防火墙，需要放行服务和端口 firewall-cmd --permanent --add-service=tftpfirewall-cmd --permanent --add-port=69/udpfirewall-cmd --reload 使用 VSFTP 搭建镜像源安装 VSFTPD 服务yum install vsftpdsystemctl start vsftpdsystemctl enable vsftpd 将光盘镜像挂载在/var/ftp/pub中。mount /dev/cdrom /var/ftp/pub 若开启了防火墙，应该放行端口和服务 firewall-cmd --permanent --add-port=20/tcp --add-port=21/tcpfirewall-cmd --permanent --add-service=ftpfirewall-cmd --reload 在浏览器中输入ftp://192.168.10.100访问成功。 使用 HTTP 搭建镜像源安装 HTTPD 服务yum install httpdsystemctl start httpdsystemctl enable httpd 将光盘镜像挂载在/var/www/html/centos7上。mount /dev/cdrom /var/www/html/centos7 若开启了防火墙，应该放行端口和服务 firewall-cmd --permanent --add-port=80/tcpfirewall-cmd --permanent --add-service=httpfirewall-cmd --reload 在浏览器中输入192.168.10.100/centos7访问成功。 Syslinux 配置安装 syslinux 服务syslinux 是一个功能强大的引导加载程序，用于获取引导文件。yum install syslinux将引导文件复制到 TFTP 主目录cp /usr/share/syslinux/pxelinux.0 /tftpboot若要图形化菜单功能（仅仅是可以上下键切换，最好一起复制了），可将/usr/share/syslinux中的menu.32或vesamenu.c32复制到/tftpboot。这里就复制vesamenu.c32，比menu.32更好。并在/tftpboot中创建目录pxelinux.cfg用于存放默认开机选项，并在该目录中创建default文件 创建存放 CentOS7 内核文件的目录mkdir /tftpboot/centos7，并将挂载镜像目录/var/ftp/pub/isolinux/中vmlinuz和initrd.img两个内核文件复制到该目录中。cp /var/ftp/pub/isolinux/&#123;vmlinuz,initrd.img&#125; /tftpboot/centos7/最好将isolinux目录下的isolinux.cfg也复制过去，该文件提供了开机选项，可以以它作为修改开机选项和菜单的模板。可以直接将内容拷贝过去，cat /var/ftp/pub/isolinux/isolinux.cfg &gt; /tftpboot/pxelinux.cfg/default。 default即isolinux.cfg简单解析 default vesamenu.c32 # 必须指定，填/tftpboot中复制的图形化文件timeout 10 # 在选择界面停留的时间（若未操作）display boot.msg # 选项的说明文件菜单的一些显示设置，不用改menu clearmenu background splash.pngmenu title CentOS 7 # 引导是显示的标题menu vshift 8menu rows 18menu margin 8#menu hiddenmenu helpmsgrow 15menu tabmsgrow 13.....在引导界面上显示的选项label linux menu label ^Install CentOS 7 kernel ./centos7/vmlinuz # vmlinuz是可引导的、压缩的内核，路径要设为相对路径（相对于tftp根目录） append initrd=./centos7/initrd.img ks=ftp://192.168.10.100/ks_config/ks.cfg quiet # 设置内核文件，要设置initrd.img的相对路径 # initrd.img全称boot loader initialized RAM disk， boot loader初始化的内存盘。在linux内核启动前，boot loader会将存储介质中的initrd文件加载到内存，内核启动时会在访问真正的根文件系统前先访问该内存中的initrd文件系统 # 后面跟着ks=ks.cfg文件的路径，http或ftp都行 # 后面还可以跟上ksdevice=eth0，当客户端有多块网卡时，此项就会让系统不提示要选择哪块网卡label check menu label Test this ^media &amp; install CentOS 7 menu default # 默认光标停留在此标签（选项）上 kernel ./centos7/vmlinuz append initrd=./centos7/initrd.img inst.stage2=hd:LABEL=CentOS\\x207\\x20x86_64 rd.live.check quiet 整个/tftpboot的目录结构如下 /tftpboot/├── centos7│ ├── initrd.img│ └── vmlinuz├── pxelinux.0├── pxelinux.cfg│ └── default└── vesamenu.c32 网络安装试验做这个实验时，要先修改/tftpboot/pxelinux.cfg/default 找到以下内容label linux menu label ^Install CentOS 7 menu default kernel ./centos7/vmlinuz append initrd=./centos7/initrd.img inst.stage2=ftp://192.168.10.100/pub quietnet.ifnames=0 biosdevname=0inst.stage2设置FTP镜像源在quiet后再加上net.ifnames=0 biosdevname=0让网卡名称为ethN，而不是默认的eno16777728这样的随机名称 创建一个新的虚拟机，不指定镜像。进入虚拟机的 BIOS 设置进入 Boot 菜单，通过-或+改变启动顺序，将Network boot from Intel E1000移到最上面。保存退出，会自动启动主机，通过网络读取 FTP 镜像源。最后进入图形化安装界面 Kickstart 配置手动配置首先创建ks.cfg，存放在/var/ftp/ks_config（FTP 源）或/var/www/html/centos/ks_config（HTTP 源）。修改/tftpboot/pxelinux.cfg/default 仍然找到这段内容，修改initrid后的内容删除原来的inst.stage2，改为ks=fs.cfg路径，HTTP同理label linux menu label ^Install CentOS 7 menu default kernel ./centos7/vmlinuz append initrd=./centos7/initrd.img ks=ftp://192.168.10.100/ks_config/ks.cfg quietnet.ifnames=0 biosdevname=0 在主目录中有系统自动创建的anaconda-ks.cfg，可以此为模板。在图像化配置安装时就是向该文件中添加配置，直到点击安装时，安装程序就会根据该配置文件安装。 anaconda-ks.cfg简单解析 分为三个部分：1. 选项指令段，用于图形化安装时除包选择外的所有手动操作2. %packages段，用于选择软件包3. 脚本段，可选。分为两种： %pre：预安装脚本段，在安装系统之前就执行的脚本。很少使用 %post：后安装脚本段，在系统安装完成后执行的脚本。必选选项：# auth验证选项auth --enableshadow --passalgo=sha512 --enableshadow|--useshadow 开启shadow文件验证 --passalgo 指定密码加密算法# bootloader指定如何安装引导程序bootloader --append=&quot;crashkernel=auto&quot; --location=mbr --boot-drive=sda --append 指定内核参数 --location 指定引导程序的位置，默认为MBR --boot-drive 指定grub安装的分区# keyboard键盘类型keyboard --vckeymap=us --xlayouts=&#x27;us&#x27; --vckeymap指定键盘分布，默认为us美式# lang指定语言lang en_US.UTF-8# rootpw指定root密码rootpw --iscrypted $6$D2fmDfXJI30ZbG0x$lenXfD98spplf7jHTmfiJ0m7CgQqJM.ddQ5hu07qiU3A5fJcRhSQA5KZolrWoSfGm2oIwJUglnRwoXth9rDGc0 --iscrypted 使用加密密码可选选项：install 表示是安装系统，若为install还需指定安装方式： cdrom 表示从光盘安装 harddrive 硬盘安装，硬盘必须是VFAT或EXT文件系统 --dir 指定从包含安装树（install-tree）的目录安装 --partition 指定从哪个分区安装 nfs --server 指定NFS服务器主机名或IP地址 --dir 指定安装树目录 --opts 指定NFS的挂载选项 url --url 后面跟上地址update 表示是升级系统graphical 表示图形模式下执行kickstart安装，默认text 文本模式下根据kickstart执行安装（手动创建一定是text）firstboot 安装后第一次启动默认会有需要手动配置的界面，应该禁用 --enable|--disable 启用|禁用ignoredisk 指定忽略的磁盘 --only-use=sdanetwork 配置网络 --bootproto 地址协议，dhcp或static。若为static需要配置IP、掩码、网关、DNS --device=ens33 设置网卡名 --onboot=off 是否在引导系统时启用设备 --ipv6=auto 开启IPv6 --no-activate --hostname=localhost.localdomain 主机名若协议为static，则需要以下选项： --ip= --netmask= --gateway= --nameserver=repo 设置repo源 --name= --baseurl=services 设置服务是否启用 --disabled= --enabled=timezone Asia/Shanghai --isUtc --nontp 指定时区selinux 设置selinux --enforcing --permissive --disabledfirewall 是否开启防火墙 --disable|--enablexconfig --startxonbootautostep 交互式，和interactive类似interactive 使用kickstart文件指定的参数交互式安装，但仍会给出每一步的选择项，如果直接下一步就使用kickstart参数cmdline 在完全非交互的命令行模式下进行安装driverdisk 指定驱动程序所在位置 --source=autopart 自动分区 --type=lvmzerombr 清除磁盘的MBRclearpart 在安装系统前清除分区 --all 清除所有分区 --initlabel 创建标签，对于没有MBR或者GPT的新硬盘，该选项是必须的 --drives=sda 清除指定的分区 --Linux 清除Linux分区 --none 不清除分区 常用cleanpart --all --initlabelpart [分区] 创建分区 --fstype 文件系统类型 --asprimary 强制为主分区 --size 设置大小（单位Mb） --grow 使用所有可用空间，即为其分配所有剩余空间。 对于根分区至少需要3G空间（即使是--grow，也还是需要指定--size）user 在系统中生成一个新用户 --name 指定用户名 --groups 指定辅助组，非默认组 --homedir 用户家目录，如果不指定则默认为/home/&lt;username&gt; --password 该用户的密码，如果不指定或省略则创建后该用户处于锁定状态 --shell 用户的shell，不指定则默认 --uid 用户UID，不指定则自动分配一个非系统用户的UIDloggin 指定安装过程中的错误日志位置 --host 指定日志将发送到那台主机上 --port 如果远程主机的rsyslog使用非默认端口，则应该指定该端口选项 --level 指定日志级别halt|reboot 安装完成后操作，halt为关机，reboot为重启，默认是halt# 软件包或软件包组# @表示包组，@base和@core默认包含%packages@^graphical-server-environment@base@core@desktop-debugging@dial-up@fonts@gnome-desktop@guest-agents@guest-desktop-agents@hardware-monitoring@input-methods@internet-browser@multimedia@print-client@x11kexec-tools%end%addon com_redhat_kdump --enable --reserve-mb=&#x27;auto&#x27;%end%anacondapwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notemptypwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyokpwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty%end 注：%addon、%anaconda、%packages、%onerror、%pre、%post必须以%end结尾 官方并不建议手工创建 kickstart 文件，因为太过复杂，且容易出错。因此，可通过system-config-kickstart图形化工具快速生成 kickstart 文件。 system-config-kickstart 配置需要安装该工具yum install system-config-kickstart打开工具后，按照以下界面配置即可。 若需要修改，则直接打开修改即可。若图形化无法添加安装软件包，就在生成的ks.cfg中添加。最终修改后的ks.cfg文件如下 installkeyboard &#x27;us&#x27;rootpw --iscrypted $1$8.DdzSgf$UIjrFmFh/4Mavb/4q7z8U.url --url=&quot;ftp://192.168.10.100/pub&quot;lang en_USfirewall --disabledauth --useshadow --passalgo=sha512graphicalselinux --disabledskipxnetwork --bootproto=dhcp --device=eth0network --hostname=system10.example.comreboottimezone Asia/Shanghaibootloader --location=mbrzerombrclearpart --all --initlabelpart /boot --fstype=&quot;xfs&quot; --size=200part / --fstype=&quot;xfs&quot; --size=5part /var --fstype=&quot;xfs&quot; --size=10services --enabled=httpd%packages@base@coretreenmapwgethttpd%end 再次进行安装，进入下面画面时，发现配置已根据 kickstart 文件填写完成。 CobblerCobbler 与 Kickstart 类似，是一个 Linux 服务器快速网络安装的服务，可以通过 PXE 快速安装、重装物理服务器和虚拟机。基于 Python 开发，支持命令行管理、web 界面管理、提供 API 接口。可以管理 DHCP，DNS，TFTP、RSYNC 以及 yum 仓库、构造系统 ISO 镜像。 Cobbler 会在请求内核文件后，再请求 Kickstart 文件（即 ks.cfg）和 OS 镜像。然后 Cobbler 加载 Kickstart 文件并接收安装 OS 镜像。 Cobbler 常见术语： distro：发行版，相当于一个操作系统镜像，包含内核和 initrd 信息以及软件包等 repository：保存一个 yum 或 rsync 存储库的镜像信息 profile：配置文件，包含 distro、kickstart 文件和 repository 等信息，作用为了修改/tftpboot/pxelinux.cfg/default文件，每生成或修改一次 profile，都会在 default 文件中修改或追加对应的 label system：目标系统，即要安装的主机，包含配置文件或镜像，IP 地址等信息 image：系统镜像 system、image、repository 用的很少，主要用 distro 和 profile。 Cobbler 安装仍然使用之前 Kickstart 的环境。必须关闭 selinux。首先安装epel-release，因为 Cobbler 位于 epel 源中。然后安装 Cobbler 及其他工具程序yum install cobbler cobbler-web pykickstart其中 cobbler-web 是 cobbler 的网页端配置工具，可不用安装。pykickstart 是用于检查 kickstart 文件语法的工具cobbler 的运行依赖于 dhcp、tftp、rsync 及 dns 服务，因此在现有环境下还要安装 rsync。yum install rsyncsystemctl enable rsyncdsystemctl start rsyncdsystemctl enable cobblerd.servicesystemctl start cobblerd.service 使用命令cobbler check进行检查，对查出的错误一一解决。 1 : The &#x27;server&#x27; field in /etc/cobbler/settings must be set to something other than localhost,or kickstarting features will not work.This should be a resolvable hostnameor IP for the boot server as reachable by all machines that will use it.2 : For PXE to be functional, the &#x27;next_server&#x27; field in /etc/cobbler/settingsmust be set to something other than 127.0.0.1,and should match the IP of the boot server on the PXE network. 这两个问题需要设置/etc/cobbler/settings，修改以下内容： # 将127.0.0.1修改为本机的IP地址next_server: 192.168.10.100server: 192.168.10.100 3 : Some network boot-loaders are missing from /var/lib/cobbler/loaders, you may run &#x27;cobbler get-loaders&#x27; to download them, or, if you only want to handle x86/x86_64 netbooting, you may ensure that you have installed a *recent* version of the syslinux package installed and can ignore this message entirely. Files in this directory, should you want to support all architectures, should include pxelinux.0, menu.c32, elilo.efi, and yaboot. The &#x27;cobbler get-loaders&#x27; command is the easiest way to resolve these requirements. 问题是需要获取 bootloaders 文件，执行cobbler get-loaders自动下载，但要求联网。也可复制，但需要的文件很多，有的不好找，最好直接执行命令。 4 : debmirror package is not installed, it will be required to manage debian deployments and repositories 安装debmirror软件包并将/etc/debmirror.conf中的dists和arches注释。 #@dists=&quot;sid&quot;;#@arches=&quot;i386&quot;; 5 : The default password used by the sample templates for newly installed machines (default_password_crypted in /etc/cobbler/settings) is still set to &#x27;cobbler&#x27; and should be changed, try: &quot;openssl passwd -1 -salt &#x27;random-phrase-here&#x27; &#x27;your-password-here&#x27;&quot; to generate new one 需要使用 openssl 生成加密密码来取代默认的密码。 openssl passwd -1 -salt &#x27;cobbler&#x27; &#x27;123456&#x27; passwd 表示生成密码 -1 表示使用MD5加密 -salt 表示使用后面提供的参数生成，后面跟上用户名和密码会生成一个加密密码，将这串字符替换掉原来的默认密码default_password_crypted: &quot;$1$cobbler$52QDrGSqGlT9d5qbjg7QY/&quot; 6 : fencing tools were not found, and are required to use the (optional) power management features. install cman or fence-agents to use them 安装cman和fence-agents，cman可能会找不到这个包，但只安装fence-agents就够了。 最后使用命令cobbler sync应用调整的参数或重启cobblerd服务，再执行一次cobbler check检查，若还有错就继续排错，若没有错误就会显示No configuration problems found. All systems go. Cobbler 默认管理 tftp 服务，默认不管理 dhcp，因此 tftp 的根目录变为/var/lib/tftpboot。如果让 Cobbler 管理 DHCP，则 Cobbler 管理 DHCP 的模板文件/etc/cobbler/dhcp.template会覆盖/etc/dhcp/dhcpd.conf。 将光盘挂载到本地，mount /dev/cdrom /mnt/mirror，然后执行cobbler import --name=CentOS7 --path=/mnt/mirror生成 distro，从本地导入的过程实际上是将系统镜像中的文件复制到/var/www/cobbler/ks_mirror/CentOS7中。在/var/www/cobbler/images中也会生成一个CentOS7-x86_64的目录，其中存放了initrd.img和vmlinuz文件。 然后，需要提供 kickstart 文件，这里继续使用 Kickstart 实验用的ks.cfg文件，将文件移动到/var/lib/cobbler/kickstarts中，并改名为CentOS7.ks，需要修改以下内容。 #如果存在ignoredisk设置，一定要注释掉，cobbler编译时不支持此语法修改镜像安装源url --url=&quot;http://http://192.168.10.100/cobbler/ks_mirror/CentOS7/&quot; 在导入镜像生成 distro 的过程中，会自动生成一个 profile。使用cobbler profile list查看。使用cobbler profile report --name=CentOS7-x86_64查看 profile 信息。 # cobbler profile report --name=CentOS7-x86_64其中profile默认使用的kickstart文件有误Kickstart : /var/lib/cobbler/kickstarts/sample_end.ks 需要通过cobbler profile edit --name=CentOS7-x86_64 --kickstart=/var/lib/cobbler/kickstarts/CentOS7.ks修改。 最好再修改内核启动参数net.ifnames和biosdevname使网卡名为ethN系列而不是用enoXXXXXX随机名。cobbler profile edit --name=CentOS7-x86_64 --kopts=&quot;net.ifnames=0 biosdevname=0&quot; 若要手动添加一个 profile，可使用cobbler profile add --name=XXX --distro=distro名 --kickstart=ks文件路径。每添加一个 profile，就是在/var/lib/tftpboot/pxelinux.cfg/default中添加一个 label，一个 label 就是开机启动时的引导选项。 LABEL CentOS7-x86_64 kernel /images/CentOS7-x86_64/vmlinuz MENU LABEL CentOS7-x86_64 append initrd=/images/CentOS7-x86_64/initrd.img ksdevice=bootif lang= text net.ifnames=0 biosdevname=0 kssendmac ks=http://192.168.10.100/cblr/svc/op/ks/profile/CentOS7-x86_64 ipappend 2 在配置完成后，执行cobbler sync同步设置。 通过浏览器访问default文件中ks参数指定的 ks 文件路径，看是否能访问，若能显示文件内容，则配置没有问题。 重启xinetd、cobblerd、dhcpd服务，以防配置未刷新。 仍然使用一台裸机进行安装，会自动进入安装界面。 使用 cobbler-web 图形化配置如果开启了防火墙，需要放行 443 端口和 https 服务，因为 Cobbler 在 CentOS7 只支持 https。 在浏览器访问https://IP地址/cobbler_web即可，输入账号密码，均为cobbler。 首先进行镜像的导入，左侧菜单的Import DVD选项配置。 菜单的Events查看事件日志。 进入distros配置，添加内核选项。也可以通过profiles配置。 设置网卡名为 ethN 系列 修改或编写 ks 文件 也可进入菜单system进行 system 配置。 参考文章 骏马金龙–无人值守 CentOS7 &gt; kickstart 文件详解 &gt; KICKSTART 无人值守安装 &gt; Cobbler-自动化部署神器 &gt; cobbler 无人值守批量安装 Linux 系统 &gt; Cobbler 原理解析Linux 就该这么学Linux 运维之道（第二版）","categories":[],"tags":[{"name":"无人值守","slug":"无人值守","permalink":"https://coconutmilktaro.top/tags/%E6%97%A0%E4%BA%BA%E5%80%BC%E5%AE%88/"},{"name":"PXE","slug":"PXE","permalink":"https://coconutmilktaro.top/tags/PXE/"},{"name":"Kickstart","slug":"Kickstart","permalink":"https://coconutmilktaro.top/tags/Kickstart/"},{"name":"Cobbler","slug":"Cobbler","permalink":"https://coconutmilktaro.top/tags/Cobbler/"}]},{"title":"Rsync文件同步服务器学习笔记","slug":"Rsync文件同步服务器学习笔记","date":"2018-08-01T12:58:22.000Z","updated":"2022-05-30T02:51:53.868Z","comments":true,"path":"2018/Rsync文件同步服务器学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Rsync%E6%96%87%E4%BB%B6%E5%90%8C%E6%AD%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"Rsync 介绍与搭建 quick check 算法介绍 rsync 的工作方式 rsync 命令使用 规则解析 Rsync 服务器搭建 Rsync 部分报错解决 Rsync+Inotify 文件自动同步 Inotify 介绍 参考文章","text":"Rsync 介绍与搭建 quick check 算法介绍 rsync 的工作方式 rsync 命令使用 规则解析 Rsync 服务器搭建 Rsync 部分报错解决 Rsync+Inotify 文件自动同步 Inotify 介绍 参考文章 Rsync 介绍与搭建Rsync（Remote Synchronize）是一个远程数据同步工具，可使本地主机不同分区或目录之间及本地和远程两台主机之间的数据快速同步镜像，远程备份等功能。 Rsync 特点： 使用 TCP 873 端口 可根据数据变化进行差异备份或增量备份，减少数据流量 传输可以通过 ssh 协议加密数据 支持拷贝特殊文件如链接，设备等 可以镜像保存整个目录树和文件系统 可以保持原来文件或目录的所有属性均不改变 可使用 rsh、ssh，或直接通过 socket 连接 支持匿名的或认证的进程模式传输 使用 Rsync 自己的 quick check 算法 rsync 传输大文件时速度大概是 scp 的 20 倍以上 rsync 同步过程由两部分模式组成：决定哪些文件需要同步的检查模式，文件同步时的同步模式。 检查模式是指按照指定规则来检查哪些文件需要被同步。 默认情况下，rsync 使用quick check算法。可以通过rsync命令选项设置指定的检查模式。 同步模式是指在文件确定要被同步后，在同步过程发生之前要做哪些额外工作。 quick check 算法介绍比较源文件和目标文件的文件大小和修改时间 mtime（修改时间），只要存在不同，就发送端会传输该文件，如果目标路径下没有文件，则 rsync 会直接传输文件。若存在差异，并不会传送整个文件，而是只传源文件和目标文件所不同的部分，实现真正的增量同步。 rsync 的增量传输体现在两个方面：文件级别的增量传输和数据块级别的增量传输。 文件级别的增量传输是指源主机上有，但目标主机上没有将直接传输该文件 数据块级别的增量传输是指只传输两文件所不同的那一部分数据。 两台计算机Host-A与Host-B，其中Host-A是源主机，即数据的发送端Sender，Host-B是目标主机，即数据的接收端Receiver。Host-A上存在文件file-A，Host-B上存在file-B。注：**file-A与file-B是同名文件。** rsync 的实现有以下过程： Host-A告诉Host-B有文件file-A要传输。 Host-B收到信息后，将文件file-B划分为一系列大小固定的数据块(大小在 500-1000 字节之间)，并以 chunk 号码对数据块进行编号，同时还会记录数据块的起始偏移地址以及数据块长度。 Host-B对每一个分割好的数据块执行两种校验：一种是 32 位的滚动弱校验（rolling checksum），另一种是 128 位的 MD5 强校验。将file-B计算出的所有滚动弱校验和强校验码跟随在对应数据块chunk[N]后形成校验码集合，然后发送给主机Host-A 不同数据块的滚动弱校验值有可能相同，但几率非常小。 Host-A通过搜索file-A 的所有该固定大小的数据块（偏移量可以任选），并计算它的校验码和校验码集合中的校验码进行匹配。 如果能匹配上校验码集合中的某个数据块条目，则表示该数据块和file-B中数据块相同，不需要传输。 如果不能匹配校验码集合中的数据块条目，则表示该数据块是非匹配数据块，它需要传输给Host-B，于是Host-A将跳转到下一个字节，从此字节处继续取数据块进行匹配。 数据匹配过程有三个层次：首先比较 hash 值，然后比较弱滚动校验，最后比较强校验。 当Host-A发现是匹配数据块时，将只发送这个匹配块的附加信息给Host-B。同时，如果两个匹配数据块之间有非匹配数据，则还会发送这些非匹配数据。 当Host-B陆陆续续收到这些数据后，会创建一个临时文件，并通过这些数据重组这个临时文件，使其内容和file-A 相同。临时文件重组完成后，修改该临时文件的属性信息(如权限、所有者、mtime 等)，然后重命名该临时文件替换掉file-B。 rsync 的工作方式 本地传输方式：首先 rsync 命令执行时，会有一个 rsync 进程，然后根据此进程 fork 另一个 rsync 进程作为连接的对端，连接建立之后，后续所有的通信将采用管道的方式。 远程 shell 连接方式：本地敲下 rsync 命令后，将请求和远程主机建立远程 shell 连接（如 ssh 连接），连接建立成功后，在远程主机上将 fork 远程 shell 进程调用远程 rsync 程序，并将 rsync 所需的选项通过远程 shell 命令（如 ssh）传递给远程 rsync。这样两端就都启动了 rsync，之后它们将通过管道的方式进行通信。 网络套接字连接远程主机上的 rsync daemon：当通过网络套接字和远程已运行好的 rsync 建立连接时，rsync daemon 进程会创建一个子进程来响应该连接并负责后续该连接的所有通信。这样两端也都启动了连接所需的 rsync，此后通信方式是通过网络套接字来完成的。 远程 shell 临时启动一个 rsync daemon：不要求远程主机上事先启动 rsync 服务，而是临时派生出 rsync daemon，它是单用途的一次性 daemon，仅用于临时读取 daemon 的配置文件，当此次 rsync 同步完成，远程 shell 启动的 rsync daemon 进程也会自动停止。 发起连接的一端称为 Client 端，就是执行 rsync 命令的一端，连接的另一端称为 Server 端。 注：当 Client 端和 Server 端都启动好 rsync 进程并建立好了 rsync 连接(管道、网络套接字)后，将使用 Sender 端和 Receiver 端来代替 Client 端和 Server 端的概念。 当两端的 rsync 连接建立后，Sender 端的 rsync 进程称为Sender进程，该进程负责 Sender 端所有的工作。Receiver 端的 rsync 进程称为Receiver进程，负责接收 sender 端发送的数据，以及完成文件重组的工作。Receiver 端还有一个核心进程 Generator 进程，该进程负责在 Receiver 端执行--delete动作、比较文件大小和 mtime 以决定文件是否跳过、对每个文件划分数据块、计算校验码以及生成校验码集合，然后将校验码集合发送给 Sender 端。 三个进程的作业流程：Generator进程的输出结果作为Sender端的输入，Sender端的输出结果作为Recevier端的输入。并且，这三个进程是完全独立、并行工作的。 数据同步方式 推 push：一台主机负责把数据传送给其他主机，服务器开销很大，比较适合后端服务器少的情况 拉 pull：所有主机定时去找一主机拉数据，可能就会导致数据缓慢 rsync 命令使用本地传输方式: rsync [OPTION...] SRC... [DEST]远程shell连接方式: Pull: rsync [OPTION...] [USER@]HOST:SRC... [DEST] Push: rsync [OPTION...] SRC... [USER@]HOST:DEST连接远程主机上的rsync daemon: Pull: rsync [OPTION...] [USER@]HOST::SRC... [DEST] rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST] Push: rsync [OPTION...] SRC... [USER@]HOST::DEST rsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST 若主机与路径用:分隔，则为 shell 连接，若为::，则为 daemon 连接。 注：源路径如果是一个目录的话，带上尾随斜线和不带尾随斜线是不一样的，不带尾随斜线表示的是整个目录包括目录本身，带上尾随斜线表示的是目录中的文件，不包括目录本身。 若只有源路径或只有目的路径，则相当于ls -l查看指定文件或目录属性。 常用选项： -a 归档模式，表示递归传输并保持文件属性。等同于&quot;-rtopgDl&quot;。-P 显示文件传输的进度信息-v 显示rsync过程中详细信息(最多支持-vvvv)-r 递归-t 保持mtime属性(最好加上，否则时间会设置为当前系统时间，在增量备份时会因为时间出错)-o 保持owner属性-g 保持group属性(属组)。-p 保持perms属性(权限，不包括特殊权限)。-D 拷贝设备文件和特殊文件。-l 如果文件是软链接文件，则拷贝软链接本身而非软链接所指向的对象。-z 传输时进行压缩提高效率。-R 使用相对路径。意味着将命令行中指定的全路径而非路径最尾部的文件名发送给服务端，包括它们的属性-u 仅在源mtime比目标已存在文件的mtime新时才拷贝。接收端判断，不会影响删除行为。--size-only 只检查文件大小（默认算法是检查文件大小和mtime不同的文件）--max-size 限制rsync传输的最大文件大小。可以使用单位后缀，还可以是一个小数值(例如：&quot;--max-size=1.5m&quot;)--min-size 限制rsync传输的最小文件大小。这可以用于禁止传输小文件或那些垃圾文件。--exclude 指定排除规则来排除不需要传输的文件。一个exclude只能指定一条规则，若有多条规则，就要写多个exclude--exclude-from 若规则有多条，可以写在文件中，并使用此选项加载规则文件--include 指定传输规则，只传输符合该规则的文件，与exclude相反--delete 接收端的rsync会先删除目标目录下已经存在，但发送端目录不存在的文件，以SRC为主，对DEST进行同步。多则删之，少则补之。注意&quot;--delete&quot;是在接收端执行的，所以它是在exclude/include规则生效之后才执行的。-b --backup 对目标上已存在的文件做一个备份，备份的文件名后默认使用&quot;~&quot;做后缀。--backup-dir 指定备份文件的保存路径，若指定，保存路径必须存在。 不指定时默认和待备份文件保存在同一目录下。 默认没有后缀，可通过&quot;--suffix=&quot;指定-e 指定所要使用的远程shell程序，默认为ssh。--port 连接daemon时使用的端口号，默认为873端口。--password-file daemon模式时的密码文件，是rsync模块认证的密码。-W --whole-file rsync将不再使用增量传输，而是全量传输。在网络带宽高于磁盘带宽时，该选项比增量传输更高效。--existing 要求只更新目标端已存在的文件，目标端还不存在的文件不传输。注意，使用相对路径时如果上层目录不存在也不会传输。--ignore-existing 要求只更新目标端不存在的文件。和&quot;--existing&quot;结合使用有特殊功能。--remove-source-files 要求删除源端已经成功传输的文件。 最常用组合-avz。 命令示例： rsync -r -R /etc/dir-1 /tmp 将/etc/dir-1目录复制到/tmp目录下，tmp目录下将会有etc/dir-1子目录rsync -r -R /etc/./dir-1 /tmp 将/etc/dir-1目录复制到/tmp目录下，但在/etc/和dir-1目录间加上了“.” 因此仅仅将dir-1目录复制过去作为子目录，tmp目录下将有dir-1子目录rsync -r -b --backup-dir --suffix=&#x27;.bak&#x27; /etc/dir-1 /tmp 备份文件，若不存在就直接复制，若已存在就添加后缀以区分rsync -r --exclude=&quot;*.txt&quot; /etc/dir-1 /tmp 将/etc/dir-1目录下的以&quot;.txt&quot;结尾的文件排除rsync -r -v --delete --exclude=&quot;*.txt&quot; /etc/dir-1 /tmp 将/tmp/dir-1中存在而/etc/dir-1不存在的文件删除，并且不删除&quot;.txt&quot;结尾的文件（即使源中不存在）sending incremental file listdeleting dir-1/a3.logdeleting dir-1/a2.logdeleting dir-1/a1.log 规则解析规则作用时间：当发送端敲出 rsync 命令后，rsync 将立即扫描命令行中给定的文件和目录(扫描过程中还会按照目录进行排序，将同一个目录的文件放在相邻的位置)，这称为拷贝树(copy tree)，扫描完成后将待传输的文件或目录记录到文件列表中，然后将文件列表传输给接收端。筛选规则的作用时刻是在扫描拷贝树时，所以会根据规则来匹配并决定文件是否记录到文件列表中(严格地说是会记录到文件列表中的，只不过排除的文件会被标记为 hide 隐藏起来)，只有记录到了文件列表中的文件或目录才是真正需要传输的内容。**筛选规则的生效时间在 rsync 整个同步过程中是非常靠前的，它会影响很多选项的操作对象，最典型的如--delete**，--delete是在 generator 进程处理每个文件列表时、生成校验码之前进行的，这样就无需为多余的文件生成校验码。 rsync 规则：通过选项--filter指定规则 exclude规则：即排除规则，只作用于发送端，被排除的文件不会进入文件列表(实际上是加上隐藏规则进行隐藏)。 include规则：即包含规则，也称为传输规则，只作用于发送端，被包含的文件将明确记录到文件列表中。 hide规则：即隐藏规则，只作用于发送端，隐藏后的文件对于接收端来说是看不见的，也就是说接收端会认为它不存在于源端。 show规则：即显示规则，只作用于发送端，是隐藏规则的反向规则。 protect规则：即保护规则，该规则只作用于接收端，被保护的文件不会被删除掉。 risk规则：即取消保护规则。是protect的反向规则。 clear规则：删除include/exclude规则列表。 上述内容都引用自骏马金龙－rsync 介绍与用法 Rsync 服务器搭建实验环境： Rsync 服务器：192.168.205.135 Rsync 客户端：192.168.205.134 首先，两台主机都需要安装 rsync，yum/dnf install rsync，fedora 已默认安装。 由于 rsync 的配置文件默认不存在，所以需要手动创建。Rsync 有三个配置文件： rsyncd.conf：主配置文件 rsyncd.secrets：密码文件 rsyncd.motd：服务器信息文件 创建/etc/rsyncd.conf文件，具体参数可通过man rsyncd.conf查看 配置文件分为两部分：全局参数，模块参数全局参数：对 rsync 服务器生效，如果模块参数和全局参数冲突，冲突的地方模块参数生效模块参数：定义需要通过 rsync 输出的目录定义的参数 # 常见全局参数：port = 873 监听端口address = 192.168.205.135 监听服务器地址uid = nobody 数据传输时使用的用户ID，默认nobodygid = nobody 数据传输时使用的组ID，默认nobodyread only = yes 是否只读max connections = 10 并发连接数，0表示无限制。超出并发连接数时若再访问，会收到稍后重试的消息use chroot = no 是否开启chroot，若设为yes，rsync会首先进行chroot设置，将根映射到模块中path指定目录 需要root权限，在同步符号连接资料时仅同步文件名，不同步文件内容transfer logging = yes 开启Rsync数据传输日志功能#log format = 设置日志格式，默认log格式为：&quot;%o %h [%a] %m (%u) %f %l&quot;# 默认log格式表达的是：&quot;操作类型 远程主机名[远程IP地址] 模块名 (认证的用户名) 文件名 文件长度字符数&quot;#syslog facility = 指定rsync发送日志消息给syslog时的消息级别，默认值是daemonlock file = /var/run/rsync.lock 设置锁文件log file = /var/log/rsyncd.log 设置日志文件pid file = /var/run/rsyncd.pid 设置进程号文件motd file = /etc/rsyncd.motd 设置服务器信息提示文件hosts allow = 192.168.205.134 设置允许访问服务器的主机# 模块配置[rsync_test]comment = rsync test 添加注释path = /root/rsync_test 同步文件或目录路径ignore errors 忽略一些IO错误# exclude = 可以指定不同步的目录或文件，使用相对路径auth users = tom,jack 允许连接的用户，可以是系统中不存在的secrets file = /etc/rsyncd.secrets 设置密码验证文件，该文件的权限要求为只读（600），该文件仅在设置了auth users才有效list = false 是否允许查看模块信息timeout = 600 可以覆盖客户指定的 IP 超时时间。确保rsync服务器不会永远等待一个崩溃的客户端。 超时单位为秒钟，0表示没有超时定义，这也是默认值。 对于匿名rsync服务器来说，一个理想的数字是600 相关系统操作： useradd tom &amp;&amp; echo &quot;redhat&quot; | passwd tom --stdinecho &quot;tom:redhat&quot; &gt;&gt; /etc/rsyncd.secretsecho &quot;jack:redhat&quot; &gt;&gt; /etc/rsyncd.secrets 添加用户到密码文件chmod 600 /etc/rsyncd.secrets 修改密码文件权限echo &quot;Welcome to Rsync&quot; &gt;&gt; /etc/rsyncd.motd# firewall-cmd --permanent --add-port=873/tcp 若开启了防火墙，就放行端口rsync --daemon --config=/etc/rsyncd.conf 启动rsyncd服务 客户端同步文件：rsync -r tom@192.168.205.135::rsync_test /root/rsync_test 自动备份脚本，可配合 crond 使用 #!/bin/bash# 本脚本是将服务器端的文件定期同步到本端# 设置服务器端模块和本端目录SRC=rsync_testDEST=/root/rsync_testServer=192.168.205.135# 设置验证用户名和密码User=tomPassword=redhat# 若本地同步目录不存在，就创建目录。同步时会删除目录中本端存在，但服务器端不存在的文件[ ! -d $DEST ] &amp;&amp; mkdir $DESTrsync -az --delete $&#123;User&#125;@$&#123;Server&#125;::$SRC $DEST/$(date +%Y%m%d) 若使用 Xinetd 服务管理 Rsync，需要先安装 xinetd。创建文件/etc/xinetd.d/rsync，添加以下内容。 service rsync &#123; disable = no socket_type = stream wait = no user = root server = /usr/bin/rsync server_args = --daemon --config=/etc/rsyncd.conf log_on_failure += USERID&#125; 然后重启 xinetd 服务即可。 Rsync 部分报错解决rsync: failed to connect to 192.168.205.135 (192.168.205.135): No route to host (113)rsync error: error in socket IO (code 10) at clientserver.c(125) [Receiver=3.1.2] 解决：防火墙问题，放行端口或直接关闭 @ERROR: auth failed on module rsync_testrsync error: error starting client-server protocol (code 5) at main.c(1648) [Receiver=3.1.2] 解决：用户名与密码问题或模块问题，检查用户名与密码是否匹配，服务器端模块是否存在 rsync: read error: Connection reset by peer (104)rsync error: error in rsync protocol data stream (code 12) at io.c(759) [Receiver=3.1.2] 解决：服务器端配置文件/etc/rsyncd.conf 问题，检查配置文件参数是否出错 Rsync+Inotify 文件自动同步Inotify 介绍Inotify是一个 Linux 特性，是inode notify的简写，能监控文件系统操作，反应灵敏，异步传输信息，比 cron 高效，通过 Inotify 能试试了解文件系统发生的所有变化。 注：Linux 内核需要大于 2.6 才集成了 Inotify，先要通过 uname -r 查看内核版本 Inotify 一些特点： 允许程序员使用标准 select 或者 poll 函数来监视事件 使用一个独立的文件描述符，可以通过系统调用获得 使用 rsync 工具与 inotify 机制相结合，可以实现触发式备份（实时同步）。 在系统/proc/sys/fs/inotify/目录中有三个文件： max_queued_events：inotify 实例事件队列可容纳的事件个数，默认 16384 max_user_instances：每个用户可以运行的 inotifywait 或 inotifywatch 命令的进程数，默认 128 max_user_watches：每个进程最多监控文件数，默认 8192 若要监控的文件量较大时，需要适当增大这三个值 可以在/etc/sysctl.conf中添加以下内容修改这三个值，或通过sysctl -w直接设置。 fs.inotify.max_queued_events =fs.inotify.max_user_instances =fs.inotify.max_user_watches = Inotify 常用的文件系统事件： 事件名 描述 IN_ACCESS 文件访问事件 IN_MODIFY 文件修改事件 IN_ATTRIB 文件属性修改事件 IN_OPEN 文件打开事件 IN_CLOSE_WRITE 可写文件被关闭事件 IN_CLOSE_NOWRITE 不可写文件被关闭事件 IN_MOVED_FROM IN_MOVED_TO 文件移动或重命名事件 IN_DELETE 文件或目录删除事件 IN_CREATE 文件或目录创建事件 IN_DELETE_SELF 自删除事件 若系统为 fedora，可直接安装dnf install inotify-tools，若系统为 CentOS，则需要先安装epel-release再安装yum install inotify-tools inotify-tools 提供两个应用程序：inotifywait和inotifywatch inotifywait命令用法： inotifywait [options] file... @&lt;file&gt; 排除监控指定文件 --exclude &lt;pattern&gt; 使用正则表达式匹配例外文件，区分大小写 --excludei &lt;pattern&gt; 使用正则表达式匹配例外文件，不区分大小写 -m|--monitor 一直监听，不退出。若不加此项，监听到一个事件后就退出 -d|--daemon 相当于-m，但是是在后台运行，需要-o或--outfile指定输出文件，或者通过再指定--syslog将错误信息输出至syslog系统日志 -r|--recursive 递归监控目录 --fromfile &lt;file&gt; 从文件中读取需要监控与例外的文件名，每行一个文件，若文件名以@开头，表示例外文件 -o|--outfile &lt;file&gt; 将事件信息输出到文件，默认输出到标准输出 -s|--syslog 将错误事件日志发送到syslog，而不是stderr -q|--quiet 静默模式，只输出事件 -qq 静默模式，什么都不输出（包括事件） --format &lt;fmt&gt; 指定输出信息格式 --timefmt &lt;fmt&gt; 设置时间格式 -c|--csv 使用CSV格式输出 -t|--timeout &lt;seconds&gt; 在指定时间内若监听到事件，就退出 -e|--event &lt;event1&gt; [ -e|--event &lt;event2&gt; ... ] 只监控指定事件 实例：inotifywait -m -d -r -o /var/log/html_monitor /var/www/html 然后进入/var/www/html进行以下操作 touch a.htmlvim a.htmlcp a.html b.htmlrm a.htmlchmod -r b.htmlmv b.html a.html 在/var/log/html_monitor中就有以下信息 /var/www/html/ CREATE a.html/var/www/html/ OPEN a.html/var/www/html/ ATTRIB a.html/var/www/html/ CLOSE_WRITE,CLOSE a.html/var/www/html/ OPEN,ISDIR/var/www/html/ ACCESS,ISDIR...../var/www/html/ MOVED_FROM b.html/var/www/html/ MOVED_TO a.html 仅仅几步操作，就生成了 170 多行事件信息。 Inotify 与 Rsync 实时同步数据要求分析：数据发布服务器既是 Rsync 服务器同时也是 Inotify 监控服务器，该服务器是用于发布数据的，将数据同步到 Web 服务器，实现 Web 服务器与此数据发布服务器的同步。 实验环境： 数据发布服务器：192.168.205.135 Web 服务器：192.168.205.134 首先需要在 Web 服务器上编写rsyncd.conf，添加模块 [www]comment = web file dictionarypath = /var/www/htmlauth users = wwwsecrets file = /etc/rsyncd.secretshosts allow = 192.168.205.135 进行系统准备： # 在Web服务器上创建用户wwwuseradd www &amp;&amp; echo &quot;redhat&quot; | passwd www --stdinecho &quot;www:redhat&quot; &gt;&gt; /etc/rsyncd.secretschmod 600 /etc/rsyncd.secrets查看/var/www是否所属于www，若不是就chown -R www:www /var/www# 在数据发布服务器上ssh-keygenssh-copy-id -i ~/.ssh/id_rsa.pub www@192.168.205.134# 在Web服务器上，同理，双向交换ssh公钥ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.205.135 数据发布服务器上触发同步脚本示例： #!/bin/bashSRC=/var/www/web_html/DST=www@192.168.205.134::www/usr/bin/inotifywait -d -r -o var/log/inotify_web -e modify,delete,create,attrib $&#123;SRC&#125; | while read linedo /usr/bin/rsync -az --delete $&#123;SRC&#125; $&#123;DST&#125; 2&gt;&amp;1done &amp; 加上执行权限chmod a+x并写入/etc/rc.local，echo &quot;脚本名&quot; &gt;&gt; /etc/rc.local 参考文章 rsync Rsync 原理详解及部署 Rsync 完全手册 inotify+rsync+mutt+msmtp 实现 linux 文件或者目录自动更新并且实现发邮件给管理员 Linux 运维之道（第二版） 真正的 inotify+rsync 实时同步 彻底告别同步慢","categories":[],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"Rsync","slug":"Rsync","permalink":"https://coconutmilktaro.top/tags/Rsync/"},{"name":"同步","slug":"同步","permalink":"https://coconutmilktaro.top/tags/%E5%90%8C%E6%AD%A5/"}]},{"title":"Linux性能监控与优化","slug":"Linux性能监控常用命令","date":"2018-08-01T12:57:03.000Z","updated":"2022-06-21T15:16:21.210Z","comments":true,"path":"2018/Linux性能监控常用命令/","link":"","permalink":"https://coconutmilktaro.top/2018/Linux%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"","text":"所有工具进行系统性能分析的思路都是相同的。 top 与 htoptop查看动态进程状态，默认每 5 秒刷新一次。 top -d：指定top刷新的时间间隔，默认是5 秒 -b：批处理模式，每次刷新分批显示 -n：指定top刷新几次就退出，可以配合-b使用 -p：指定监控的pid，指定方式为-pN1 -pN2 ...或-pN1, N2 [,...] -u：指定要监控的用户的进程，可指定UID或用户名 top - 02:20:36 up 6:48, 1 user, load average: 0.00, 0.00, 0.00Tasks: 145 total, 2 running, 143 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 2029396 total, 1600964 free, 161512 used, 266920 buff/cacheKiB Swap: 2097148 total, 2097148 free, 0 used. 1670652 avail Mem Tasks 为进程数 us：用户进程（非优雅进程，即未修改优先级）占用 cpu 百分比 sy：系统进程（内核进程）占用 cpu 百分比 ni：进程空间内改变过优先级的进程占 cpu 百分比 id：空闲 cpu 百分比。如果系统慢但这个值很高，则可以推断不是 CPU 负载的问题 wa：I/O 等待时间比率。若系统慢但这个值低，则也可排除磁盘或网络 I/O 负载问题 hi：不可中断睡眠时间比率 si：可中断睡眠时间比率 st：被偷走时间比率，一般为虚拟机占用 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND PR：优先级 NI：nice 值，负数为高优先级，正数为低优先级 VIRT：虚拟内存总量 RES：进程实际使用内存 SHR：共享内存 S：进程状态。有五种状态：D-不可中断的睡眠态 R-运行态 S-睡眠态 T-停止 Z-僵尸态 TIME+：进程使用 CPU 的时间总计（单位 1/100 秒） 进入 top 视图后的操作 1：查看每个逻辑 CPU 的状况 P：按 cpu 使用率排序（默认） M：按内存使用率排序 N：按 PID 排序 T：根据 TIME+进行排序 H：切换为线程 l：切换负载信息 m：切换显示内存信息（只是将数字改为类似进度条） t：切换显示进程和 CPU 状态信息（只是将数字改为类似进度条） k：指定要杀死的进程号 A：切换显示模式。有四个窗口，在左上角显示，按 a（后一个）或 w（前一个）进行窗口切换Def：默认字段组 Job：任务字段组 Mem：内存字段组 Usr：用户字段组 d 或 s：修改刷新间隔 f：选择要添加显示的字段，标上*为已选的 R：反向排序 c：显示进程的完整命令路径名 V：树视图，命令会按进程的父子关系显示 u：显示指定用户的进程 n 或#：设置最多显示的进程量 r：设置指定进程的 nice 优先级 F: 选择要排序的字段 htop默认没有安装，需要dnf install htop 可通过鼠标点击操作，h查看帮助 uptime,free 和 vmstatuptime获取主机运行时间，查询系统负载 09:05:53 up 12 days,13:33,2 users,load average: 0.01,0.02,0.05 09:05:53–当前时间 up 12 days, 13:33–系统启动时长 2 users–当前登录系统的用户数 load average: 0.01,0.02,0.05–系统在最近的 1,5,15 分钟的平均负载 负载率(load)，即特定时间长度内，cpu 运行队列中的平均进程数(包括线程)，一般平均每分钟每核的进程数小于 3 都认为正常，大于 5 时负载已经非常高。Linux 运行队列包括正在运行的、在等待的、处于可中断睡眠态（IO 等待）的进程。若为多核 CPU，还需要除以核数。 平均负载最佳值为 1，意味着每个进程都能立刻访问 CPU，并且没有丢失 CPU 周期 平均负载为 1 表示，单 CPU 系统处于恒定负载，若为 4 则表示系统处于它可承受负载能力的 4 倍。明确负载是：CPU 密集型（等待 CPU 资源的进程）、RAM 密集型（频繁使用的 RAM 被移入交换区）和 I/O 密集型（争夺磁盘或网络 I/O 的进程）非常重要。 CPU 密集型系统比 I/O 密集型和 RAM 密集型相应度更高，因为磁盘 I/O 或 RAM 资源耗尽时，进程会逐渐变慢直至停止，而 CPU 密集型仍能快速执行命令。 free查看内存与 swap 分区使用状况，实际是从/proc/meminfo中读取数据的 free -b 单位b -k 单位kb（默认） -m 单位mb -g 单位gb -h 自动选择单位 -l 显示高内存、低内存详细统计数据 -t 显示内存与swap的总和 -s N 设置刷新周期（单位秒），ctrl+C退出 -c N 设置刷新次数 -w 分开显示buffers和cachetotal used free shared buff/cache available# cache为缓存：把读取的数据放在内存中，再次读的话就直接从内存中读了，加快读取# buffer为缓冲：写入数据时，把分散的写入操作保存到内存中，然后集中写入硬盘，减少磁盘碎片与硬盘反复寻道，加快写入 vmstat报告进程、内存、分页、块 IO、中断、CPU 活动信息，能够显示平均数据和实时样本。 vmstat [options] [delay [count]] -a 显示内存信息 -f 显示自从系统启动以来产生的子进程数量 -m 显示slab信息 -n 与-a类似 -s 事件计数器和内存状态 -d 磁盘状态 -D 磁盘状态概述 -p 分区 磁盘分区统计信息 -S 单位 输出与-n一致，但可指定单位 -t 显示时间戳，输出比-n增加了一项CST时间戳 刷新间隔 [次数] 设置持续刷新间隔及刷新次数 虚拟内存模式 vmstatprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----r b swpd free buff cache si so bi bo in cs us sy id wa stProc： r：可运行进程的数量（正在运行+等待运行） b：不可中断睡眠进程的数量memory： swpd：虚拟内存使用量 free：空闲内存 buff：缓冲区内存 cache：用作缓存的内存swap： si：swap in,从磁盘换入的内存数量/s so：swap out,交换到磁盘的内存数量/sio： bi：block in,从块设备收到的块数/s bo：block out,发送到块设备的块数/ssystem： in：interrupt,每秒中断数量 cs：context switch,每秒上下文切换数量cpu： us：非内核代码运行时间 sy：内核代码运行时间 id：idle,空闲花费时间，包含IOwait时间 wa：wait,IO等待花费时间 st：steal,虚拟软件花费时间 磁盘模式 vmstat -ddisk- ------------reads------------ ------------writes----------- -----IO------ total merged sectors ms total merged sectors ms cur secdisk：磁盘名reads： total：成功读取的总量 merged：合并后分组的读 sectors：成功读取的扇区 ms：读取花费时间（单位毫秒）writes： total：成功写入的总量 merged：合并后分组的写 sectors：成功写入的扇区 ms：写入花费时间（单位毫秒）IO： cur：正在进行的IO sec：IO花费时间（单位秒） 一旦 Linux 将一个文件载入到 RAM 中，当程序使用完这个文件，并不会将它从 RAM 中删除，而是缓存在 RAM 中，如果还有程序要访问这个文件，会大大提高读取速度。若系统需要为活动进程提供 RAM，则经过一段时间，系统的可用 RAM 就会越来越少。 想要知道进程确切使用了多少内存，需要将总消耗内存减去缓存文件大小。若一个进程转为空闲状态，则 Linux 会将占用的内存释放。 mpstat、 iostat 与 sarmpstat与iostat都在sysstat包中，若没有这两个命令，则需要安装sysstatsysstat 包包含以下命令： iostat mpstat pidstat sar tapestat 若要进行长时间的负载记录和统计，需要启动 sysstat 服务。systemctl start sysstat.service，启用 sysstat 后，会每 10 分钟收集一次系统状态，并存储到/var/log/sa或/var/log/sysstat中若是 debian 系的系统，还需要修改配置/etc/sysconfig/sysstat，将ENABLED修改为 true，开启收集系统动态数据。 mpstat用于报告在多处理器服务器上每个可用 CPU 的统计数据。 mpstat [ 选项 ] [ &lt;时间间隔&gt; [ &lt;次数&gt; ] ] -A 相当于-u -I ALL -P ALL -I &#123;SUM|CPU|SCPU|ALL&#125; 报告中断统计数据 SUM 报告每个处理器中断的总数量，显示CPU编号和intr/s一个或多个CPU每秒接收每个独特中断的个数 CPU 显示intr/s，但排列难以阅读 SCPU 显示intr/s，排版容易阅读 ALL 显示所有中断统计信息 -P &#123;cpu编号|ON|ALL&#125; cpu 指明统计的cpu编号（0开始） ON 每个在线CPU的统计数据 ALL 所有CPU的统计数据 -u 统计CPU使用率，输出与-P一致 时间间隔interval [次数times] 指定报告时间间隔及次数，最后会生成平均值 mpstat -uCPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle CPU：CPU 编号 %usr：用户级别（应用）执行时 CPU 使用率 %nice：用户级别使用 nice 优先级执行时 CPU 使用率 %sys：系统级别（内核）执行时 CPU 使用率（不包括硬件软件中断服务的时间） %iowait：系统未完成磁盘 I/O 请求期间，CPU 空闲时间百分比 %irq：CPU 硬件中断时间百分比 %soft：CPU 软件中断时间百分比 %steal：虚拟化软件为其他虚拟 CPU 服务时，虚拟 CPU 非主动等待时间百分比 %guest：CPU 运行虚拟处理器花费时间百分比 %idle：CPU 空闲时间百分比 iostat当 I/O 等待时间占 CPU 时间比例很高时，首先要检查系统是否正在大量使用交换空间，若还有大量可用 RAM，则查看哪个进程占用了大量 I/O。 avg-cpu: %user %nice %system %iowait %steal %idle 9.71 0.01 4.89 0.09 0.00 85.30Device tps kB_read/s kB_wrtn/s kB_read kB_wrtnloop0 0.00 0.00 0.00 147 0......sda 7.57 96.06 114.12 39466421 46888861......sdb 0.00 0.01 0.00 3562 144loop13 0.00 0.00 0.00 1419 0sdc 0.09 9.42 0.01 3869460 3816 tps：设备每秒传输量（I/O 请求） kB_read/s：每秒从设备读取的数据量 kB_wrtn/s：每秒向设备写入的数据量 kB_read：从设备读取的总数据量 kB_wrtn：向设备写入的总数据量 当系统处于 I/O 高负载时，首先观察每个分区的负载，并确定分区中存放的是什么服务的数据，然后确定高负载 I/O 操作是写入还是读取，从而分析是什么服务占用了大量 I/O。 sarsar 用于收集系统的各种负载信息。数据存在/var/log/sa/下的saX文件中，X 为当天是本月的几号，如当月 19 号，则会生成日志/var/log/sa/sa19。 -B 分页状况-b I/O 和传输速率信息状况-d 块设备状况-F [ MOUNT ] 文件系统统计信息-H 交换空间利用率-I &#123; &lt;int_list&gt; | SUM | ALL &#125; 中断统计-n &#123; &lt;keyword&gt; [,...] | ALL &#125; 网络统计 DEV 网络接口 NFS NFS客户端 NFSD NFS服务器端 SOCK Sockets (v4) IP IP负载(v4) TCP TCP负载(v4)-q 队列长度和平均负载-r [ ALL ] 内存利用率信息-S 交换空间利用率信息-u [ ALL ] CPU 利用率信息-v 内核表统计信息-W 交换信息-w 任务创建与系统转换信息-y TTY 设备信息 sar直接输出当天的 CPU 统计信息。默认 10 分钟收集一次。 12:25:14 LINUX RESTART (4 CPU)12时35分01秒 CPU %user %nice %system %iowait %steal %idle12时45分01秒 all 5.67 0.00 4.74 0.11 0.00 89.4712时55分01秒 all 5.67 0.00 4.60 0.06 0.00 89.67......14时25分01秒 all 5.58 0.00 4.74 0.03 0.00 89.6514时35分01秒 all 5.61 0.00 4.72 0.03 0.00 89.63平均时间: all 5.65 0.00 4.71 0.05 0.00 89.60 sar -r输出 RAM 统计信息 20:20:03 kbmemfree kbavail kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty12:30:02 268208 451380 549032 67.18 3268 275912 312828 10.73 211764 134792 012:40:02 268152 451324 549088 67.19 3268 275912 312828 10.73 211764 134792 0......14:30:02 265232 448896 552008 67.55 3268 276348 312824 10.73 214348 132792 014:40:02 265244 448908 551996 67.54 3268 276348 312824 10.73 214356 132792 0Average: 265875 449326 551365 67.47 3268 276153 312999 10.74 213254 133725 0 sar -b输出磁盘统计信息 20:20:03 tps rtps wtps bread/s bwrtn/s12:30:02 0.38 0.04 0.34 3.04 9.7712:40:02 0.02 0.00 0.02 0.00 0.2812:50:02 0.05 0.00 0.05 0.00 0.78......14:40:02 0.03 0.00 0.03 0.00 0.47Average: 0.10 0.01 0.09 0.31 1.69 tps 为每秒总传输的数据量，是 rtps 和 wtps 的总和。bread/s 为平均每秒读取的数据量。 sar -s和-e通常组合使用，指定要查看的项在某段时间内的统计 # sar -s 12:33:00 -e 13:21:0012:40:02 CPU %user %nice %system %iowait %steal %idle12:50:02 all 0.16 0.00 0.47 0.01 0.00 99.3613:00:02 all 0.12 0.00 0.48 0.01 0.00 99.4013:10:02 all 0.11 0.00 0.46 0.01 0.00 99.4213:20:02 all 0.12 0.00 0.45 0.01 0.00 99.42Average: all 0.13 0.00 0.47 0.01 0.00 99.40 若要查看本月指定几号的记录，则用-f指定统计日志文件 # sar -r -f /var/log/sa/sa19 -s 13:00:00 -e 13:30:0013:00:02 kbmemfree kbavail kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty13:10:02 265584 448928 551656 67.50 3268 276048 313296 10.75 212760 134228 013:20:02 265520 448888 551720 67.51 3268 276052 313296 10.75 212776 134232 0Average: 265552 448908 551688 67.51 3268 276050 313296 10.75 212768 134230 0 ps 和 pstreepsps 命令有两种风格：BSD 和 Unix。BSD 格式的参数前不加-，Unix 格式会在参数前加- 查看所有进程 ps ax # a表示此tty下的所有程序（不区分用户），x表示所有程序（不区分tty终端机），若增加u参数，可以用户为主的格式来显示程序状况ps -ef # -e显示所有程序，只显示PID、TTY、TIME、CMD，-f增加显示UID、PPID、C、STIMEps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDUSER：进程发起用户PID：进程号%CPU，%MEM：CPU，内存占用率VSZ：虚拟内存（单位kb）RSS：常驻内存（实际物理内存）（单位kb）TTY：该进程在哪个终端运行STAT：进程状态 S：可中断睡眠 &lt;：高优先级 s：子进程 +：位于后台 R：运行（Running） T：停止状态（Terminate） Z：僵尸进程（Zombie）START————进程开启时间TIME：进程已启动时间COMMAND：产生进程的命令名ps -ef中不同的几个PPID：父进程IDC：CPU占用率STIME：进程启动时间 显示用户进程 ps -f -u [用户名1,用户名2...] #-u指定用户，可指定多个，不能加-e，不然等于没指定例：ps -f -u apache 显示指定进程 ps -f -C [进程] # -C指定进程名，进程名必须是精确的，不能用通配符。同样不能指定-e例：ps -f -C httpdps -f -p [进程号] # -p指定进程号 通过 cpu 或内存占用对进程排序 ps -ef --sort=[+|-]pcpu,[+|-]pmem--sort用于指定多个字段，pcpu为按CPU排序，pmem为按内存排序，+为升序，-为降序例：ps -ef --sort=-pcpu | head -6 显示CPU占用排名前五的进程 以树显示进程层级关系 ps -f --forest例：ps -f --forest -C httpd 查看指定父进程下的所有子进程 ps --ppid [PPID] 显示进程的线程 ps -f -L -C [进程]或-p [进程号] #显示指定进程的线程例：ps -f -L -C httpd 指定要显示的列 ps -o pid,uname,pcpu,pmem,comm,etime其中：uname为用户名，etime为进程已运行时间 通过watch命令将 ps 变为实时查看器 watch -n 指定指令执行间隔 -d 高亮显示指令输出信息不同之处例：watch -n 1 &#x27;ps -e -o pid,uname,cmd,pmem,pcpu --sort=-pcpu,-pmem | head -11&#x27; pstree查看进程树，常用选项 -p 显示进程PID-g 显示进程组ID，即PGID-u 显示进程所属用户-a 显示进程的命令及参数-H PID 高亮显示指定进程及它的所有父进程-T 只显示进程，不显示线程-t 显示完整的线程名-s 显示进程的父进程-n 按PID排序输出-Z 显示selinux上下文（需要开启selinux） 参考文章 10 basic examples of Linux ps command ps 命令的 10 个例子 Linux 性能优化大师 DevOps 故障排除 Linux 服务器运维最佳实践","categories":[{"name":"系统运维","slug":"系统运维","permalink":"https://coconutmilktaro.top/categories/%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"运维","slug":"运维","permalink":"https://coconutmilktaro.top/tags/%E8%BF%90%E7%BB%B4/"},{"name":"Linux","slug":"Linux","permalink":"https://coconutmilktaro.top/tags/Linux/"},{"name":"性能","slug":"性能","permalink":"https://coconutmilktaro.top/tags/%E6%80%A7%E8%83%BD/"}]},{"title":"LVM逻辑卷与RAID磁盘阵列学习笔记","slug":"LVM逻辑卷与RAID磁盘阵列学习笔记","date":"2018-08-01T12:55:08.000Z","updated":"2022-06-21T15:16:21.208Z","comments":true,"path":"2018/LVM逻辑卷与RAID磁盘阵列学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/LVM%E9%80%BB%E8%BE%91%E5%8D%B7%E4%B8%8ERAID%E7%A3%81%E7%9B%98%E9%98%B5%E5%88%97%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"LVM 逻辑卷LVM 概述LVM(Logical Volume Manager)逻辑卷管理器，可以灵活调整存储空间大小，并不会对现有数据造成任何损坏。红帽系的系统全部默认开启了 LVM。 几个关键术语： PV(Physical Volume，物理卷)：最低层的存储设备（硬盘，分区） VG(Volume Group，卷组)：单个或多个 PV 构成的存储资源池，不能直接存放数据 LV(Logical Volume，逻辑卷)：从 VG 中划分出的存储空间，可直接存放数据 PE(Physical Extend，物理扩展单元)：逻辑层面上最小的存储单元（类似 block），一个 PE 大小 4MB，是告诉 PV，表明存储单元位于 PV 的哪个位置。PE 大小必须满足 2 的指数幂 LE(Logical Extend，逻辑扩展单元)：与 PE 类似，默认情况下，PE 与 LE 一样大。LE 告诉 LV，表明存储单元处于 VG 的哪个位置 LVM 创建流程： 使用几块硬盘或分区（未创建文件系统）创建物理卷，并将分区的识别号设置为 LVM，即8e 将多个物理卷组合成一个卷组，卷组会根据指定的 PE 大小将空间划分为多个 PE，在 LVM 中存储以 PE 为单元 在卷组中分出逻辑卷，这些逻辑卷在外界看来就是多个独立的硬盘分区 对逻辑卷制作文件系统，并挂载 LVM 写入机制：LV 是从 VG 中划分出来的，LV 中的 PE 很可能来自于多个 PV。在向 LV 存储数据时，有多种存储机制，其中两种是： 线性模式(linear)：先写完来自于同一个 PV 的 PE，再写来自于下一个 PV 的 PE。 条带模式(striped)：一份数据拆分成多份，分别写入该 LV 对应的每个 PV 中，所以读写性能较好，类似于 RAID 0。 尽管striped读写性能较好也不建议使用该模式，因为 lvm 的着重点在于弹性容量扩展而非性能，要实现性能应该使用 RAID 来实现，而且使用striped模式时要进行容量的扩展和收缩将比较麻烦。默认的是使用线性模式。 引用自骏马金龙的 LVM 详解 LVM 创建与调整实验环境： CentOS7 /dev/sdb 大小 20G 将/dev/sdb分为 4 个分区，每个分区 5G，并将磁盘标识号设为8e，表示 LVM # fdisk -l /dev/sdbDisk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0xde948938 Device Boot Start End Blocks Id System/dev/sdb1 2048 10487807 5242880 8e Linux LVM/dev/sdb2 10487808 20973567 5242880 8e Linux LVM/dev/sdb3 20973568 31459327 5242880 8e Linux LVM/dev/sdb4 31459328 41943039 5241856 8e Linux LVM 准备裸磁盘与裸分区，制作 PV使用pvcreate [裸磁盘/裸分区]创建 PV # pvcreate /dev/sdb1 /dev/sdb2 /dev/sdb3 /dev/sdb4 Physical volume &quot;/dev/sdb1&quot; successfully created. Physical volume &quot;/dev/sdb2&quot; successfully created. Physical volume &quot;/dev/sdb3&quot; successfully created. Physical volume &quot;/dev/sdb4&quot; successfully created. 可使用pvdisplay [磁盘/分区]查看 pv 信息 pvdisplay选项列表 若不指定磁盘或分区，就是查看所有PV -m # 查看该设备中PE的使用分布图 -s # 查看pv简短信息 pvs或pvscan也可查看所有 PV 信息 # pvscan PV /dev/sda2 VG centos lvm2 [&lt;19.00 GiB / 0 free] PV /dev/sdb4 lvm2 [&lt;5.00 GiB] PV /dev/sdb2 lvm2 [5.00 GiB] PV /dev/sdb1 lvm2 [5.00 GiB] PV /dev/sdb3 lvm2 [5.00 GiB] Total: 5 [&lt;39.00 GiB] / in use: 1 [&lt;19.00 GiB] / in no VG: 4 [&lt;20.00 GiB] 最后一行显示的是&quot;pv的总容量/已使用的pv容量/空闲的pv容量&quot; pvremove [分区]删除 PVpvmove [分区]删除 PV 中的所有数据 制作 VGvgcreate [VG名][多个裸磁盘]制作卷组 vgcreate选项列表 -s # 指定PE大小。不指定默认大小4M -l # 卷组上允许创建的最大逻辑卷数 -p # 卷组中允许添加的最大物理卷数 在创建VG后，只有VG没有数据时才能修改属性，如PE大小 # vgcreate -s 8M my_vg1 /dev/sdb1 /dev/sdb2 Volume group &quot;my_vg1&quot; successfully created# vgcreate -s 8M my_vg2 /dev/sdb3 /dev/sdb4 Volume group &quot;my_vg2&quot; successfully created vgdisplay [VG名]查看 VG 信息vgs和vgscan也可显示所有 VG 信息 vgreduce [VG名] [PV名]从 VG 中删除指定 PVvgextend [VG名] [PV名]添加 PV 到 VG 中 # vgreduce my_vg1 /dev/sdb2 Removed &quot;/dev/sdb2&quot; from volume group &quot;my_vg1&quot;# vgextend my_vg1 /dev/sdb2 Volume group &quot;my_vg1&quot; successfully extended vgchange [选项] [VG名] 修改卷组的属性经常被用来设置卷组是处于活动状态或非活动状态。处于活动状态的卷组无法被删除，必须使用vgchange命令将卷组设置为非活动状态后才能删除。 -a y|n 设置卷组的活跃|非活跃状态 vgremove [VG名]删除 VG 创建逻辑卷 LVlvcreate -L N -n [LV名][VG名]创建逻辑卷 lvcreate选项列表 -L N # 指定逻辑卷大小N，若N不是PE的整数倍，系统会自动将LV大小变大为PE整数倍 -n # 指定LV名 -l n # 指定PE个数，即通过PE个数指定逻辑卷大小 # lvcreate -L 6G -n my_lv1 my_vg1 Logical volume &quot;my_lv1&quot; created.# lvcreate -l 1000 -n my_lv2 my_vg2 Logical volume &quot;my_lv2&quot; created.# lvscan ACTIVE &#x27;/dev/centos/swap&#x27; [2.00 GiB] inherit ACTIVE &#x27;/dev/centos/root&#x27; [&lt;17.00 GiB] inherit ACTIVE &#x27;/dev/my_vg2/my_lv2&#x27; [7.81 GiB] inherit ACTIVE &#x27;/dev/my_vg1/my_lv1&#x27; [6.00 GiB] inherit lvdisplay [LV名]查看指定或所有 LV 详细信息lvs和lvscan可查看全部 LV 信息 lvextend [选项] +[扩容大小] [LV]用于扩展逻辑卷的空间大小，而不中断应用程序对逻辑卷的访问 lvextend选项 -L 指定逻辑卷的大小 -l 指定逻辑卷的大小（LE数） 若不添加加号，则要指定扩容后的大小（一定要比增容前大） 若添加加号，则要指定要扩容的大小 # lvextend -L +100M /dev/my_vg1/my_lv1 Rounding size to boundary between physical extents: 104.00 MiB. Size of logical volume my_vg1/my_lv1 changed from 6.00 GiB (768 extents) to 6.10 GiB (781 extents). Logical volume my_vg1/my_lv1 successfully resized.# lvextend -l +10 /dev/my_vg2/my_lv2 Size of logical volume my_vg2/my_lv2 changed from 7.81 GiB (1000 extents) to 7.89 GiB (1010 extents). Logical volume my_vg2/my_lv2 successfully resized. lvreduce [选项] -[缩小大小] [LV]减少 LVM 逻辑卷占用的空间大小。有可能会删除逻辑卷上已有的数据参数与用法与lvextend一致lvresize [选项] [+|-][扩大或缩小大小] [LV]调整 LVM 逻辑卷的空间大小，可以增大空间和缩小空间，可能会使逻辑卷上已有的数据丢失参数与用法与lvextend与lvreduce一致。 若该 LV 已制作完文件系统并挂载完成，而要对该 LV 进行容量的改变操作，首先需要umount将挂载取消，然后使用上述命令进行容量改变操作，然后再制作文件系统并挂载。然而，通过任何查看命令都会发现 LV 大小并没有改变。因此，在改变大小后，还要继续以下操作。 若文件系统为ext4，则需要执行resize2fs [LV路径] 若文件系统为xfs，则需要执行xfs_growfs [LV路径]xfs 文件系统只支持增大分区空间的情况，不支持减小的情况。硬要减小的话，只能在减小后将逻辑分区重新通过 mkfs.xfs 命令重新格式化才能挂载上，这样的话这个逻辑分区上原来的数据就丢失了。xfs是不支持裁剪的，ext是支持裁剪的，所以xfs尽量不要缩小 LV 建议在修改容量后执行e2fsck -f [LV路径]检查是否修改后的大小会影响数据。 # e2fsck -f /dev/my_vg1/my_lv1e2fsck 1.42.9 (28-Dec-2013)Pass 1: Checking inodes, blocks, and sizesPass 2: Checking directory structurePass 3: Checking directory connectivityPass 4: Checking reference countsPass 5: Checking group summary information/dev/my_vg1/my_lv1: 11/400624 files (0.0% non-contiguous), 64167/1599488 blocks lvremove [LV名]删除指定 LV LVM 快照LVM 提供快照功能，可将逻辑卷的数据进行备份，并可快速恢复 实验选用/dev/my_vg1/my_lv1，并将该 LV 挂载在/my_lv1 # lvdisplay /dev/my_vg1/my_lv1 --- Logical volume --- LV Path /dev/my_vg1/my_lv1 LV Name my_lv1 VG Name my_vg1...... LV Size 6.10 GiB...... lvcreate -s -n [快照名] -L [快照大小] [LV路径]创建快照 -s 创建快照-n 指定快照名-L 快照大小 # lvcreate -s -L 1G -n my_snapshot /dev/my_vg1/my_lv1 Using default stripesize 64.00 KiB. Logical volume &quot;my_snapshot&quot; created.可通过lvscan查看，最后多了一条快照信息# lvscan ACTIVE &#x27;/dev/centos/swap&#x27; [2.00 GiB] inherit ACTIVE &#x27;/dev/centos/root&#x27; [&lt;17.00 GiB] inherit ACTIVE &#x27;/dev/my_vg2/my_lv2&#x27; [7.89 GiB] inherit ACTIVE Original &#x27;/dev/my_vg1/my_lv1&#x27; [6.10 GiB] inherit ACTIVE Snapshot &#x27;/dev/my_vg1/my_snapshot&#x27; [1.00 GiB] inherit 在/my_lv1中创建文件dd if=/dev/zero of=/my_lv1/files count=1 bs=100M然后将该 LV 卸载umount /my_lv1最后将快照恢复lvconvert --merge /dev/my_vg1/my_snapshot # lvconvert --merge /dev/my_vg1/my_snapshot Merging of volume my_vg1/my_snapshot started. my_lv1: Merged: 69.90% my_lv1: Merged: 100.00%快照恢复过后会自动删除 将 LV 重新挂载到/my_lv1，发现文件夹中已经没有任何文件了，说明快照恢复成功。 RAID 磁盘阵列RAID 概述Redundant Array of Independent Disks 独立硬盘组，作用是防止硬盘物理损坏以及增加存储设备的吞吐量。常见的 RAID 形式有：RAID 0、RAID 1、RAID 3、RAID 5、RAID 6、RAID 10、RAID 01、RAID 50。 RAID 0：将多个磁盘合并为 1 个大磁盘，读写速度极大提高，但不具冗余，因为通过硬件或软件串联，数据是依次被写入各个硬盘，所以任何一块损坏都会导致数据丢失。 RAID 1：两组以上的 N 个硬盘相互做镜像，让数据被多块硬盘同时写入。一块损坏可立刻通过热交换恢复数据。由于做备份，硬盘空间只有 50%。 RAID 3：将数据条块化分布于不同的硬盘上，使用简单的奇偶校验，并用单块磁盘存放奇偶校验信息。如果一块磁盘失效，奇偶盘及其他数据盘可以重新产生数据;如果奇偶盘失效则不影响数据使用。RAID 3 对于大量的连续数据可提供很好的传输率，但对于随机数据来说，奇偶盘会成为写操作的瓶颈。 RAID 5：使用硬盘分割技术，至少需 3 块硬盘。既提高传输速度，也可实现镜像备份。但采用奇偶校验信息，写速度相当慢，读速度与 RAID0 相近，且镜像保障程度也不及 RAID1。空间利用率高。是在所有磁盘上交叉存储数据与奇偶校验信息，并不是单独保存在某块内存中，而是分别互相保存在每一块硬盘，raid5 并不是备份实际硬盘数据，而是在硬盘出现故障后通过奇偶校验信息尝试恢复数据。RAID5的利用率为总可用盘容量的(n-1)/n RAID 6：相较 RAID5 增加第二个独立的奇偶校验信息块，数据可靠性非常高，需要四个以上硬盘 RAID 7：全称是最优化的异步高 I/O 速率和高数据传输率，不仅仅是一种技术，它还是一个独立存储计算机，自身带的操作系统和管理工具，完全可以独立运行。采用了非同步访问，极大地减轻了数据写瓶颈，提高了 I/O 速度，存储计算机操作系统可使主机 I/O 传递性能达到最佳，能够自动执行恢复操作，并可管理备份磁盘的重建过程。该技术已被 raid7 公司垄断。 RAID 10（企业主要用）/01： 10 为 1+0 先镜射再分割数据，两个两个组成 raid1，再两组两组组成 raid0，拥有较高速度与较高数据保护性，但需要 4 块以上且数量为偶数的硬盘数，因为使用 raid1，所以利用率也是 50%。安全性比 01 强，速度也比 01 强，恢复速度快于 5 01 为 0+1 先分割再镜射，两个两个组 raid0，再两组两组组 raid1 RAID 50：至少需要 6 块硬盘，将数据分为条带同时存入多个磁盘，以数据校验位保证数据安全，目的在于提高 RAID5 的读写性能 RAID 基础搭建RAID 的创建管理由mdadm命令实现 mdadm [模式] [RAID设备名] [选项] [成员设备] -a # 检测设备名 -As # 激活raid（停止的时候） -n # 指定设备数量 -c # 指定数据块大小 -l # 指定raid级别 -C # 创建 -v # 显示过程 -G # 修改raid -f # 模拟设备损坏 -r # 移除设备 [raid] -a [磁盘] 将磁盘添加到raid阵列 [raid] -r [磁盘] 将磁盘移出raid阵列 -Q # 查看摘要信息 -D # 查看详细信息（cat /proc/mdstat查看阵列状态） -S # 停止阵列 -x # 备份盘个数 --detail # 查看RAID阵列 实验目的：搭建RAID 10实验环境 CentOS7 /dev/sdb分出 4 分区，每个 5G 大小 mdadm -Cv /dev/md0 -a yes -n 4 -l 10 /dev/sdb1 /dev/sdb2 /dev/sdb3 /dev/sdb4创建 RAID 可通过cat /proc/mstat查看简要 RAID 信息 # cat /proc/mdstatPersonalities : [raid10]md0 : active raid10 sdb4[3] sdb3[2] sdb2[1] sdb1[0] 10475520 blocks super 1.2 512K chunks 2 near-copies [4/4] [UUUU] 也可通过mdadm -D /dev/md0查看指定 RAID 信息 # mdadm -D /dev/md0/dev/md0: Version : 1.2 Creation Time : Thu Aug 2 09:56:25 2018 Raid Level : raid10 Array Size : 10475520 (9.99 GiB 10.73 GB) # Array Size总共能使用的空间，因为是raid10，所以总可用空间为400M左右，除去元数据，大于370M左右 Used Dev Size : 5237760 (5.00 GiB 5.36 GB) # Used Dev Size每颗raid组或设备上的可用空间，也即每个RAID1组可用大小为190M左右 Raid Devices : 4 # raid中设备的个数 Total Devices : 4 # 总设备个数，包括raid中设备个数，备用设备个数等 Persistence : Superblock is persistent Update Time : Thu Aug 2 09:57:17 2018 State : clean # 当前raid状态，有clean/degraded(降级)/recovering/resyncing Active Devices : 4 Working Devices : 4 Failed Devices : 0 Spare Devices : 0 Layout : near=2 # RAID10数据分布方式，有near/far/of set，默认为near，即数据的副本存储在相邻设备的相同偏移上。 # near=2表示要备份2份数据 Chunk Size : 512KConsistency Policy : resync Name : bogon:0 (local to host bogon) UUID : 1aa0ce46:4762919a:71542e42:47b8bc7b Events : 17 Number Major Minor RaidDevice State 0 8 17 0 active sync set-A /dev/sdb1 1 8 18 1 active sync set-B /dev/sdb2 2 8 19 2 active sync set-A /dev/sdb3 3 8 20 3 active sync set-B /dev/sdb4 制作文件系统并挂载该 RAIDmkfs.ext4 /dev/md0 &amp;&amp; mount /dev/md0 /mnt/md0 模拟 RAID 损坏，这里模拟损坏/dev/sdb3mdadm /dev/md0 -f /dev/sdb3此时查看 RAID 信息 # mdadm -D /dev/md0...... State : clean, degraded Active Devices : 3 # 活跃的设备仅3台 Working Devices : 3 # 工作中的设备仅3台 Failed Devices : 1 # 出现故障的设备1台 Spare Devices : 0...... Number Major Minor RaidDevice State 0 8 17 0 active sync set-A /dev/sdb1 1 8 18 1 active sync set-B /dev/sdb2 - 0 0 2 removed 3 8 20 3 active sync set-B /dev/sdb4 2 8 19 - faulty /dev/sdb3 # /dev/sdb3的状态为faulty 由于 raid10 允许一组 raid1 存在故障，不会影响使用。 进行恢复，首先要卸载文件系统umount /mnt/md0然后移除/dev/sdb3，mdadm /dev/md0 -r /dev/sdb3最后再次添加进 RAID 组，mdadm /dev/md0 -a /dev/sdb3立刻查看 RAID 状态 # mdadm -D /dev/md0...... Active Devices : 3 Working Devices : 4 Failed Devices : 0 Spare Devices : 1 Layout : near=2 Chunk Size : 512KConsistency Policy : resync Rebuild Status : 38% complete Name : bogon:0 (local to host bogon) UUID : 1aa0ce46:4762919a:71542e42:47b8bc7b Events : 30 Number Major Minor RaidDevice State 0 8 17 0 active sync set-A /dev/sdb1 1 8 18 1 active sync set-B /dev/sdb2 4 8 19 2 spare rebuilding /dev/sdb3 3 8 20 3 active sync set-B /dev/sdb4可以看出此时活跃设备仍为3台，但工作设备已变为4台，空闲设备为1台Rebuild Status : 38% complete 表示正在重建该设备在/dev/sdb3的状态上也可看出正在重建rebuilding 若要彻底停用该阵列，只需要先卸载文件系统，然后执行mdadm -S /dev/md0即可。 使用RAID 5进行备份mdadm -Cv /dev/md0 -n 3 -x 1 -l 5 /dev/sdb1 /dev/sdb2 /dev/sdb3 /dev/sdb4其中-n 3表示选择 3 块磁盘做主盘，-x 1表示选 1 张做备份盘，加起来一共 4 张磁盘。此时查看 RAID 信息 # mdadm -D /dev/md0/dev/md0: Version : 1.2 Creation Time : Thu Aug 2 10:35:01 2018 Raid Level : raid5 Array Size : 10475520 (9.99 GiB 10.73 GB) Used Dev Size : 5237760 (5.00 GiB 5.36 GB) Raid Devices : 3 Total Devices : 4 Persistence : Superblock is persistent Update Time : Thu Aug 2 10:35:28 2018 State : clean Active Devices : 3 Working Devices : 4 Failed Devices : 0 Spare Devices : 1 Layout : left-symmetric Chunk Size : 512KConsistency Policy : resync Name : bogon:0 (local to host bogon) UUID : 9d0e9a0d:38372c14:2ad4666f:feb13f5c Events : 18 Number Major Minor RaidDevice State 0 8 17 0 active sync /dev/sdb1 1 8 18 1 active sync /dev/sdb2 4 8 19 2 active sync /dev/sdb3 3 8 20 - spare /dev/sdb4可看出/dev/sdb4做了备份盘，状态为空闲 参考资料 百度百科–RAID 磁盘阵列 &gt; RAID 基础，RAID10 与 RAID01 比较，RAID10 与 RAID5 比较 &gt; 骏马金龙–RAID &gt; 骏马金龙–LVM &gt; 图文并茂 RAID 技术全解Linux 就该这么学","categories":[{"name":"系统运维","slug":"系统运维","permalink":"https://coconutmilktaro.top/categories/%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://coconutmilktaro.top/tags/Linux/"},{"name":"LVM","slug":"LVM","permalink":"https://coconutmilktaro.top/tags/LVM/"},{"name":"RAID","slug":"RAID","permalink":"https://coconutmilktaro.top/tags/RAID/"}]},{"title":"STP学习笔记","slug":"STP学习笔记","date":"2018-07-31T14:19:08.000Z","updated":"2022-05-30T02:51:53.868Z","comments":true,"path":"2018/STP学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/STP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"基于华三网络 STP RSTP","text":"基于华三网络 STP RSTP STP生成树协议（Spinning Tree Protocol，IEEE 802.1D）是二层网络中用于消除环路的协议。 简单实现原理： 通过阻断冗余链路来消除桥接网络中可能存在的路径回环 当前活动路径发生故障时，激活冗余备份链路，恢复网络连通性 桥之间交换 BPDU（Bridge Protocol Data Unit，桥协议数据单元），来保证设备完成生成树的计算。 BPDU 分为两类： 配置 BPDU（Configuration BPDU）：进行生成树计算和维护生成树拓扑的报文 TCN BPDU（Topology BPDU）：当拓扑结构改变时，用来通知相关设备的报文 STP 消除环路的思想： 选择树根节点 确定最短路径 阻塞冗余链路 桥的角色： 根桥：通过选举成为生成树树根的桥 指定桥：除根桥外的桥 端口角色： 根端口 指定端口 Alternate 端口 路径开销：用于衡量桥之间路径的优劣，每条链路都有开销值，路径开销等于路径上全部链路的开销之和。 根路径开销：网桥到根桥的最短的路径开销 配置 BPDU： 网桥通过交换配置 BPDU 来获取 STP 计算所需的参数 配置 BPDU 基于二层组播方式发送，目的地址01-80-C2-00-00-00 配置 BPDU 由根桥周期（Hello Time，默认 2s）发送 配置 BPDU 老化时间 Max Age（默认 20s），当超时后，网桥又会发送以自身为根的配置 BPDU 配置 BPDU 包含以下信息： 根桥 ID（Root ID） 根路径开销（Root Path Cost） 指定桥 ID（Designated Bridge ID） 指定端口 ID（Designated Port ID） 配置 BPDU 的比较原则：小的优先 首先比较根桥 ID 再比较根路径开销 再比较指定桥 ID 再比较指定端口 ID 最后比较桥的端口 ID 根桥选举： 每台设备的各个端口在初始化时生成以自己为根桥的配置信息，向外发送自己的配置信息 各网桥将各个端口收到的配置 BPDU 和自己的配置 BPDU 做比较，得出优先级最高的配置 BPDU，从而知道哪台设备为根桥。 网桥保存该最优的配置 BPDU，并从指定端口发送该配置 BPDU，告诉其他设备根桥的信息 网络收敛后，根桥向外发送配置 BPDU，其他设备也对该配置 BPDU 进行转发 确认端口角色： 根桥上的所有端口为指定端口 非根桥上到根的路径开销最小的端口为根端口 每个物理段的根路径开销最小的桥为指定桥，指定桥上连着该物理段的端口为指定端口 既不是指定端口，又不是根端口的端口为 Alternate 端口，进行阻塞，不转发普通的以太网数据帧 收到低优先级的配置 BPDU 时的处理：通常非根桥不会主动发送配置 BPDU，只有当网桥的指定端口收到一个低优先级的配置 BPDU 时，网桥会立即回应一个配置 BPDU，确保新加入的网桥尽快确认根桥和端口角色。 端口状态： Disabled：未启用 STP 的端口，不收发 BPDU，但能接收转发数据 Blocking：Alternate 端口，接收但不发送 BPDU，不能接收转发数据 Listening：接收并发送 BPDU，不接收或转发数据，不进行地址学习 Learning：接收并发送 BPDU，不接收或转发数据，开始进行地址学习 Forwarding：接收并发送 BPDU，接收或转发数据，同时进行地址学习 端口被选为指定端口或根端口后，需要从 Blocking 经过 Listening 和 Learning 才能到达 Forwarding。期间有两次 Forward Delay，每次为 15 秒。设置 Forward Delay 的作用为：使 BPDU 消息有一个充分时间再网络中传播。 拓扑改变后处理： 如图：SWD 的 0/1 端口故障导致 hostA 中断，经过 MaxAge，SWE 的 0/1 口配置 BPDU 老化，变为 Listening，经过两个 ForwardDelay，0/1 口变为 Forwarding 状态。 但此时 HostA 仍不能连通 HostB，因为 SWA/B/C 上 Mac 表未老化，HostB 给 HostA 发的包仍会被发到 SWD，被 SWD 丢弃。需要等 Mac 地址表老化（5min），才能重新学习。 为减少拓扑收敛时间，STP 使用 TCN BPDU 将最长的收敛时间缩减到 50 秒（MaxAge+2xForwardDelay） 使用 TCN BPDU 后的处理： 网桥感知拓扑变化，产生 TCN BPDU 从根端口发给根桥。（TCN BPDU 的发送周期为 Hello Time，若上游超时无回复就会一直发） 若上游不是根桥，则上游会将要发送的配置 BPDU 的 TCA 置位，作为收到 TCN 的确认，并发给下游。然后再发 TCN BPDU 给根桥，依次如此传递直到根桥收到。 根桥收到后，将要发送的配置 BPDU 的 TCA 置位，作为收到 TCN 的确认，并将 TC 置位，用于通知全网拓扑变化。 下面的网桥收到消息后，将自身的 MAC 地址老化时间从 5min 变为 ForwardDelay。 网桥发送 TCN BPDU 的条件（满足一个）： 有端口转变为 Forwarding，且该网桥至少包含一个指定端口 有端口从 Forwarding 或 Learning 转变为 Blocking STP 的缺陷： 收敛时间过长，两倍的 Forwarding Delay 导致连通性至少要几十秒才能恢复 若网络拓扑频繁变化，网络会频繁失去连通性（例如：主机频繁上下线，会产生大量 TCN） RSTPRSTP（Rapid Spanning Tree Protocol，快速生成树协议，IEEE 802.1W）是 STP 的优化版。RSTP 相较 STP 的改进： RSTP 减少了端口的状态 RSTP 增加了端口的角色 RSTP 的配置 BPDU 的格式和发送方式有变化 拓扑改变时的处理不同，实现更快的收敛 RSTP 的端口状态： Discarding：对应 Disabled+Blocking+Listening，不能收发数据，不能地址学习 Learning：不能收发数据，开始学习 MAC 地址，能收发 BPDU Forwarding RSTP 端口角色变化：将 STP 的 Alternate 分为 Alternate 和 Backup 当阻塞端口收到更优的配置 BPDU 来自其他网桥时，该端口为 Alternate 当阻塞端口收到更优的配置 BPDU 来自本网桥时，该端口为 Backup RSTP 使用 RST BPDU。与 BPDU 的区别： 协议版本号为0x02，标识 RSTP 类型变为0x02，标识 RST BPDU 使用了 Flags 的全 8 位 增加了 Version 1 Length 字段 网桥自行从指定端口发送 RST BPDU，不需要等待来自根桥的 RST BPDU。RST BPDU 的老化时间为 3xHello Time。 收到低优先级 RST BPDU 的处理：阻塞端口可立刻做出响应。 RSTP 快速收敛机制： 边缘端口机制：边缘端口直接进入转发状态，无需延时，不会触发拓扑改变。当边缘端口收到 BPDU 后，会转变为非边缘端口 根端口快速切换机制：若旧的根端口已进入阻塞状态，且新的根端口连接的对端网桥的指定端口为 Forwarding，则新拓扑的根端口可直接进入 Forwarding 指定端口快速切换机制：指定端口通过与相连的网桥进行一次握手（P/A 握手机制），直接进入 Forwarding 握手请求报文：Proposal 握手回应报文：Agreement 条件：必须在点对点链路 拓扑改变触发条件：只有非边缘端口转变为 Forwarding，产生拓扑改变 拓扑改变处理： 在两倍 Hello Time 时间内向所有的其他指定端口和根端口发送 TC 置位的 BPDU 报文 清除除了接收到 TC 报文的端口外的所有其他指定端口和根端口的学习的 MAC 地址表 当 RSTP 与 STP 混用时： 若 RSTP 端口连续三次接收到 STP 的 BPDU，则该端口切换回 STP 切换到 STP 的 RSTP 端口就失去快速收敛特性 当 STP 与 RSTP 混用时，最好将 STP 设备放在网络边缘","categories":[],"tags":[]},{"title":"组播学习笔记","slug":"组播学习笔记","date":"2018-07-31T14:12:09.000Z","updated":"2022-06-21T15:47:56.634Z","comments":true,"path":"2018/组播学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/%E7%BB%84%E6%92%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"基于华三网络学习笔记（理论） 组播概述 组播组管理协议 IGMPv2","text":"基于华三网络学习笔记（理论） 组播概述 组播组管理协议 IGMPv2 组播概述组播用于实现点到多点的传输。 效率高，分布式应用，多点传输 数据源仅发送一份数据包，链路仅传输一份数据包，只有组播组中接受者能收到数据包 尽最大努力交付 无拥塞控制 数据报重复 数据包无序交付 组播需求问题与相关技术： 如何标识接受者—–组播地址机制 接受者动态加入或离开组播组—–组成员关系管理 组播报文如何在网络中转发—-组播报文转发过程 组播报文转发路径（组播转发树）构建—–组播路由协议 组播地址：组播地址范围：224.0.0.0---239.255.255.255 本地协议预留组播地址：224.0.0.0---224.0.0.255，属于局部范围，不会被路由器转发 用户组播地址：224.0.1.0---238.255.255.255，用户可用组播地址，全网有效232.0.0.0/8为 SSM 组地址，其余属于 ASM 组 本地管理组地址：239.0.0.0---239.255.255.255，特定本地范围有效，属于 ASM 组 组播 MAC 地址：以太网：01-00-5e-xx-xx-xx 组播地址映射：组播 MAC 地址中高 24 位固定为0x01005E，第 25 位为 0，低 23 位来自组播 IP 地址的低 23 位。组播 IP 地址的高 4 位为1110，标识组播，而低 28 位只有 23 位被映射到组播 MAC 地址，即有 5 位的丢失，一共会有2^5即 32 个 IP 地址公用一个组播 MAC，也就是可能会接受所在组播组外的其他组播数据。 举例： 组播IP地址为：224.231.123.14换成二进制：11100000 11100111 01111011 00001110组播MAC地址高24位为01005E，即0000 0001 0000 0000 0101 1110 1110|00001|11001110111101100001110 组播IP地址，共32位000000010000000001011110|0|11001110111101100001110 组播MAC地址，共48位组播IP地址中第二部分，案例中00001部分，在MAC地址中没有任何对应，一共有32种可能 组播组管理协议常用的组播组管理协议为 IGMP（Internet Group Management Protocol 因特网组管理协议）。主机通过 IGMP 通知路由器加入或离开某个组播组，路由器通过 IGMP 周期查询组播组成员是否处于活动状态，收集成员关系并维护 IGMP 有三个版本： IGMPv1：定义了基本的组成员查询和报告过程 IGMPv2：添加了组成员快速离开机制 IGMPv3：添加了成员可指定接受或拒绝组播源的报文，支持 SSM 模型 IGMPv2IGMP 报文： Type：IGMP 报文类型，包含Membership Query、Report、Leave Group Max Reps Time：最大响应时间，只有Membership Query使用该字段 checksum：校验和 Group Address：组地址。不同报文填的不同。普遍查询填 0，特定组查询填指定组播组地址，报告报文和离开报文填组播组地址 IGMPv2 原理： 当同一网段中有多个 IGMP 路由器时，通过查询器选举机制（最小接口 IP）选出唯一查询器。 查询器周期发送普遍查询消息General Query，目的地址224.0.0.1，TTL为 1，进行组成员关系查询。主机收到后发送报告消息Report响应。也有称为Membership Report，但本篇统一为Report 主机若要加入组播组，可直接向查询器发送Report，离开组播组时，直接发送Leave，目的地址为224.0.0.2，通告所有组播路由器。查询器收到Leave后，会发送特定组查询消息Group-Specific Query确定该组所有成员是否都离开。 若是加入，查询器则会查看组播转发表项，若不存在就添加，表项为(组播源IP地址,组播组IP)，若为*表示任意源。组播转发表项还包含：组播指定报文的入接口、出接口等 案例完整流程： IGMP Snooping：解决二层组播。原因：组播数据在二层以广播发送主机发往 IGMP 查询器的报告消息经过交换机时，交换机会监听并将组播 MAC 和端口做映射，建立表项。当交换机收到组播数据时，就按表项转发，也就只向组成员发送了 IGMPv3 概述： 1.可对源过滤 2.新的报文类型与格式 3.报告报文的组播地址为 224.0.0.22 4.取消成员报告抑制机制IGMPv3 主机为接口上每个组播组维护一个表项（组地址，过滤模式，源列表）过滤模式：INCLUDE：只接收来自源列表的组播源的数据包EXCLUDE：只接收不在源列表的组播源的数据包三种状态：当前状态，过滤模式改变状态，源列表改变状态。对应三种记录当主机接口维护的组状态变化时，会主动发送组记录类型为过滤模型变化或源列表变化的报告报文。当接收到查询报文时，会响应记录类型为当前状态的报告报文 组播分发树模型：是组播数据的转发数据，分为最短路径树(S,G)和共享树(*,G)组播转发机制：逆向路径转发组播路由协议：域内：DVMRP（基于路径矢量协议）、MOSPF（基于 OSPF）、PIM 域间：MSDP、MBGP域内协议：基于 SPT：PIM DM、DVMRP、MOSPF 基于 RPT：PIM SM组播模型：ASM 任意信源组播：接收端只选择加入组播组，不能选择组播源 SSM 指定信源组播：接收端可以指定组播源 组播分发树：由组播路由协议建立的无环传输路径SPT 最短路径树：组播源到接受者的最短路径。要为每个组播源建一棵最短路径树缺点：路由器必须为每个组播源保存路由信息RPT 共享树：以某个路由器作为树根，该路由器称为汇聚点 RP，以 RP 为树根建立到每个接收者的最短路径树。所有组播源和接收者都使用这棵树收发报文。组播源先向 RP 发数据，再由 RP 发送到所有接收者。优点：路由器保留的路由信息很少缺点：数据报文先要经过 RP，再到达接收者，对 RP 的可靠性和性能要求高组播报文转发机制RPF 逆向路径转发原因：组播报文是发送给一组接收者的，路由器收到组播报文后，必须根据报文的源地址确定正确的入接口和下游方向，然后向下游方向转发。该过程就是 RPF。目的：确保组播数据沿正确路径传输，避免出现环路检查过程：在单播路由表上查找组播源（分发树为 SPT）或 RP（分发树为 RPT）对应的 RPF 接口，若数据包是在 RPF 端口接收到的，则 RPF 检查成功，转发数据包。否则检查失败，直接丢弃PIM 协议无关组播：与单播路由无关，但仍然依靠单播路由表进行组播路由。使用 RPF 转发报文。分为两种模式：PIM-DM（密集模式），PIM-SM（稀疏模式）PIM-DM：用于小型网络中接收者较多且密集的情况，采用“推”方式将流量泛洪。邻居发现：路由器周期发送 PIM Hello 消息，发现其他 PIM 路由器，建立邻居关系，判断叶子网络，选举 DR（若运行的是 IGMPv1，通过 Hello 选举，其他版本就不需要选举 DR）。扩散-剪枝：将组播数据扩散到每个节点，每个节点创建(S,G)表项（包含出接口（除 RPF 接口外所有连接 PIM-DM 邻居或组播组成员的接口）与入接口列表），若节点没有该组播组成员，就向上游发送 Prune 剪枝消息，若共享网段有路由器上有接收者，就向上游发送 Join 消息，覆盖其他路由器发送的 Prune。 扩散-剪枝过程周期进行。最终形成 SPT。Prune 消息发送情况：1.若路由器(S,G)表中出接口表为空 2.路由器从非 RPF 接口收到组播报文，会触发断言 Assert 机制，断言失败一方会向成功一方发送 Prune 消息。断言 Assert：若同一网段有多个组播路由器，相同报文可能会被重复发送，通过断言选取网段唯一转发组播数据的路由器。过程：路由器在重复接收到报文的接口上发送 Assert 消息，包含 S，G，单播路由的优先级，开销 Metric。先比较路由优先级（高的胜），再比较开销（小的胜），再比较本地接口 IP 地址（大的胜）。当一台路由器上游接口故障时，该路由器将 Metric 值设为无穷大并广播 Assert，引发新一轮断言，保证流量不会长时间中断。状态刷新机制：与组播源直连的路由器发送 State Refresh，其他路由器收到后重置剪枝超时定时器，并向所有连接 PIM-DM 邻居的路由器发送该消息，对于处于转发的接口，消息中剪枝位为 0，处于剪枝的接口，剪枝位为 1。周期发送该消息可使剪枝状态的接口维持状态，减少不必要的扩散。嫁接：当被剪枝的节点上出现接收者时，节点会主动向上游发送嫁接 Graft 消息，上游收到后回复 Graft Ack 消息确认，节点从剪枝状态变为转发状态。两个消息都是单播发送。PIM-SM：用于中大型网络中，组播组成员相对分散，范围较广，采用“拉”方式。核心任务是构造维护 RPT，选出 RP 作为共享树的根。过程：组播源侧 DR 向 RP 注册，将注册报文单播发给 RP，该报文到达 RP 后触发建立 SPT，组播源把数据沿 SPT 发给 RP，再由 RP 沿 RPT 发给接收者 ASM 模型：任意源模型。任何发送者都可作为组播源向组播组发数据，接收者无法预先知道组播源位置，但可以在任意时间加入离开组播组SSM 模型：指定信源组播模型。接收端能指定组播源。SSM 模型无需 RP，无需构建 RPT，无需组播源注册过程。","categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"组播","slug":"组播","permalink":"https://coconutmilktaro.top/tags/%E7%BB%84%E6%92%AD/"}]},{"title":"OSPF学习笔记","slug":"OSPF学习笔记","date":"2018-07-31T04:36:00.000Z","updated":"2022-05-30T02:51:53.861Z","comments":true,"path":"2018/OSPF学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/OSPF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"基于华三网络学习笔记（理论） 本篇包含以下内容 OSPF 特性与基本术语 OSPF 报文 OSPF 邻居建立维护与状态机 链路状态广播 LSA OSPF 特殊区域","text":"基于华三网络学习笔记（理论） 本篇包含以下内容 OSPF 特性与基本术语 OSPF 报文 OSPF 邻居建立维护与状态机 链路状态广播 LSA OSPF 特殊区域 OSPF 特性与基本术语Open Shortest Path First 开放最短路径优先 属于 IGP，优先级 AS 内部 10，外部 150 采用链路状态算法 SPF 防环 封装在 IP 报文中，协议号 89 度量值为开销 cost=带宽参考值/接口带宽，参考值通常为 100M，若求得的数小于 1，则 cost 就取 1 报文更新方式为触发更新+周期更新（30min。LSA 老化时间 60min） 增量更新（通过 LSA），组播更新报文 组播地址 224.0.0.5（主要，所有 OSPF 路由器都能收到）或 224.0.0.6（DR、BDR 可收到） 没有跳数限制，可用于大规模组网 路由生成过程： 生成 LSA 描述自身接口状态（链路开销、IP 地址等） 同步 OSPF 区域内每台路由器的 LSDB（通过交换 LSA） SPF 算法计算路由：每个路由器以自身为根计算最短路径树（即根到各节点的开销都是最小的），加入路由表，若两条路径开销相同，则都加入表中形成等价路由。 开启 OSPF 的路由器上与路由转发相关的三张表： 邻居表：记录建立了邻居关系的路由器 LSDB 表：记录所有链路状态信息，需要实时同步 路由表：记录经 SPF 算法计算的路由 OSPF 选路原则： 按路由类型优先级：区域内路由&gt;区域间路由&gt;第一类外部路由&gt;第二类外部路由 类型相同，选路由开销小的 以上都相同，形成等价路由 两类外部路由： 第一类外部路由：偏向于 AS 内部的选路，并不关心 AS 外的开销。用于控制入 AS 的路由选路。如图中 RTA，若选择第一类外部路由，则关心 AS 内部的开销，会选择开销较小的 RTB 路线。 第二类外部路由：偏向与 AS 外部的选路，并不关心 AS 内的开销。用于控制出 AS 的路由选路。如图中 RTA，若选择第二类外部路由，则关心 AS 外部的开销，会选择开销较小的 RTC 路线。 若同一网段的路由信息同时通过第一类外部路由和第二类外部路由学习到，在其他条件相同的情况下，会优选第一类外部路由。 骨干区域：area 0，负责转发非骨干区域之间的路由。区域间路由规则： 非骨干区域必须与骨干区域相连 非骨干区域之间不能传递路由，必须通过骨干区域 骨干区域传出的路由不能传回非骨干区域 OSPF 防环：从一个区域学习到的路由不会再向该区域注入。非骨干区域间不能直接通信 当骨干区域被分割或非骨干区域不与骨干区域相连时，可通过虚连接解决。两台 ABR（区域边界路由器）通过一个非骨干区域建立一条逻辑通道，对于通道上的路由器是透明的。 划分区域的好处： 减少了区域内 LSDB 中链路状态信息的数量 便于管理 减少路由震荡的影响范围 OSPF 路由器类型： 区域内路由器 Internal：所有接口都属于同一个区域 区域边界路由器 Area Border：连接骨干与非骨干区域（物理上或逻辑上） 骨干路由器 Backbone：至少有一个接口属于骨干区域，即所有区域内和区域边界路由器都是骨干路由器 自治系统边界路由器 Autonomous System Border：与其他 AS 路由器交换路由信息，不一定在 AS 的边界，只要该路由器引入外部路由，就是 ASBR。 Router ID 用来在 AS 中唯一标识一个路由器，RouterID 的选取优先级如下：局部 &gt; 全局 &gt; 自动选举局部：创建 OSPF 进程时同时指定 router-id全局：系统视图下指定 router id自动选举：环回口中最大的，若无环回口，则选取接口中 IP 地址最大的 网络类型： Broadcast 广播：当链路层为以太网协议时，默认为 Broadcast，以组播地址发报文(.5|.6)，需要 DR，Hello 定时器 10s，邻居失效时间 40s。 NBMA 非广播多点可达网络：当链路层为帧中继或 ATM 时，默认为 NBMA，以单播发报文，需要 DR，Hello 定时器 10s，邻居失效时间 40s。 P2P 点到点：当链路层为 PPP、HDLC 时，默认 P2P，以组播发报文(.5)，不需要 DR，Hello 定时器 30s，邻居失效时间 120s。 P2MP 点到多点：需要手动修改，以组播发报文(.5)，不需要 DR，Hello 定时器 30s，邻居失效时间 120s。 OSPF 报文OSPF 五种报文： Hello报文：发现维护邻居关系，包含定时器、DR、BDR 和已知邻居 DD报文：数据库描述报文，描述本地 LSDB 中 LSA 摘要，进行主从关系协商，用于路由器间 LSDB 同步 LSR(Request)报文：链路请求报文，向对方请求所需 LSA（通过比对 DD 报文知道自己缺哪些 LSA） LSU(Update)报文：链路状态更新报文，向对方发送所要求的 LSA LSAck报文：链路状态确认报文，对收到的 LSA 进行确认 OSPF 邻居建立维护与状态机邻居建立与维护： 组播发送 Hello 报文（.5），双方协商参数，若验证、区域等都相同，则表示邻居发现 邻居周期交换 Hello 报文，若邻居失效时间超时未收到 Hello 则认为邻居失效，将该邻居从邻居表中删除 DR/BDR 选举：目的：减少邻接关系的数量，所有路由信息都发给 DR（指定路由器），再由 DR 发 LSA若不设置 DR/BDR，则邻接关系数量R=n(n-1)/2个若设置 DR/BDR，则邻接关系数量R=2(n-2)+1个 BDR 是 DR 的备份。若 DR 失效，BDR 立刻成为 DR。DR/BDR 选举原则： 首先比较 Hello 报文中的优先级，最高的为 DR，次高的为 BDR，若为 0 不参加选举。 优先级相同则比较 RouterID，大的优 选举完毕后，即使有更优的路由器加入区域，也不会更换 DR/BDR（可以在用户视图重置 ospf 进程，使 ospf 重新选举） 只有广播和 NBMA 网络选举 DR/BDR 剩余路由器成为 DRother，只与 DR、BDR 建立邻接关系 邻接关系建立： 初始状态，A 的邻居为Down，由于邻居表为空，所以 DR 字段置为0.0.0.0，发送 Hello 报文，B 收到 Hello 报文后，将 A 添加进邻居表中，邻居状态变为Init，两个路由器比较 RouterID，大的（假设 A）会在后面的 Hello 报文中将 DR 字段设为自己的 RouterID。 B 收到 Hello 报文，发现邻居表中有自己的 RouterID，于是将邻居表中 A 状态变为2-way。B 也收到后，同理。若当前两台路由器都是 DRother，则邻接状态就会维持在2-way。只有其中一个是 DR 或 BDR，才会继续建立关系 若进一步建立邻接关系，A 会将 B 状态设为ExStart，并发送一个不包含 LSA 的 DD 报文，开始主从协商。其中 DD 报文包含 MS 位，最开始该 MS 位置 1，表示路由器以自己为 Master。Master 路由器的作用就是在交换 DD 报文时，主动发送 DD 报文，并控制报文的序列号。Slave 路由器仅能接受 Master 指定的序列号并被动发送 DD。 B 收到 DD 后，将发送方的状态设为ExStart。对比 RouterID，若大（假设 A）就在 DD 报文中将 MS 位也置为 1，表明自己是 Master，并回复。B 收到 DD 报文后，同意 A 为 Master，将 MS 位置 0，表明自身 Slave，采用 A 规定的序列号向 A 发 DD 报文，此时 DD 报文中包含 LSA 摘要，A 收到后将 B 状态改为Exchange。B 收到 A 的 DD 报文后也将 A 状态改为Exchange。 A 与 B 都对 DD 报文的 LSA 与 LSDB 进行比对，若 LSA 信息在 LSDB 中都存在，就直接进入Full状态。若一方 LSDB 不完全包含 LSA，则向另一方请求，并将对方状态置为Loading，发送 LSR。另一方收到后根据 LSR 返回 LSU。再次比对后相同就进入Full。 OSPF 状态机其中有三个稳定状态：Down、2-way、Full down：未启动 ospf init：收到对方 hello 包，但 hello 包中的邻居表没有自己 2-way：收到对方 hello 包，且在 hello 包中看到自己 exstart：互相发送空的 DD 协商主从报文，以决定谁发 DD 报文 exchange：交换真正的 DD 报文 loading：交互路由信息 full：路由学习完毕，邻接关系建立 影响 OSPF 建立邻接关系的因素： area 是否一致 接口是否开启 OSPF 接口是否开启验证 是否启用了静默接口，或开启过滤 是否处于特殊区域 Hello/Dead 定时器是否一致 Router-id 是否不同 链路两端接口掩码是否不同（广播类型链路 hello 会携带掩码信息） 两端 MTU 是否不同（若不同会一直在 Exstart 状态） 链路状态广播 LSALSA 老化时间 3600s（1 小时），每 1800s（半小时）ospf 就会泛洪一次全部路由信息 报文字段： LS age：LSA 产生后经过的时间（单位秒） LS type：LSA 类型（1-11） Link State ID：LSA 链路 ID，根据 LSA 类型而定 Advertising Router：始发 LSA 的路由器 ID，也称 LSA 通告路由器LS type、Link State ID、Advertising Router三个参数唯一标识一个 LSA LSA sequence number：LSA 序列号，用于判断是否是最新的 LSA LS checksum：LSA 信息的校验和 length：LSA 总长度 LSDB 更新过程：收到一条 LSA 更新报文，在 LSDB 中查找该 LSA，若未找到就将这条 LSA 加入 LSDB，若找到，就对比 LSA 的序列号，若该条的大，就更新，否则不更新。 LSA 类型： 一类：Router LSA，描述区域内部与路由器直连的链路信息，所有 OSPF 路由器始发，仅在区域内传播。不携带掩码信息 二类：Network LSA，记录广播或 NBMA 上所有路由器 RouterID，DR 始发，仅在区域内传播。携带网段掩码信息，和一类 LSA 共同计算网段一类和二类 LSA 解决了区域内部的通信 三类：Network Summary LSA，包含区域网段与开销，传播给相邻区域，ABR 始发，区域间传播。实际就是收集一类和二类的 LSA。每个三类 LSA 包含一个网段一、二、三类 LSA 解决了区域内和区域间通信 四类：ASBR Summary LSA，描述 ASBR 的 RouterID 和开销，传播给非 ASBR 区域，ABR 始发。辅助五类 LSA，实现到达 ASBR。告诉 OSPF 内部路由器如何到达 ASBR 五类：AS External LSA，描述到 AS 外部的路由，包含外部网段、开销等，传播给整个 OSPF 系统，ASBR 始发。每个五类 LSA 包含一个网段。 七类：NSSA Exteranl LSA，只在 NSSA 中传播，描述到 AS 外部的路由，ASBR 始发 路由聚合：ABR 和 ASBR 可将具有相同前缀的路由聚合发布。 安全： 协议报文验证：通过验证的 OSPF 报文才能被接收（路由器+接口都要配置验证：Simple 或 MD5） 禁止端口发送 OSPF 报文：该端口成为被动端口（静默），不再发送 Hello 报文 过滤计算出的路由：通过过滤规则的路由才加入路由表 过滤三类 LSA：设置规则过滤外部路由（本地有效） OSPF 特殊区域OSPF 特殊区域： Stub：不允许注入四、五类 LSA。不能存在 ASBR。虚连接不可穿过。若有多个 ABR 可能产生次优路由 Totally Stub：不允许注入三、四、五类 LSA。虚连接不可穿过。ABR 会产生一条 0.0.0.0/0 的三类 LSA NSSA：不允许四、五类 LSA，允许七类 LSA。虚连接不可穿过。该区域存在一个 ASBR，该区域不希望接收其他 ASBR 的外部路由。七类默认路由 LSA 由 ASBR 产生，在 NSSA 中传播，当到达 ABR 时，会转换为五类 LSA 传到别的区域。Totally STub 和 NSSA 都是 Stub 的变形或改进 Totally NSSA：不允许注入三、四、五类 LSA，而是用七类默认路由取代。虚连接不可穿过。","categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"OSPF","slug":"OSPF","permalink":"https://coconutmilktaro.top/tags/OSPF/"}]},{"title":"BGP学习笔记","slug":"BGP学习笔记","date":"2018-07-31T01:19:44.000Z","updated":"2022-05-30T02:51:53.775Z","comments":true,"path":"2018/BGP学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/BGP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"基于华三网络学习笔记（理论） 本篇包含以下内容 BGP 特性与基本术语 BGP 消息与状态机 BGP 路由属性 BGP 选路规则 控制 BGP 路由 BGP 补充知识点","text":"基于华三网络学习笔记（理论） 本篇包含以下内容 BGP 特性与基本术语 BGP 消息与状态机 BGP 路由属性 BGP 选路规则 控制 BGP 路由 BGP 补充知识点 BGP 特性与基本术语Border Gateway Protocol 边界网关协议。用于自治系统间，进行不同 AS 间路由传递。 路径矢量路由协议 基于 TCP，端口号 179 EGP 协议，优先级 255 支持路由聚合与 CIDR 只发送增量路由 路由信息中携带经过的所有 AS 路径表 支持 CIDR 和路由聚合 丰富的路由属性、强大的路由过滤和路由策略 可传输大量路由（基于 TCP 的可靠传输和滑动窗口） 只能点对点连接（TCP 的点对点） 基本术语 BGP 发言者：发送 BGP 消息的路由器 Router ID：32 位，在 AS 中唯一标识一台主机，必须配置 BGP 对等体 Peer：相互交换消息的 BGP 发言者互为对等体，也可称 BGP 邻居。 IBGP 对等体：处于同一 AS 的对等体，不需要直连从 IBGP 获得的路由不会向其他 IBGP 邻居发布（为了防环）从 IBGP 获得的路由是否发个 EBGP 邻居与是否同步有关（为了防止路由黑洞）全连接：为解决部分连接导致的无法学习路由，每两个路由器都建立 IBGP 邻居则可以保持 AS 内的所有 BGP 路由器路由信息相同 EBGP 对等体：处于不同 AS 的对等体，且通常要求直连从 EBGP 获得的路由会发布给所有 IBGP 邻居EBGP 的 TTL=1，所以只能是直连对端接口，不可跨设备（可修改 TTL 实现跨设备）而 IBGP 的 TTL=255 可与 AS 内任意 BGP 路由器建邻居 BGP 防环： 对于 EBGP：使用 AS-PATH 对于 IBGP：禁止将从 IBGP 邻居学到的路由发布出去。缺点：有路由器学不到路由。解决：全连接 BGP 同步：IBGP 与 IGP 之间同步，避免转发黑洞。收到 IBGP 邻居发布的路由后，会查看该路由是否在 IGP 表中，只有 IGP 表中存在，才会置为有效并发布，否则无效不发布。路由器默认关闭同步。 BGP 消息与状态机BGP 所有消息都是消息头+消息体，消息头长度 19 字节，包含以下字段： Marker：16 字节，用于 BGP 验证的计算，不使用验证时所有位都置为 1 Length：2 字节，BGP 消息总长度（包括报文头） Type：1 字节，BGP 消息的类型，取值为 1 到 5，分别表示 Open、Update、Notification、Keepalive、Route-Refresh BGP 消息种类： Open：用于建立 BGP 邻居。TCP 连接后的第一个消息，进行参数协商。包含以下字段： BGP 版本 AS 号 routerID Hold Time：保存时间，若超时仍未收到对端的 Keepalive 或 Update 消息，则认为 BGP 连接中断。建立对等体时要协商该参数并保持一致 认证信息或多协议扩展等功能 Update：在邻居间交换路由信息（发布或撤销）。可通告一类相同属性的可达路由和不可达路由。包含以下字段： 不可达路由字段长度。单位字节，若为 0 表示没有Withdrawn Routes Withdrawn Routes 不可达路由列表，即存放被撤销的路由 路径属性字段长度。单位字节，若为 0 表示没有Path Attibutes Path Attibutes，存放与 NLRI 相关的所有路径属性列表，每个路径属性由一个 TLV 三元组构成。 NLRI 可达路由的前缀和前缀长度二元组，存放一类相同属性的可达路由 Notification：错误通知（消息错误或断开 BGP 连接）。包含以下字段： 差错码，指定错误类型 差错字码，提示错误类型的详细信息 数据，出错部分的数据。用于辅助发现错误的原因，依赖于差错码和差错子码 Keepalive：维护邻居关系或对 Open 消息回应。只有消息头。周期发送，默认周期 30s。 Route-refresh：要求对等体重新发送指定地址族的路由 BGP 状态机： Idle：空闲。初始状态，等待 Start 事件。一旦有 Start，就向邻居发起 TCP 建立请求 Connect：连接。等待 TCP 建立完成。若 TCP 完成，状态改为 Open-sent。若失败，状态改为 Active。 Active：活跃。TCP 未成功建立。若超时，会返回 Connect。若成功，进入 Open-sent 状态。 Open-sent：Open 消息已发送。已发出 Open 消息，等待邻居的 Open 消息。若收到邻居的 Open 消息且无错误，进入 OpenConfirm 状态，并发送 Keepalive。否则，进入 Notification。 OpenConfirm：Open 消息已确认。Keepalive 已发送，等待邻居的 Keepalive。若收到邻居的 Keepalive，则进入 Established 状态。若收到 Notification，则断开连接 Established：BGP 连接建立。可发送 Update 交换路由，发送 Keepalive 维护连接，若收到 Notification 则断开连接 BGP 路由属性 公认必遵属性：BGP 路由器必须识别，必须存在于 Update ORIGIN：定义路由信息来源类型：IGP–路由产生于 AS 内 EGP–路由通过 EGP 学到 Incomplete–路由来源不确定优先级：IGP&gt;EGP&gt;Incomplete AS_PATH：路由更新经过的 AS 路径列表，保证 AS 间无环。可用于路由选择和过滤当 BGP 将一条路由通告到其他 AS 时，会把本地 AS 号添加到 AS-PATH 最前优先选择 AS-PATH 最短的路由。若向 EBGP 邻居发送路由更新修改，IBGP 间不修改 NEXT_HOP：路由下一跳向邻居发布路由时，会将下一跳设为自己与对端连接的端口从 EBGP 邻居得到的路由发给 IBGP 邻居时，不会修改下一跳 公认可选属性：BGP 路由器必须识别，不必须存在于 Update LOCAL_PREF：用于 IBGP 选择离开 AS 时的路由，表明 BGP 路由器的优先级仅在 IBGP 对等体间交换。默认值 100 可选传递属性：在 AS 间可传递，路由器可不支持，仍可接收并通告 COMMUNITY AGGREGATOR 可选非传递属性：若 BGP 路由器不支持，属性会被忽略，且不通告 MED：度量值。告诉 EBGP 邻居进入 AS的路由。仅在相邻 AS 间交换，收到 MED 的 AS 不会再通告给其他 AS通常只比较来自同一 AS 的 MED 私有 BGP 属性： Preferred-value：对从邻居学习到的路由分配优先级本地有效，不通告。初始为 0 对于 BGP 路由处理： 接收 BGP 路由 路由过滤、属性设置 路由优选 发布策略 发布路由过滤、属性设置、路由聚合 BGP 选路规则路由选路优先级（高到低）： 丢弃下一跳不可达的路由 Preferred-value 选大 LOCAL_PREF 选大 聚合路由，本地路由 AS_PATH 选小 ORIGIN 按优先级选 MED 选小 依次选从 EBGP、联盟、IBGP 学到的路由 下一跳度量值最低 CLUSTER_LIST 选短 ORIGINATOR_ID 最小 RouterID 最小路由器发布的路由 地址最小的邻居发布的路由 BGP 一定能选出唯一的最优路由，且可以负载分担路由下一跳不一定是直连邻居，原因：IBGP 发布路由不改变下一跳。路由器会查找直连可达地址，到达要发布路由的下一跳（去往该下一跳的路由为依赖路由，过程为路由迭代）。路由器支持基于迭代的负载分担。 BGP 路由发布策略： 只发布最优路由 只发布自己使用的路由 发布所有从 EBGP 邻居学到的路由给所有 BGP 邻居（IBGP 和 EBGP） 不把从 IBGP 邻居学到的路由发布给 IBGP 邻居 IBGP 路由发到 EBGP：BGP 同步关–直接发布。BGP 同步开–IGP 也发布时才发布 BGP 连接建立后，发布所有 BGP 路由 BGP 下一跳原则：若从 EBGP 邻居学到的路由传给 IBGP 邻居时下一跳不变，可能会导致 BGP 设备因为下一跳不可达而不加入路由表。解决：应在 EBGP 路由传给 IBGP 邻居时将下一跳改为自身 BGP 路由不优的原因：1.同步打开，但网络不满足同步要求 2.下一跳不可达 BGP 源 IP 地址原则：BGP 设备收到一个 BGP 报文，会检查报文源 IP 地址，若与 peer 所指 IP 地址一致，则设备接收该报文，若不一致，则丢弃报文，在建环回口建立 BGP 关系时，要修改 BGP 报文源 默认情况，BGP 使用到达对等体的最佳路由作为出接口作为与对等体建 TCP 连接的源接口将建立 TCP 的源接口配置为环回口，在网络中存在冗余链路时不会因为某个接口或链路故障而使 BGP，提高了可靠性和稳定性 控制 BGP 路由常用属性：preferred-value、Local-preference、MED、next-hop-local路由首选值Preferred-value：优选大的。默认从对等体学来的路由首选值为 0本地优先级Local-preference：判断离开 AS 的最佳路由AS 路径过滤表AS_PATH list：一个基于 AS 表的 ACL，使用正则表达式对路由携带的 AS 路径属性域进行匹配 正则表达式： ^ 匹配字符串的开始 $ 匹配字符串的结束 * 匹配*前的字符（串）0 或多次 + 匹配+前的字符（串）1 或多次 . 通配符，匹配任何一个字符 _ 下划线，匹配一个符号 - 连接符，连接两个字母或数值 ( ) 字符组，一般与-连用 [ ] 匹配[ ]中任意一个字符 常用正则组合： ^$只匹配本地路由 .*匹配所有路由 ^100匹配 AS100、1001 等邻居的路由 ^100_只匹配 AS100 邻居发的路由 _100$匹配 AS100 始发的路由 _100_匹配经过 AS100 的路由 BGP 补充知识点BGP 对等体组 peer group：具有某些相同属性的对等体集合，可分为 IBGP 或 EBGP 对等体组 BGP 团体属性：一组具有相同特征目的地址的集合，与所在 AS 无关，一条路由可以有多个团体属性。 公认团体属性INTERNET：有这一属性的路由可以被通告给所有对等体。路由缺省属于该团体NO_EXPORT：该团体路由不能被发布到本地 AS 外，若使用联盟，不能发布到联盟外NO_ADVERTISE：不能被通告任何 BGP 对等体NO_EXPORT_SUBCONFED：不能被发布到任何其他 AS BGP 聚合两种聚合：手动、自动自动：聚合为自然路由。只能引入 IGP 子网路由聚合，不能对 BGP 邻居学来的或 network 发布的路由进行聚合。手动：手动配置灵活的聚合，可以对从 BGP 邻居学习的、引入 IGP 的、network 生成的路由聚合 BGP 反射作用：可代替 IBGP 对等体全连接原理：允许设备从 IBGP 对等体接收到的路由信息发布给特定 IBGP 对等体，这些网络设备称为路由反射器。","categories":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"BGP","slug":"BGP","permalink":"https://coconutmilktaro.top/tags/BGP/"}]},{"title":"Docker Compose学习笔记","slug":"Docker-Compose学习笔记","date":"2018-07-30T09:32:29.000Z","updated":"2022-05-30T02:51:53.788Z","comments":true,"path":"2018/Docker-Compose学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Docker-Compose%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"环境：docker：18.06.1，docker-compose：1.22.0 docker18.06 对应的 Compose 文件格式版本为 3.7 本篇包含以下内容： docker-compose 介绍 Compose 文件格式 docker-compose 示例 官方 wordpress 案例 官方 Django 案例 官方 Flask 案例","text":"环境：docker：18.06.1，docker-compose：1.22.0 docker18.06 对应的 Compose 文件格式版本为 3.7 本篇包含以下内容： docker-compose 介绍 Compose 文件格式 docker-compose 示例 官方 wordpress 案例 官方 Django 案例 官方 Flask 案例 docker-compose 介绍Docker Compose是一个编排多容器分布式部署的工具，提供命令集管理容器化应用的完整开发周期，包括服务构建，启动和停止。在配置文件中，所有的容器通过 services 来定义，然后使用 docker-compose 脚本来启动，停止和重启应用，和应用中的服务以及所有依赖服务的容器。 Compose 的特性 通过项目名称将单个主机隔离成多个环境，能将应用环境复制多份，还能防止使用相同名称的服务的应用间的干扰 能够保护卷中的数据，如果 Compose 发现存在之前运行过的容器，它会把旧容器中的数据卷拷贝到新的容器中 只会重新创建改变过的容器，Compose 会缓存用于创建容器的配置信息，当你重启服务时，如果服务没有被更改，Compose 就会重用已经存在的容器，加快了修改应用的速度 编排：Orchestration，根据被部署的对象间的耦合关系以及被部署对象对环境的依赖，制定部署流程中各个动作的执行顺序，部署过程中所需要的依赖文件和被部署文件的存储位置和获取方式，以及如何验证部署成功。这些信息都会在编排工具中以制定格式定义并保存。 部署：Deployment，按照编排所指定的内容和流程，在目标机器上执行编排指定环境初始化，存放指定的依赖和文件，运行指定的部署动作，按照编排中规则确认是否部署成功。 以上编排和部署定义摘选自《docker 容器与容器云》 docker-compose 安装 使用 pip 快速安装pip install docker-compose或通过 compose 的 github 库按提示安装docker-compose 的 github curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-compose docker-compose 参数选项： docker-compose [-f &lt;arg&gt;...] [options] [COMMAND] [ARGS...] -f, --file FILE 指定compose文件，默认为docker-compose.yml -p, --project-name NAME 指定项目名，默认为所在目录名 --verbose 显示详细过程信息 --log-level LEVEL 设置日志级别(DEBUG, INFO, WARNING, ERROR, CRITICAL) --no-ansi 不打印ANSI控制字符 -H, --host HOST 指定要连接的主机 --tls 使用tls，就是指--tlsverify --tlscacert CA_PATH 指定只承认该CA颁发的证书 --tlscert CLIENT_CERT_PATH TLS证书路径 --tlskey TLS_KEY_PATH TLS密钥路径 --tlsverify 使用TLS --skip-hostname-check 不根据客户端证书中指定的名称检查守护程序的主机名 --project-directory PATH 指定工作目录，默认为compose文件所在目录 --compatibility Compose将尝试将v3文件中的部署密钥转换为其非Swarm等效项 docker-compose 命令： **build**：构建或重构服务（services） build [options] [--build-arg key=val...] [SERVICE...] --compress 使用gzip压缩构建上下文 --force-rm 始终移除中间容器 --no-cache 构建镜像时不使用缓存 --pull 总是尝试拉取最新镜像 -m, --memory MEM 设置构建镜像的内存上限 --build-arg key=val 设置服务的构建时变量 **bundle**：从 Compose 文件生成 Docker 包 镜像必须存储摘要，这需要与 Docker Registry 进行交互。如果没有为所有镜像存储摘要，可以使用docker-compose pull或docker-compose push来获取。 bundle [options] --push-images 在打包时自动推送已使用build指定的服务的镜像 -o, --output PATH 包文件路径，默认为&quot;项目名.dab&quot; **config**：校验并查看 compose 文件 config [options] --resolve-image-digests 将镜像标签写入摘要 -q, --quiet 静默模式，只校验配置，不打印信息 --services 列出所有服务 --volumes 列出所有数据卷 **down**：停止并删除容器、网络、镜像、数据卷 默认能删除的内容： Compose 文件中定义的服务的容器 Compose 文件的networks中定义的网络 默认网络（如果使用） down [options] --rmi type 删除镜像，必须指定类型： &#x27;all&#x27;: 删除任何服务使用的所有镜像 &#x27;local&#x27;: 只删除没有通过image指定自定义标签的镜像 -v, --volumes 删除在Compose文件的&quot;volumes&quot;中声明的命名卷和附加到容器的匿名卷。 --remove-orphans 为服务删除没有在compose文件中声明的容器 -t, --timeout TIMEOUT 指定几秒后关闭（默认10s） **events**：从容器接收实时事件 events [options] [SERVICE...] --json 使用json格式输出事件 **exec**：在运行的容器中执行命令 exec [options] [-e KEY=VAL...] SERVICE COMMAND [ARGS...] -d, --detach 后台运行命令 --privileged 给进程额外的权限 -u, --user USER 指定运行命令的用户 -T 禁止分配伪终端，exec默认分配一个伪终端 --index=index 设置容器的索引（如果一个服务有多个容器），默认为1 -e, --env KEY=VAL 设置环境变量 -w, --workdir DIR 设置工作目录 **images**：列出镜像 images [options] [SERVICE...] -q, --quiet 静默模式，只显示镜像号 **kill**：杀死容器 kill [options] [SERVICE...] -s SIGNAL 发送给容器的SIGNAL，默认为SIGKILL **logs**：显示容器的输出 logs [options] [SERVICE...] --no-color 单色输出 -f, --follow 按照日志输出 -t, --timestamps 显示时间戳 --tail=&quot;all&quot; 显示日志的末尾行数 **pause**：暂停服务 pause [SERVICE...] **ps**：列出容器，执行此命令时必须cd到项目的根目录下 ps [options] [SERVICE...] -q, --quiet 静默，只显示容器号 --services 显示服务 --filter KEY=VAL 根据属性过滤服务 **port**：显示用于绑定的公共端口 port [options] SERVICE PRIVATE_PORT --protocol=proto 选择协议，tcp或udp，默认tcp --index=index 设置容器的索引，默认为1 **pull**：拉取服务镜像 pull [options] [SERVICE...] --ignore-pull-failures 忽略拉取失败的镜像 --parallel 并行拉取多个镜像，默认开启，官方不推荐 --no-parallel 禁止并行拉取多个镜像 -q, --quiet 静默模式，不显示拉取信息 --include-deps 同时拉取依赖的服务 **push**：推送服务镜像 push [options] [SERVICE...] --ignore-push-failures 忽略推送失败的镜像 **restart**：重启服务 restart [options] [SERVICE...] -t, --timeout TIMEOUT 指定几秒后重启（默认10s） **rm**：删除停止的容器 rm [options] [SERVICE...] -f, --force 不询问确认删除 -s, --stop 在删除前自动停止容器 -v 删除任何关联的匿名数据卷，默认不会删除 **run**：运行一次性命令 run [options] [-v VOLUME...] [-p PORT...] [-e KEY=VAL...] [-l KEY=VALUE...] SERVICE [COMMAND] [ARGS...] -d, --detach 后台运行 --name NAME 设置容器名 --entrypoint CMD 覆盖容器的ENTRYPOINT -e KEY=VAL 设置环境变量 -l, --label KEY=VAL 添加或覆盖标签 -u, --user=&quot;&quot; 以指定用户执行，可设置用户名或uid --no-deps 不启动相连的服务，默认依赖的服务也会启动 --rm 在运行后删除容器，不与-d兼容 -p, --publish=[] 发布公共端口 --service-ports 通过已启用并映射到主机的端口执行命令 --use-aliases 在容器连接的网络中使用服务的网络别名 -v, --volume=[] Bind mount挂载一个数据卷 -T 禁止分配伪终端（tty），默认run会分配一个 -w, --workdir=&quot;&quot; 容器中的工作目录 **start**：启动已存在的容器 start [SERVICE...] **stop**：停止运行中的容器，并不会删除它们 stop [options] [SERVICE...] -t, --timeout TIMEOUT 指定几秒后关闭（默认10s） **top**：显示服务的进程 top [SERVICE...] **unpause**：恢复暂停的服务 unpause [SERVICE...] **up**：创建并启动容器 默认会启动相连的服务。 up [options] [--scale SERVICE=NUM...] [SERVICE...] -d, --detach 后台运行。与--abort-on-container-exit不兼容 --no-color 单色输出 --quiet-pull 静默拉取 --no-deps 不启动连接的服务 --force-recreate 强制重建服务（即使配置和镜像都没变） --always-recreate-deps 重建依赖的服务，与--no-recreate不兼容 --no-recreate 若容器存在就不会重建，与--force-recreate和-V不兼容 --no-build 即使镜像丢失也不重建镜像 --no-start 在构建服务后不启动该服务 --build 在启动容器前先构建镜像 --abort-on-container-exit 如果任何容器停止，就停止所有容器，与-d不兼容 -t, --timeout TIMEOUT 设置容器几秒后关闭（默认10s） -V, --renew-anon-volumes 重建匿名卷而不是从以前的容器中恢复数据。 --remove-orphans 删除服务的compose文件中未定义的容器 --exit-code-from SERVICE 返回指定服务的退出码 Implies --abort-on-container-exit. --scale SERVICE=NUM 将SERVICE扩展到NUM个实例。会覆盖Compose文件中的&quot;scale&quot;设置（如果存在） Compose 文件格式Compose 文件采用 YAML，文件名以.yml或.yaml结尾，默认应存放在项目的根目录中，文件名应为docker-compose.yml。在 Compose 文件中无需再指定 Dockerfile 中已定义的项。 文件格式为 3.7，所以 compose 文件最开始要写上version: &quot;3&quot; 然后定义服务services，在services:下添加服务，开始对服务的配置。 build：用于指定在构建时应用的配置选项。 context：用于指定构建上下文。 dockerfile：用于指定 Dockerfile 文件 args：用于给 Dockerfile 文件中ARG定义的参数传参 version: &quot;3&quot;services: webapp: build: ./dir 可以这样直接指定上下文路径 webapp: build: 也可以作为具有在上下文中指定的路径的对象 context: ./dir 然后通过context指定上下文路径 当提供的值是相对路径时，context被解释为相对于Compose文件的位置。此目录也是发送到Docker daemon的构建上下文 build: context: . dockerfile: webapp.dockerfile 还可以指定Dockerfile文件 args: 可以为Dockerfile文件传参 args1: 123 args2: 345 也可以这样表示： args: - args1=123 - args2=345 image: webapp:tag 可以指定构建镜像，会生成一个名为webapp，并打上tag标签的镜像 在群集模式下使用Compose文件（版本3）部署堆栈时，将忽略image选项。 docker stack命令仅接受预先构建的图像。 注：YAML 布尔值（true，false，yes，no，on，off）必须用引号括起来，以便解析器将它们解释为字符串。 cache_from：指定 Docker 引擎用于实现缓存的镜像列表 labels：使用标签将元数据添加到生成的镜像中，可使用数组或字典 shm_size：为构建的容器设置/dev/shm分区的大小，指定字节数或字节值字符串，如2mb或2000000 target：根据 Dockerfile 中的定义构建指定的阶段 build: context: . target: prod cap_add和cap_drop：添加或删除容器功能 cap_add: - ALLcap_drop: - NET_ADMIN - SYS_ADMIN command：覆盖容器启动后默认执行的命令 command: bundle exec thin -p 3000或使用列表表示：command: [&quot;bundle&quot;, &quot;exec&quot;, &quot;thin&quot;, &quot;-p&quot;, &quot;3000&quot;] container_name：自定义该容器名称。由于 Docker 容器名称必须是唯一的，因此如果指定了自定义名称，则无法将服务扩展到多个容器 volumes：卷挂载路径设置。格式：宿主机源路径:容器目的路径[:访问权限]，默认访问权限为读写。可使用相对路径，相对于 compose 文件所在目录。 links：链接到另一个服务中的容器，格式：服务名[:别名] external-links：链接到 docker-compose.yml 外部的容器，甚至并非 Compose 管理的容器。 expose：暴露端口，但不映射到宿主机，只被连接的服务访问。最好使用字符串表示数字，因为 YAML 会解析xx:yy这种数字格式为 60 进制，容器端口小于 60 可能出错。 ports：暴露端口信息，格式：[宿主机IP:][端口:]容器端口，可用-表示一个端口范围 docker-compose 示例官方 wordpress 案例只需要一个docker-compose.yml文件即可 version: &quot;3.3&quot;services: db: image: mysql:5.7 volumes: - db_data:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: wordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: depends_on: - db image: wordpress:latest ports: - &quot;8000:80&quot; restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress WORDPRESS_DB_NAME: wordpressvolumes: db_data: &#123;&#125; 然后通过 8000 端口访问。命令docker-compose down删除容器和默认网络，但保留 WordPress 数据库。 命令docker-compose down --volumes删除容器，默认网络和 WordPress 数据库。 官方 Django 案例配置 Dockerfile FROM pythonENV PYTHONUNBUFFERED 1RUN mkdir /codeWORKDIR /codeCOPY requirements.txt /code/RUN pip install -r requirements.txtCOPY . /code/ 配置 requirements.txt Django&gt;=2.0psycopg2&gt;=2.7 配置 compose 文件 version: &quot;3&quot;services: db: image: postgres web: build: . command: python manage.py runserver 0.0.0.0:8000 volumes: - .:/code ports: - &quot;8000:8000&quot; depends_on: - db 创建 Django 项目 docker-compose run web django-admin startproject djangoapp . 启动后，django 项目就创建完成了，当前目录就是 django 的根目录，编辑settings.py，修改数据库配置 DATABASES = &#123; &#x27;default&#x27;: &#123; &#x27;ENGINE&#x27;: &#x27;django.db.backends.postgresql&#x27;, &#x27;NAME&#x27;: &#x27;postgres&#x27;, &#x27;USER&#x27;: &#x27;postgres&#x27;, &#x27;HOST&#x27;: &#x27;db&#x27;, &#x27;PORT&#x27;: 5432, &#125;&#125; 同时根据 compose 文件中的暴露地址修改 settings 中的 ALLOWED_HOSTS ALLOWED_HOSTS = [&#x27;0.0.0.0&#x27;] 然后docker-compose up启动，通过配置的 8000 端口访问 $ docker-compose ps Name Command State Ports---------------------------------------------------------------------------------djangoapp_db_1 docker-entrypoint.sh postgres Up 5432/tcpdjangoapp_web_1 python manage.py runserver ... Up 0.0.0.0:8000-&gt;8000/tcp$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES17743c2c1145 djangoapp_web &quot;python manage.py ru…&quot; 10 minutes ago Up 17 seconds 0.0.0.0:8000-&gt;8000/tcp djangoapp_web_14a5e0f4f5559 postgres &quot;docker-entrypoint.s…&quot; 43 minutes ago Up 18 seconds 5432/tcp djangoapp_db_1 官方 Flask 案例创建app.py import timeimport redisfrom flask import Flaskapp = Flask(__name__)cache = redis.Redis(host=&#x27;redis&#x27;, port=6379)def get_hit_count(): retries = 5 while True: try: return cache.incr(&#x27;hits&#x27;) except redis.exceptions.ConnectionError as exc: if retries == 0: raise exc retries -= 1 time.sleep(0.5)@app.route(&#x27;/&#x27;)def hello(): count = get_hit_count() return &#x27;Hello World! I have been seen &#123;&#125; times.\\n&#x27;.format(count) 配置requirements.txt flaskredis 创建 Dockerfile FROM python:3.7-alpineWORKDIR /codeENV FLASK_APP app.pyENV FLASK_RUN_HOST 0.0.0.0RUN apk add --no-cache gcc musl-dev linux-headersCOPY requirements.txt requirements.txtRUN pip install -r requirements.txtCOPY . .CMD [&quot;flask&quot;, &quot;run&quot;] 创建 compose 文件 version: &quot;3&quot;services: web: build: . ports: - &quot;5000:5000&quot; redis: image: &quot;redis:alpine&quot; 然后docker-compose up启动即可，通过 5000 端口访问也可添加 volume，存储项目代码 version: &quot;3&quot;services: web: build: . ports: - &quot;5000:5000&quot; volumes: - .:/code environment: FLASK_ENV: development redis: image: &quot;redis:alpine&quot; 参考文章 Docker 三剑客之 Compose-一 Docker 三剑客之 Compose-二 Docker 三剑客之 Compose-三 docker-compose 教程（安装，使用, 快速入门） docker-compose 官方文档 Docker 系列之（五）：使用 Docker Compose 编排容器","categories":[{"name":"云计算","slug":"云计算","permalink":"https://coconutmilktaro.top/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://coconutmilktaro.top/tags/docker/"},{"name":"docker-compose","slug":"docker-compose","permalink":"https://coconutmilktaro.top/tags/docker-compose/"},{"name":"云计算","slug":"云计算","permalink":"https://coconutmilktaro.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"容器编排","slug":"容器编排","permalink":"https://coconutmilktaro.top/tags/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"}]},{"title":"Kubernetes","slug":"Kubernetes","date":"2018-07-13T12:33:37.000Z","updated":"2022-06-28T18:31:02.499Z","comments":false,"path":"2018/Kubernetes/","link":"","permalink":"https://coconutmilktaro.top/2018/Kubernetes/","excerpt":"","text":"Kubernetes 概述Kubernetes 协调一个高可用的计算机集群，这些计算机连接起来作为一个单元工作，以更有效的方式自动化跨集群分发和调度应用程序容器。kubernetes 具有完备的集群管理能力，包括多层次的安全防护和准入机制、多租户应用之称能力、透明的服务注册和服务发现机制、内建智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制，以及多粒度的资源配额管理能力。因此，K8s 是一个全新的基于容器技术的分布式架构解决方案以及一个一站式完备的分布式系统开发和支撑平台。 在 K8s 中，服务（Service）是分布式集群架构的核心。一个 Service 对象的关键特征： 一个唯一指定的名字 一个虚拟 IP（Cluster IP、Service IP、VIP）和端口号 提供某种远程服务能力 被映射到提供这种服务能力的一组容器应用上 Service 服务进程都基于 Socket 通信对外服务，一个 Service 通常由多个相关服务进程提供服务，每个服务进程都有独立 Endpoint 访问点，但是 k8s 能让我们通过 service 连接到指定的服务。service 一旦创建就不再变化，意味着不需要关心服务 IP 的变化了。 k8s 通过 pod 对象，将每个服务进程包装到相应 pod 中，使其成为 pod 中的一个容器。为了简历 service 和 pod 的映射关系，k8s 首先给每个 pod 打上标签 label，然后给相应的 service 定义标签选择器（label selector）。这样，service 便能选择所有符合标签选择器的 pod。 pod 运行在节点中，一个节点可运行几百个 pod。每个 pod 中有两种容器，一种是 Pause 容器，只有一个，一种是业务容器，业务容器共享 Pause 容器和 Volume 挂载卷，使得数据交换更加高效，因此可以在设计时将多个密切相关的服务进程放入同一个 pod。并不是每个 pod 和里面运行的容器都能映射到一个 service，只有提供服务的一组 pod 才会被映射成一个服务。 K8s 集群节点的两种角色：Master 管理节点和 Nodes 工作节点。 每个节点都有一个 Kubelet，它是管理节点并与 Master 通信的代理。 该节点还应具有用于处理容器操作的工具，例如 Docker 或 rkt。 处理生产流量的 Kubernetes 集群应至少有三个节点。 节点使用主服务器公开的 Kubernetes API 与 Master 进行通信。用户还可以直接使用 Kubernetes API 与群集进行交互。 K8s 对外提供容器服务偏向于 Mesos 方式，即用户提交容器集群运行所需要的资源的申请（通常是一个配置文件），然后有 k8s 负责完成这些容器的调度，自动为容器选择运行的宿主机。 K8S 的功能： 自动化容器的部署和复制 ，控制器维护 pod 副本数量，保证一个 pod 或一组同类 pod 数量始终可用 随时扩展或收缩容器规模，弹性伸缩，自动缩放 pod 副本数 将容器组织成组（pod），并且提供容器间的负载均衡，集群服务入口为 ClusterIP 服务发现与注册，可使用环境变量或 DNS 插件保证容器中程序发现 pod 入口 使用数据卷，实现 pod 间共享数据 应用程序健康检查，保证健壮性 很容易地升级应用程序容器的新版本，滚动更新，服务不中断，一次更新一个 pod 服务编排，通过文件描述部署服务 资源监控，node 节点集成 cAdvisor 资源收集，通过 Heapster 汇总整个集群节点资源数据，存储到 InfluxDB 提供认证和授权，支持属性访问控制（ABAC）、角色访问控制（RBAC） k8s 结构与组件Master节点组件：Master 也可以叫做控制平面（Control Plane） kube-apiserver：K8s API，集群的统一入口，以 HTTP API 提供接口服务，所有对象资源的增删改查和监听工作都交给 API Server 处理，再交给 etcd 存储（可选），是集群控制的入口进程。是无状态应用，可运行多个以用于平衡实例请求的流量。 kube-controller-manager：控制器管理，k8s 资源对象的自动化控制中心，可理解为资源对象的大总管。处理集群中常规后台任务，一个资源对应一个控制器。 kube-scheduler：用于 watch 监听 apiserver 的资源变动，并根据调度算法调度到合适的后端节点，从而创建 pod。 Master 流程概述：用户通过 API、web、CLI 向 apiserver 发送请求，kube-scheduler 监听 apiserver 的资源变动，同时通过调度算法从 node 中选出最适合的节点开始调度，并将调度的结果存在 etcd 中。 Node节点组件：Node 也可以叫做数据平面（Data Plane） kubelet：Master 在 Node 上的 Agent，管理本机运行的容器的生命周期，如创建容器、挂载数据卷，获取节点状态等工作，将每个 pod 转换为一组容器。 kube-proxy：在 Node 节点上实现 pod 网络代理，维护网络规划和四层负载均衡工作，实现k8s service 的通信和负载均衡。 Container-Runtime：底层容器引擎，如 docker、rkt 等。 Node 流程概述：kubelet 监听 apiserver 的资源变动，在符合的 node 上通过 kubelet 调用相关 docker 进行后续打包构建。Node 节点能在运行期间动态添加到 k8s 集群中，在默认情况下 kubelet 会向 Master 注册自己，这也是 Kubernetes 推荐的 Node 管理方式。一旦 Node 被纳入集群管理范围， kubelet 进程就会定时向 Master 节点汇报自身的情报，例如操作系统、 Docker 版本、机器的 CPU 和内存情况，以及当前有哪些 Pod 在运行等 ， 这样 Master 可以获知每个 Node 的资源使用情况，并实现高效均衡的资源调度策略。而某个 Node 超过指定时间不上报信息时，会被 Master 判定为“失联”， Node 的状态被标记为不可用（ Not Ready ），随后 Master 会触发“工作负载大转移”的自动流程 。 第三方服务： etcd：分布式键值存储系统，用于保持集群状态，如 pod、service 对象信息。 cloud-controller-manager：K8s 与云厂商提供的服务能力对接的关键组件，也称 k8s cloudprovider。 k8s 基础对象k8s 提供如 Pod、Service、Namespace、Volume 的基础对象 pod：最小的部署单元，包含一组容器和卷。同一个 Pod 里的容器共享同一个网络命名空间，可以使用 localhost 互相通信。Pod 是短暂的，不是持续性实体。 service：是一个应用服务抽象，定义了pod 逻辑集合和访问这个 pod 集合的策略，对外提供一个访问入口，会有一个集群的 IP 地址，会将目的是该 IP 的请求负载均衡到 pod 的容器。 volume：存储卷，支持很多类型的存储系统，如分布式存储、临时存储、网络存储等 namespace：名称空间，即资源的作用域 label：标签，用于区分对象（pod，service），每个对象可以有多个标签，可通过标签关联对象。 基于基本对象的更高层次抽象，称为 Controller 控制器，提供额外的功能。 ReplicaSet：下一代的 Replication Controller。确保任何给定时间指定的 pod 副本数量，并提供声明式更新等功能。 Replication Controller：确保任意时间都有指定数量的 Pod 副本在运行。能根据指定的副本数量动态增加或删除副本。ReplicaSet 和 Replication Controller 的区别：前者支持新的基于集合的标签，后者仅支持基于等式的标签 Deployment：管理 ReplicaSets 和 pod，提供声明式更新等功能。官方建议用 Deployment 管理 ReplicaSets，而不是直接使用 ReplicaSets。 StatefulSet：适合持久化的应用，具有唯一的网络标识符（IP 地址）、持久存储、有序部署、扩展、删除和滚动更新。 DaemonSet：确保所有或一些节点运行同一个 pod，当节点加入 k8s 集群中，pod 会被调度到该节点上运行，当节点从集群中删除时，pod 也会被删除。 Job：一次性任务，运行完成后 pod 销毁。 Pod每个 Pod 都有一个特殊的被称为“根容器”的 Pause 容器 。 Pause 容器对应的镜像属于 Kubenetes 平台的一部分，除了 Pause 容器，每个 Pod 还包含一个或多个紧密相关的用户业务容器。 设置该 Pause 容器的目的： 引入与业务无关且不宜死亡的 Pause 容器作为 pod 的根容器，以它的状态代表整个容器组的状态。 pod 的多个业务容器共享 Pause 容器的 IP，共享 Pause 容器挂接的 Volume，既简化了密切关联的业务容器之间的通信问题，也解决了它们之间的文件共享问题。 k8s 为每个 pod 分配一个 pod IP，pod 的容器组共享 pod IP，k8s 要求底层网络支持集群内任意两个 pod 之间的 TCP/IP 直接通信，通常通过虚拟二层网络实现（如 Flannel、Open vSwitch）。在 k8s 中，一个 pod 里的容器能直接与另一主机上的 pod 的容器通信。 pod 类型： 普通 pod：一旦被创建，就会放入 etcd 存储，随后被 k8s master 调度到某个具体 node 上并进行绑定，该 pod 被对应 Node 上的 kubelet 进程实例化成一组相关 docker 容器并启动。若 pod 中某个容器停止，则 k8s 会直接重启这个 pod，若 node 宕机，则 k8s 会将 node 上所有 pod 重新调度到其他 node。 静态 pod：没有被存放在 etcd 中，而是存放在某个具体的 node 上一个文件中，只能在此 node 上运行启动。 Pod Volume：被定义在 pod 上，然后被各自的容器挂载到自己的文件系统中，能实现一些扩展。 每个 pod 都可以对其能使用的计算资源设置限额，包括 CPU 和内存，cpu 的资源单位为 cpu 数量，在 k8s 中以千分之一的 cpu 配额为最小单位（m 表示），cpu 配额是绝对值。通常一个容器的 cpu 配额被定义为 100-300m，即占用 0.1-0.3 个 cpu。内存配额也是绝对值，单位是内存字节数。 在 k8s 中，进行配额需设置两个参数： Requests：该资源的最小申请量，系统必须满足要求 Limits：该资源的最大允许使用量，不能突破，当容器试图超过这个量时，k8s 会杀掉该容器并重启 通常会把 Requests 设置为一个较小的数值，符合容器平时负载的要求，把 Limits 设为峰值负载下最大资源占用量。 例：spec: containers: - name: db image: mysql resources: requests: memory: &quot;64Mi&quot; cpu: &quot;250m&quot; limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; 在 Kubernetes 中运行的 Pod 正在一个私密的隔离网络上运行。默认情况下，它们可以从同一个 kubernetes 集群中的其他 pod 和服务中看到，但不能在该网络之外。当我们使用 kubectl 时，我们通过 API 端点进行交互以与我们的应用程序进行通信。 kubectl 命令可以创建一个代理，将代理转发到群集范围的专用网络。 Labellabel 标签时一个 key-val 键值对，可被附加到各种资源对象上，一个资源对象可定义任意数量 label，同个 label 可被添加到任意数量资源对象上。label 通常在资源对象定义时确定，也可在对象创建后动态添加或删除。 常见 label 示例： 版本标签：release: stable、release: canary 环境标签：environment: dev、environment: qa、environment: production 架构标签：tier: frontend、tier: backend、tier: middleware 分区标签：partition: customerA、partition: customerB 质量管控标签：track: daily、track: weekly 通过 Label Selector（标签选择器）查询与筛选指定 label 的资源对象，实现了类似 sql 的简单查询机制。有两种 label selector 表达式： 基于等式：例：name=xxx（具有标签）和env!=xxx（不具有标签） 基于集合：例：name in (xxx,xxxx)（具有标签）和name not in (xxx,xxxx)（不具有标签） 多个表达式之间用,分隔。 管理对象 RC 和 Service 通过 selector 字段设置关联 pod 的 label 例：# podapiVersion: v1kind: Podmetadata: name: web-1 labels: app: web-1# 管理对象apiVersion: v1kind: ReplicationController # 或 Servicemetadata: name: web-2spec: replicas: 1 selector: app: web-1 ..... 其他管理对象如 Deployment、ReplicaSet、DaemonSet、Job 可通过 Selector 使用集合筛选条件matchLabels定义一组 label，和直接写在selector中一样。matchExpressions定义一组基于集合的筛选条件，可用的条件运算符包含In、NotIn、Exists、DoesNotExists。若同时设置了matchLables和matchExpressions，则为 AND 关系。 selector:matchLabels:app: mywebmatchExpressions: - &#123; key: tier, operator: In, values: [frontend] &#125; - &#123; key: environment, operator: NotIn, values: [dev] &#125; label selector 的常见使用场景： kube-controller 在 RC 上定义的 label selector 来筛选要监控的 pod 副本数量，使 pod 副本数量始终符合预期 kube-proxy 通过 Service 的 label selector 选择对应 pod，自动建立每个 Service 到对应 pod 的请求转发路由表，实现 Service 的智能负载均衡 对某些 node 定义特殊 label，并在 pod 定义文件中使用 NodeSelector 标签调度策略，kube-scheduler 可实现 pod 定向调度的特性。 Replication Controller用于声明某种 pod 的副本个数在任何时刻都符合某个预期值。RC 包含以下部分： Pod 的期望数量 筛选 Pod 的 Label Selector 当 Pod 副本数量小于预期时，用于创建新 Pod 的 Pod 模板 例：......spec: replicas: 2 selector: version: v2 ...... 当定义一个 RC 并提交到 K8s 集群后，Master 上的 Controller Manager 就会得到通知，定期巡检系统中存活的目标 Pod，确保目标 Pod 实例数量刚好等于 RC 的期望值，若超出则停掉些 Pod，若不足则新建些 Pod。可通过修改 RC 的期望值，实现 Pod 的动态缩放（Scaling）。删除 RC 不会影响该 RC 已创建的 Pod，若要删除所有指定 Pod，可以将replicas设为 0 并更新即可。 滚动升级（Rolling Update）：旧版本的 Pod 每停止一个，就同时创建一个新版本的 Pod，通过 RC 就很容易实现。 RC 的升级版 Replica Set，支持两种 Label Selector，但目前很少单独使用，主要是被 Deployment 调用，从而形成一套 Pod 创建、删除、更新的编排机制。RepliaSet 和 Deployment 逐渐替代了 RC。 DeploymentDeployment 用于更好解决 Pod 的编排问题，为此，Deployment 在内部使用了 ReplicaSet 来实现。Deployment 能让我们随时知道 Pod 的部署进程。 Deployment 的典型应用场景： 创建一个 Deployment 对象来生成对应的 ReplicaSet 并完成 Pod 副本创建 检查 Deployment 状态来查看部署是否完成（Pod 是否达到指定数量） 更新 Deployment 创建新 Pod 若当前 Deployment 不稳定，则回滚到上一个 Deployment 版本 暂停 Deployment 以便一次性修改多个 PodTemplateSpec 配置项，之后再恢复 Deployment 进行新发布 扩展 Deployment 应对高负载 查看 Deployment 状态，了解发布是否成功 清除不再需要的旧版本 ReplicaSet StatefulSet很多服务，尤其是中间件集群，如 Mysql、MongoDB、Akka、Zookeeper 等，都是有状态的。这些集群有以下共同点： 每个节点都有固定的身份 ID，通过 ID 集群中成员可互相发现并通信 集群规模是比较固定的，集群规模不能随意变动 集群中的每个节点都是有状态的，通常会持久化数据到永久存储中 若磁盘损坏，则集群中某个节点无法正常运行，集群功能受损 StatefulSet 可以看作 Deployment/RC 的一个特殊变种，有以下特性： StatefulSet 中每个 Pod 都有稳定、唯一的网络标识，可用于发现集群中其他成员 StatefulSet 控制的 Pod 副本的启停顺序是受控的，操作第 n 个 pod 时，前 n-1 个 Pod 已经运行且准备好 StatefulSet 中 pod 采用稳定的持久化存储卷，通过 PV 或 PVC 实现，删除 Pod 默认不会删除与 StatefulSet 相关的存储卷 StatefulSet 除了与 PV 卷捆绑使用存储 pod 状态数据，还要与 Headless Service 配合使用，每个 StatefulSet 定义都要声明属于哪个 Headless Service。 Headless Service 和普通 Service 的区别在于：Headless Service 没有 ClusterIP。若解析 Headless Service 的 DNS 域名，则返回的是该 Service 对应所有的 Pod 的 Endpoint 列表 StatefulSet 在 Headless Service 基础上又为 Headless Service 控制的每个 Pod 实例创建一个 DNS 域名，格式为： $(podname).$(Headless Service name) Service每个 Service 就算是一个微服务。Service 定义一个服务的访问入口，前端的应用 pod 通过该入口地址访问其背后的一组 pod 副本组成的集群，而 Service 通过 Label Selector 与后端 pod 对接。最终系统由多个提供不同业务能力又相互独立的微服务单元组成，服务间通过 TCP/IP 网络通信，形成强大的弹性网络。 客户端如何访问由多个 Pod 副本组成的集群？通过 node 上的 kube-proxy 进程，kube-proxy 是一个智能的负载均衡器，负责把对 Service 的请求转发到后端的某个 Pod 实例上，并在内部实现服务的负载均衡和会话保持，且 Service 没有共用一个负载均衡器的 IP，每个 Service 都被分配一个全局唯一虚拟 IP，称为 Cluster IP，导致每个服务变成具备唯一 IP 地址的节点，服务调用变成了 TCP 网络通信。Service 一旦创建，k8s 就自动为它分配一个可用 Cluster IP，且在 Service 的整个生命周期中，Cluster IP 不会发生改变。所以服务发现只要用 Service 的 Name 和 Cluster IP 做 DNS 映射即可。 kind: ServiceapiVersion: v1metadata:name: Service Namespec:selector:app: Selector Labeltype: LoadBalancer | ClusterIP | NodePortports: - name: name-of-the-port port: 80 targetPort: 8080 # 提供服务的容器内暴露的端口。若不指定targetPort，则默认targetPort和Port相同 ClusterIP：Service 通过 Cluster 内部 IP 对外提供服务，只有 Cluster 内的节点和 Pod 可访问。默认为 ClusterIP NodePort：Service 通过 Cluster 节点的静态端口对外提供服务。CLuster 外部可直接通过:访问服务。若不指定 nodePort 参数，则 k8s 会从 30000-32767 中选一个作为端口号。 LoadBalancer：Service 利用 Cloud Provider 特有的 Load Balancer 对外提供服务，Cloud Provider 负责将 Load Balancer 的流量导向 Service k8s 服务支持多个 Endpoint，并要求每个 Endpoint 都定义一个名称来区分。 ports: - name: name-of-the-port port: 80 targetPort: 80 - name: name-of-the-port port: 808 targetPort: 8080 Cluster IP 是一种虚拟 IP，但更像一个伪造的 IP，原因如下： Cluster IP 仅作用于 Service 对象，并由 K8s 管理分配 IP 地址（来自 Cluster IP 地址池） Cluster IP 无法被 ping，因为没有一个实体网络对象来响应 Cluster IP 只能结合 Service Port 组成一个具体的通信端口，单独的 Cluster IP 不具备 TCP/IP 通信基础，并且属于 k8s 集群这个封闭的空间，外部若要访问该端口，需要做额外工作 k8s 集群内，Node IP 网、Pod IP 网与 Cluster IP 网之间通信采用 k8s 自己的特殊路由规则，不同于传统 IP 路由。 因为 Cluster IP 是集群内部的地址，外界无法直接访问该地址，所以若有要提供给外界的服务，需要用添加 NodePort 参数 kind: ServiceapiVersion: v1metadata:name: tomcat-servicespec:selector:tier: frontendtype: NodePortports: - port: 8080 nodePort: 8888 # 定义了NodePort，则外界可通过Node IP:nodePort 访问tomcat服务 NodePort 实现方式：在 K8s 集群的每个 Node 上都为需要外部访问的 Service 开启一个对应的 TCP 监听端口（kube-proxy 进程开的），外部系统只要用任意一个 Node IP+NodePort 即可访问该服务。 若集群中有多个 Node，则需要借助负载均衡器，外部请求访问负载均衡器 IP，由负载均衡器转发流量到某个 Node 的 Nodeport 上。对于每个 Service，通常要配置一个对应的 Load Balancer，k8s 提供了自动化创建方案，只要将 spec.type 的值设为 LoadBalancer，k8s 就会自动创建一个对应的负载均衡器，并返回它的 IP 地址供外部访问。 Job批处理任务通常并行启动多个计算进程处理一批工作项，处理完成后整个批处理任务结束。可通过 k8s Job 启动一个批处理任务。Job 也是一种特殊的 pod 副本自动控制器，但与 RC 的区别如下： job 控制的 pod 副本是短暂运行的，且不能自动重启（RestartPolicy 都被设为了 Never）。k8s 也提供 CronJob，能够反复定时执行某批处理任务。 job 控制的 pod 副本的工作模式能够多实例并行计算 Volumevolume 是 Pod 中能被多个容器访问的共享目录，被定义在 pod 上，被该 pod 的容器挂载到各自的具体目录，Volume 的生命周期也与 Pod 一致，与容器不相关，即 pod 中容器重启停止不影响 volume，K8s 也支持多种类型 Volume。 volumes: - name: name of the volume mountPath: Path to mountcontainers: - image: image name: my-name volumeMounts: - name: Name of the Volume mountPath: Path to mount k8s 提供的 volume 类型： emptyDir：在 Pod 分配到 Node 时创建的，初始内容为空，无须指定宿主机上的目录文件。可用作临时空间、中间过程 CheckPoint 的临时保存目录、一个容器需从另一容器中获取数据的目录 hostPath：在 Pod 上挂载宿主机上的文件或目录。可用于容器的应用日志文件永久保存、定义 hostPath 为宿主机的/var/lib/docker使容器内应用能访问 docker 文件系统。 需要注意：不同 node 上相同配置的 Pod 可能因为宿主机目录和文件不同而对 volume 的目录文件访问结果不一致。若使用资源配额管理，则 k8s 无法将 hostPath 在宿主机上的资源纳入管理volumes: - name: Volume Name hostPath: path: Path to mount gcePersistentDisk：使用谷歌公有云的永久磁盘(PersistentDisk，PD)，PD 上的内容会被永久保存。Pod 被删除也只是 PD 卸载，而不是删除。 awsElasticBlockStore：使用亚马逊公有云的 EBS Volume NFS例：volumes: - name: nfs nfs: server: nfs-server.localhost path: &quot;/&quot; iSCSI：iSCSI 的目录挂载到 Pod 中 flocker：用 Flocker 管理存储卷 GlusterFS：用 GlusterFS 的目录挂载到 Pod 中 RBD：用 Ceph 块设备共享存储（Rados Block Device）挂载到 Pod 中 gitRepo：挂载一个空目录，从 git 库中 clone 一个仓库供 pod 使用 secret：一个 Secret Volume 为 Pod 提供加密信息 Persistent VolumePersistent Volume（简称 PV）可被理解为 k8s 集群中某个网络存储对应的一块存储 PV 只能是网络存储，不属于任何 Node，但可在 Node 上访问 PV 不是被定义在 Pod 上的，而是独立于 Pod 定义的 PV 比 Volume 支持更多的存储类型 Namespace命名空间多用于实现多租户资源隔离，将集群内部的资源对象分配到不同的 Namespace 中，逻辑上形成分组的不同项目、小组、用户组等，不同组共享集群资源同时还能被分别管理。 集群启动后会自动创建一个叫 default 的 Namespace，若不指明 namespace，则创建的 Pod 等资源就使用该 namespace。 apiVersion: v1kind: Namespacemetadata:name: name AutonationAutonation 注解，是用户任意定义的附加信息，以便外部工具查找。 通常 Annotation 记录： build 信息、release 信息、Docker 镜像信息等 日志库、监控库、分析库等资源库的地址信息 程序调试工具信息 团队联系信息 ConfigMap问题：如何在运行时修改配置文件中的内容？常规想法：通过 Docker Volume 将主机上的配置文件映射到容器中引出问题：这种方法必须在主机上先创建配置文件，才能映射到容器。若是在分布式环境下，配置文件的管理与一致性很难控制。K8S 解决方案：所有配置项都当作 key-value 字符串，作为 Map 表中的一个项，整个 Map 的数据可被持久化存储在 k8s 的 Etcd 中，然后提供 API 方便 k8s 组件或应用操作这些数据，这个 Map 就是 ConfigMap 资源对象。接着，K8S 提供一种内建机制，将存储在 etcd 中的 ConfigMap 通过 Volume 映射变成 Pod 内的配置文件，不论 Pod 调度到哪个主机，都能自动完成映射。若 ConfigMap 中的键值对改变，则 Pod 上的配置文件也会更新。 ConfigMap 典型用法： 生成为容器内的环境变量 设置容器启动命令的启动参数（需设为环境变量） 以 Volume 形式挂载为容器内部的文件或目录 k8s 如何进行版本升级k8s 在声明资源对象时，有个关键属性放在最开头，apiVersion: v1。K8s 采用“核心+外围扩展”的设计思路，在保持平台核心稳定的同时，具备持续演进升级的优势。k8s 大部分常见的核心资源对象都归属于 v1 这个核心 API。随着 k8s 版本升级，一些资源对象会引入新的属性，在不影响当前功能的情况下，有两种做法： 在设计数据库表时，会在每个表增加一个很长的备注字段，之后扩展的数据就以某种格式（xml、json 或简单字符串）放入备注字段，因此表结构没改变，程序风险也小，但不美观 直接修改数据库，增加列，但程序改动大，风险大，虽然看上去美观 于是，k8s 采用先方法 1，再方法 2 的做法。先采用方法 1，等新特性稳定成熟后，采用方法 2 升级到正式版。为此，k8s 为每个资源对象增加了一个类似备注字段的属性Annotations，以实现方法 1 的升级。 例：先是方法1apiVersion: v1kind: podmetadata: ..... annotations: pod.beta.kubernetes.io/init-containers: &#x27;[ &#123; &quot;name&quot;: &quot;init-mydb&quot;, &quot;image&quot;: &quot;busybox&quot;, &quot;command&quot;: [......] &#125; ]&#x27;.....等成熟后采用方法2升级以后apiVersion: v1kind: podmetadata: ......spec: initContainers: - name: init-mydb image: busybox command: - xxx ... K8s 开放接口K8s 开放以下接口，用于对接不同后端，实现不同业务逻辑。 CRI：Container Runtime Interface 容器运行时接口，提供计算服务 CNI：Container Network Interface 容器网络接口，提供网络服务 CSI：Container Storage Interface 容器存储接口，提供存储服务 CRICRI 中定义了容器和镜像的服务的接口，因为容器运行时与镜像的生命周期是彼此隔离的。CRI 包含了 Protocol Buffers、gRPC API、运行库支持以及开发标准规范和工具。CRI 在 kubelet 启动时默认启动。 无论 docker 还是 rkt 都用到了 kubelet 内部接口，导致定制开发难度增加，因此 CRI 接口规范用定义清晰的抽象层清除这一壁垒，当开发者能专注于容器运行时本身。 kubelet 使用 gRPC 框架通过 unix socket 与 CRI 代理（shim）进行通信，这个过程中 kubelet 是客户端，shim 是服务端。 Protocol Buffers 包含两个 gRPC 服务：ImageService、RuntimeService ImageService 提供仓库拉取镜像、查看、移除镜像功能 RuntimeService 负责 Pod 和容器生命周期管理以及与容器交互 目前支持 CRI 的后端： cri-o：cri-o 是 Kubernetes 的 CRI 标准的实现，并且允许 Kubernetes 间接使用 OCI 兼容的容器运行时，可以把 cri-o 看成 Kubernetes 使用 OCI 兼容的容器运行时的中间层。 cri-containerd：基于 Containerd 的 Kubernetes CRI 实现 rkt：CoreOS 开发的容器运行时 frakti：基于 hypervisor 的 CRI docker CNI目前主流容器网络模型主要有 docker 公司提出的 Container Network Model（CNM）和 CoreOS 提出的 Container Network Interface（CNI）。而 k8s 采用的是 CNI 模型。 CNM：主要通过 network sandbox、endpoint、network 三个组件实现 network sandbox：容器内部网络栈，包括网络接口、路由表、DNS 等配置管理。一个 sandbox 能包含多个 endpoint。 endpoint：用于将容器内 sandbox 与外界相连的接口。一般用 veth 对、open vswitch 的内部 port 等技术实现。一个 endpoint 只能加入一个 network network：可直接互联的 endpoint 的集合，通过 linux 网桥、VLAN 等技术实现。一个 network 包含多个 endpoint CNI 由一组用于配置 Linux 容器的网络接口的规范和库组成，同时还包含了一些插件。在 CNI 只涉及两个概念：容器和网络，CNI 仅关心容器创建时的网络分配，和当容器被删除时释放网络资源。该接口只有四个方法，添加网络、删除网络、添加网络列表、删除网络列表。 CNI 插件包含 3 个基本接口定义：添加 ADD、删除 DELETE、检查 CHECK、版本检查 VERSION CNI 的设计考量： 容器运行时必须在调用任何插件之前为容器创建一个新的网络命名空间。然后，运行时必须确定这个容器应属于哪个网络，并为每个网络确定哪些插件必须被执行。 网络配置采用 JSON 格式，可以很容易地存储在文件中。网络配置包括必填字段，如 name 和 type 以及插件（类型）。网络配置允许字段在调用之间改变值。为此，有一个可选的字段 args，必须包含不同的信息。 容器运行时必须按顺序为每个网络执行相应的插件，将容器添加到每个网络中。 在完成容器生命周期后，运行时必须以相反的顺序执行插件（相对于执行添加容器的顺序）以将容器与网络断开连接。 容器运行时不能为同一容器调用并行操作，但可以为不同的容器调用并行操作。 容器运行时必须为容器订阅 ADD 和 DEL 操作，这样 ADD 后面总是跟着相应的 DEL。 DEL 可能跟着额外的 DEL，但是，插件应该允许处理多个 DEL（即插件 DEL 应该是幂等的）。 容器必须由 ContainerID 唯一标识。存储状态的插件应该使用（网络名称，容器 ID）的主键来完成。 给定的容器 ID 必须只能添加到特定的网络一次。 CNI 插件必须支持的操作： 将容器添加到网络 从网络中删除容器 CSICSI 用于在 k8s 和外部存储系统之间建立一套标准的存储管理接口，通过该接口为容器提供存储服务。 k8s 通过 PV、PVC、Storageclass 已经提供了基于插件的存储管理机制，但是这些存储服务都是基于 in-tree 方式提供。因此，k8s 推出与容器对接的存储接口标准 CSI，基于 CSI 的存储插件机制也称为 out-of-tree 方式。 in-tree：存储插件的代码必须放在 k8s 主干代码库才能被 k8s 调用，属于紧耦合开发模式，若存储插件代码出错可能会影响 k8s 的核心组件，存在安全和可靠性问题out-of-tree：存储提供方只需要基于接口标准进行存储插件实现，就能使用 k8s 原生存储机制为容器提供存储服务，实现存储提供方代码与 k8s 彻底解耦 CSI 存储关键组件： CSI Controller：提供存储服务视角对存储资源和存储卷进行管理操作。k8s 推荐将其部署为单实例 Pod，可使用 StatefulSet 和 Deployment 控制器进行部署，设置副本数量为 1，保证为一种存储插件只运行一个控制器实例。 CSI Node：对主机 Node 上的 Volume 进行管理和操作，k8s 建议部署为 DaemonSet，在每个 Node 上都运行一个 Pod k8s 部署要点若是在测试环境： 可使用单 master，单 etcd node 节点按需分配 存储直接用 nfs 或 glusterfs 若是在生产环境： 高可用的 etcd 集群（需定期备份 etc 数据文件），建立 3 或 5 或 7 个节点，保证冗余能力 master 一定要高可用 kube-apiserver 要多实例部署，借助 haproxy、nginx 或 keepalived 实现高可用 kube-scheduler 和 kube-controller-manager 只能有一个活动实例，但可以有多个备用（主备） node：数量越多，冗余和负载能力越强 集群存储建议：Ceph、GlusterFS、iSCSI 及云存储 kubectl启用自动补全。 echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrcsource ~/.bashrc 确认关闭 swap，并将/proc/sys/net/bridge/bridge-nf-call-iptables设为 1。在 Node 上也要这样设置 swapoff -asysctl -w net.bridge.bridge-nf-call-iptables=1 若要设置详细的系统参数，可以添加内容到/etc/sysctl.d/kubernetes.conf中，并sysctl -p /etc/sysctl.d/kubernetes.conf net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1net.ipv4.ip_forward=1net.ipv4.tcp_tw_recycle=0 # tcp_tw_recycle 和 Kubernetes 的 NAT 冲突，必须关闭 ，否则会导致服务不通vm.swappiness=0vm.overcommit_memory=1vm.panic_on_oom=0fs.inotify.max_user_watches=89100fs.file-max=52706963fs.nr_open=52706963net.ipv6.conf.all.disable_ipv6=1 # 不使用的 IPV6 协议栈，防止触发 docker BUGnet.netfilter.nf_conntrack_max=2310720 Kubectl 常用操作 创建资源对象kubectl create -f xxx.yml # 可同时指定多个-f进行一次性创建kubectl create -f &lt;目录&gt; # 创建目录下所有.yml、.json文件定义的对象 查看资源对象kubectl get pods # 查看所有Pod列表kubectl get rc,service # 查看RC和Service列表 描述资源对象kubectl describe nodes &lt;node name&gt; # 显示node的详细信息kubectl describe pods/&lt;pod name&gt; # 显示pod的详细信息kubectl describe pods &lt;rc name&gt; # 显示由RC管理的pod的信息 删除资源对象kubectl delete -f pod.yml # 删除指定pod文件定义的podkubectl delete pods,services -l name=&lt;label name&gt; # 删除指定label的Podkubectl delete pods --all # 删除所有pods 执行容器命令kubectl exec &lt;pod name&gt; &lt;command&gt; # 在pod的容器中执行命令，默认为第一个容器kubectl exec &lt;pod name&gt; -c &lt;container name&gt; &lt;command&gt; # 指定pod中的某个容器执行kubectl exec -ti &lt;pod name&gt; -c &lt;container name&gt; /bin/bash # 登录某个容器 查看容器日志kubectl logs &lt;pod name&gt; # 查看pod容器输出到stdout的日志kubectl logs -f &lt;pod name&gt; -c &lt;container name&gt; # 跟踪查看pod容器的日志，相当于tail -f 创建或更新资源对象kubectl apply -f app.yml # 类似create， 若对象不存在则创建，存在则更新 在线编辑运行中的资源对象kubectl edit deploy nginx # 编辑运行中的资源对象 将 pod 开放端口映射到本地kubectl port-forward --address 0.0.0.0 pod/nginx 8888:80 # 将pod的80端口映射到本地的8888端口 在 pod 和本地之间复制文件kubectl cp pod名:&lt;容器内路径&gt; &lt;宿主机本地路径&gt; 资源对象的标签设置kubectl label namespaces default &lt;labalname=xxx&gt; # 为default namespace设置标签 检查可用 API 资源类型列表kubectl api-resources # 检查特定类型资源是否已经定义，列出所有资源对象类型 查看支持的 API 版本kubectl api-versions 使用命令行插件自定义插件，先编写一个可执行文件，文件名必须为kubectl-&lt;plugin name&gt;，复制到$PATH环境变量指定的目录中，就可通过kubectl &lt;plugin name&gt;执行该自定义插件了。 K8s 集群安全若是在一个安全的内网环境，则 k8s 各组件可通过 http 通信，若是要对外服务，则最好要使用 https。k8s 提供基于 CA 签名的双向数字证书认证方式以及简单的基于 HTTP Base 或 Token 的认证方式。 证书可分为三大类： root CA apiserver：apiserver 自己的证书 apiserver-kubelet-client：kubelet 连接 apiserver 时的客户端证书 etcd CA etcd-server：etcd 服务器端证书 etcd-peer：etcd 对等证书，用于 etcd 集群中 https 通信 etcd-healthcheck-client：etcd 健康检查的客户端证书 apiserver-etcd-client：apiserver 连接 etcd 的客户端证书 front-proxy CA front-proxyserver-client：apiserver 中的聚合器 aggregator 在前端的客户端证书 证书默认存放在/etc/kubernetes/pki中 # tree /etc/kubernetes/pki/etc/kubernetes/pki├── apiserver.crt├── apiserver-etcd-client.crt├── apiserver-etcd-client.key├── apiserver.key├── apiserver-kubelet-client.crt├── apiserver-kubelet-client.key├── ca.crt├── ca.key├── etcd│ ├── ca.crt│ ├── ca.key│ ├── healthcheck-client.crt│ ├── healthcheck-client.key│ ├── peer.crt│ ├── peer.key│ ├── server.crt│ └── server.key├── front-proxy-ca.crt├── front-proxy-ca.key├── front-proxy-client.crt├── front-proxy-client.key├── sa.key└── sa.pub 查看证书过期时间 openssl x509 -in /etc/kubernetes/pki/front-proxy-client.crt -noout -text | grep Not Not Before: Mar 31 09:00:17 2020 GMT Not After : Mar 31 09:00:17 2021 GMT 基于 CA 签名的双向数字证书的生成过程： 为 kube-apiserver 生成一个数字证书，并用 CA 证书签名 为 kube-apiserver 进程配置证书相关的启动参数，包括 CA 证书（用于验证客户端证书签名真伪）、自己的经过 CA 签名的证书以及私钥 为每个访问 kubernetes API Server 的客户端（controller manager、scheduler、kubelet、kube-proxy、kubectl）进程都生成自己的数字证书，并都用 CA 证书签名，在相关启动参数中添加 CA 证书参数 第一步：生成客户端密钥 openssl genrsa -out ca.key 2048 第二步：用私钥为证书请求文件签名，生成证书文件 openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=kubenode1&quot; -days 5000 -out ca.crt其中/CN的值为master的主机名 第三步：生成 apiserver 的私钥 openssl genrsa -out server.key 2048 第四步：创建配置文件master_ssl.cnf，用于 x509 v3 版本的证书 [req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][v3_req]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_name[alt_name]DNS.1 = kubernetesDNS.2 = kubernetes.defaultDNS.3 = kubernetes.default.svcDNS.4 = kubernetes.default.svc.cluster.localDNS.5 = kubenode1 # master hostnameIP.1 = 192.168.60.3 # master IPIP.2 = 192.168.10.1 # kubernetes.default&#x27;s ClusterIP 其中，ClusterIP 可通过命令查看kubectl get svc kubernetes -o yaml # kubectl get svc kubernetes -o yamlapiVersion: v1kind: Servicemetadata: creationTimestamp: &quot;2020-03-31T09:00:34Z&quot; labels: component: apiserver provider: kubernetes .......spec: clusterIP: 192.168.10.1 # 属于init-default.yml中serviceSubnet的网段 ports: - name: https port: 443 protocol: TCP targetPort: 6443 sessionAffinity: None type: ClusterIPstatus: loadBalancer: &#123;&#125; 第五步：创建证书签名请求文件 server.csr 和证书文件 server.crt openssl req -new -key server.key -subj &quot;/CN=kubenode1&quot; -config master_ssl.cnf -out server.csropenssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 5000 -extensions v3_req -extfile master_ssl.cnf -out server.crt 此时目录下有以下相关文件 ├── ca.crt # CA证书├── ca.key # CA私钥├── ca.srl # CA签发证书的序列号记录文件├── server.crt # 服务端证书├── server.csr # 证书签名请求，核心内容是一个公钥└── server.key # 服务端私钥 把这些文件都移动到/var/run/kubernetes中 第六步：设置 kube-apiserver 的启动参数KUBE_API_ARGS KUBE_API_ARGS=&quot;--client-ca-file=/var/run/kubernetes/ca.crt --tls-private-key-file=/var/run/kubernetes/server.key --tls-cert-file=/var/run/kubernetes/server.crt --secure-port=6443 --insecure-port=0&quot; 重启 kube-apiserver 服务 第七步：设置 kube-controller-manager 的客户端证书、私钥和启动参数 openssl genrsa -out cs_client.key 2048openssl req -new -key cs_client.key -subj &quot;/CN=kubenode1&quot; -out cs_client.csropenssl x509 -req -in cs_client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out cs_client.crt -days 5000 第八步：创建/etc/kubernetes/kubeconfig文件，配置客户端证书相关参数 apiVersion: v1kind: Configusers: - name: controllermanager user: client-certificate: /var/run/kubernetes/cs_client.crt client-key: /var/run/kubernetes/cs_client.keyclusters: - name: local cluster: certificate-authority: /var/run/kubernetes/ca.crt server: https://192.168.60.3:6443contexts: - context: cluster: local user: controllermanager name: my-contextcurrent-context: my-context 设置 controller manager 的启动参数，然后重启 kube-controller-manager： --service-account-key-file=/var/run/kubernetes/server.key--root-ca-file=/var/run/kubernetes/ca.crt--kubeconfig=/etc/kubernetes/kubeconfig 第九步：设置 kube-scheduler 启动参数，然后重启 kube-scheduler --kubeconfig=/etc/kubernetes/kubeconfig 第十步：设置每个 Node 上 kubelet 的客户端证书、私钥和启动参数先复制 apiserver 上的 ca.crt 和 ca.key 到 Node 上，生成客户端 crt 文件。其中/CN的值为本 Node 的 IP 地址 openssl genrsa -out kubelet_client.key 2048openssl req -new -key kubelet_client.key -subj &quot;/CN=192.168.60.4&quot; -out kubelet_client.csropenssl x509 -req -in kubelet_client.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 5000 -out kubelet_client.crt 将这些文件都复制到/var/run/kubernetes中 第十一步：创建 Node 上的 kubeconfig 文件，配置客户端证书等参数 apiVersion: v1kind: Configusers: - name: kubelet user: client-certificate: /var/run/kubernetes/kubelet_client.crt client-key: /var/run/kubernetes/kubelet_client.keyclusters: - name: local cluster: certificate-authority: /var/run/kubernetes/ca.crt server: https://192.168.60.3:6443contexts: - context: cluster: local user: controllermanager name: my-contextcurrent-context: my-context kubelet 的启动参数，并重启 kubelet --kubeconfig=/etc/kubernetes/kubeconfig 第十二步：设置 kube-proxy，同上设置启动参数，并重启 kube-proxy 深入理解 PodK8s 对系统中长时间运行的容器的要求为：主程序需要一直在前台执行。若创建一个 Pod 中的 docker 容器执行的命令是在后台运行，则 kubelet 在创建 pod 后运行后台命令，然后认为 pod 执行结束，销毁该 pod。若 pod 定义了 RelicationController，则 pod 销毁后又自动创建，导致不断循环。因此，K8s 需要自己创建 docker 镜像并以一个前台命令作为启动命令。 若无法改造为前台命令，则可以使用 Supervisor 辅助进行前台运行功能 Pod 特征： 通过容器各自的 IPC，使得同个 pod 的容器可在 pod 中通信 同一个 pod 的容器之间可通过 localhost 相互访问，使这一组容器被绑定在了一个环境中 每个容器集成 Pod 的名称 每个 Pod 有一个平滑共享网络名称空间的 IP 地址 Pod 内部共享存储卷 静态 Pod由 kubelet 管理的仅存在于特定 Node 上的 Pod，不能通过 ApiServer 管理，无法与 ReplicationController、Deployment 及 DaemonSet 关联，kubelet 也无法对它们进行安全检查，仅由 kubelet 创建，并在 kubelet 所在主机上运行。 两种创建静态 Pod 的方式：配置文件、HTTP 配置文件方式：设置 kubelet 启动参数--pod-manifest-path或者在 kubelet 配置文件中设置staticPodPath（推荐），kubelet 会自动定期扫描该目录，根据里面的 json 和 yaml 文件进行操作。因为无法通过 APIserver 管理，所以删除 pod 命令并不会删除该 pod，只是将状态变为 Pending。若要删除 pod 一定要到该 Node，把该 pod 配置文件删掉。 HTTP：设置 kubelet 启动参数--manifest-url，会定期从该 URL 下载 Pod 配置文件并创建。 Pod 配置管理创建 ConfigMap 资源对象 yaml 文件方式创建 apiVersion: v1kind: ConfigMapmetadata: name: cm-appvarsdata: apploglevel: info appdatadir: /var/data kubectl create -f cm-appvars.yml查看详情： # kubectl describe configmaps cm-appvarsName: cm-appvarsNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Data====apploglevel:----infoappdatadir:----/var/dataEvents: &lt;none&gt; 或者直接放配置，要注意缩进 apiVersion: v1kind: ConfigMapmetadata: name: cm-appconfigdata: tomcat-server-xml: | &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt; &lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt; &lt;Listener className=&quot;org.apache.catalina.startup.VersionLoggerListener&quot;/&gt; &lt;Listener className=&quot;org.apache.catalina.core.AprLifecycleListener&quot; SSLEngine=&quot;on&quot;/&gt; &lt;Listener className=&quot;org.apache.catalina.core.JreMemoryLeakPreventionListener&quot;/&gt; &lt;Listener className=&quot;org.apache.catalina.mbeans.GlobalResourcesLifecycleListener&quot;/&gt; &lt;Listener className=&quot;org.apache.catalina.core.ThreadLocalLeakPreventionListener&quot;/&gt; &lt;Service name=&quot;Catalina&quot;&gt; &lt;Connector port=&quot;8080&quot; relaxedPathChars=&quot;[]|&quot; relaxedQueryChars=&quot;[]|&#123;&#125;^&amp;#x5c;&amp;#x60;&amp;quot;&amp;lt;&amp;gt;&quot; maxThreads=&quot;150&quot; minSpareThreads=&quot;25&quot; connectionTimeout=&quot;20000&quot; enableLookups=&quot;false&quot; maxHttpHeaderSize=&quot;8192&quot; protocol=&quot;HTTP/1.1&quot; useBodyEncodingForURI=&quot;true&quot; redirectPort=&quot;8443&quot; acceptCount=&quot;100&quot; disableUploadTimeout=&quot;true&quot; bindOnInit=&quot;false&quot;/&gt; ...... &lt;/Service&gt; &lt;/Server&gt; tomcat-loggingproperties: &quot;1catalina.org.apache.juli.AsyncFileHandler.level = FINE 1catalina.org.apache.juli.AsyncFileHandler.directory = $&#123;catalina.base&#125;/logs 1catalina.org.apache.juli.AsyncFileHandler.prefix = catalina. 1catalina.org.apache.juli.AsyncFileHandler.encoding = UTF-8 ...... java.util.logging.ConsoleHandler.level = FINE java.util.logging.ConsoleHandler.formatter = org.apache.juli.OneLineFormatter java.util.logging.ConsoleHandler.encoding = UTF-8&quot; kubectl 直接创建，通过--from-file或--from-literal指定内容，from-file是直接将文件内容作为值，from-literal后面跟着键值对。--from-file kubectl create configmap jira-server-xml --from-file=/opt/atlassian/jira/conf/server.xml --from-literal kubectl create configmap cm-appenv --from-literal=loglevel=info --from-literal=appdir=/var/data 容器对 ConfigMap 的使用有以下两种方法 通过环境变量获取 ConfigMap 内容apiVersion: v1kind: Podmetadata: name: cm-test-pod labels: name: cm-test-podspec: containers: - name: cm-test image: busybox resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env|grep APP&quot;] env: - name: APPLOGLEVEL valueFrom: configMapKeyRef: name: cm-appenv key: loglevel - name: APPDATADIR valueFrom: configMapKeyRef: name: cm-appenv key: appdir restartPolicy: Never # 需要注意：环境变量的名称受POSIX规范约束，不能以数字开头，且不能包含特殊字符 创建完成后，查看 pod# kubectl get podsNAME READY STATUS RESTARTS AGEcm-test-pod 0/1 Completed 0 22s 查看 pod 的输出日志，说明 pod 内已经读取到了# kubectl logs cm-test-podAPPDATADIR=/var/dataAPPLOGLEVEL=info 通过 Volume 挂载的方式将 ConfigMap 的内容挂载为容器内部的文件或目录apiVersion: v1kind: Podmetadata: name: cm-test-pod-2 labels: name: cm-test-pod-2spec: containers: - name: cm-test image: kubeguide/tomcat-app:v1 resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 8080 volumeMounts: - mountPath: /configfiles name: serverxml volumes: - name: serverxml configMap: name: cm-appconfig items: - key: tomcat-server-xml path: server.xml - key: tomcat-loggingproperties path: logging.properties 创建该 pod，然后进入该 Pod 查看，配置文件已添加成功# kubectl exec -it cm-test-pod-2 -- bash# ls /configfiles/logging.properties server.xml 若引用 ConfigMap 时不指定 items，则该方法在容器内的目录下会为每个 item 生成一个文件名为key-&lt;key键值&gt;的文件 ConfigMap 的限制条件： ConfigMap 必须在 Pod 之前创建 ConfigMap 受 NameSpace 限制，只有在相同 Namespace 的 Pod 才能引用 ConfigMap 的配额管理还未能实现 kubelet 只支持可被 apiserver 管理的 pod 使用 ConfigMap。静态 Pod 无法引用 ConfigMap ConfigMap 在 Pod 内只能挂载为目录，且若已存在该目录，则直接覆盖。所以最好将文件挂载在一个临时目录，并通过 cp 或 link 命令将配置移动到实际目录下 在容器内获取 Pod 信息在容器中可通过 Downward API 获取所在 Pod 的信息，仍然是通过环境变量或 Volume 挂载的方式将 Pod 信息注入容器内部可以通过Downward API获取以下信息： 能通过 fieldRef 获得： metadata.name metadata.namespace metadata.uid metadata.labels[&#39;&lt;KEY&gt;&#39;] - Pod 标签 &lt;KEY&gt; 的值 (例如, metadata.labels[&#39;mylabel&#39;]） metadata.annotations[&#39;&lt;KEY&gt;&#39;] - Pod 的注解 &lt;KEY&gt; 的值（例如, metadata.annotations[&#39;myannotation&#39;]） 能通过 resourceFieldRef 获得： 容器的 CPU limit 容器的 CPU request 容器的内存 limit 容器的内存 request 容器的临时存储 limit 容器的临时存储 request 环境变量 将 Pod 信息注入为环境变量 apiVersion: v1kind: Podmetadata: name: dapi-test-pod labels: name: dapi-test-podspec: containers: - name: test-container image: busybox resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP restartPolicy: Never Downward API 提供变量： metadata.name：pod 名 status.podIP：pod IP。（IP 为 status 而非 metadata，是因为 IP 不是元数据，而是状态数据） metadata.namespace：pod Namespace 创建完成后，查看日志 # kubectl logs dapi-test-podPOD_IP=10.38.0.1KUBERNETES_SERVICE_PORT=443KUBERNETES_PORT=tcp://192.168.10.1:443HOSTNAME=dapi-test-podSHLVL=1HOME=/rootPOD_NAME=dapi-test-podKUBERNETES_PORT_443_TCP_ADDR=192.168.10.1PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binKUBERNETES_PORT_443_TCP_PORT=443KUBERNETES_PORT_443_TCP_PROTO=tcpKUBERNETES_SERVICE_PORT_HTTPS=443KUBERNETES_PORT_443_TCP=tcp://192.168.10.1:443POD_NAMESPACE=defaultKUBERNETES_SERVICE_HOST=192.168.10.1PWD=/ 将容器资源信息注入为环境变量 apiVersion: v1kind: Podmetadata: name: dapi-test-pod-container-vars labels: name: dapi-test-pod-container-varsspec: containers: - name: test-container image: busybox command: [&quot;sh&quot;, &quot;-c&quot;] args: - printenv CPU_REQUEST CPU_LIMIT; printenv MEM_REQUEST MEM_LIMIT; resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; env: - name: CPU_REQUEST valueFrom: resourceFieldRef: containerName: test-container resource: requests.cpu - name: CPU_LIMIT valueFrom: resourceFieldRef: containerName: test-container resource: limits.cpu - name: MEM_REQUEST valueFrom: resourceFieldRef: containerName: test-container resource: requests.memory - name: MEM_LIMIT valueFrom: resourceFieldRef: containerName: test-container resource: limits.memory restartPolicy: Never 创建完成后，查看日志 # kubectl logs dapi-test-pod-container-vars11134217728134217728 Volume 挂载 apiVersion: v1kind: Podmetadata: name: kubernetes-downwardapi-volume-example-2spec: containers: - name: client-container image: k8s.gcr.io/busybox:1.24 command: [&quot;sh&quot;, &quot;-c&quot;] args: - while true; do echo -en &#x27;\\n&#x27;; if [[ -e /etc/podinfo/cpu_limit ]]; then echo -en &#x27;\\n&#x27;; cat /etc/podinfo/cpu_limit; fi; if [[ -e /etc/podinfo/cpu_request ]]; then echo -en &#x27;\\n&#x27;; cat /etc/podinfo/cpu_request; fi; if [[ -e /etc/podinfo/mem_limit ]]; then echo -en &#x27;\\n&#x27;; cat /etc/podinfo/mem_limit; fi; if [[ -e /etc/podinfo/mem_request ]]; then echo -en &#x27;\\n&#x27;; cat /etc/podinfo/mem_request; fi; sleep 5; done; resources: requests: memory: &quot;32Mi&quot; cpu: &quot;125m&quot; limits: memory: &quot;64Mi&quot; cpu: &quot;250m&quot; volumeMounts: - name: podinfo mountPath: /etc/podinfo volumes: - name: podinfo downwardAPI: items: - path: &quot;cpu_limit&quot; resourceFieldRef: containerName: client-container resource: limits.cpu divisor: 1m - path: &quot;cpu_request&quot; resourceFieldRef: containerName: client-container resource: requests.cpu divisor: 1m - path: &quot;mem_limit&quot; resourceFieldRef: containerName: client-container resource: limits.memory divisor: 1Mi - path: &quot;mem_request&quot; resourceFieldRef: containerName: client-container resource: requests.memory divisor: 1Mi 进入容器查看： / # ls /etc/podinfo/cpu_limit cpu_request mem_limit mem_request/ # cat /etc/podinfo/cpu_limit250 Pod 生命周期与重启策略Pod 状态如下： Pending：apiserver 已创建该 pod，但 pod 中还有容器镜像没有创建（可能在下载） Running：pod 内容器都已创建，且至少有一个容器在运行、正在启动或重启状态 Succeeded：pod 内容器都成功执行后退出，且不会再重启 Failed：pod 内容器都已退出，但至少有一个容器退出为失败状态 Unknown：无法获取该 pod 状态 当某个容器异常退出或健康检查失败时，kubelet 会根据 RestartPolicy 的设置进行相应操作pod 重启策略如下： Always：当容器失效，由 kubelet 自动重启该容器 OnFailure：当容器终止运行且退出码不为 0 时，由 kubelet 自动重启该容器 Never：不论容器什么状态，kubelet 都不会重启该容器 每种控制器对 Pod 的重启策略要求如下： RC、DaemonSet：必须设为 Always，保证容器持续运行 Job：OnFailure 或 Never，确保容器执行完成后不再重启 kubelet：在 Pod 失效时自动重启，不论将 RestartPolicy 设为什么值，都不会对 Pod 进行健康检查 kubelet 重启失效容器的时间间隔以sync-frequency X 偶数倍计算，最长延时 5min，且在成功重启后的 10min 后重置该时间。 Pod 健康检查和服务可用性检查k8s 通过两类探针检查 pod 健康状态：LivenessProbe、ReadinessProbe。kubelet 定期执行这两类探针诊断容器健康。 LivenessProbe：用于判断容器是否存活（Running），若判定为不健康，则 kubelet 杀死该容器并根据重启策略处理。若容器不包含该探针，则 kubelet 认为该容器的探针返回值永远为 Success ReadinessProbe：用于判断容器服务是否可用（Ready），只有达到 Ready 状态，Pod 才能接受请求。 对于被 Service 管理的 Pod，Service 与 Pod Endpoint 的关联关系也基于 Pod 是否 Ready 进行设置。 若运行过程中 Ready 变为 False，则系统自动将其从 Service 的后端 Endpoint 列表中隔离出去。 若恢复到 Ready，则再将 Pod 加回 Endpoint 列表。 这样就保证客户端在访问 Service 时不会被转发到服务不可用的 Pod 实例上。 LivenessProbe 告诉 K8s 容器是否需要通过重启实现自愈。ReadinessProbe 告诉 K8s 容器是否已准备好加入到 Service 负载均衡池中对外提供服务。LivenessProbe 探测是重启容器，ReadinessProbe 探测是将容器设为不可用，从而不接收 Service 转发的请求。若不特意配置，则 K8s 对两种检查采取相同默认行为，通过判断容器启动进程的返回值是否为 0 判断探测是否成功。 两种探针都有三种实现方式： ExecAction：在容器内部执行一个命令，若该命令的返回码为 0，则表明容器健康apiVersion: v1kind: Podmetadata: name: liveness-exec labels: name: liveness-execspec: containers: - name: liveness-exec image: busybox resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; args: - /bin/sh - -c - echo ok &gt; /tmp/health; sleep 10; rm -rf /tmp/health; sleep 600 livenessProbe: exec: command: # 查看/tmp/health文件是否存在作为是否存活条件 - cat - /tmp/health initialDelaySeconds: 15 timeoutSeconds: 1 结果应为 failed，通过 describe 查看信息，可看到Warning Unhealthy 18s (x6 over 107s) kubelet, kubenode2 Liveness probe failed: cat: can&#x27;t open &#x27;/tmp/health&#x27;: No such file or directoryNormal Killing 18s (x2 over 88s) kubelet, kubenode2 Container liveness-exec failed liveness probe, will be restarted 导致 kubelet 不断杀死并重启 TCPSocketAction：通过容器 IP 和端口号进行 TCP 检查，若能建立 TCP 连接，则说明健康apiVersion: v1kind: Podmetadata: name: liveness-tcp labels: name: liveness-tcpspec: containers: - name: liveness-tcp image: nginx resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 80 livenessProbe: tcpSocket: port: 80 initialDelaySeconds: 30 timeoutSeconds: 1 HTTPGetAction：通过容器的 IP、端口号和 HTTP get，若响应状态码大于等于 200 小于 400，则认为容器健康apiVersion: v1kind: Podmetadata: name: liveness-httpget labels: name: liveness-httpgetspec: containers: - name: liveness-httpget image: nginx resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 80 livenessProbe: httpGet: path: /_status/healthz port: 80 initialDelaySeconds: 30 timeoutSeconds: 1 每种探测方式都需要设置：initialDelaySeconds 和 timeoutSeconds 参数。 initialDelaySeconds：启动容器后进行首次健康检查的等待时间 timeoutSeconds：健康检查发送请求后等待响应的超时时间。若超时则 kubelet 认为该容器无法提供服务，并重启该容器 Pod 调度Deployment 与 RCDeployment 和 RC 的主要功能就是自动部署一个容器应用的多个副本并持续监控副本数量，在集群中始终维持指定的副本数量 apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployspec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 创建 nginx 的 deployment，副本数为 3。创建完成后查看 deployment 状态，以及 ReplicaSet 信息 # kubectl get deployments nginx-deployNAME READY UP-TO-DATE AVAILABLE AGEnginx-deploy 3/3 3 3 49s# kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deploy-d46f5678b 3 3 3 115s Deployment 配置文件 spec 重点参数： replicas：副本数量 tempalte：Pod 模板 template.metadata：Pod 元数据，至少要一个 Label tamplate.spec：Pod 规格，定义 Pod 中每个容器的属性 NodeSelector若在实际情况中需要将 Pod 调度到指定的 Node 上，则可以通过 Node 的标签 Label 和 Pod 的 nodeSelector 相匹配来实现。 首先给 node 打标签 kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt; 例如：给 kubenode2 打上app=redis-master标签 kubectl label nodes kubenode2 app=redis 创建一个 redis 的 deployment，部署到 kubenode2 上 apiVersion: apps/v1kind: Deploymentmetadata: name: redis-masterspec: selector: matchLabels: app: redis-master template: metadata: labels: app: redis-master spec: containers: - name: redis image: redis ports: - containerPort: 6379 nodeSelector: app: redis 创建后查看 pod，确认部署到了 kubenode2 # kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESredis-master-7f84cc5d4d-8mrfr 1/1 Running 0 70s 10.38.0.6 kubenode2 &lt;none&gt; &lt;none&gt; 若多个 Node 都打了相同标签，则 scheduler 在分配时会根据算法选择一个可用的 Node 进行调度。若指定了标签，但是集群中不存在打了该标签的 node，则 Pod 无法成功调度 k8s 目前偏向于发展亲和性调度，随着节点亲和性越来越能表达 NodeSelector 的功能，最终会淘汰掉 NodeSelector。Node 亲和性调度极大扩展了 Pod 的调度能力，调度功能包含：节点亲和性（NodeAffinity）和 Pod 亲和性（PodAffinity）。增强了一些功能： 可使用软限制、优先采用等限制方式，调度器在无法满足优先需求时，会退而求其次继续运行该 pod 可根据节点上正在运行的其他 pod 的标签来进行限制，即可定义一种规则描述 Pod 之间的亲和或互斥关系 NodeAffinity有两种节点亲和性表达： RequiredDuringSchedulingIgnoredDuringExecution：必须满足指定规则才能调度 Pod，类似 NodeSelector，相当于硬限制。 PreferredDuringSchedulingIgnoredDuringExecution：强调优先满足指定规则，但不强求，相当于软限制。还可设置权重，定义先后顺序。 其中 IgnoredDuringExecution 的意思为：若 pod 所在节点在 pod 运行期间标签发生了更改，不再符合亲和性要求，则系统会忽略变化，pod 仍在该节点上运行。 例：创建 pod，只运行在 amd64 的节点，且尽量运行在磁盘为 ssd 的节点 apiVersion: v1kind: Podmetadata: name: node-affinity-test labels: name: node-affinity-testspec: containers: - name: node-affinity-test image: google/pause affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - amd64 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: disk-type operator: In values: - ssd NodeAffinity 语法支持的操作符包括： In NotIn Exists DoesNotExist Gt Lt NodeAffinity 规则注意事项： 若同时定义了 nodeSelector 和 nodeAffinity，则必须两个条件都得到满足时 pod 才能最终运行在指定 node 上 若 nodeAffinity 指定了多个 nodeSelectorTerms，则其中一个能匹配成功即可 若在 nodeSelectorTerms 中有多个 matchExpressions，则一个节点必须满足所有 matchExpressions 才能运行该 Pod PodAffinity根据在节点上正在运行的 Pod 标签而不是节点的标签进行判断和调度，要求对节点和 Pod 两个条件进行匹配。若在具有标签 X 的 node 上运行了一个或多个符合条件 Y 的 Pod，则 Pod 应该运行在该 node 上。X 指一个集群中的节点、区域等概念，通过 k8s 内置节点标签的 key 进行声明，该 key 名字为 topologyKey，意为节点所属 topology 范围。 因为 pod 是属于某个命名空间的，所以 Y 表示的是一个或全部命名空间的一个 Label Selector。pod 亲和性表达和节点亲和性表达是相同的。 先创建一个参考 pod，有两个自定义标签 security 和 app。其中 security 是用于亲和性调度，app 是用于互斥性调度 apiVersion: v1kind: Podmetadata: name: pod-flag labels: name: pod-flag security: s1 app: nginxspec: containers: - name: pod-flag image: nginx 该 pod 在 kubenode2 进行亲和性调度测试 apiVersion: v1kind: Podmetadata: name: pod-affinity labels: name: pod-affinityspec: containers: - name: pod-affinity image: google/pause affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - s1 topologyKey: kubernetes.io/hostname 匹配包含 security=s1 的 pod 的 node。查看创建情况 # kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-affinity 1/1 Running 0 10s 10.38.0.2 kubenode2 &lt;none&gt; &lt;none&gt;pod-flag 1/1 Running 0 5m26s 10.38.0.1 kubenode2 &lt;none&gt; &lt;none&gt; 进行互斥性调度测试 apiVersion: v1kind: Podmetadata: name: pod-anti-affinity labels: name: pod-anti-affinityspec: containers: - name: pod-anti-affinity image: google/pause affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: security operator: In values: - s1 topologyKey: kubernetes.io/hostname podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx topologyKey: kubernetes.io/hostname Taints 与 Tolerations与亲和性相反，Taint（污点）让 Node 拒绝 Pod 的运行。 Taint 需要配合 Toleration（容忍）使用，让 Pod 避开那些不适合的 Node，在 node 设置了 taint 后，除非 node 声明能容忍这些污点，否则无法在这些 node 上运行。Toleration 是 Pod 属性，让 Pod 能运行在标注了 Taint 的 Node 上。 例：给 node 添加污点，键为 key，值为 value，效果为 NoSchedule，除非 Pod 容忍，否则不调度到该 Node # kubectl taint node kubenode2 key=value:NoSchedulenode/kubenode2 tainted 此时创建 deployment 等资源对象时，不会部署到污点 node 上。 # kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-toleration-deployment-5f6d8b47d6-8kk8q 1/1 Running 0 45s 10.32.0.2 kubenode3 &lt;none&gt; &lt;none&gt;nginx-toleration-deployment-5f6d8b47d6-fbqkm 1/1 Running 0 45s 10.32.0.3 kubenode3 &lt;none&gt; &lt;none&gt;nginx-toleration-deployment-5f6d8b47d6-vm7vx 1/1 Running 0 45s 10.32.0.4 kubenode3 &lt;none&gt; &lt;none&gt; 然后在 Deployment 中声明 Toleration，表示允许调度到该污点 Node ...... spec: containers: - name: nginx-toleration-deployment image: nginx resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; ports: - containerPort: 80 tolerations: - key: &quot;key&quot; operator: &quot;Equal&quot; value: &quot;value&quot; effect: &quot;NoSchedule&quot;或者 tolerations: - key: &quot;key&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; 其中键值和 effect 需要和污点设置一致，并满足： operator 值为 Exists，这样无需指定 value operator 值为 Equal，需要指定 value 并一致 若不指定 operator，则默认值为 Equal 空的 key 配合 Exists 能匹配所有键值 空的 effect 能匹配所有的 effect 除了 NoSchedule，还有另外两个 effect PreferNoSchedule：NoSchedule 的软限制版本，只是尽量不调度到污点 node，不强制 NoExecute：在该 node 上正在运行的所有无对应 Toleration 配置的 Pod 立刻被驱逐，且具有相应 Toleration 的 Pod 永远不被驱逐 同个 Node 可配置多个污点，pod 也可设置多个容忍，处理多个污点和容忍的顺序为： 先列出节点的所有污点 忽略 Pod 的容忍能匹配的部分 剩下的没有忽略的污点就是对 Pod 不会调度到的节点 特殊情况： 若剩余污点中存在 NoSchedule，则调度器不会把该 Pod 调度到这个节点 若剩余污点中没有 NoSchedule，但有 PreferNoSchedule，则调度器尽量不把 Pod 调度到这个节点 若剩余污点中有 NoExecute，且 Pod 已经在该节点上运行，则会被驱逐，若还没有在该节点上运行，则不再会被调度到该节点 对于 NoExecute，若需要让已经在运行的 Pod 被逐出前还能运行一段时间，而不是立刻被逐出，则可以添加参数 tolerationSeconds: 3600 若要将一些节点专门给特定应用使用，则可以通过添加污点 kubectl taint nodes &lt;node_name&gt; dedicated=&lt;group_name&gt;:NoSchedule 然后给这些应用 Pod 加入相应 Toleration，带有合适 Toleration 的 Pod 就能使用这样的 Node。 若集群中有特殊硬件设备如 GPU，希望只有相关特定 Pod 使用这些设备，可设置污点 kubectl taint nodes &lt;node_name&gt; special=true:NoSchedule或PreferNoSchedule 出于安全考虑，默认 k8s 不会将 Pod 调度到 Master，若希望 master 也当做 Node 使用，则可以使用污点 kubectl taint node &lt;master_name&gt; node-role.kubernetes.io/master- 若要恢复 Master Only 状态，则使用 kubectl taint mode &lt;master_name&gt; mode-role.kubernetes.io/master=&quot;&quot;:NoSchedule Pod Priority PreemptionDaemonSetDaemonSet 用于管理在集群中每个 Node 上仅运行一份 Pod 的副本实例。DaemonSet 的 Pod 调度策略和 RC 类似，除了使用系统内置算法在每个 Node 上进行调度，也可以在 Pod 的定义中使用 NodeSelector 或 NodeAffinity 进行调度。 例：集群的每个 Node 都创建一个 fluentd-elasticsearch，并挂载两个主机目录。DaemonSet 的配置与 Deployment 几乎一致 apiVersion: apps/v1kind: DaemonSetmetadata: name: fluented-logging labels: app: fluented-loggingspec: selector: matchLabels: app: fluented-logging template: metadata: labels: app: fluented-logging spec: containers: - name: fluented-logging image: ist0ne/fluentd-elasticsearch resources: limits: memory: &quot;128Mi&quot; cpu: &quot;500m&quot; env: - name: FLUENTD_ARGS value: -q volumeMounts: - name: varlog mountPath: /var/log readOnly: false - name: containers mountPath: /var/lib/docker/containers readOnly: false volumes: - name: containers hostPath: path: /var/lib/docker/containers - name: varlog hostPath: path: /var/log 启动后查看 # kubectl get daemonsetsNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEfluented-logging 2 2 2 2 2 &lt;none&gt; 4m39s# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESfluented-logging-h8dwx 1/1 Running 0 104s 10.32.0.2 kubenode3 &lt;none&gt; &lt;none&gt;fluented-logging-l68n2 1/1 Running 0 104s 10.38.0.5 kubenode2 &lt;none&gt; &lt;none&gt; 两台 node 节点都自动部署好了。 DaemonSet 自动滚动升级，在更新 DaemonSet 模板时，旧 pod 副本会自动删除，同时新 pod 副本自动创建。只需添加策略，与 containers 同级： updateStrategy: type: RollingUpdate DaemonSet 的常见应用场景： 节点上运行存储，如 Glusterd 或 Ceph 节点上运行日志收集，如 Flunentd 或 Logstash 节点上运行监控，如 Prometheus Exporter 或 Collectd K8s 也在运行自己的 DaemonSet 组件 # kubectl get daemonsets.apps --namespace=kube-systemNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEkube-proxy 3 3 3 3 3 kubernetes.io/os=linux 8dweave-net 3 3 3 3 3 &lt;none&gt; 8d JobJob 用于定义并启动一个批处理任务，能并行或串行处理一批工作项（work item）。批处理包含以下模式： Job Template Expansion：一个 Job 对应一个 Work Item，适合工作项少、每个工作项要处理数据量大的场景 Queue With Pod Per Work Item：一个任务队列存放工作项，一个 job 作为消费者去完成这些工作项。job 会启动 N 个 Pod，每个 Pod 对应一个工作项 Queue With Variable Pod Count：同上，但是 job 启动的 pod 数量是可变的 Single Job With Static Work Assignment：一个 job 产生多个 Pod，采用程序静态方式分配任务项，而不是采用队列动态分配 Job 类型： Non-parallel jobs：一个 Job 只启动一个 Pod，除非 Pod 异常，才会重启该 Pod，一旦 Pod 正常结束，job 就结束 Parallel jobs with a fixed completion count：并行 Job 会启动多个 Pod，此时需要设定 Job 的 spec.completions 参数，当正常结束的 Pod 数量到达该数值后，Job 结束。spec.parallelism 参数控制并行数（同时启动多少个 Job 处理） Parallel Jobs with a work queue：并行 Job 有一个独立 Queue，工作项在 Queue 中存放，不能设置 spec.completions参数。有以下特性： 每个 Pod 都能独立判断决定是否有任务项需要处理 若某个 Pod 正常结束，则 Job 不会再启动新 Pod 若一个 Pod 成功结束，则此时应该不存在其他 Pod 还在工作的情况，而是都处于结束和退出的状态 若所有 pod 都结束了，且至少一个 Pod 成功结束，则 job 成功结束 job 的重启策略只能为 Never 或 OnFailure，且必须设置。 apiVersion: batch/v1kind: Jobmetadata: name: myjobspec: parallelism: 3 completions: 9 template: metadata: name: myjob spec: containers: - name: job image: hello-world imagePullPolicy: IfNotPresent restartPolicy: OnFailure 查看 job 信息情况 # kubectl get jobs.batchNAME COMPLETIONS DURATION AGEmyjob 9/9 10s 66s Cronjob类似 Linux Cron，定时启动 Job apiVersion: batch/v1beta1kind: CronJobmetadata: name: hellospec: schedule: &quot;*/1 * * * *&quot; # 每分钟执行一次 jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh -c - date restartPolicy: OnFailure 初始化容器 Init ContainerPod 升级与回滚只要对 Deployment 的 Pod 定义进行修改并应用到 Deployment 对象上，即可完成 Deployment 的自动更新操作。若更新中发生错误，则可以通过回滚恢复 Pod 版本。 Deployment 升级例：创建 nginx 的 Deployment，版本为 1.7.9 apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployspec: selector: matchLabels: app: nginx replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 然后需要升级到 1.17.9 版本，先通过set命令设置参数 # kubectl set image deployment/nginx-deploy nginx=nginx:1.17.9deployment.apps/nginx-deploy image updated 此时已经开始滚动升级了，立刻查看更新过程 # kubectl rollout status deployment nginx-deployWaiting for deployment &quot;nginx-deploy&quot; rollout to finish: 1 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 1 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 1 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 1 old replicas are pending termination...Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 1 old replicas are pending termination...deployment &quot;nginx-deploy&quot; successfully rolled out 再次查看 Pod 列表，能看到 pod 名称都已经更新了，可通过describe查看具体 Pod 情况，能看到 Pod 镜像已经更新。 滚动升级的流程 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 14s (x2 over 87m) deployment-controller Scaled up replica set nginx-deploy-5bf87f5f59 to 1 Normal ScalingReplicaSet 11s (x2 over 87m) deployment-controller Scaled down replica set nginx-deploy-5df494d57d to 2 Normal ScalingReplicaSet 11s deployment-controller Scaled up replica set nginx-deploy-5bf87f5f59 to 2 Normal ScalingReplicaSet 8s (x2 over 93m) deployment-controller Scaled up replica set nginx-deploy-5bf87f5f59 to 3 Normal ScalingReplicaSet 8s deployment-controller Scaled down replica set nginx-deploy-5df494d57d to 1 Normal ScalingReplicaSet 6s deployment-controller Scaled down replica set nginx-deploy-5df494d57d to 0查看RS情况# kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deploy-5bf87f5f59 3 3 3 94mnginx-deploy-5df494d57d 0 0 0 90m 在整个升级过程中，系统会保证至少有两个 Pod 有用，并最多同时运行 4 个 Pod。默认情况下，Deployment 确保 Pod 总数至少为所需副本数量（DESIRED）-1，即最多一个不可用，Pod 总数最多比所需 Pod 数多一个，即最多一个浪涌值（maxSurge=1）。这样，升级过程中 Deployment 就能保证服务不中断，且副本数量始终维持为用户指定数量。 更新策略在 Deployment 定义中，可通过spec.strategy指定 Pod 更新的策略，目前支持两种策略： Recreate 重建：设置spec.strategy.type=RecreateDeployment 在更新 Pod 时，会先杀掉所有正在运行的 Pod，然后重新创建 Pod RollingUpdate 滚动更新：默认，设置spec.strategy.type=RollingUpdate，Deployment 会以滚动更新方式逐个更新 Pod，并可通过参数maxUnavailable和maxSurge控制滚动更新的过程 maxUnavailable：指定 Deployment 在更新过程中不可用 Pod 的数量上限，可以是数字，或 Pod 期望副本数的百分比（会向下取整），默认为 25% maxSurge：指定在 Deployment 更新过程中 Pod 总数超过 Pod 期望副本数部分的最大值，值类型同上，默认为 25% maxSurge越大，初始创建的新副本数量越多。maxUnavailable越大，初始销毁的旧副本数量越多。 多重更新（Rollover）：若在更新时再次发起更新，则会立刻将之前正在更新的 RS 停止扩容，且将其加入到旧版本 RS 列表中，并开始缩容至 0。对于 Pod，Deployment 会立刻杀死创建的中间版本的 Pod，并开始创建最后指定版本的 Pod。 Deployment 回滚默认情况所有 Deployment 的发布历史记录都被保留在系统中，以便随时进行回滚。 可通过以下命令查看 Deployment 更新状态 # kubectl rollout status deployment nginx-deploydeployment &quot;nginx-deploy&quot; successfully rolled out若更新出现问题，则会卡住# kubectl rollout status deployment nginx-deployWaiting for deployment &quot;nginx-deploy&quot; rollout to finish: 1 out of 3 new replicas have been updated...# kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deploy-57574fd9dd 1 1 0 3m48snginx-deploy-5bf87f5f59 0 0 0 6m44snginx-deploy-5d85b5fb59 3 3 3 4m9s# kubectl get podsNAME READY STATUS RESTARTS AGEnginx-deploy-57574fd9dd-k9hk2 0/1 ImagePullBackOff 0 73s # 镜像拉取出错nginx-deploy-5d85b5fb59-gl2ch 1/1 Running 0 88snginx-deploy-5d85b5fb59-lg79f 1/1 Running 0 91snginx-deploy-5d85b5fb59-vtlt2 1/1 Running 0 86s 此时需要先查询之前的稳定版本的 Deployment，注意 REVISION 版本 # kubectl rollout history deployment nginx-deploydeployment.apps/nginx-deployREVISION CHANGE-CAUSE1 kubectl create --filename=nginx-deploy.yml --record=true2 kubectl create --filename=nginx-deploy.yml --record=true9 kubectl create --filename=nginx-deploy.yml --record=true10 kubectl create --filename=nginx-deploy.yml --record=true 可通过--revision查看指定版本的详细信息 # kubectl rollout history deployment nginx-deploy --revision 2deployment.apps/nginx-deploy with revision #2Pod Template: Labels: app=nginx pod-template-hash=7b45d69949 Annotations: kubernetes.io/change-cause: kubectl create --filename=nginx-deploy.yml --record=true Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt; 此时要退回到 revision 2，则可以指定版本回滚 # kubectl rollout undo deployment nginx-deploy --to-revision=2deployment.apps/nginx-deploy rolled back 对于复杂的 Deployment 配置修改，为避免频繁触发 Deployment 的更新操作，可先暂停 Deployment 的更新操作，然后进行配置修改，再恢复 Deployment，一次性触发完整的更新操作。 暂停 Deployment 的更新 # kubectl rollout pause deployment nginx-deploydeployment.apps/nginx-deploy paused 修改完成后恢复 Deployment 的部署 # kubectl rollout resume deployment nginx-deploydeployment.apps/nginx-deploy resumed 注：暂停 Deployment 期间是不能进行回滚的 RC 滚动升级K8s 通过配置文件进行 例：RC redis-master 的 v1 版本升级到 v2 版本 # RC的v1版本的配置apiVersion: v1kind: ReplicationControllermetadata: name: redis-master labels: name: redis-master version: v1...... spec: containers: - name: redis-master image: kubeguide/redis-master:1.0 ports: - containerPort: 6379# kubectl get rc -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORredis-master-v1 3 3 0 3m18s redis-master-v1 kubeguide/redis-master:1.0 name=redis-master,version=v1# RC的v2版本的配置，是修改的v1的配置文件apiVersion: v1kind: ReplicationControllermetadata: name: redis-master-v2 labels: name: redis-master version: v2...... spec: containers: - name: redis-master-v2 image: kubeguide/redis-master:2.0 ports: - containerPort: 6379 minReadySeconds: 5 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 Pod 扩缩容手动扩缩容kubectl scale deployment &lt;deployment-name&gt; --replicas=&lt;replicas-number&gt; 若设置为比当前副本数量更小的数字，则会杀死一些正在运行的 pod # kubectl get deploymentsNAME READY UP-TO-DATE AVAILABLE AGEnginx-deploy 3/3 3 3 5h42m[root@kubenode1 ~]# kubectl scale deployment nginx-deploy --replicas=5deployment.apps/nginx-deploy scaled# kubectl get deploymentsNAME READY UP-TO-DATE AVAILABLE AGEnginx-deploy 5/5 5 5 5h43m K8s弹性伸缩三种弹性伸缩： CA（Cluster Autoscaler）：Node级别自动扩缩容，通过cluster-autoscaler组件（主要是在云服务商上进行创建新节点，Cluster AutoScaler会监听Node资源使用情况。还有一种就是通过ansible等方式进行扩容） HPA（Horizontal Pod Autoscaler）：Pod个数自动扩缩容 VPA（Vertical Pod Autoscaler）：Pod配置（如CPU、内存）自动扩缩容，通过addmin-resizer组件（主要对象是有些有状态的服务，不能横向扩容） HPAHorizontal Pod Autoscaler（Pod 横向自动扩容），也是一种资源对象。通过追踪分析指定 RC 控制的所有目标 Pod 的负载情况，来确定是否需要针对性调整目标 Pod 的副本数量。由于需要监控Node的性能信息，所以依赖Metrics Server组件。HPA 有两种方法作为 Pod 负载的度量指标： CPUUtilizationPercentage，是目标 Pod 所有副本自身 CPU 利用率的算数平均值（Pod自身CPU利用率=Pod当前CPU使用量/Pod Request）。 若某一时刻该值超过 80%，则意味着当前 Pod 副本数量不足以支撑更多请求，需要动态扩容，而当请求高峰过去，CPU 利用率又降下来，则副本数也自动减少到一个合理值。 通常是 1min 的平均值 K8s 通过基础性能数据监控框架（Kubernetes Monitoring Architecture）支持 HPA。该框架中 K8s 定义了标准化 API 接口 Resource Metrics API，方便客户端（如 HPA）获取性能数据 应用自定义的度量指标（如 TPS、QPS） 例：apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata: name: php-apache namespace: defaultspec: maxReplicas: 10 minReplicas: 1 scaleTargetRef: # HPA控制一个叫php-apache的Deployment中的Pod副本 kind: Deployment name: php-apache targetCPUUtilizationPercentage: 90 # 该值超过90%则触发自动扩容 为防止副本数量因扩容大幅波动，K8s在Controller manager中设置了冷却时间，即在每次扩缩容后的冷却时间。 horizontal-pod-autoscaler-downscale-dalay: 缩容冷却，默认5min horizontal-pod-autoscaler-upscale-dalay: 扩容冷却，默认3min 目前除了autoscaling/v1（仅支持基于CPU使用率）外，还有autoscaling/v2beta2，支持了多指标以及自定义指标。 从 v1.18 开始，v2beta2 API 允许通过 HPA 的 behavior 字段配置扩缩行为。 在 behavior 字段中的 scaleUp 和 scaleDown 分别指定扩容和缩容行为。 可以两个方向指定一个稳定窗口，以防止扩缩目标中副本数量的波动。 类似地，指定扩缩策略可以控制扩缩时副本数的变化率。在 spec 字段的 behavior 部分可以指定一个或多个扩缩策略。 当指定多个策略时，默认选择允许更改最多的策略，就是说每个操作周期都会计算当前该策略能操作的pod数，然后挑能操作pod数最多的策略执行。 behavior: scaleDown: policies: - type: Pods value: 4 periodSeconds: 60 - type: Percent value: 10 periodSeconds: 60 当用于扩缩的指标持续抖动时，可以使用稳定窗口来限制副本数上下振动。 scaleDown: stabilizationWindowSeconds: 300 默认扩缩容行为，即未设置的默认配置 behavior: scaleDown: # 缩容配置 stabilizationWindowSeconds: 300 # 稳定窗口为300s policies: - type: Percent # 按照百分率 value: 100 # 可以直接降低到最小允许的副本数，即minReplicas配置。 periodSeconds: 15 # 每15s为一个操作周期 scaleUp: # 扩容配置 stabilizationWindowSeconds: 0 policies: - type: Percent value: 100 # 可以直接扩容到最大允许的副本数，即maxReplicas配置。 periodSeconds: 15 - type: Pods value: 4 periodSeconds: 15 selectPolicy: Max 深入理解 Service通过创建 Service 可为一组具有相同功能的容器应用提供一个统一的入口地址，并且将请求负载分发到后端的各个容器应用上。 两种方法创建 Service： 先创建 RC 或 Deployment，然后执行kubectl expose rc|deployment &lt;rc-name|deployment-name&gt; 然后查看服务# kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 192.168.10.1 &lt;none&gt; 443/TCP 4d1hwebapp-tomcat ClusterIP 192.168.10.144 &lt;none&gt; 8080/TCP 5m47swebapp-tomcat-deployment ClusterIP 192.168.10.67 &lt;none&gt; 8080/TCP 5m40s 就可通过 ClusterIP 加上端口访问该服务了 直接 YAML 创建 ServiceapiVersion: v1kind: Servicemetadata: name: webapp-tomcat-servicespec: selector: app: webapp-tomcat-service ports: - port: 808 targetPort: 8080 Service 的关键字段为 ports 和 selector。posts 为提供给外部访问的端口，selector 为后端 pod 的 label。 Service 的 Pod 被 k8s 进行负载负载均衡，具体有两种分发策略： RoundRobin：轮询（默认） SessionAffinity：基于客户端 IP 进行会话保持。相同客户端 IP 的请求分到同一个 Pod 上。若要修改为此模式，则需要在配置中 spec 下添加sessionAffinity: ClientIP 若服务开放多个端口，则需要在每个端口定义内加上name定义。且端口定义中可指定传输层协议，通过添加protocol定义。 apiVersion: v1kind: Servicemetadata: name: app-dnsspec: selector: app: app-dns clusterIP: 192.168.10.200 ports: - port: 53 protocol: TCP name: dns-tcp - port: 53 protocol: UDP name: dns-udp 外部服务 Service若应用需要连接一个外部数据库，或将另一个集群或 Namespace 中服务作为服务的后端，这时需要创建一个无 Label Selector 的 Service。 例：一个无标签 http 服务，开放端口 80，但指向另一个 http 服务，目的 IP 为 10.1.1.2 apiVersion: v1kind: Servicemetadata: name: nolabel-http-servicespec: ports: - port: 80 targetPort: 80 protocol: TCPapiVersion: v1kind: Endpointsmetadata: name: nolabel-http-servicesubsets:- addresses: - ip: 10.1.1.2 ports: - port: 80 分别创建一个无标签 Service 和一个 Endpoint，因为系统不会自动创建 Endpoint，且该 Endpoint 需要和 Service 同名，指向实际后端访问 IP。 Headless Service若需要自己控制负载均衡策略，而不使用 Service 默认负载策略，则可使用 Headless Service，不为 Service 设置 ClusterIP，仅通过 Label Selector 将后端 Pod 列表返回给调用的客户端。不指定特定的 ClusterIP，访问该 service 将会返回所有标签为 app=nginx 的 Pod 列表，然后客户端自定义策略如何处理该列表。StatefulSet 就是使用 Headless Service 为客户端返回多个服务地址的。Headless Service 主要应用在去中心化的应用集群。 例：创建 Headless nginx 服务 apiVersion: v1kind: Servicemetadata: name: headless-nginxspec: selector: app: nginx ports: - port: 80 clusterIP: None 查看该服务 # kubectl get service headless-nginxNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEheadless-nginx ClusterIP None &lt;none&gt; 80/TCP 49m Apache Cassandra 简介Apache Cassandra 是一套开源分布式 NoSQL 数据库，并不是单个数据库，而是由一组数据库节点共同构成的一个分布式的集群数据库。由于 Cassandra 使用的是去中心化模式，所以在集群中的一个节点启动后，需要获知集群中新节点的加入，对此 Cassandra 使用 Seed 完成节点之间发现和通信。 Cassandra 使用了 Google 设计的 BigTable 的数据模型，与关系型数据库或 key-value 数据库不同，Cassandra 使用的是宽列存储模型(Wide Column Stores)，每行数据由row key唯一标识之后，可以有最多 20 亿个列，每个列有一个column key标识，每个column key下对应若干value。这种模型可以理解为是一个二维的 key-value 存储，即整个数据模型被定义成一个类似map&lt;key1, map&lt;key2,value&gt;&gt;的类型。Cassandra 的数据并不存储在分布式文件系统如 GFS 或 HDFS 中，而是直接存于本地。与 BigTable 一样，Cassandra 也是日志型数据库，即把新写入的数据存储在内存的 Memtable 中并通过磁盘中的 CommitLog 来做持久化，这种系统的特点是写入比读取更快，因为写入一条数据是顺序计入 commit log 中，不需要随机读取磁盘以及搜索。 Cassandra 结合 Headless Service 可实现 Cassandra 节点之间互相查找和集群的自动搭建。 通过 Service 动态查找 Pod由于 pod 的创建和销毁都会实时更新 Service 的 Endpoint 数据，所以可动态对 Service 的后端 Pod 进行查询，因此一个 Cassandra 节点只需要查询到其他节点就能自动组成一个集群。 Cassandra 节点加入集群的过程 新节点出现会更新 Service 的 Endpoint Master 获取 Service 的后端 Endpoint，将新 Pod 加入集群 创建服务 cassandra，并打上标签 app=cassandra，且 selector 也选择 app=cassandra。 apiVersion: v1kind: Servicemetadata: name: cassandra labels: app: cassandraspec: selector: app: cassandra ports: - port: 9042 创建 RC apiVersion: v1kind: ReplicationControllermetadata: name: cassandraspec: replicas: 2 selector: app: cassandra template: metadata: name: cassandra labels: app: cassandra spec: containers: - name: cassandra image: cassandra ports: - containerPort: 9042 name: cql - containerPort: 9160 name: thrift volumeMounts: - mountPath: /cassandra_data name: data command: - /run.sh res volumes: - name: data emptyDir: &#123;&#125; 从集群外部访问 Pod 和 Service核心组件运行机制API-ServerAPIserver 的功能： 提供 K8s 各资源对象的增删改查及 Watch 等 REST 接口，是整个系统的数据总线和数据中心 是集群管理的 API 入口 是资源配额控制的入口 提供完备的集群安全机制 默认 kube-apiserver 在 master 上 8080 端口提供服务，也可启动 HTTPS 安全端口启动安全机制。 常见 REST API： apiserver 支持的资源对象列表：localhost:8080/api/v1 pod 列表：localhost:8080/api/pods service 列表：localhost:8080/api/services RC 列表：localhost:8080/api/replicationcontrollers 等 若只要对外暴露部分 REST，则需要在 Master 或其他节点运行 proxy，在 8001 端口启动代理拒绝客户端访问 RC 的 API kubectl proxy --reject-paths=&quot;^/api/v1/replicationcontrollers&quot; --port=8001 --v=2 然后访问该 REST 查看 # curl localhost:8001/api/v1/replicationcontrollersForbidden K8s apiserver 本身就是一个 Service，名叫 kubernetes，且 ClusterIP 就是地址池的第一个地址，端口是 HTTPS/443 # kubectl get serviceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 192.168.10.1 &lt;none&gt; 443/TCP 6d2h K8s 设计通过以下方式最大程度保证 API server 的性能： API server 的底层代码性能高，使用了协程和队列的并发代码 普通 List 接口结合异步 Watch 接口，解决了资源对象高性能同步问题并提高了响应事件的灵敏度 采用 etcd 解决了数据可靠性问题，提升了 APIserver 数据访问层的性能 API Server 架构： API 层：以 REST 提供各种 API 访问控制层：当客户端访问 API 时负责对用户身份的鉴权，进行访问控制（许可逻辑 Admission Controll） 注册表层：K8s 将所有资源对象都保存在注册表（Registry）中，包含了资源对象类型、如何创建资源对象、如何转换资源版本、如何将资源编码和解码为 Json 或 ProtoBuf 格式 etcd 数据库 完整 Pod 的调度机制 List-Watch API server 通过 etcd 提供的 Watch API 监听 etcd 上发生的数据操作，如 Pod 创建更新等，etcd 会及时通知 APIserver。当一个 ReplicaSet 对象被创建并保存到 etcd 后，etcd 立刻发送一个 Create 事件给 API Server。 API Server 通过自身的 Watch 接口获取它们感兴趣的任务资源对象的相关事件。 K8s List-Watch 实现数据同步，客户端先调用 API server 的 List 接口获取相关资源对象的全量数据并缓存到内存中，然后启动对应资源对象的 Watch 协程，接收到 Watch 事件后根据事件类型对内存的全量资源对象列表同步修改，能达到近乎实时的数据同步 所有 K8s 内建的资源对象实现都包含以下功能： 元数据（Schema）定义：定义了资源对象的数据结构 校验逻辑：确保用户提交的资源对象属性的合法性 CRUD 代码 自动控制器：确保对应资源对象的数量、状态、行为始终符合用户期望 Controller Manager在 K8s 中，每个 Controller 都是一个功能系统，通过 APIServer 提供的 List-Watch 接口实时监控集群中特定资源的状态变化，当发生各种故障导致资源对象状态变化时，Controller 会尝试将其状态调整为期望的状态。Controller Manager 是 K8s 各种 Controller 的管理者，是集群内的管理控制中心和自动化的核心。 Replication Controller与资源对象 ReplicationController（RC）不同，在 Controller Manager 中的 Replication Controller 为副本控制器，核心作用为确保任何时候集群中某个 RC 关联的 Pod 副本数量保持预定值。注：只有 Pod 的重启策略为 Always 时（RestartPolicy=Always），Replication Controller 才会管理该 Pod 的操作。通常情况下 Pod 对象被成功创建后就不会消失，除非 Pod 处于 succeed 或 failed 状态的时间过长，该 Pod 就会被系统自动回收，然后副本控制器在其他节点上重新创建该 Pod 副本。 Pod 可通过修改标签来脱离 RC 的管控，一般用在将 Pod 从集群中迁移、数据修复等调试，对于被迁移的 Pod 副本，RC 会自动新建一个副本替换被迁移的副本。若要删除一个 RC 控制的所有 Pod，只需将 RC 定义的副本数设为 0 并应用即可。 Node ControllerNode Controller 通过 API Server 实时获取 Node 的相关信息，实现管理和监控集群中的各 Node 的相关控制功能。 工作流程： Controller Manager 若在启动时设置了cluster-cidr参数，则会为每个没有设置Spec.PodCIDR的 Node 都生成一个 CIDR 地址，然后再用该 CIDR 地址设置Spec.PodCIDR，从而防止 CIDR 地址冲突 逐个读取 Node 信息，尝试修改 nodeStatusMap 的节点状态信息，将该信息与 Node Controller 的 nodeStatusMap 的节点信息比较。 若没收到 kubelet 发的节点信息、或者是第一次收到节点信息、或者处理过程中节点状态变为非健康状态，则在 nodeStatusMap 中保存该节点状态信息，且修改探测时间与节点状态变化时间为 Master 节点的系统时间 若指定时间内收到新的节点信息，且节点状态未发生变化，则在 nodeStatusMap 中保存该节点状态，且修改探测时间为 Master 节点的系统时间、节点状态变化时间为上次节点变化时间 若指定时间内收到新的节点信息，且节点状态发生变化，则在 nodeStatusMap 中保存该节点状态，且修改探测时间和节点状态变化时间为 Master 节点的系统时间 若指定时间内未收到节点信息，则设置节点状态为未知，并通过 API server 保存节点状态 逐个读取节点信息，根据节点状态删除或同步节点信息 ResourceQuota ControllerResourceQuota Controller（资源配额管理）确保了指定资源对象在任何时刻都不会超量占用系统物理资源。目前支持三个层次的配额管理： 容器级别：对 CPU 和内存 Pod 级别：对 Pod 内所有容器的资源 Namespace 级别：多租户级别的资源限制，包括：Pod 数量、ReplicationController 数量、Service 数量、ResourceQuota 数量、Secret 数量、PV 数量 配额管理通过 Admission Control（准入控制）来控制，提供两种方式： LimitRanger：作用于 Pod 和 Container ResourceQuota：作用于 Namespace 若在 Pod 中声明了 LimitRanger，则通过 APIserver 请求创建或修改资源时，Admission Control 会计算当前配额的使用情况，若不符合配额约束，则创建失败。若在 Namespace 中声明了 ResourceQuota，则 ResourceQuota Controller 负责定期统计和生成该 Namespace 下各个资源对象的资源使用总量并写入 etcd 的 resourceQuotaStatusStorage 目录（resourceQuotas/status），然后这些统计被 Admission Control 使用，确保相关 Namespace 下资源配额总量不超过 ResourceQuota 的限定值。 Namespace Controller用户通过 APIServer 创建新的 Namespace 并保存在 etcd 中，Namespace Controller 定时通过 API Server 读取这些 Namespace 信息。 若 Namespace 被 API 标识为优雅删除（DeletionTimestamp属性，删除期限），则将该 Namespace 状态设为 Terminating 并保存在 etcd 中，同时 Namespace Controller 删除该 Namespace 下的 ServiceAccount、RC、Pod、Secret、PersistentVolume、ListRange、ResourceQuota 和 Event 等资源对象。 Service Controller 和 Endpoints ControllerEndpoints 表示了一个 Service 对应的所有 Pod 副本的访问地址，Endpoints Controller 就是负责生成和维护所有 Endpoints 对象的控制器。Endpoints Controller 监听 Service 和对应 Pod 副本的变化 若检测到 Service 被删除，则删除和该 Service 同名的 Endpoints 对象 若检测到 Service 被创建或修改，则根据该 Service 信息获取相应 Pod 列表，然后创建更新对应 Endpoints 对象 若检测到 Pod 事件，则更新对应 Service 的 Endpoints 对象 Service Controller 监听 Service 变化，若该 Service 是 LoadBalancer 类型，则 Service Controller 确保在外部云平台上该 Service 对应的 LoadBalancer 实例被相应地创建、删除或更新。 Schedulerkubeletkubeproxy 参考文章 十分钟带你理解 Kubernetes 核心概念 Kubernetes Handbook——Kubernetes 中文指南/云原生应用架构实践手册 Kubernetes 中文社区 | 中文文档 和我一步步部署 kubernetes 集群 docker 容器与容器云 Docker 高级应用实战——李振良——视频课程 Kubernetes 权威指南（第四版） 每天 5 分钟玩转 Kubernetes Kubernetes 1.12.2 版，使用 docker 镜像安装 Kubernetes：如何解决从 k8s.gcr.io 拉取镜像失败问题 Kubernetes: 21 天完美通关","categories":[{"name":"云计算","slug":"云计算","permalink":"https://coconutmilktaro.top/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"云计算","slug":"云计算","permalink":"https://coconutmilktaro.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://coconutmilktaro.top/tags/Kubernetes/"}]},{"title":"Docker存储学习笔记","slug":"Docker存储学习笔记","date":"2018-07-06T05:33:01.000Z","updated":"2022-05-30T02:51:53.789Z","comments":true,"path":"2018/Docker存储学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Docker%E5%AD%98%E5%82%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"主要是对 docker 文档(v18.03)的翻译以及自己的学习笔记本篇主要包含以下内容 Docker 存储介绍 BindMount bind-propagation 传播挂载 Selinux 标签 Volume 数据卷 Tmpfs 数据卷容器 数据卷备份、还原与迁移 备份一个容器 还原一个容器 删除一个数据卷 存储引擎 参考资料","text":"主要是对 docker 文档(v18.03)的翻译以及自己的学习笔记本篇主要包含以下内容 Docker 存储介绍 BindMount bind-propagation 传播挂载 Selinux 标签 Volume 数据卷 Tmpfs 数据卷容器 数据卷备份、还原与迁移 备份一个容器 还原一个容器 删除一个数据卷 存储引擎 参考资料 Docker 存储介绍Docker 为容器提供了两种存放数据的资源： storage driver：管理的镜像层和容器层特点：Copy-on-Write。新数据会存放在最上层容器层，修改现有数据会先从镜像层复制到容器层，修改后的数据直接保存在容器中，镜像层保持不变。若多层中有同名文件，用户只能看到最上层的文件。可通过 docker info 查看到。Docker 优先使用默认的 storage driver。 data volume 数据卷特点：是目录或文件，不是磁盘，volume 数据可以被永久的保存，即使使用它的容器已经销毁。分为两种 volume：bind mount 和 docker managed volume 存储驱动：目前 docker 支持五种存储驱动。详见存储引擎 AUFS Btrfs Device Mapper Overlay ZFS 原理： Copy-On-Write 写时复制所有驱动都是用到 Cow 写时复制（copy-on-write），只在需要写时才复制。可以让所有容器都共享一个 image 文件系统，所有数据都从 image 读取，容器需要写操作时，才将要写的文件从 image 复制到自己的文件系统，即所有写操作都是对 image 中副本的修改。有效提高了磁盘利用率。 allocate-on-demand 用时分配在要写入一个文件时才按需分配空间 驱动： AUFS：一种 Union FS 联合文件系统，文件级存储。支持将不同目录挂载到同一个虚拟文件系统，下层文件系统只可读，最上层可写。若要修改，AUFS 会创建一个该文件的副本，放在可写层，结果也保存在可写层。 Overlay：一种 UnionFS，文件级存储。只有两层：upper 层和 lower 层 Device Mapper：RHEL 下 Docker Engine 的默认存储驱动，基于同名级卷管理技术框架的存储引擎。 默认情况下，在容器内创建的所有文件都存储在可写容器层中，仅存储在主机系统的内存中，即tmpfs方式，永远不会写入主机系统的文件系统，因此一旦容器停止，容器内所有文件的改动都会丢失，所以需要通过一些机制将文件保存以至于在容易停止后仍不会丢失。 docker 有三种存储容器数据的方式： bind mount：可将容器数据存放在宿主机的任何位置。 volumes：通过创建数据卷将容器数据持久化到文件系统。 tmpfs：数据存放在内存中，不会写入文件系统。 BindMountbind mount可使容器中的文件存储在主机系统的任何位置。Docker 主机或 Docker 容器上的非 Docker 进程可以随时修改它们。bind mount 非常高效，但它们依赖于具有特定目录结构的主机文件系统。 docker 提供两种选项进行 bind mount。 -v或--volume：三个字段组成，冒号分隔。 第一个字段：卷名，若是匿名卷，则可省略 第二个字段：文件或目录在容器中安装的路径 第三个字段：可选项，例如ro，默认是可读可写 --mount：由多个键值对组成，用逗号分隔，键和值用=连接。以下是提供的键： type：挂载的类型，可以是 bind，volume 或 tmpfs。 source或src：挂载源，即卷名。匿名卷可省略。 destination或dst或target：挂载到的容器中的指定目录或文件路径 readonly：以只读方式挂载，可选。 volume-opt：卷选项，可指定多次，也是键值对形式 bind-propagation：绑定传播，有以下选择：rprivate，private，rshared，shared，rslave，slave注：–mount 不支持设置 selinux 的z或Z选项 如果使用-v或--volume绑定安装 Docker 主机上尚不存在的文件或目录，则-v 会为您创建端点。它始终作为目录创建。而如果使用--mount，Docker 不会自动为您创建它，但会生成错误。 注：bind mount 允许访问敏感文件使用 bind mount 的一个副作用：可以通过容器中运行的进程更改主机文件系统，包括创建，修改或删除重要的系统文件或目录。这是一种强大的功能，可能会产生安全隐患，包括影响主机系统上的非 Docker 进程。 当挂载一个 bind mount 或非空数据卷到容器中的一个非空目录，则该目录中原有的文件会被掩盖（并非删除），而只显示挂载的卷内容。当挂载一个空数据卷到容器中的一个非空目录，则该目录中的文件都会复制到该卷中。若启动容器时指定了一个不存在的数据卷，则会自动创建一个卷。 使用-v挂载docker run -v &lt;源目录或文件&gt;:&lt;容器中目录或文件&gt;# 路径都需要绝对路径# 若添加单个文件，主机源文件必须存在，否则会当做一个新目录挂载到容器使用--mount挂载docker run --mount type=bind,src=&lt;源目录或文件&gt;,dst=&lt;容器目录或文件&gt; docker inspect 容器查看是否挂载了数据卷 bind-propagation 传播挂载在指定的 bind mount 或数据卷上挂载是否能被复制到挂载的目录中去。用于做动态，可通过编排工具方便实现。 有以下几种选项： shared：源挂载的子挂载会暴露给副本挂载，副本挂载的子挂载也会复制到源挂载。 slave：类似于 shared，但只在一个方向上。如果源挂载暴露了子挂载，则副本挂载可以看到它。但是，如果副本挂载暴露了子挂载，则源装载无法看到它。 private：此挂载是私人的。其中的子挂载不会暴露给副本挂载，副本挂载的子挂载不会暴露给源挂载。 rshared：与 shared 相同，但传播也扩展到嵌套在任何源或副本挂载中的挂载点。 rslave：与 slave 相同，但传播也扩展到嵌套在任何源或副本挂载中的挂载点。 rprivate：默认值。与 private 相同，源或副本挂载中任何位置的挂载点都不会沿任一方向传播。 Selinux 标签如果使用 selinux，则可以添加z或Z选项以修改要挂载到容器的主机文件或目录的 selinux 标签。这会影响主机本身上的文件或目录，并且可能会影响到 Docker 外部。 z选项表示绑定装载内容在多个容器之间共享。 Z选项表示绑定装载内容是私有且非共享的。使用Z选项绑定安装系统目录（例如/home或/usr）会导致主机无法运行。 当 bind mount 和 service 一起使用时，会自动忽略 selinux 标签（z和Z）还有ro。 Volume 数据卷卷是保存 Docker 容器生成和使用的数据的首选机制，并且卷完全由 Docker 管理。写入容器的可写层需要存储驱动程序来管理文件系统，存储驱动程序使用 Linux 内核提供联合文件系统 UFS。而数据卷是经过特殊设计的目录，可以绕过联合文件系统，为多个容器提供访问。数据卷的目的在于数据永久化，完全独立于容器的生存周期，容器删除时挂载的数据卷不会被删除。 特点： 卷比 bind mount 更容易备份或迁移。 卷适用于 Linux 和 Windows 容器。 可以在多个容器之间更安全地共享卷。 在容器启动时初始化，若挂载点已有数据，则会被拷贝到新初始化的数据卷中 数据卷变化不会影响镜像更新 卷驱动程序允许在远程主机或云提供程序上存储卷，加密卷的内容或添加其他功能。 可通过docker volume create 数据卷名创建数据卷。每创建一个 volume，就会在/var/lib/docker/volumes中创建一个同名目录。若不指定数据卷名，就会随机生成一个 volume ID 作为数据卷名。 docker volume命令 create 创建数据卷inspect 查看数据卷信息ls 查看所有数据卷prune 删除未使用的数据卷rm 删除指定数据卷 在创建 volume 时或启动使用为创建卷的容器时，可以指定卷的驱动。数据卷驱动可通过docker plugin install [选项] 驱动名。如果卷驱动程序要求您传递选项，则必须使用--mount标志来装入卷 docker 提供-v和--mount选项进行挂载。使用与 bind mount 基本一致。如果需要指定卷驱动程序选项，则必须使用--mount。将卷与服务一起使用时，仅支持--mount。 注：挂载数据卷不支持单个文件，只能是目录。且不权限控制，均为可读写。因为容器配置文件里的可以指定docker inspect查看，发现Mounts中的Source字段，其中已指定了源，源为：/var/lib/docker/volumes/容器长ID/_data。 以上两种方法数据源其实还是宿主机中的，并不是真正放在 volume container 中，可以在通过 dockerfile 的ADD将数据打包进镜像并指定VOLUME，将ADD指定的目录与VOLUME设为一致，此法称为data-packed volume container。 Tmpfs使用tmpfs mount创建容器时，容器可以在容器的可写层之外创建文件。tmpfs 挂载是临时的，仅保留在主机内存中。当容器停止时，将删除 tmpfs 挂载，该数据不可被共享，Docker 默认使用 tmpfs 挂载。--tmpfs：安装 tmpfs 挂载而不允许指定任何可配置选项，并且只能与独立容器一起使用。也可以通过--mount指定type=tmpfs。 数据卷容器容器挂载数据卷，其他容器通过挂载该容器实现数据共享，挂载数据卷的容器称为数据卷容器。 创建容器时--volumes-from 数据卷容器创建数据卷容器注：由于数据卷容器仅是提供数据，所以只要 create，不用 rundocker rm -v 容器 在删除容器时一并删除数据卷。但是，只要有容器还在使用该数据卷，数据卷就不会删除。宿主机上的数据卷若删除，就真的没了。 &gt; docker volume create volume_1&gt; docker volume create volume_2# 创建数据卷容器，挂载volume_1，volume_2&gt; docker create \\ -v volume_1:/volume1 \\ -v volume_2:/volume2 \\ --name vol_container \\ alpine# 创建容器挂载数据卷容器&gt; docker run -it \\ --volumes-from vol_container \\ --name test \\ alpine/ # lsbin home mnt run sys vardev lib proc sbin tmp volume1etc media root srv usr volume2# 数据卷容器中挂载的数据卷也被挂载到新容器的根目录了，也可通过-v设置挂载点 数据卷备份、还原与迁移使用数据卷能方便地进行数据备份、迁移和还原。 备份一个容器首先创建一个数据卷，用于存放备份数据。docker volume create vol_backup然后创建一个数据卷容器，挂载该数据卷 docker create --name backup_container\\ -v vol_backup:/backup \\ alpine 接着运行要备份容器，挂载数据卷容器 backup_container，并将要备份的数据打包放入数据卷。 docker run --rm \\ --volumes-from backup_container \\ -v /backup \\ alpine tar -cvf /backup/data.tar /usr /var 在主机的/var/lib/docker/volumes/vol_backup/_data/中出现了打包后的data.tar 还原一个容器docker run --rm \\ --volumes-from backup_container \\ -v /backup \\ alpine \\ bash -c &quot;cd /dbdata &amp;&amp; tar xvf /backup/data.tar --strip 1&quot; 删除一个数据卷删除的数据卷有两种情况： 命名卷在容器外部有指定源，删除了容器，数据卷并不会被删除 匿名卷没有指定源，在删除容器时，该匿名卷也会被删除。也可在创建容器时，加上--rm参数，关闭时自动删除容器和匿名卷。 存储引擎参考资料Docker 官方文档-存储每天 5 分钟玩转 docker 容器技术","categories":[{"name":"云计算","slug":"云计算","permalink":"https://coconutmilktaro.top/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://coconutmilktaro.top/tags/docker/"},{"name":"存储","slug":"存储","permalink":"https://coconutmilktaro.top/tags/%E5%AD%98%E5%82%A8/"}]},{"title":"常见VPN技术笔记","slug":"常见VPN技术笔记","date":"2018-06-18T01:00:37.000Z","updated":"2022-06-28T18:58:04.796Z","comments":true,"path":"2018/常见VPN技术笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/%E5%B8%B8%E8%A7%81VPN%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/","excerpt":"华三网络学习笔记（理论） 本篇包含以下知识点 VPN 概述 GRE-VPN L2TP-VPN IPSEC-VPN IPSec SA IKE IPSec 包处理流程 安全协议 GRE-OVER-IPSEC IPSEC-OVER-GRE MPLS BGP-MPLS-VPN 多 VPF 组网 MP-BGP","text":"华三网络学习笔记（理论） 本篇包含以下知识点 VPN 概述 GRE-VPN L2TP-VPN IPSEC-VPN IPSec SA IKE IPSec 包处理流程 安全协议 GRE-OVER-IPSEC IPSEC-OVER-GRE MPLS BGP-MPLS-VPN 多 VPF 组网 MP-BGP VPN 概述Virtual Private Network 虚拟私有网，利用共享公共网络仿真 WAN 设施，构建私有的专用网络。基于 IP 的 VPN 体系的核心是使用 Tunnel 隧道技术。 VPN 的优势 快速构建，降低部署周期 与私有网络一样的安全性、可靠性与可管理性 提高了基础资源的利用率 简化了用户端的配置和维护工作 概念术语 承载协议：在公网传输时使用的协议 封装协议：用于标识承载协议中封装的数据包，放置在承载协议头与载荷协议头间 载荷协议：最初封装数据包的协议 隧道协议：决定如何实现隧道的协议 主要 VPN 技术L2 VPN 技术 L2TP VPN：二层隧道协议，可实现拨号 VPN 与专线 VPN PPTP VPN：点到点隧道协议，支持 PPP 在 IP 网络上的隧道封装，使用增强 GRE 技术为传输的 PPP 报文提供流控与拥塞控制的封装 MPLS L2 VPN：多协议标签交换L3 VPN 技术 GRE VPN：通用路由封装，可在任意协议中封装任意协议的封装方法 IPSEC VPN：IP 安全，并不是单个协议，而是一系列协议组成的数据安全体系，包括 AH、ESP、IKE 等，实现对数据私密性、完整性保护与源校验 BGP/MPLS VPN：多协议 BGP，利用 MPLS 与 MP-BGP 技术 SSL VPN：安全套接字层，使用 SSL 协议实现远程 VPN GRE-VPNGeneric Routing Encapsulation 通用路由封装，是一种能在任意协议中封装任意协议的封装方法，可以直接使用 GRE 封装建立 GRE 隧道，为 IP 协议，协议号 47。以下为GRE 封装包的格式。 GRE 头包含了 2 字节的Protocol Type，用于指示载荷协议类型，IP协议为0x0800。此外还有扩展 GRE 头，增加了Key和Sequence Number，具备标识数据流和分组次序的能力。IP 协议使用协议号 47 标识 GRE 头，说明 IP 头后跟着 GRE 头。而 GRE 头中 protocol type 若为 0x0800，说明 GRE 头后跟着 IP 头。 双方通过 Tunnel 接口（逻辑接口）建立隧道，再通过实际物理接口进行转发。tunnel 口为载荷协议服务，物理口为承载协议服务。 GRE 隧道通信过程： 隧道起点路由查找：隧道两端的路由器必须是私网边界路由器。收到数据包时查找 ip 路由表 加封装：查找路由表确认下一跳为 tunnel 口，则进行 GRE 封装。 承载协议路由转发：对封装后的包进行公网路由查询。 中途转发：即公网转发 解封装：到达对端私网边界路由器后，该路由器检查 IP 地址，若是自己则解开 IP 头，发现有 GRE 头，就交给目标 Tunnel 口，tunnel 口解开 GRE 头 隧道终点路由转发：解开 GRE 头后，发现私网 IP 目的地址，然后查表转发。 每个运行 GRE 的路由器只有一个路由表，公网和私网之间只能通过不同的路由加以区分，因此公网私网 IP 地址能重复，且公网私网必须采用不同策略。 连接到私网的物理接口和 Tunnel 口属于私网路由 AS，采用一致的私网路由策略。 连接到公网的物理接口属于公网路由 AS，必须和公网采用一致的路由策略。 优点： 支持多种协议 支持 IP 路由协议和组播 缺点： 点对点隧道 静态配置隧道 缺乏安全性 不可分配地址空间 部署复杂 静态路由配置：配置到达目的 IP 的私网网段路由，下一跳为对端 Tunnel 口的地址。动态路由配置：将隧道和私网作为一个 AS 看待，动态路由需要将 Tunnel 口和私网都包括。 Tunnel 口虚假状态：GRE 本身不对隧道状态维护，系统默认根据接口状态设置 Tunnel 状态，即若物理链路中出现故障，物理口仍为 UP，则隧道口也为 UP，而此时隧道却不通。只存在于静态配置时。解决：tunnel 口 keepalive 机制：允许路由器探测隧道口的实际状态。路由器会从 tunnel 口周期发 keepalive 消息，默认周期 10s，若连续 3 次未收到，则认为隧道不通，会自动删除该 tunnel 口为出接口的静态路由。 L2TP-VPN对 PPP 协议链路提供隧道，允许二层链路端点和 PPP 会话点驻留在不同设备上，并采用分组交换技术进行信息交互。使用 PSTN/ISDN 拨号、xDSL 直接连接到 ISP 位于本地的 POP（存在点），或直接连接到 Internet 获得 IP 通信服务，然后 ISP 设备或用户设备建立 L2TP 通道连接对端。 L2TP 特点： L2TP 支持对用户和隧道的验证，和对客户端的动态地址分配。可使用 PPP 验证，也可使用 LAC 提供的 AAA 验证 具备点到网络特性。适合单个或少量接入 不提供加密，但可结合 IPSec 等加密 面向连接，为信息提供一定的可靠性 L2TP 组件： LAC：L2TP 访问集中器，隧道就在 LAC 和 LNS 间建立 LNS：L2TP 网络服务器 NAS：网络访问服务器，抽象概念，是远程访问的接入点，可以是 LAC 或 LNS 两种拓扑方式： 独立 LAC：ISP 提供 LAC，不依赖 IP 接入点。条件：ISP 支持 L2TP，验证系统支持 VPDN 属性特点：终端用户不需要配置 VPN 拨号软件，只需要执行普通拨号，登录一次就可以接入企业网。 客户 LAC：远程系统安装 VPDN 客户端，直接对 LNS 发起连接请求。不依赖 LAC，验证只能由 LNS 执行条件：远程系统必须接入 Internet，远程系统需要安装专用客户端软件并配置特点：只需配置 VPN 软件即可与企业网建立 VPN 连接。对用户的验证只能由 LNS 端执行。 L2TP 封装：L2TP 以 UDP/IP 为承载协议，UDP 端口号为 1701。 L2TP 头中Type字段标识消息类型，若为 1 表示控制消息，若为 0 表示数据消息。Tunnel ID字段标识 L2TP 控制连接，即隧道标识符，是在隧道建立时通过Assigned Tunnel ID AVP交换的。Session ID字段用于标识一个隧道中的各个会话，是在隧道建立时通过Assigned Session ID AVP交换的。 控制连接：在 L2TP 隧道内部，建立维护和释放会话与隧道控制消息：LAC 与 LNS 间交换的隧道内消息，用于对隧道操作。包含 AVP（属性值对，通过发送 AVP 对隧道建立维护或释放，即管理隧道和会话） L2TP 协议操作： 建立控制连接：由 PPP 触发（1）LAC 使用任意 UDP 端口向 LNS 的 UDP1701 端口发起连接（SCCRQ打开控制连接请求）（2）LNS 将连接重定向到一个随机 UDP 端口并回应（SCCRP打开控制连接应答）（3）LAC 收到后返回确认（SCCCN打开控制连接已确认）（4）LNS 收到后再确认，隧道建立（ZLB零长度体）若要执行隧道验证，可在SCCRQ或SCCRP中加上Challenge AVP（挑战 AVP）发起验证，接收方要在回应中加上Challenge Response AVP（挑战响应 AVP）。 建立会话：前提为控制连接的建立。由 PPP 模块触发LAC 发起：（1）LAC 向 LNS 发送ICRQ（入呼叫请求）发起会话建立（2）LNS 收到请求后返回ICRP（入呼叫应答）（3）LAC 收到应答后返回ICCN（入连接已连接）（4）LNS 再回应ZLB，会话建立LNS 发起：（1）LNS 发送OCRQ（出呼叫请求）发起会话建立（2）LAC 收到后返回OCRP（出呼叫应答）（3）LAC 执行呼叫，返回OCCN（出呼叫已连接）（4）LNS 收到后回应ZLB，会话建立会使用Tunnel ID和Session ID区分不同隧道和会话 隧道状态维护LAC 与 LNS 互发Hello消息维持会话，默认周期 60s，若三次未收到对方的 Hello 消息，则认为隧道断开。 关闭会话与控制链接关闭会话：（1）LAC 发送CDN（呼叫断开通知），通知 LNS 关闭会话（2）LNS 收到后返回ZLB，并关闭会话关闭控制连接：（1）LAC 发送StopCCN（停止控制连接通知），通知 LNS 关闭控制连接（2）LNS 收到后回应ZLB，并关闭控制连接 L2TP 验证 1.对拨入的远程系统 PPP 验证2.LAC 与 LNS 间隧道验证3.LNS 对远程系统再次 PPP 验证。方式分为三种：1）代理验证：LAC 将从远程系统得到的验证信息和自身的验证信息都发给 LNS2）强制 CHAP 验证：LNS 直接对远程系统进行 CHAP 验证3）LCP 重协商：LNS 与远程系统重新进行 LCP 协商，采用相应虚拟模板接口上配置的验证方式进行验证 LAC 端对远程系统用户的 AAA 验证包括： 本地验证：需要 LAC 端配置本地用户名、密码、服务类型等信息，与用户输入的通过对比进行验证。 远程验证：需要与 RADIUS 或 TACACS 服务器协同验证，需要在 RADIUS 或 TACACS 服务器上配置用户验证信息，LAC 将用户输入的信息发送给验证服务器进行验证。 下图为：独立LAC隧道会话建立 下图为：客户LAC隧道会话建立 IPSEC-VPN一种网络层安全保障机制。可实现访问控制、机密性、完整性校验、数据源校验、拒绝重播报文等。IPSec 是可扩展的体系，不受限于任何一种特定算法，可引入多种验证算法、加密算法、密钥管理机制。缺点：复杂，消耗大量资源、数据延迟、点对点、不支持组播 IPSec SAIPSec SA：IPSec 安全联盟，安全服务通过 SA 实现。SA 是双方的安全协定，包括协议、算法、密钥。SA 是单向的，入站和出站数据流分别由入站 SA 和出站 SA 处理。 SA 的三元组： SPI：安全参数索引，32 位数值 IP 目的地址：对方 IP 地址 安全协议标识符：标识 AH 或 ESP SA 建立方式： 手工配置：两端手动设置参数 自动协商：双方通过 IKE 生成维护 SA 具有生存时间，有两种方式： 时间：每隔定时长更新 SA 流量：每传输一定流量更新 SA SA 协商信息存放在 SPD（安全策略数据库），SPD 的项指向 SAD（安全联盟数据库）相应项 IKEIKE：因特网密钥交换。基于 UDP 协议，端口号 500，为 IPSec 提供自动协商交换密钥、建立 SA 服务，实际提供安全服务的是 IPSec，采用 DH 算法交换密钥（精髓）。且可以定时更新密钥和 SA，提供了完善的前向安全性。可以为 IPSec 自动重新建立 SA，允许 IPSec 提供抗重播服务（通过 SPI 值）。 采用 ISAKMP 的密钥交换框架体系IKE 安全机制： 身份验证：预共享密钥（默认）、RSA 数字签名、DES 数字签名 DH 密钥交换 PFS 完善前向安全性：通过在第二阶段再进行一次 DH 交换，使 IKE SA 密钥与 IPSec SA 密钥无派生关系 协商两个阶段： 阶段 1：建立一个 IKE SA，为阶段 2 提供保护分为主模式 main 和野蛮模式 aggressive 主模式：强制实现的阶段 1 交换模式。共三步，六条消息 策略协商：A 向 B 发送本地 IKE 策略，B 查找匹配的策略，并确认其中协商属性包括：加密算法，散列算法（MD5、SHA 等），验证方法（预共享密钥、DSS、RSA），DH 组信息（默认 MODP 768），DH 公共值，IKE 生存时间，身份信息 DH 交换：A 向 B 发起密钥生成信息，B 生成密钥并回应。 ID 交换验证：A 向 B 发送身份和验证数据，B 回应身份验证。 野蛮模式：远程拨号时，由于拨号用户 IP 无法确定，可以使用野蛮模式 A 向 B 发送本地 IKE 策略，开始 DH 交换 B 查找匹配策略，回应验证信息 A 接收确认信息并验证，生成密钥，向 B 发送验证载荷 B 验证 阶段 2：在 IKE SA 的保护下完成 IPSec SA 的协商。采用快速模式 野蛮模式安全性差于主模式，但过程简单快速。在不知道对端 IP 且需要使用预共享密钥的情况下，必须用野蛮模式。 IPSec 包处理流程出站包处理： 查找 SPD，三种结果：丢弃、旁路安全服务：直接转发、提供安全服务：查找 IPSec SA 若第一步结果是提供安全服务，就在 SAD 中找 IPsec SA，若找到就根据参数提供安全服务，若找不到就查找 IKE SA 若找到 IKE SA，就只要创建 IPSec SA，若找不到 IKE SA，就要先创建 IKE SA，再创建 IPSec SA。 入站包处理： 检查目的地址是否本地，若是则检查数据包是否被 IPSec 保护 若被 IPSec 保护，则查找 IPSec SA，若不被 IPSec 保护，则交给上层 若找到 IPSec SA，则解封装，若未找到则丢弃 安全协议 AH：验证头，提供完整性保护、数据源验证、抗重播服务。不支持机密性保护。IP 协议，协议号 51。AH 头格式：Next Header：8 位，指示 AH 头后的载荷协议类型Payload Length：8 位，指示 AH 的长度并减 2，单位为 32 位SPI：32 位任意数值，用于和目的 IP 地址和安全协议标识结合，唯一标示一个 SASequence Number：32 位无符号整数，SA 建立时为 0，随着数据包发送而增大，接收方通过该值确定数据包的序列Authentication Data：包含该数据包的完整性校验值ICV（使用 HMAC 算法对 IP 头+AH 头+载荷+共享密钥加密计算），变长且必须是 32 位的整数倍。AH 强制实现HMAC-MD5-96和HMAC-SHA-1-96两种验证算法 ESP：封装安全载荷，有 AH 所有功能且支持加密。包含 ESP 头和 ESP 尾。IP 协议，协议号 50。ESP 头格式：Next Header：同上。强制包含。SPI：同上Sequence Number：同上Payload Data：Next Header描述的数据，即载荷数据，长度为字节的整数倍。若加密，ESP 强制实现了基础加密算法 DES-CBC。强制包含。Padding：填充，使载荷数据达到指定长度。Pad Length：填充长度，范围为 0 到 255。强制包含。Authentication Data：ICV，长度由验证算法决定。可选，验证服务开启时才包含。同样强制实现HMAC-MD5-96和HMAC-SHA-1-96ESP 尾格式：Padding、Pad Length、Next Header IPSec 有两种工作模式： 传输模式：保护端到端安全，两个终端间直接运行 IPSec，所有加解密、协商都是端系统完成，网络设备完全不参与 IPSec，只进行正常路由转发。 隧道模式：保护站点到站点安全，两个安全网关间运行 IPSec，整个数据包都计算 AH 或 ESP 头，AH 或 ESP 头加上数据都被封装在一个新的 IP 包中。所有加解密、协商都是安全网关完成，终端主机不参与。 因此 AH 和 ESP 对两种工作模式分别有封装的方式： AH 传输模式原 IP 包、AH 头与密钥通过散列函数（如 RSA）生成校验值。将校验值封装在 AH 头中，再由 TCP 和原 IP 头封装。 隧道模式新 IP 头（根据隧道起点终点建立隧道 IP 头）、AH 头与原 IP 包生成校验值。将校验值封装在 AH 头中，封装原 IP 包，再用新 IP 头封装。 ESP 传输模式原 IP 包（不包括原 IP 头）、ESP 尾与密钥加密（通过 DES 等算法）生成密文。将生成的密文与 ESP 头和验证密钥通过数字签名算法（通过 RSA）生成校验值。最后将用 ESP 头和原 IP 头封装密文和校验值。 隧道模式将整个原 IP 包、ESP 尾、加密密钥通过加密算法（如 DES）生成密文。将密文与 ESP 头和验证密钥通过散列函数（如 RSA）生成校验值。用 ESP 头和新 IP 头封装密文和校验值。 GRE-OVER-IPSEC使用Gre Over IPsec的原因：GRE 不保证数据机密性与完整性，不能数据源验证。 特性 GRE IPSec 多协议 支持 不支持 虚接口 支持 不支持 组播 支持 不支持 路由协议 支持 不支持 IP 协议族 支持 支持 机密性 不支持 支持 完整性 不支持 支持 数据源验证 不支持 支持 封装：原始 IP 包被封装在 GRE 隧道包中。GRE 隧道包被封装在 IPSec 包中。 IPSEC-OVER-GREMPLSMulti Protocol Labal Switching：多协议标签交换MPLS 使用定长标签封装网络层分组。标签位于数据链路层和网络层之间，称为 2.5 层。多协议指：MPLS 被多种二层协议封装，也可封装多种三层协议 两种工作模式： 帧模式：用于 PPP、以太网、帧中继 信元模式：作用于 ATM 组成： LSR：位于 MPLS 内部的核心交换机或路由器，提供标签交换和分发 LER：位于 MPLS 网络边缘，提供标签映射、移除和分发 FEC：转发等价类，转发过程中以等价方式处理的一组数据分组，可根据 IP 地址、隧道、COS 标识创建 FECLSP：标签交换通道，属于同一个 FEC 的数据流在每个节点赋予一个确定的标签，按照一个确定的标签转发表项进行转发，每个 FEC 流会有固定的转发路径，该路径就成为该 FEC 的 LSP MPLS 标签：4 个字节。分为四个字段 Label：标签值，20 位，标签转发表的关键索引 EXP：标识 QoS 优先级，3 位 S：栈底标识，1 位，若为 1 说明是最后一个标签，若为 0 说明后面还有 MPLS 标签。可实现多层 MPLS 标签嵌套 TTL：存活时间，8 位，每经过一台 LSR，TTL 就减 1 链路层协议为 MPLS 分配的标识： PPP：0x0281 以太网或 HDLC：0x8847 帧中继：0x0080 标签分配协议：用于在 LSR 间分配标签，建立 LSP。有以下四种： LDP 标签分发协议：最通用 CR-LDP 基于路由受限的标签分发协议：可进行路由约束、QoS，用于流量工程 RSVP-TE 基于流量工程扩展的资源预留协议：用于流量工程中 MPLS 标签分配 MP-BGP 多协议扩展 BGP 协议：为 BGP 路由分配 MPLS 标签 LDP 消息类型： 发现Discover消息：LDP 邻居的发现维护 会话Session消息：LDP 邻居会话的建立维持与终止 通告Advertisement消息：向 LDP 邻居宣告 Label、地址等信息 通知Notification消息：向 LDP 邻居通知事件或错误 所有 LDP 消息都采用 TLV 结构，具有扩展性（TLV：Type-Length-Value 类型-长度-值） LDP 会话建立维护： 邻居发现：互发Hello消息，组播地址224.0.0.2，UDP 端口 646 TCP 连接：LSR-ID 大的，即 IP 地址大的一方主动发起，TCP 端口 646 会话建立：Master 发出初始化 Initialization 消息，携带协商参数。协商成功 Session 建立 会话维持：互发 Keepalive 消息维持会话。LSR 之间发送 Label mapping 消息，形成标签转发表。期间若收到任何差错消息都会关闭会话，断开 TCP 连接。 LDP 邻居状态机：两台 LDP 邻居间建立LDP Session后，状态会维持在Operational 标签分配过程：上下游根据数据转发方向而定。LDP Session建立后路由器根据路由表分配标签，生成 MPLS 标签转发表标签转发表包含：入标签IN，出标签OUT，出接口next-hop标签为随机生成，16 以下系统保留 标签分配模式： DOD 下游按需标记分配：上游 LSR 向下游 LSR 发送标签请求信息。下游 LSR 为此 FEC 分配标签，通过标签映射消息反馈给上游 LSR。原则：下游设备需要收到上游的标签请求才能分配标签 DU 下游自主标记分配：下游 LSR 在 LDP 会话建立后主动向上游 LSR 发布标签映射消息，不需等待上游请求 标签控制模式： 有序：只有最下游设备能分发标签，上游设备只有收到了下游的标签映射消息，才能再向上游发送标签映射信息。使得 MPLS 的转发是端到端的 独立：不管是不是最下游，不管是否收到下游的标签映射信息，都向上游发送标签映射信息。任何的数据流经过 MPLS 网络都可进行 MPLS 转发 标签保持方式：收到下游的标签映射后，是否记录标签信息的原则 保守：只保留下一跳邻居的标签，丢弃所有非下一跳邻居发来的标签优点：节约空间缺点：当网络故障时，LSP 收敛较慢 自由：保留来自邻居的所有标签。优点：网络故障后，路由切换时收敛快缺点：消耗空间 常用组合：DU + 有序 + 自由 MPLS 转发：第一阶段：标签 PUSH：报文进入 MPLS 网络，LER 设备发现报文目的 IP 地址有关联的标签，对报文进行压标签。该报文就变为了 MPLS 报文第二阶段：标签 SWAP：报文在 MPLS 网络内进行标签交换。第三阶段：标签 POP：报文转出 MPLS 网络时，在最后一跳弹出标签。倒数第二跳的设备上标签表的出标签为 3，说明此为倒数第二跳。一旦包查找到出标签为 3，就直接弹出标签。 BGP-MPLS-VPN解决了传统 VPN 的问题：1.实现隧道动态建立 2.解决本地地址冲突问题 3.VPN 私网路由易于控制 多 VPF 组网多 VRF 技术用于解决同一台设备（PE）上地址冲突问题。存在以下路由器角色： CE：直接与 ISP 相连的用户设备 PE：公网边缘路由器，与 CE 相连，负责 VPN 接入 P：公网核心路由器，负责路由与快速转发 实现：将一台路由器划分为多个 VRF，每个 VRF 相互独立，拥有各自的路由表、端口、协议，每个 VRF 类似一台虚拟路由器。未划分 VRF 的路由在公网路由表中。各个 VRF 与各自的网络运行一个实例，该实例学到的路由只能加入该 VPN 路由表。实例与所属 VPN 进行绑定。并且，端口与 VPN 绑定，与 VRF 绑定的接口只会出现在该 VRF 对应的路由表中，当报文从该接口进入路由器后只能查询该 VRF 对应的路由表。确保了不同 VRF 数据间不会冲突。 多 VRF 与路由协议多实例：各 VRF 与各自用户网络之间运行一个路由实验，该路由实例学习到的路由只能加入该 VPN 的路由表。各个路由实例与所属 VPN 绑定，互相独立，只能学到各自的邻居信息。 MP-BGPMP-BGP（Multi Protocol BGP，多协议 BGP）是对 BGP 根据特性（TCP 连接、TLV 扩展属性位）进行扩展的协议。MP-BGP 相对于 BGP 的新增特性： 普通 BGP 只能传递 IPv4 信息，MP-BGP 能承载多个协议路由信息。 新增了MP_REACH_NLRI和MP_UNREACH_NLRI两个属性，并新增了扩展团体属性（Extended_Communities）。 MP-BGP 可传递 BGP MPLS VPN、L2VPN、6PE 等路由信息。 MP_REACH_NLRI和MP_UNREACH_NLRI两个属性都是路由更新消息属性。MP_REACH_NLRI代替了原 BGP 更新消息中的NLRI和Next-hop。增加了地址族的描述 Address-Family、私网 Label 和 RD，包含原有的 Next-hop。若地址族描述为 VPNv4，则 NLRI 包含两个部分，一个是私网标签（一个 MPLS 标签），第二部分是 VPNv4 地址（RD+IPv4 地址）MP_UNREACH_NLRI代替了原 BGP 更新消息中的Withdrawn Routes。可撤销通过MP_REACH_NLRI发布的各种地址族的路由。包含Address-Family和Withdrawn Routes。 VPNv4 地址族主要用于 PE 路由器间传递 VPN 路由，并只存在于 MP-BGP 路由信息和 PE 设备的私网路由表中，即只出现在路由的发布学习过程中，在穿越 ISP 公网时，数据包头是没有 VPNv4 地址的。 下图为MP_REACH_NLRI属性 下图为MP_UNREACH_NLRI属性 BGP 的扩展团体属性：RT（Route Target）路由目标。本质是每个 VPN 实例表达自己的路由取舍方式。RT 的格式有三种，都表示 RT。 0x0002：2 字节的 AS 号，加上 4 字节的用户自定义数字，如100:1、200:1 0x0102：4 字节的 IP 地址，加上 2 字节的用户自定义数字，如192.168.1.1:1、10.1.1.1:2 0x0202：4 字节的 AS 号，加上 2 字节的用户自定义数字 通常设置为冒号后的数字设置为 VPN 实例编号。 RT 由两个部分组成：Export Target和Import Target。MP-BGP 在 PE 间交互私网路由时，需要遵循以下规则： 在 PE 设备上，发送某一个 VPN 用户的私网路由给 BGP 邻居时，需要在扩展团体属性区域中增加该 VPN 的Export Target属性。 在 PE 设备上，需要将收到的 MP-BGP 路由的扩展团体属性中携带的 RT 值与本地每个 VPN 的Import Target对比，若存在交集，则可以将该路由添加进实例的路由表。 通过对 RT 的操作可实现两种模式：Hub-Spoke和Extranet。Hub-Spoke模式：用户总部可与每个分布互通，但每个 VPN 分布之间禁止互通。Extranet模式：使指定的节点可以与其他节点互通。 RD（Route Distinguisher）路由区分。本质就是用于私网路由的撤销，因为在撤销路由时是不能携带属性值的（包括 RT），PE 在删除路由时无法判断撤销哪个 VPN 的路由。长度 6 字节。RD 有两种格式： 2 字节的 AS 号，加上 4 字节用户自定义数，如100:1 4 字节的 IP 地址，加上 2 字节用户自定义数，如192.168.1.1:1 只要保证存在相同地址的两个 VPN 实例的 RD 不同即可，但最好为每个 VPN 实例配置一个 RD。若两个 VPN 实例中存在相同 IP 地址，则这两个实例一定不能互访，间接互访也不行。 私网标签 Label，用于帮助 PE 判断该报文前往的 VPN，是通过 MPLS 的多标签嵌套实现的。 BGP MPLS VPN 实现分为以下步骤： 公网隧道建立：公网 IGP 协议开启，PE 间互通。 本地 VPN 建立：PE 上设置本地 VPN 并设置 RD、RT 属性，然后将 VPN 与接口绑定，即配置 VRF。 私网路由的学习：PE 与 CE 间运行路由协议多实例，各 VPN 实例进行路由学习。PE 间建立 MP-BGP 邻居，下游 LSR 分配标签，建立标签转发表。并会生成一条 MP-BGP 更新消息，包含 VPNv4 路由前缀（即 IP 地址）、下一跳地址、RT 属性、私网标签。PE 设备会对比 RT 值，若通过就会记录该路由信息并发布给本地 VPN。 私网数据转发：数据会根据标签转发表进行转发。","categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"vpn","slug":"vpn","permalink":"https://coconutmilktaro.top/tags/vpn/"}]},{"title":"FTP笔记","slug":"FTP笔记","date":"2018-06-05T16:07:17.000Z","updated":"2022-05-30T02:51:53.799Z","comments":true,"path":"2018/FTP笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/FTP%E7%AC%94%E8%AE%B0/","excerpt":"本篇笔记包含以下内容 FTP 原理 VSFTP 搭建 服务器端 配置文件/etc/vsftpd/vsftpd.conf简单解析 认证访问控制 使用本地用户认证 使用虚拟账户 常用客户端软件 TFTP 原理","text":"本篇笔记包含以下内容 FTP 原理 VSFTP 搭建 服务器端 配置文件/etc/vsftpd/vsftpd.conf简单解析 认证访问控制 使用本地用户认证 使用虚拟账户 常用客户端软件 TFTP 原理 FTP 原理File Transfer Protocol 文件传输协议，基于 TCP 协议，采用 C/S 模式，控制连接端口 21，数据连接端口 20。 控制连接：负责 FTP 客户端与服务器交互命令与信息的传输，在整个会话过程中始终打开。 数据连接：负责客户端与服务器数据的传输，传输完毕就会关闭 文件类型： 一共有４种，但目前主流仅支持以下两种。 ASCII：默认模式，发送方将文件转为 ASCII 码传输，适合文本文件传输 二进制：也称图像文件传输模式，按比特流传输，适合程序文件传输 格式控制：有三种选项，但目前主流配置只允许非打印。非打印：表示文件中不含有垂直格式信息。 数据结构：有四种选项，但主流配置只允许文件结构。文件结构认为数据是一个连续的字节流。 传输模式：有四种选项，但主流仅允许流方式，文件以字节流形式传输。对于文件节后，发送方在文件结束处提示关闭数据连接。 数据传输方式： 主动 PORT 首先客户端（随机端口）与服务器（21 端口）TCP 三次握手建立连接，建立控制连接通道 客户端向服务器发送 PORT 命令，告知服务器使用主动模式。其中 PORT 命令携带参数（客户端 IP 地址, P1, P2），P1 与 P2 用于标识客户端数据连接的临时端口号，具体为 256*P1+P2，IP 地址也是四段，每段用逗号分隔 服务器收到 PORT 命令后按照参数用 20 端口与客户端指定端口三次握手建立数据传输通道。 数据传输完毕，发送方发送 FIN 报文，关闭数据连接 注：若客户端在防火墙内部网络，主动方式会出现问题，因为客户端提供的端口是随机的，防火墙若未放行该端口，则无法建立 FTP 连接。此时需要使用被动方式建立连接 被动 PASV 首先客户端（随机端口）与服务器（21 端口）TCP 三次握手建立连接，建立控制连接通道 客户端向服务器发送 PASV 命令，参数与 PORT 一致。但 IP 是服务器的，标识的是服务器端的临时端口号。 客户端用随机端口与服务器的指定临时端口 TCP 三次握手建立数据连接通道。 数据传输完毕，发送方发送 FIN 报文，关闭数据连接 FTP 应答格式：服务器端处理完命令后，会将状态信息，如命令是否执行成功、出错类型、是否就绪等，通过控制连接发送给客户端，即应答。应答的目的就是对数据传输过程进行同步，也为了让客户端了解服务器目前的状态。 FTP 应答由 3 个 ASCII 码数字组成，并跟随解释性文本符号。数字面向机器，文本面向用户。 第一位： 1：确定预备应答：仅仅是在发送另一个命令前期待另一个应答时启动 2：确定完成应答：要求的操作已完成，可接受新命令 3：确定中间应答：该命令已被接受，另一个命令必须被发送 4：暂时拒绝完成应答：请求的命令没有执行，但差错状态是暂时的，命令以后可以再发。 5：永久拒绝完成应答：该命令不被接受，并要求不要再重试。 第二位： 0：语法错误 1：一般性的解释信息 2：与控制和数据连接有关 3：与认证和账户登录过程有关 5：与文件系统有关 第三位：未明确规定，指示对第二位的进一步细化。 常见 FTP 应答： 110：重新启动标记应答 120：服务在多久时间内准备 125：数据连接打开，传输开始 150：文件状态正常，打开数据连接端口 200：命令执行成功 202：命令执行失败 211：系统状态或是系统求助响应 212：目录的状态 213：文件的状态 214：帮助信息 215：名称系统类型 220：新的联机服务准备 221：服务控制连接关闭，可注销 225：数据连接开启，但无传输动作 226：关闭数据连接端口，请求的文件操作成功 227：进入 passive modes 250：请求的文件操作完成 331：用户名已接受，需要输入密码 332：登录时需账号信息 350：请求的命令需要进一步的命令 421：无法提供服务，关闭控制连接 425：无法开启数据连接 426：关闭联机，终止传输 450：请求的操作未执行 451：命令终止，有本地错误 452：未执行命令，磁盘空间不足 500：格式错误，无法识别命令 501：参数语法错误 502：命令执行失败 503：命令顺序错误 504：命令所接的参数不正确 530：未登录 532：存储文件需要账户登录 550：未执行请求的操作 551：请求的命令终止，类型未知 552：请求的文件终止，储存位溢出 553：未执行请求的命令，名称不正确 VSFTP 搭建Very Secure FTP 安全文件传输软件。针对系统的程序权限设计，有以下特点： 将 PID 的权限降低 使用 chroot 机制 服务的启动者就是一个一般用户 任何需要执行具有较高执行权限的 VSFTP 指令都由特殊的上层程序控制 VSFTPD 有两种启动方式： stand alone：CentOS 默认使用该方式启动 VSFTPD，适合主要用于提供大量下载的任务，服务速度快。使用 systemd 管理就是 stand alone super daemon：适合内部人员小范围使用。使用 xinetd 管理就是 super daemon 服务器端安装 vsftpd 服务yum install vsftpdsystemctl enable vsftpdsystemctl start vsftpd 安装完在/etc/vsftpd中有四个默认文件： ftpusers：指定哪些用户不能访问 FTP 服务器，即黑名单 user_list：实行访问控制的用户列表 vsftpd.conf：VSFTP 主配置文件 vsftpd_conf_migrate.sh：VSFTPD 操作的一些变量和设置的脚本 配置文件/etc/vsftpd/vsftpd.conf简单解析listen = YES # IPv4监听，默认是以StandAlone方式启动listen_ipv6 = NO # IPv6监听 # ipv6监听若v4为yes则v6必须为no,同理v6为yes则v4为nolisten_address = # 监听的IP地址listen_port = 21 # 监听的端口port_enable = YES # 开启端口监听ftp_data_port = 20 # 数据传输端口20connect_from_port_20=YES # 数据连接的端口号pasv_enable = YES # 是否启用被动连接pasv_max_port = # 被动连接的最大端口号pasv_min_port = # 被动连接的最小端口号connect_timeout = 60 # 主动连接若60秒tcp无法建立就不建立了accept_timeout = 60 # 被动连接若60秒tcp无法建立就不建立了max_clients = 2000 # 最多允许2000用户同时登录（0为不限制）max_login_fails = 3 # 最多允许3次登录失败max_per_ip = 20 # 同一地址最多允许多少连接（0为不限制）data_connection_timeout = 300 # 数据连接超时时间（数据无响应）就断开idle_session_timeout = 300 # 用户登录上后无操作时间300s则断开user_config_dir = /etc/vsftpd/conf #dirmessage_enable=YES # 是否启用目录提示信息，默认YES。 # 当用户进入某个目录时，会先检查该目录是否存在message_file参数指定的文件 # 若有就显示文件中的内容，通常用于放置欢迎语或目录说明message_file = # 设置文件路径，该文件用于存放目录的说明或欢迎语（当dirmessage_enable为YES时生效）xferlog_enable=YES # 是否启用详细记录上传下载的日志功能，日志文件路径由xferlog_file指定xferlog_file = /var/log/xferlog # 设置文件路径，该文件用于存放目录的说明或欢迎语（当xferlog_enable为YES时生效）pam_service_name=vsftpd # PAM认证服务配置文件名，放在/etc/pam.d/目录中tcp_wrappers=YES # 开启TCP_wrappers防火墙，用于在一定程度上限制某种服务的访问权限ftpd_banner = # 登录FTP服务器时的欢迎语，默认为空download_enable = YES # 是否允许下载userlist_enable = YES # 是否启用用户名单（对名单中的用户进行访问控制）chroot_list_enable = YES # 是否启用锁定用户在自己的主目录中的功能。 # 被锁定的用户登录FTP服务器后只能进入自己的主目录，不能进入其他目录，默认为NO，应该禁用。 # 锁定的用户名单文件由chroot_list_file参数指定chroot_local_user = YES # 是否将用户限制在自己的根目录中chroot_list_file = /etc/vsftpd/chroot_list # 实行或不实行chroot的用户名单，默认就是该文件。文件中是一个用户一行记录 # 若chroot_list_enable为enable，则该文件中的用户会chroot # 若chroot_local_user为enable（chroot_list_enable为enbale仍为前提），则该文件中的用户不会chrootwrite_enable = YES # 是否允许修改，默认NOuserlist_enable=YES # 当用户登录FTP服务器时，在输入完账户名后，服务器会根据userlist_file中指定的用户列表进行控制 # 若在该文件中，则禁止该用户输入密码。默认NOuserlist_file = /etc/vsftpd/user_list # 禁止访问的，默认该文件uesrlist_deny = NO # 是否不允许该文件中的用户登录ftp，需要userlist_enable=YES # NO表示只允许该文件中的用户访问，YES表示禁止文件中的用户登录FTPuse_localtime = YES # VSFTPD默认使用GMT（格林威治）时间，为防止文档时间错误，需要改为YES，即使用本地时间banner_file = /etc/vsftpd/welcome.txt # 当用户登录时会显示的文字，文件必须存在 认证访问控制VSFTPD 提供三种认证方式： 匿名访问 anoymous：无需认证即可登入 本地用户 local：使用 ftp 服务器中的用户登录 虚拟用户：创建独立 ftp 账户。是最安全的 匿名访问无需提供真正用户名和密码就能登录 FTP 服务器。最好不要开启匿名登录，若要开启就进行限制行为。 只允许匿名用户使用少量基本的操作命令 限制文件下载数量，不要允许上传 限制匿名登录的最大同时联机数 配置文件相关参数： anonymous_enable = YES # 是否允许匿名用户登录，默认YES # 因为是匿名模式，所以要开启。否则尽量不要匿名访问anon_umask = 077 # 匿名用户创建文件的umask值，默认077，此时匿名传递的文档权限为600anon_root = /var/ftp/anon # 匿名用户的ftp根目录anon_upload_enable = NO # 允许匿名用户上传文件（这项生效的条件为write_enable为YES且ftp匿名用户对该目录有写权限），默认NOanon_mkdir_writable_enable = NO # 是否允许匿名用户创建目录（需要该目录的父目录的写权限），默认为NOanon_other_writable_enable = NO # 是否开放匿名用户其他写入权限，最好关闭anon_world_readable = YES # 仅允许匿名用户下载可读文档anon_max_rate = 0 # 匿名用户最大传输速率，0为不限制，单位字节/秒# 传输速率的控制大概有20%的上下浮动，即范围为速度*80%到120%no_anon_password = NO # 匿名用户登录是否需要密码（NO为需要，YES为不需要，默认为NO） # 若为需要，登录时输入任意字符串即可 在客户端上匿名登录 ftp 服务器：只要在输用户名时输入 anonymous，并任意输入字符串作为密码 本地用户使用 ftp 服务器本地的用户进行登录，会更加安全，也是最常用的方式。 配置文件相关参数： local_enable = YES # 允许本地用户登录ftp，默认YES，在实际工作环境中，应该将这项设为NOlocal_umask = 022 # 本地用户上传文件的umask，默认为077local_root = /var/ftp # 本地用户的根目录local_max_rate = 0 # 本地用户最大传输速率，0为不限制，单位字节/秒 虚拟用户 本地用户登录时会自动转为虚拟用户，即使有大量用户登录，但最终也仅仅转为一个虚拟用户，避免了创建大量的系统用户。 guest_enable = YES # 是否开启虚拟账户guest_username = ftp # 虚拟账户映射的用户名 使用本地用户认证创建 FTP 用户限制该用户仅能登录 FTP 服务器useradd ftpuser -s /sbin/nologin并设置密码。为该用户创建一个主目录，即用户登录 FTP 后的根目录。mkdir -p /data/ftp/ftpuser/pub。其中/data/ftp/ftpuser为用户ftpuser的主目录，该目录不得上传文件，该目录下的pub目录供 ftpuser 用户上传文件。usermod -d /data/ftp/ftpuser ftpuserchmod a-w /data/ftp/ftpuserchmod a+w -R /data/ftp/ftpuser/pub 配置文件中几条修改项： local_enable = YESlocal_root = /data/ftp 使用虚拟账户首先创建虚拟用户文件/etc/vsftpd/visualusers，文件中列出虚拟用户名和密码 ftp_visual_1123456ftp_visual_2234567 生成虚拟用户数据库（可选）。需要工具libdb4-utils（Berkeley DB 工具，CentOS 中是该软件包） db_load -T -t hash -f /etc/vsftpd/visualusers /etc/vsftpd/visualusers.db ，然后修改该备份文件的访问权限chmod 600 /etc/vsftpd/&#123;visualusers,visualusers.db&#125; 创建 PAM 文件，设置账户验证。 PAM 配置文件位于/etc/pam.d/vsftpd，该文件的名称取决于 vsftpd 主配置文件的pam_service_name字段。将默认配置注释，然后添加以下内容： auth required /lib64/security/pam_userdb.so db=/etc/vsftpd/visualusers.dbaccount required /lib64/security/pam_userdb.so db=/etc/vsftpd/visualusers.db 所有的虚拟用户都需要映射到一个真实的系统用户，因此需要添加一个系统账户并设置家目录。 useradd -s /sbin/nologin -d /home/virtual virtual 查看修改主配置文件（有的不用改）： local_enable = YESchroot_local_user = YESpam_service_name = vsftpduser_config_dir = /etc/vsftpd/visual_config # 设置虚拟用户配置文件主目录 创建虚拟用户配置文件的存放目录/etc/vsftpd/visual_config，这样可以为每个账户做单独的权限设置 创建用户visual的独立配置（举例）： # vim /etc/vsftpd/visual_config/visualguest_enable = YESguest_username = ftp_visual_1anon_max_rate = 100000 常用客户端软件TFTP 原理Trivial File Transfer Protocol 简单文件传输协议，基于 UDP 协议，端口号 69特点： 仅提供简单文件传输功能（上传，下载） 无存取授权与认证机制，无目录功能 由客户端发起 下载过程： 客户端向服务器发送读请求 服务器根据请求回应数据报文（块编号从 1 开始） 客户端收到数据后回应确认报文。重复 2.3 步直至完成下载 上传过程： 客户端向服务器发送写请求 服务器回应确认报文（块编号为 0） 客户端发送数据报文（块编号从 1 开始） 服务器收到后回应确认报文。重复 3，4 步直至上传完成 文件传输时，将文件分成多个文件块，封装到数据报文中并打上文件块编号 传输文件模式： netASCII：对应 FTP 的 ASCII 模式 octet：对应 FTP 二进制模式 协议报文： RRQ 读请求报文 WRQ 写请求报文 数据报文 确认正确/错误报文 报文的头两个字节是操作码字段，1 为读请求，2 为写请求，3 为数据报文，4 为确认正确，5 为错误。文件传输过程中读写出错就发送差错报文，数据传输就停止，差错报文不会被确认也不会重传。TFTP 每次传输的数据报文中文件块大小固定为 512 字节，若文件大小刚好是 512 字节的整数倍，则传完文件后还要再发一个空文件块的数据报文表明文件传输完成。","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"ftp","slug":"ftp","permalink":"https://coconutmilktaro.top/tags/ftp/"}]},{"title":"Iptables、Selinux与防火墙笔记","slug":"Iptables、Selinux与防火墙笔记","date":"2018-06-05T16:04:20.000Z","updated":"2022-05-30T02:51:53.804Z","comments":true,"path":"2018/Iptables、Selinux与防火墙笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Iptables%E3%80%81Selinux%E4%B8%8E%E9%98%B2%E7%81%AB%E5%A2%99%E7%AC%94%E8%AE%B0/","excerpt":"本片包含以下内容： Iptables Netfilter/Iptables 框架 iptables 规则 iptables 应用 Selinux firewalld 参考文章","text":"本片包含以下内容： Iptables Netfilter/Iptables 框架 iptables 规则 iptables 应用 Selinux firewalld 参考文章 IptablesNetfilter/Iptables 框架Netfilter 是 Linux 2.4.x 引入的一个子系统，它作为一个通用的、抽象的框架，为每种网络协议都提供一整套的 hook 函数的管理机制，使得诸如数据包过滤、网络地址转换(NAT)和基于协议类型的连接跟踪成为了可能。Netfilter 的架构就是在整个网络流程的若干位置放置了一些检测点（HOOK），而在每个检测点上登记了一些处理函数进行处理。 Netfilter 采用的关键技术： 连线跟踪（Connection Tracking）：是包过滤、地址转换的基础，它作为一个独立的模块运行。在协议栈低层截取数据包，将当前数据包及其状态信息与历史数据包及其状态信息进行比较，从而得到当前数据包的控制信息，根据这些信息决定对网络数据包的操作，达到保护网络的目的。 包过滤（Packet Filtering）：检查通过的每个数据包的头部，然后根据规则处理 地址转换（NAT）：网络地址转换分为源 NAT（SNAT）、目的 NAT（DNAT）和端口转换（PNAT）。SNAT 修改数据包的源 IP，DNAT 修改数据包的目的 IP。SNAT 在数据包送出之前的最后一刻做好转换工作，DNAT 在数据包进入后立刻完成转换。 包处理（Packet Mangling）：可以设置或改变数据包的服务类型（TOS），改变包的生存期（TTL），在包中设置标志值，利用该标志值可以进行带宽限制和分类查询。 资料摘自百度百科-netfilter Netfilter 为 IPv4 定义了 5 个 hook 函数，这些 hook 函数会在数据报流过协议栈的 5 个关键点被调用。 NF_IP_PRE_ROUTING：刚刚进入网络层的数据包通过此点（已完成版本号，校验和等检测）， 目的地址转换在此点进行 NF_IP_LOCAL_IN：经路由查找后，送往本机的数据包通过此检查点，INPUT 包过滤在此点进行 NF_IP_FORWARD：要转发的包通过此检测点，FORWARD 包过滤在此点进行 NF_IP_POST_ROUTING：所有马上要通过网络设备出去的包通过此检测点，内置的源地址转换 SNAT 功能（包括地址伪装）在此点进行 NF_IP_LOCAL_OUT：本机进程发出的包通过此检测点，OUTPUT 包过滤在此点进行 Netfilter 所有的过滤规则都以模块存放在/usr/lib/modules/$(uname -r)/kernel/net/netfilter/目录中。在 Linux 内核版本 2.6 前，netfilter 分为IPv4版和IPv6版，分别存放在/usr/lib/modules/$(uname -r)/kernel/net/ipv4和/usr/lib/modules/$(uname -r)/kernel/net/ipv6中，Linux2.6 后进行了整合，使得 Netfilter 更加简单高效。 iptables 规则iptables 是一个工具，位于用户空间，用于插入，修改，删除数据包过滤表的规则。 iptables 分为三部分： 表：分为四张表 raw 表：是否对该数据包进行状态跟踪 mangle 表：为数据包设置标记，修改数据包（TOS，TTL，MARK） nat 表：修改数据包中源、目的 IP 地址或端口 filter 表：过滤数据包（对数据包） 顺序：raw -&gt; mangle -&gt; nat -&gt; filter 链：分为五条链 在路由选择前处理数据包（PREROUTING） 处理流入的数据包（这条规则起到保证内网不被侵犯的关键作用）（INPUT） 处理流出的数据包（OUTPUT） 处理转发的数据包（FORWARD） 在路由选择后处理数据包（POSTROUTING） 链顺序： 入站：prerouting -&gt; input 出站：output -&gt; postrouting 转发：prerouting -&gt; forward -&gt; postrouting 规则：规则被分组在链中，规则被添加到相应的链中，链被添加在表中。规则表默认是允许，则规则链就是被禁止的规则。若规则表是禁止的，则规则链就是被允许的规则 完整包过滤流程： 包到达网络接口 进入 raw 表的 prerouting 链（在连接跟踪前处理数据包） 连接跟踪（若要做） 进入 mangle 表的 prerouting 链，修改数据包 进入 nat 表的 prerouting 链，做 DNAT（目标地址转换，改变数据包目的地址使包能到达内网某服务器），但不做过滤 路由判断 若是要转发：进入mangle 表 forward 链，然后进入filter 表的 forward 链过滤，进入mangle 表的 postrouting 链，进入nat 表的 postrouting 链，做SNAT，但不过滤，然后数据包离开本机。 若是发给本地的：进入mangle 表的 input 链，进入filter 表的 input 链，对数据包过滤，然后交给本地程序，处理完后先判断路由，进入raw 表的 output 链，连接跟踪对包的处理，进入mangle 表的 output 链，可修改数据包但不过滤，进入nat 表的 output 链，做 NAT，然后路由，进入filter 表的 output 链，可过滤包，进入mangle 表的 postrouiting 链，进入nat 表的 postrouting 链，做SNAT 但不过滤，包离开本机。 iptables 应用iptables 有八种匹配后的触发动作： ACCEPT：允许通过 DROP：丢弃 REJECT：拒绝 LOG：记录日志（syslog） DNAT：目的地址转换 SNAT：源地址转换 MASQUERADE：地址欺骗 REDIRECT：重定向 iptables [-t 表名] 选项 [链名] [条件] [-j 控制类型（上面八种）] -P INPUT (DROP|ACCEPT) 设置默认策略 -L 查看规则链 -F 清空规则链 -A 在链末尾加新规则 -I num 在链头部加新规则 -D num 删除指定规则 -R 替换指定链中的一条匹配规则 -N 创建一个新链 -X 删除指定用户的定义链，若未指定就删除所有用户链 -C 检查数据包是否与指定链规则匹配 -Z 将指定链中的所有规则byte计数器清零 匹配参数： -s 匹配源IP/mask，加！表示除该IP -d 匹配目的地址 -i [网卡] 匹配流入网卡的数据 -o [网卡] 匹配流出网卡的数据 -p [协议] 匹配协议 -n ip地址会以数字显示 --dport num 匹配目标端口号 --sport num 匹配源端口号 SelinuxfirewalldRHEL7 中 firewalld 取代了 iptables。firewalld 将所有网络流量都分类汇集到 zones，然后通过 zones 管理防火墙规则。 firewalld 匹配规则： 数据包进入系统，首先检查源 IP 地址和网卡接口，若与某个 zone 匹配，则按照该 zone 的规则过滤。每个 zone 都有开启或关闭的服务和端口列表，数据包根据列表决定是否放行。如果数据包不与任何定义的 zone 匹配，则进入默认 zone，默认 zone 的名称为public。firewalld 提供以下默认 zone：home/drop/work/internal/block/public/trusted/dmz/external，在 fedora 中，还会默认提供FedoraWorkstation和FedoraServer两个 zone。 firewall 命令： firewall-cmdStatus Options --state 返回firewalld状态 --reload 重新加载firewalld，会保留状态信息 --complete-reload 重载firewalld，不会保留状态信息 --runtime-to-permanent Create permanent from runtime configuration --permanent 设置为永久配置 --get-default-zone 显示默认zone --set-default-zone=&lt;zone&gt; 设置默认zone --get-active-zones 显示活跃的zone --get-zones 显示所有预设的zone --get-services 显示所有预设服务 --list-all-zones 列出所有zone的信息 --new-zone=&lt;zone&gt; 创建新的zone --delete-zone=&lt;zone&gt; 删除指定zone --load-zone-defaults=&lt;zone&gt; 加载zone的默认配置 --zone=&lt;zone&gt; 指定zone进行设置 --info-zone=&lt;zone&gt; 显示指定zone的信息 --list-all 列出活跃的zone的信息 --list-services 列出放行的服务 --add-service=&lt;service&gt; 添加放行的服务 --remove-service=&lt;service&gt; 取消放行服务 --query-service=&lt;service&gt; 返回服务是否放行 --list-ports 列出zone中放行的端口 --add-port=&lt;portid&gt;[-&lt;portid&gt;]/&lt;protocol&gt; 放行端口 --remove-port=&lt;portid&gt;[-&lt;portid&gt;]/&lt;protocol&gt; 取消放行端口 --query-port=&lt;portid&gt;[-&lt;portid&gt;]/&lt;protocol&gt; 返回端口是否放行 --list-protocols 列出指定区域添加的协议 --add-protocol=&lt;protocol&gt; 为指定区域添加协议 --remove-protocol=&lt;protocol&gt; 去除协议 --query-protocol=&lt;protocol&gt; 查询是否协议被添加到区域 --list-source-ports 查看区域中定义的源端口 --add-source-port=&lt;portid&gt;[-&lt;portid&gt;]/&lt;protocol&gt; 添加设置源端口 --remove-source-port=&lt;portid&gt;[-&lt;portid&gt;]/&lt;protocol&gt; 去除源端口 --query-source-port=&lt;portid&gt;[-&lt;portid&gt;]/&lt;protocol&gt; 查询源端口是否属于该区域 --list-rich-rules 列出所有rich rules --add-rich-rule=&lt;rule&gt; 添加rich rules --remove-rich-rule=&lt;rule&gt; 去除rich rules --query-rich-rule=&lt;rule&gt; 查询指定rich rules是否属于该域 --list-interfaces 列出区域中的网卡 --add-interface=&lt;interface&gt; 在区域中添加网卡 --query-interface=&lt;interface&gt; 查询网卡是否属于一个区域 --remove-interface=&lt;interface&gt; 从区域移除网卡 --list-sources 查看区域中定义的源 --add-source=&lt;source&gt;[/&lt;mask&gt;] 添加源 --query-source=&lt;source&gt;[/&lt;mask&gt;] 查询区域中是否有指定源 --remove-source=&lt;source&gt;[/&lt;mask&gt;] 去除区域中指定源 参考文章 firewall-cmd","categories":[{"name":"系统运维","slug":"系统运维","permalink":"https://coconutmilktaro.top/categories/%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"iptables","slug":"iptables","permalink":"https://coconutmilktaro.top/tags/iptables/"},{"name":"安全","slug":"安全","permalink":"https://coconutmilktaro.top/tags/%E5%AE%89%E5%85%A8/"},{"name":"Linux","slug":"Linux","permalink":"https://coconutmilktaro.top/tags/Linux/"},{"name":"Selinux","slug":"Selinux","permalink":"https://coconutmilktaro.top/tags/Selinux/"},{"name":"防火墙","slug":"防火墙","permalink":"https://coconutmilktaro.top/tags/%E9%98%B2%E7%81%AB%E5%A2%99/"},{"name":"firewalld","slug":"firewalld","permalink":"https://coconutmilktaro.top/tags/firewalld/"}]},{"title":"DHCP笔记","slug":"DHCP笔记","date":"2018-06-05T15:59:29.000Z","updated":"2022-05-30T02:51:53.784Z","comments":true,"path":"2018/DHCP笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/DHCP%E7%AC%94%E8%AE%B0/","excerpt":"本篇包含以下内容 DHCP 原理 DHCP 服务器配置 服务器端 客户端 DHCP 中继","text":"本篇包含以下内容 DHCP 原理 DHCP 服务器配置 服务器端 客户端 DHCP 中继 DHCP 原理DHCP（Dynamic Host Configuration Protocol）用于为客户端动态分配 IP 地址、子网掩码并设置网关等信息。前身为 BOOTP 协议，工作在应用层，基于 UDP 协议，端口号 67（服务器端），68（客户端），还有一个 546 端口用于 DHCPv6 的客户端。 DHCP 与 BOOTP 最重要的区别：DHCP 支持租期，BOOTP 不支持，BOOTP 分配的 IP 地址是永久的。 DHCP 提供三种分配方式： 手动分配：静态绑定固定 IP，这些 IP 固定给特定设备使用（打印机，DNS，web 服务器等） 自动分配：服务器给客户端分配租期无限长的 IP 地址，只有客户释放，其他客户才能使用该地址 动态分配：服务器给客户端分配租期有限长的 IP 地址，一旦租期到期而未续约，地址就会释放。 DHCP 的基本原则：尽可能为客户端分配原来使用的地址。DHCP 的分配顺序：1.静态分配的 2.客户端曾经会用过的 3.最先找到的可用 IP。 DHCP 报文与请求过程 DHCP 工作过程 发现阶段： 提供阶段： 选择阶段： 确认阶段： 重新申请： 更新租约： DHCP 报文 Discover：客户端第一次向服务器发送的请求报文，广播发送 Offer：服务器对客户端 Discover 的回应，包含分配的 IP、掩码、网关等信息，广播或单播发送 Request：客户端发送给服务器的请求报文，包括服务器的选择与租期更新等，单播或广播发送（根据客户端状态） Release：客户端若想释放当前地址，则单播发送给服务器 Ack/Nak：服务器对客户端的回应，请求报文正确时回复 Ack，否则回复 Nak Decline：客户端收到服务器的 Ack 后，对获取的 IP 进行确认，使用 ARP，若发现该 IP 已被使用，则广播向服务器发送 Decline 报文，拒绝使用该 IP。 Inform：当客户端通过其他方式已获取了 IP，若还需要向服务器索取其他配置信息时，会向服务器发送 Inform，若服务器能根据要求分配则会回复 Ack，否则不操作。 DHCP 续约 更新状态：使用时间达到租约的 50%，客户端进入更新状态，单播向服务器发送 Request，服务器若同意续约则回复 Ack，否则回复 Nak 重新绑定状态：使用时间达到租约的 87.5%，客户端进入重新绑定状态。客户端广播 Request 请求，请求对有效租期进行更新。进入该状态的原因：客户端未收到服务器对续约 Request 的回应。若 Request 未收到回应，客户端会在一定时间内重发 Request 报文，若直到租期结束也未更新租期，则被迫释放 IP 地址。 DHCP 中继DHCP 只适用于客户端与服务器在同网段（原因：广播请求），但可以通过中继使客户端可向其他网段的 DHCP 服务器请求。实现：中继路由器收到请求广播报文，便向服务器单播发送，同理服务器也单播回应中继，中继再广播回应客户端。 DHCP 服务器配置实验环境：全部为 CentOS-7 服务器端：system2 192.168.163.102 客户端：system3 192.168.163.103 服务器端 安装 DHCP 服务yum install dhcp dhcp-devel dhcp为服务器端基础组件，dhcp-devel为服务器开发工具 开机自启 `systemctl enable dhcpd` `systemctl start dhcpd` 修改配置文件/etc/dhcp/dhcpd.conf注：该文件是空的，可以参考模板添加项，模板为/usr/share/doc/dhcp-4.2.5/dhcpd.conf.example，其中 dhcp 的版本号可能不一致，可用find命令查找 以下为配置文件常见参数的解析，可通过man 5 dhcpd.conf查看完整配置参数解析 # 服务器名server-name “system2”;# DNS域名option domain-name &quot;example.org&quot;;# DNS服务器域名（最多指定三个）option domain-name-servers ns1.example.org, ns2.example.org;# 默认租期，单位秒。在默认租期内，可以进行续约操作default-lease-time 600;# 最大租期，单位秒。max-lease-time 7200;# 最大租期即客户端IP租约时间的最大值，当客户端超过默认租约时间，虽此时已无法续约，但DHCP会仍然允许用户在最大租约时间内使用该IP，之后就收回该IP# dhcp与dns动态信息更新模式（必选）# 三种选择：interim--dns互动更新 ad-hoc--特殊dns更新 none--不支持# 全局设置中一定要有这个，否则不能成功启动#ddns-update-style none;# 如果该DHCP服务器是本地网络的授权服务器，则需要取消注解#authoritative;# 忽略客户端更新ignore client-updates;# 设置网关option routers 192.168.163.254;# 日志类型log-facility local7;# 设置ntp服务器ntp-server [IP地址]# 子网设置subnet 10.5.5.0 netmask 255.255.255.224 &#123; # 设置地址池 range 10.5.5.26 10.5.5.30; option domain-name-servers ns1.internal.example.org; option domain-name &quot;internal.example.org&quot;; option routers 10.5.5.1; # 广播地址 option broadcast-address 10.5.5.31; default-lease-time 600; max-lease-time 7200;&#125;# shared-network用于跨网段分配IP地址，多用于中继，形成超级作用域shared-network 224-29 &#123; # 配置子网1 subnet 10.17.224.0 netmask 255.255.255.0 &#123; option routers rtr-224.example.org; &#125; # 配置子网2 subnet 10.0.29.0 netmask 255.255.255.0 &#123; option routers rtr-29.example.org; &#125;&#125;# host指定客户端客户端的IP地址绑定# hostname仅仅是标识，无意义host hostname &#123; # 指定目标主机 hardware ethernet [MAC地址]; # 绑定IP地址 fixed-address [ip地址];&#125; 注：划分子网时，如果选择直接配置多作用域实现动态 IP 分配的任务，则必须要为 DHCP 服务器添加多块网卡，并配置多个 IP 地址，否则 DHCP 服务器只能分配与其现有网卡 IP 地址对应网段的作用域。 DHCP 租约文件/var/lib/dhcpd/dhcpd.leases租约数据库文件用于保存一系列的租约声明，其中包含客户端的主机名、MAC 地址、分配到的 IP 地址，以及 IP 地址的有效期等相关信息。这个数据库文件是可编辑的 ASCII 格式文本文件。每当发生租约变化的时候，都会在文件结尾添加新的租约记录。 客户端需要安装先dhclient，然后修改/etc/sysconfig-network-scripts/ifcfg-ens33（根据实际网卡名称），修改BOOTPROTO=dhcp，重启网络。 DHCP 中继DHCP 中继代理（DHCP Relay Agent）用于转发其他网段的客户端 DHCP 请求。当客户端请求","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"dhcp","slug":"dhcp","permalink":"https://coconutmilktaro.top/tags/dhcp/"}]},{"title":"HTTP协议基础笔记","slug":"HTTP协议笔记","date":"2018-05-31T04:02:09.000Z","updated":"2022-05-30T02:51:53.802Z","comments":true,"path":"2018/HTTP协议笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/HTTP%E5%8D%8F%E8%AE%AE%E7%AC%94%E8%AE%B0/","excerpt":"Web 概述 HTTP 概述 HTTP 报文 HTTP 请求方式 HTTP 首部 HTTP 请求报文（Request） HTTP 响应报文（Response） 状态码 HTTP Cookie HTTP 连接 HTTP 缓存 HTTP 安全 内容安全策略 CSP HTTP 公钥锁定 HPKP HTTP 严格传输安全 HSTS X-Content-Type-Options X-Frame-Options X-XSS-Protection","text":"Web 概述 HTTP 概述 HTTP 报文 HTTP 请求方式 HTTP 首部 HTTP 请求报文（Request） HTTP 响应报文（Response） 状态码 HTTP Cookie HTTP 连接 HTTP 缓存 HTTP 安全 内容安全策略 CSP HTTP 公钥锁定 HPKP HTTP 严格传输安全 HSTS X-Content-Type-Options X-Frame-Options X-XSS-Protection Web 概述WWW（World Wide Web）环球信息网，也称万维网，由 W3C 万维网联盟管理。万维网并不等同互联网，万维网只是互联网所能提供的服务其中之一，是靠着互联网运行的一项服务。WWW 的三种技术：HTML，HTTP，URL URI： 统一资源标识符，表示服务器资源名称，给定了 URI，HTTP 便能解析资源。URI 有两种形式： URL 统一资源定位符：描述特定服务器上指定资源的位置，是固定的。包含三部分： 方案 scheme：说明访问资源使用的协议方式，如 http:// 服务器因特网地址，即域名或 IP 地址 指定服务器上的指定资源，即访问路径几乎所有 URI 都是 URL URN 统一资源名：特定资源的唯一名称，与资源所在地址无关，资源可以四处搬运。 HTTP 概述超文本传输 ​​ 协议 Hyper Text Transfer Protocol（HTTP）是用于传输诸如 HTML 的超媒体文档的应用层协议，用于 Web 浏览器和 Web 服务器之间的通信，基于 TCP/IP，采用 C/S 模式。HTTP 是无状态协议，意味着服务器不会在两个请求之间保留任何数据（状态）。 HTTP 的三个常见版本： HTTP/1.0：客户端只能从 web 服务器获取一个 web 资源 HTTP/1.1：客户端能在一个连接上获取多个 web 资源（有数量限制，超出部分请求被阻塞） HTTP/2.0：多流并行，一个连接可获取多个 web 资源 特点： 简单快速：HTTP 协议简单，报文简单易懂，HTTP 服务器程序规模 小，通信速度快。 灵活：HTTP 允许传输任意类型的数据对象。正在传输的类型由Content-Type加以标记。 可扩展：通过 HTTP 首部，只要服务端和客户端就新首部达成语义一致，新功能就可以被轻松加入进来。 无状态：在同一个连接中，两个执行成功的请求之间是没有关系的。但是可以通过 HTTP 的头部扩展和 HTTP Cookies 解决，把 Cookies 添加到头部中，让每次请求都能共享相同的上下文信息，来创建有状态的会话。 HTTP 报文HTTP 请求方式 GET：请求访问已被 URL 识别的资源，不会对信息产生影响，每次 GET 方法都是相同的，GET 放在 URL 首部，GET 提交的数据大小一般限制为 1024 字节，大小随浏览器而。采用明文传输，速度快。只产生一个 TCP 数据包。GET 能被缓存。 POST：请求服务器传输信息实体的主体，POST 放在报文中，没有具体限制，由于放在报文中所以无法看见，安全，form 表单必须使用 POST。会产生两个 TCP 数据包。POST 不可被缓存。POST 提交数据大小没有限制。 PUT：传输文件，在请求报文的主体中包含文件内容，然后保存到请求 URL 指定的位置（出于安全，网站一般不会用） HEAD：获取报文首部，用于确认 URI 的有效性及资源更新的日期时间等 DELETE：请求 URL 删除指定的资源，与 PUT 相反（同样一般不用） OPTIONS：查询指定 URL 资源支持的方法 TRACE：追踪路径，让服务器将之前的请求通信返还给客户端 CONNECT：要求在与代理服务器通信时建立隧道，实现用隧道协议进行 TCP 通信，主要使用 SSL（安全套接层）和 TLS（传输层安全）协议把通信内容加密后经过网络传输。 GET 与 POST 的区别： GET POST 请求访问已被 URL 识别的资源 将实体提交到指定资源 放在 URL 首部，明文传输，可见 封装在报文中，不可见 产生一个 TCP 包 产生两个 TCP 包 能被缓存 不可被缓存 数据传输大小随浏览器而定，一般为 1024 字节 没有具体限制 HTTP 首部HTTP 请求报文（Request）GET https://developer.mozilla.org/zh-CN/docs/Web/HTTP HTTP/2.0Host: developer.mozilla.orgUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:60.0) Gecko/20100101 Firefox/60.0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2Accept-Encoding: gzip, deflate, brCookie: dwf_sg_task_completion=False; messages=&quot;67ba01ba64d7aac589a5e34d72bb89050449f64e$[[\\&quot;__json_message\\&quot;\\0541\\05430\\054\\&quot;Redirected from https://developer.mozilla.org/en-US/docs/Web/Guide/HTML/Forms\\&quot;\\054\\&quot;wiki_redirect\\&quot;]]&quot;Connection: keep-alive 报文解析：第一行为请求行，包含三个部分： 执行动作：即请求方式。 请求目标：完整路径，通常是一个 URL，或者是协议、端口和域名的绝对路径 绝对路径，末尾加上?与查询字符串，称为原始形式 完整 URL，称为绝对形式，通常 GET 使用 域名:端口，仅在使用 CONNECT 建立 HTTP 隧道时才使用 星号形式*，配合 OPTIONS 方法使用，代表整个服务器 HTTP 版本 第二行以后的内容都称为 HTTP 头（Headers），可分为： 通用（一般）头（general headers）：适用于请求和响应消息，但与最终消息主体中传输的数据无关。例：Connection，Date，Keep-Alive等 请求头（request headers）：包含有关要获取的资源或客户端本身更多信息例：Host，User-Agent，Accept，Accept-Encoding等 响应头（response headers）：包含有关服务器响应的补充信息，如其位置或服务器本身（名称和版本等）。例：Server，Set-Cookie，X-Frame-Options，Etag等 实体头（entity headers）：包含有关实体主体的更多信息，比如主体长(Content-Length)度或其 MIME 类型。例：Content-Type，Content-Length，Content-Encoding等 消息头也可以根据代理对其的处理方式分为： 端到端消息头：这类消息头必须被传输到最终的消息接收者，也即，请求的服务器或响应的客户端。中间的代理服务器必须转发未经修改的端到端消息头，并且必须缓存它们。 逐跳消息头：这类消息头仅对单次传输连接有意义，不能通过代理或缓存进行重新转发。这些消息头包括 Connection, Keep-Alive, Proxy-Authenticate, Proxy-Authorization, TE, Trailer, Transfer-Encoding 及 Upgrade。注意，只能使用 Connection 来设置逐跳一般头。 请求的最后一部分是它的 body。不是所有的请求都有一个 body：例如获取资源的请求，GET，HEAD，DELETE 和 OPTIONS，通常它们不需要 body。 有些请求将数据发送到服务器以便更新数据：常见的的情况是 POST 请求（包含 HTML 表单数据）。Body 大致可分为两类： Single-resource bodies，由一个单文件组成。该类型 body 由两个 header 定义： Content-Type 和 Content-Length. Multiple-resource bodies，由多部分 body 组成，每一部分包含不同的信息位。通常是和 HTML Forms 连系在一起。 HTTP 响应报文（Response）HTTP/2.0 200 OKcontent-type: text/css; charset=&quot;utf-8&quot;access-control-allow-origin: *cache-control: max-age=315360000, public, immutabledate: Tue, 05 Jun 2018 16:24:09 GMTlast-modified: Tue, 05 Jun 2018 15:59:27 GMTserver: meinheld/0.6.1strict-transport-security: max-age=63072000x-content-type-options: nosniffx-xss-protection: 1; mode=blockcontent-encoding: gzipvary: Accept-Encodingage: 336026x-cache: Hit from cloudfrontvia: 1.1 ec1e0045303188984bc160bff8921bbd.cloudfront.net (CloudFront)x-amz-cf-id: D0Bs8VzYr7qYkVfUiXYivsQqygQPctoPwabrnWC0zSPCwYedUyHAWg==X-Firefox-Spdy: h2 响应报文解析： 第一行为响应行，包含三个字段： HTTP 版本 状态码 状态信息 第二行开始为响应头，与请求头类似 以及可选项，比起请求报文，响应报文中更常见地包含获取的资源 body。 响应的最后一部分是 body。不是所有的响应都有 body：具有状态码 (如 201 或 204) 的响应，通常不会有 body。 Body 大致可分为三类： Single-resource bodies，由已知长度的单个文件组成。该类型 body 由两个 header 定义：Content-Type 和 Content-Length。 Single-resource bodies，由未知长度的单个文件组成，通过将 Transfer-Encoding 设置为 chunked 来使用 chunks 编码。 Multiple-resource bodies，由多部分 body 组成，每部分包含不同的信息段。但这是比较少见的。 状态码类别： 1XX：信息类——-&gt;请求正在处理，属于临时响应 2XX：成功类——-&gt;请求正常处理完毕 3XX：重定向——-&gt;需要附加操作完成请求 4XX：客户端错误—-&gt;服务器无法处理请求 5XX：服务器错误—-&gt;服务器处理请求出错 常见状态码： 1XX 信息响应 100 Continue：迄今为止的所有内容都是可行的，客户端应该继续请求，如果已经完成，则忽略它。 101 Switching Protocol：响应客户端的 Upgrade 标头发送的，并且指示服务器也正在切换的协议。 102 Processing (WebDAV)：服务器已收到并正在处理该请求，但没有响应可用。 2XX 成功响应 200 OK：正常，客户端的请求在服务器被正常处理 201 Created：该请求已成功，并因此创建了一个新的资源。这通常是在 PUT 请求之后发送的响应。 202 Accepted：请求已经接收到，但还未响应，没有结果。意味着不会有一个异步的响应去表明当前请求的结果，预期另外的进程和服务去处理请求，或者批处理。 203 Non-Authoritative Information：服务器已成功处理了请求，但返回的信息可能来自另一来源。 204 No Content：正常处理，但无资源返回，响应报文中不含实体主体 205 Reset Content：服务器成功处理了请求，且没有返回任何内容。 206 Partial Content：客户端进行了范围请求且服务器成功执行了这部分的 GET 请求 3XX 重定向 300 Multiple Choice：针对请求，服务器可执行多种操作。 服务器可根据 user agent 选择一项操作，或提供操作列表供请求者选择。 301 Moved Permanently：永久性重定向，请求的资源已被分配了新的 URI（以后都用新 URI） 302 Found：临时性重定向，请求的资源被暂时的移动到了由 Location 头部指定的 URL 上（本次使用新 URI 访问） 303 See Other：请求对应的资源存在另一个 URI,应该使用 GET 方法定向获取请求的资源当 301、302、303 响应状态码返回，几乎所有浏览器都会把 POST 改成 GET，并删除请求报文内的主体，之后请求自动再次发送 304 Not Modified：如果客户端发送了一个带条件的 GET 请求且该请求已被允许，而文档的内容（自上次访问以来或者根据请求的条件）并没有改变，则服务器应当返回这个状态码。304 响应禁止包含消息体，因此始终以消息头后的第一个空行结尾。 305 Use Proxy：请求者只能使用代理访问请求的网页。 （已不再使用，但目前仍能生效） 307 Temporary Redirect：服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。 308 Permanent Redirect：资源现在永久位于响应头中Location指定的另一个 URI。 4XX 请求错误 400 Bad Request：响应状态码表示由于语法无效，服务器无法理解该请求 401 Unauthorized：发送的请求需要有通过 http 认证（BASIC 认证、DIGEST 认证）的认证信息。该响应必须包含一个适用于被请求资源的 WWW-Authenticate 信息头用以询问用户信息。若之前已经进行了一次请求，则表示用户认证失败 403 Forbidden：服务器端有能力处理该请求，但是拒绝授权访问 404 Not Found：服务器端无法找到所请求的资源 405 Method Not Allowed：禁用请求中指定的方法。 406 Not Acceptable：无法使用请求的内容特性响应请求的网页。 407 Proxy Authentication Required：此状态代码与 401（未授权）类似，但指定请求者应当授权使用代理。 408 Request Timeout：服务器等候请求时发生超时。 409 Conflict：服务器在完成请求时发生冲突。 服务器必须在响应中包含有关冲突的信息。 410 Gone：如果请求的资源已永久删除，服务器就会返回此响应。 411 Length Required：服务器不接受不含有效内容长度标头字段的请求。 412 Precondition Failed：服务器未满足请求者在请求中设置的其中一个前提条件。 413 Payload Too Large：服务器无法处理请求，因为请求实体过大，超出服务器的处理能力。 414 URI Too Long：请求的 URI 过长，服务器无法处理。 415 Unsupported Media Type：请求的格式不受请求页面的支持。 416 Requested Range Not Satisfiable：如果页面无法提供请求的范围，则服务器会返回此状态代码。 417 Expectation Failed：服务器未满足”期望”请求标头字段的要求。 5XX 服务器错误 500 Internal Server Error ：服务器端错误，所请求的服务器遇到意外的情况并阻止其执行请求。 502 Bad Gateway：服务器作为网关或代理，从上游服务器收到无效响应。 503 Server Unavailable：服务器暂时处于超负载或者正在停机维护，现在无法处理请求 504 Gateway Timeout：服务器作为网关或代理，但是没有及时从上游服务器收到请求。 505 HTTP Version Not Supported：服务器不支持请求中所用的 HTTP 协议版本。 HTTP CookieHTTP Cookie（也叫 Web Cookie 或浏览器 Cookie）是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。Cookie 使基于无状态的 HTTP 协议记录稳定的状态信息成为了可能。 Cookie 主要用于以下三个方面： 会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息） 个性化设置（如用户自定义设置、主题等） 浏览器行为跟踪（如跟踪分析用户行为等） 当服务器收到 HTTP 请求时，服务器可以在响应头里面添加一个 Set-Cookie 选项。浏览器收到响应后通常会保存下 Cookie，之后对该服务器每一次请求中都通过 Cookie 请求头部将 Cookie 信息发送给服务器。另外，Cookie 的过期时间、域、路径、有效期、适用站点都可以根据需要来指定。 Set-Cookie: &lt;cookie名&gt;=&lt;cookie值&gt; 会话期 Cookie 是最简单的 Cookie：浏览器关闭之后它会被自动删除，也就是说它仅在会话期内有效。会话期 Cookie 不需要指定过期时间（Expires）或者有效期（Max-Age）。需要注意的是，有些浏览器提供了会话恢复功能，这种情况下即使关闭了浏览器，会话期 Cookie 也会被保留下来，就好像浏览器从来没有关闭一样。 持久性 Cookie 可以指定一个特定的过期时间（Expires）或有效期（Max-Age）。Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; 当 Cookie 的过期时间被设定时，设定的日期和时间只与客户端相关，而不是服务端。 标记为 Secure 的 Cookie 只应通过被 HTTPS 协议加密过的请求发送给服务端。但即便设置了 Secure 标记，敏感信息也不应该通过 Cookie 传输，因为 Cookie 有其固有的不安全性，Secure 标记也无法提供确实的安全保障。 Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Secure; HttpOnly Domain 和 Path 标识定义了 Cookie 的作用域：即 Cookie 应该发送给哪些 URL。 Domain 标识指定了哪些主机可以接受 Cookie。如果不指定，默认为当前文档的主机（不包含子域名）。如果指定了 Domain，则一般包含子域名。例如，如果设置 Domain=mozilla.org，则 Cookie 也包含在子域名中（如developer.mozilla.org）。 Path 标识指定了主机下的哪些路径可以接受 Cookie（该 URL 路径必须存在于请求 URL 中）。以字符 /作为路径分隔符，子路径也会被匹配。 SameSite Cookie 允许服务器要求某个 cookie 在跨站请求时不会被发送，从而可以阻止跨站请求伪造攻击（CSRF）。 Set-Cookie: key=value; SameSite=StrictStrict：浏览器将只发送相同站点请求的cookie(即当前网页URL与请求目标URL完全一致)。如果请求来自与当前location的URL不同的URL，则不包括标记为Strict属性的cookie。若不开启SameSite，则设为None 每个 Cookie 都会有与之关联的域（Domain）如果 Cookie 的域和页面的域相同，那么称这个 Cookie 为第一方 Cookie（first-party cookie）如果 Cookie 的域和页面的域不同，则称之为第三方 Cookie（third-party cookie.）一个页面包含图片或存放在其他域上的资源（如图片广告）时，第一方的 Cookie 也只会发送给设置它们的服务器。通过第三方组件发送的第三方 Cookie 主要用于广告和网络追踪。大多数浏览器默认都允许第三方 Cookie，但是可以通过附加组件来阻止第三方 Cookie Cookie 的一个极端使用例子是僵尸 Cookie（或称之为“删不掉的 Cookie”），这类 Cookie 较难以删除，甚至删除之后会自动重建。它们一般是使用 Web storage API、Flash 本地共享对象或者其他技术手段来达到的。 HTTP 连接连接管理是一个 HTTP 的关键话题：打开和保持连接在很大程度上影响着网站和 Web 应用程序的性能。在 HTTP/1.x 里有多种模型：短连接, 长连接, 和 HTTP 流水线。 HTTP 的连接管理适用于两个连续节点之间的连接，如 hop-by-hop，而不是 end-to-end。当模型用于从客户端到第一个代理服务器的连接和从代理服务器到目标服务器之间的连接时(或者任意中间代理)效果可能是不一样的。HTTP 协议头受不同连接模型的影响，比如 Connection 和 Keep-Alive，就是 hop-by-hop 协议头，它们的值是可以被中间节点修改的。 短连接短连接是 HTTP 最早期的模型，也是 HTTP/1.0 的默认模型。每一个 HTTP 请求都由它自己独立的连接完成，这意味着发起每一个 HTTP 请求之前都会有一次 TCP 握手，而且是连续不断的。短连接破坏了 TCP 具备的能力，新的冷连接降低了其性能。在 HTTP/1.1 中，只有当 Connection 被设置为 close 时才会用到这个模型。 长连接一个长连接会保持一段时间，重复用于发送一系列请求，节省了新建 TCP 连接握手的时间，还可以利用 TCP 的性能增强能力。当然这个连接也不会一直保留着：连接在空闲一段时间后会被关闭(服务器可以使用 Keep-Alive 协议头来指定一个最小的连接保持时间)。缺点：就算是在空闲状态，它还是会消耗服务器资源，而且在重负载时，还有可能遭受 DoS attacks 攻击。这种场景下，可以使用非长连接，即尽快关闭那些空闲的连接，也能对性能有所提升。把 Connection 设置成 close 以外的其它参数都可以让其保持长连接，通常会设置为 retry-after。在 HTTP/1.1 里，默认就是长连接的，协议头都不用再去声明它。 流水线默认情况下，HTTP 请求是按顺序发出的。下一个请求只有在当前请求收到应答过后才会被发出。由于会受到网络延迟和带宽的限制，在下一个请求被发送到服务器之前，可能需要等待很长时间。流水线是在同一条长连接上发出连续的请求，而不用等待应答返回。这样可以避免连接延迟。理论上讲，性能还会因为两个 HTTP 请求有可能被打包到一个 TCP 消息包中而得到提升。并不是所有类型的 HTTP 请求都能用到流水线：只有 idempotent 方式，比如 GET、HEAD、PUT 和 DELETE 能够被安全的重试：如果有故障发生时，流水线的内容要能被轻易的重试。 HTTP 流水线在现代浏览器中并不是默认被启用的，因为： Web 开发者并不能轻易的遇见和判断那些搞怪的代理服务器的各种莫名其妙的行为。 正确的实现流水线是复杂的：传输中的资源大小，多少有效的 RTT 会被用到，还有有效带宽，流水线带来的改善有多大的影响范围。 流水线受制于 HOL（Head-Of-Line）问题。 域名分片作为 HTTP/1.x 的连接，请求是序列化的，哪怕本来是无序的，在没有足够庞大可用的带宽时，也无从优化。一个解决方案是，浏览器为每个域名建立多个连接，以实现并发请求。曾经默认的连接数量为 2 到 3 个，现在比较常用的并发连接数已经增加到 6 条。如果尝试大于这个数字，就有触发服务器 DoS 保护的风险。 如果服务器端想要更快速的响应网站或应用程序的应答，它可以迫使客户端建立更多的连接。例如，不要在同一个域名下获取所有资源，假设有个域名是 www.example.com，可以把它拆分成好几个域名：www1.example.com、www2.example.com、www3.example.com。所有这些域名都指向同一台服务器，浏览器会同时为每个域名建立 6 条连接(在这个例子中，连接数会达到 18 条)。 HTTP 缓存缓存大致分为两类： 私有缓存：只能用于单独用户。浏览器缓存拥有用户通过 HTTP 下载的所有文档。这些缓存为浏览过的文档提供向后/向前导航，保存网页，查看源码等功能，可以避免再次向服务器发起多余的请求。它同样可以提供缓存内容的离线浏览。 共享缓存可以被多个用户使用。例如，ISP 或你所在的公司可能会架设一个 web 代理来作为本地网络基础的一部分提供给用户。这样热门的资源就会被重复使用，减少网络拥堵与延迟。 常见的 HTTP 缓存只能存储 GET 响应，对于其他类型的响应则无能为力。缓存的关键主要包括 request method 和目标 URI（一般只有 GET 请求才会被缓存）。 普遍的缓存案例: 一个检索请求的成功响应: 对于 GET 请求，响应状态码为：200，则表示为成功。一个包含例如 HTML 文档，图片，或者文件的响应。 永久重定向: 响应状态码：301。 错误响应: 响应状态码：404 的一个页面。 不完全的响应: 响应状态码 206，只返回局部的信息。 除了 GET 请求外，如果匹配到作为一个已被定义的 cache 键名的响应。 HTTP/1.1 定义的 Cache-Control头用来区分对缓存机制的支持情况， 请求头和响应头都支持这个属性。通过它提供的不同的值来定义缓存策略。 禁止缓存：缓存中不得存储任何关于客户端请求和服务端响应的内容。Cache-Control: no-store 强制确定缓存：每次有请求发出时，缓存会将此请求发到服务器，服务器端会验证请求中所描述的缓存是否过期，若未过期（注：实际就是返回 304），则缓存才使用本地缓存副本。Cache-Control: no-cache 共有缓存：该响应可以被任何中间人（比如中间代理、CDN 等）缓存。若指定了 public，则一些通常不被中间人缓存的页面（比如 带有 HTTP 验证信息（帐号密码）的页面 或 某些特定状态码的页面），将会被其缓存。Cache-Control: public 私有缓存：该响应是专用于某单个用户的，中间人不能缓存此响应，该响应只能应用于浏览器私有缓存中。默认为 private。Cache-Control: private 缓存过期机制： max-age：指定的是距离请求发起的时间的秒数Cache-Control: max-age=31536000 expires：指定的是日期Expires: Wed, 21 Oct 2015 07:28:00 GMT 缓存会定期地将一些副本删除，这个过程叫做缓存驱逐。服务器更新一个资源时，不可能直接通知客户端更新缓存，所以双方必须为该资源约定一个过期时间，在该过期时间之前，该资源（缓存副本）就是新鲜的，当过了过期时间后，该资源（缓存副本）则变为陈旧的。驱逐算法用于将陈旧的资源（缓存副本）替换为新鲜的，注意，一个陈旧的资源（缓存副本）是不会直接被清除或忽略的。当客户端发起一个请求时，缓存检索到已有一个对应的陈旧资源（缓存副本），则缓存会先将此请求附加一个If-None-Match头，然后发给目标服务器，以此来检查该资源副本是否是依然还是算新鲜的。若服务器返回了 304 (Not Modified)（该响应不会有带有实体信息），则表示此资源副本是新鲜的。若服务器通过 If-None-Match 或 If-Modified-Since判断后发现已过期，那么会带有该资源的实体内容返回。 对于请求，服务器会先查看Cache-control: max-age，若不含该属性则会去查看是否包含 Expires 属性。如果 max-age 和 expires 属性都没有，则会找头里的 Last-Modified 信息，如果有，缓存的寿命就等于头里面 Date 减去 Last-Modified除以 10，即Date - Last-Modified / 10 缓存验证确认：当使用了must-revalidate指令，那就意味着缓存在考虑使用一个陈旧的资源时，必须先验证它的状态，已过期的缓存将不被使用。 Cache-Control: must-revalidate 用户点击刷新按钮时会开始缓存验证。如果缓存的响应头信息里含有Cache-control: must-revalidate的定义，在浏览的过程中也会触发缓存验证。另外，在浏览器偏好设置里设置 Advanced-&gt;Cache 为强制验证缓存也能达到相同的效果。 当缓存的文档过期后，需要进行缓存验证或者重新获取资源。只有在服务器返回强校验器或者弱校验器时才会进行验证。 ETag 响应头作为缓存的一种强校验器，是一个对用户代理不透明的值。对于像浏览器这样的 HTTP 用户代理，不知道 ETag 代表什么，不能预测它的值是多少。如果资源请求的响应头里含有 ETag, 客户端可以在后续的请求的头中带上 If-None-Match头来验证缓存。 例：ETag: &quot;51142bc1-7449-479b075b2891b&quot; Last-Modified响应头可以作为一种弱校验器，因为它只能精确到一秒。如果响应头里含有这个信息，客户端可以在后续的请求中带上 If-Modified-Since 来验证缓存。 当向服务端发起缓存校验的请求时，服务端会返回 200 ok表示返回正常的结果或者 304 Not Modified(不返回 body)表示浏览器可以使用本地缓存文件。304 的响应头也可以同时更新缓存文档的过期时间。 Vary 头Vary HTTP 响应头决定了对于后续的请求头，如何判断是请求一个新的资源还是使用缓存的文件。当缓存服务器收到一个请求，只有当前的请求和原始（缓存）的请求头跟缓存的响应头里的 Vary 都匹配，才能使用缓存的响应。 例：Vary: User-Agent 使用 vary 头有利于内容服务的动态多样性。例如，使用 Vary: User-Agent 头，缓存服务器需要通过 User-Agent 判断是否使用缓存的页面。如果需要区分移动端和桌面端的展示内容，利用这种方式就能避免在不同的终端展示错误的布局。 HTTP 安全内容安全策略 CSPHTTP 公钥锁定 HPKPHTTP 严格传输安全 HSTSX-Content-Type-OptionsX-Frame-OptionsX-XSS-Protection参考文章 HTTP | MDN https://developer.mozilla.org/zh-CN/docs/Web/HTTP关于 HTTP 协议，一篇就够了 https://www.cnblogs.com/ranyonsue/p/5984001.html开发之前应该了解的 HTTP https://blog.csdn.net/qq_35414779/article/details/78981151","categories":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"http","slug":"http","permalink":"https://coconutmilktaro.top/tags/http/"}]},{"title":"HAProxy笔记","slug":"HAProxy笔记","date":"2018-05-31T04:01:06.000Z","updated":"2022-05-30T02:51:53.801Z","comments":true,"path":"2018/HAProxy笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/HAProxy%E7%AC%94%E8%AE%B0/","excerpt":"本篇包含以下内容： HAProxy 介绍 HAProxy 配置文件 源码包安装 HAProxy 操作 HAproxy 配置文件 web 查看状态 ACL 配置 HAProxy+Keepalived 搭建 参考文章","text":"本篇包含以下内容： HAProxy 介绍 HAProxy 配置文件 源码包安装 HAProxy 操作 HAproxy 配置文件 web 查看状态 ACL 配置 HAProxy+Keepalived 搭建 参考文章 HAProxy 介绍HAProxy 是一个提供高可用性、负载均衡，以及可基于 TCP（第四层）和 HTTP（第七层）的应用代理软件。 HAProxy 适合处理高负载 Web 站点的 HTTP 请求，这些站点通常需要会话保持或七层处理，HAProxy 完全支持数以万计的并发连接，并且能使后端的 Web 服务器不会暴露。HAProxy 还支持服务器健康检查，当后端服务器出现故障后，HAProxy 会自动移除该服务器，在故障排除后自动将该服务器加入。 HAProxy 特点： 支持连接拒绝：通过限制连接防御攻击蠕虫，降低被 DDOS 攻陷的可能 支持全透明代理 支持健康检查 实现会话保持 实现 HTTP 重写与重定向 自带服务器状态监控页面，实现监控统计 原生配置了 SSL 证书 支持虚拟主机 支持双机热备 支持服务器健康检查 单进程 支持 RDP 协议（远程桌面协议） HAProxy 支持的代理模式： 基于四层的 TCP 代理：仅在客户端和服务器间进行流量转发。可用于邮件服务（SMTP、POP3 等）、内部协议通信服务器、MySQL、https 等 四层根据负载均衡算法直接修改报文的目的 IP 地址，然后发送给后端，相当于一个转发的功能。 基于七层的 HTTP 代理：能分析应用层协议，且能根据协议信息灵活控制访问。 七层是查看请求报文内容，根据内容选择后端服务器。不仅可以根据 IP 和端口进行负载分流，还能根据 URL、域名、浏览器、语言等报文参数决定负载均衡策略。 在七层模式下，负载均衡器与客户端还有后端服务器会分别建立一次 TCP 连接，因此七层负载对设备的要求更高，而处理能力也不如四层负载。 HAProxy 的 frontend 和 backend 功能： frontend：ACL 规则匹配，根据任意 HTTP 请求头内容做规则匹配，然后将请求重定向到指定的 backend backend：事先定义的 server pool，等待前端将请求转到的服务器组 HAproxy 实现性能最大化的做法： 单进程、事件驱动模型降低了上下文切换的开销和内存占用 O(1)事件检查器允许其在高并发连接中对任何连接的任何事件实现即时探测 在任何可用情况下，单缓冲（Single Buffering）机制能以不复制任何数据的方式完成读写操作，能够节约大量 CPU 时钟周期（CPU 主频的倒数。周期小说明执行速度变快）及内存带宽。 MRU 内存分配器在固定大小的内存池中可实现即时内存分配，能显著减少创建一个会话的时长。 借助内核的splice()系统调用，可实现零复制转发（Zero-copy forwarding），还可实现零复制启动（Zero-Starting）。 树型存储：实现了O(logN)的低开销保持计时器命令、保持运行队列命令、管理轮询和最少连接队列。 优化了 HTTP 首部分析，避免在分析过程中重读任何内存区域 降低了系统调用，大部分工作在用户空间中完成，如时间读取、缓冲聚合、问价描述符的启用和禁用等。 HAproxy 进程消耗比系统空间消耗低 20 倍以上，在某些系统上，HAproxy 的七层性能可能超过硬件负载均衡设备。 负载均衡器的性能评估要素 会话率：单位时间内的处理请求数。该指标决定了一个负载均衡器是否能将所有请求分发出去，依赖于 CPU 性能。 当关闭了 keep-alive 连接保持功能，session/s（每秒会话数）和 requests/s（每秒请求数）或者 hits/s（每秒命中数）是一样的。 当开启了 keep-alive 连接保持功能，requests/s 或者 hits/s 要高很多 会话并发能力：并发处理能力。当并发会话数上升时，session/s 会下降。该指标被系统允许的最大文件描述符数量以及内存大小限制。 数据率：处理数据能力。当传输大的对象时，会增加并发会话数，可获得更高的数据率，这时 session 的创建与销毁是最少的。使用 MB/s 或 GB/s 为单位 HAProxy 与 LVS 的异同 LVS 是基于Linux 内核实现的一种负载，HAProxy 是基于第三方应用实现的负载 LVS 仅是四层 IP 负载均衡，HAProxy 提供四层与七层负载，提供 TCP 和 HTTP 应用的负载 LVS 的状态检测功能单一，HAProxy 因为能在四层和七层负载，可支持端口、URL、脚本等多种状态检测方式 HAProxy 的整体性能低于 LVS，LVS 拥有接近硬件设备的网络吞吐和连接负载能力 HAProxy 负载均衡算法： roundrobin：与 LVS 的轮询一致。 static-rr：与 LVS 的 wrr 一致。 leastconn：与 LVS 的 Least conn 一致。 source：类似 LVS 的源地址散列 source hashing。对源 IP 进行哈希。同一客户端 IP 访问同一台服务器 uri：类似 LVS 的目的地址散列 destination hashing。对目的地址进行哈希，同一请求的 URI 总是访问同一个服务器 url_param：根据 URL 参数调度。将同一个用户信息都发往同一个后端服务器。 hdr(name)：根据 HTTP 请求头锁定每一次 HTTP 请求。若缺少头，则用 rr 代替 rdp-cookie(name)：查询每个 TCP 请求，并哈希 RDP cookie，用于退化的持久模式，使同一个用户或会话 ID 总是发送到同一台服务器。若没有 cookie，则使用 rr 代替。 HAProxy 配置文件直接通过yum install haproxy安装（版本可能很老），版本为 1.6。或者在haproxy 下载源码包，版本会更加新。安装后会自动创建用户 haproxy。可以通过systemctl管理。 源码包安装下载的是 1.8.14 版本的源码包。进入解压目录 make PREFIX=/usr/local/haproxy1.8 TARGET=linux2628# 其中TARGET是指定Linux内核版本，一定要写，Linux2.6以及3.X都是写linux2628make install PREFIX=/usr/local/haproxy1.8 若使用源码安装，则不会自动创建 haproxy 用户及用户组，需要手动创建。并且没有任何配置文件，都要手动创建。有模板文件，在解压目录的example/目录中，叫option-http_proxy.cfg，在安装目录中创建一个conf目录，再将该配置文件复制过去。 HAProxy 操作HAProxy 命令： haproxy -v 显示版本 -vv 显示详细的构建选项信息 -d 进入debug模式 -f 指定配置文件 -dM[&lt;byte&gt;] poisons memory with &lt;byte&gt; (defaults to 0x50) -D 后台运行; -C changes to &lt;dir&gt; before loading files. -q 静默模式 -c 检查配置文件语法 -n 设置最大连接数，默认2000 -m 限制可用的内存量，单位MB -N sets the default, per-proxy maximum # of connections (2000) -L 设置本地peer name，默认为主机名 -p writes pids of all children to this file -de 禁止使用epoll()函数 -dp 禁止使用poll()函数 -dS disables splice usage (broken on old kernels) -dR disables SO_REUSEPORT usage -dV 禁止服务器端的SSL -sf/-st [pid ]* finishes/terminates old pids.常用操作：选项不可连起来，只能分开haproxy -c -f /etc/haproxy/haproxy.cfg 检查配置文件，一定要-c -f都指定haproxy -D -f /etc/haproxy/haproxy.cfg 以daemon模式启动haproxy -vv 显示编译与启动信息killall haproxy 关闭HAProxy HAproxy 配置文件HAProxy 主配置文件/etc/haproxy/haproxy.cfg HAProxy 的配置有五个部分：global，defaults，frontend，backend，listen global：设置全局配置参数，属于进程级的配置，和操作系统配置有关 defaults：默认参数的配置。默认会自动被引用到下面的 frontend、backend 和 listen 部分中。如果在 frontend、backend 和 listen 部分中也配置了与 defaults 部分一样的参数，那么 defaults 部分参数对应的值自动被覆盖。 frontend：设置接收用户请求的前端虚拟节点 backend：设置后端服务器集群的配置 listen：是 frontend 部分和 backend 部分的结合体。是为了兼容 1.3 版本以前的配置而保留下来的。可以不用。 全局 global 配置： global进程管理与安全性参数 log 127.0.0.1 local2 [level] #日志使用local2输出到本地，后面还可以添加日志等级 chroot /var/lib/haproxy #改变haproxy的工作目录 pidfile /var/run/haproxy.pid #指定PID文件路径 user haproxy #执行HAProxy进程的用户 group haproxy #执行HAProxy进程的用户组 daemon #后台执行 stats socket /var/lib/haproxy/stats #用户访问统计数据的接口 stats maxconn 10 #默认stats socket仅限10个并发连接 nbproc 1 #启动的haproxy进程的个数，只能用于守护进程模式性能调整参数 maxconn 4000 #最大连接数 spread-checks #设置HealthCheck时间间隔 noepoll #禁用使用epoll事件轮询系统 nopoll #禁用poll事件轮询系统 nosplice #禁用套接字之间使用内核tcp拼接 为了配置文件中的日志参数，创建独立的日志文件，需要修改/etc/rsyslog.conf，添加以下参数，然后重启rsyslog服务 local2.* /var/log/haproxy.log 注：HAProxy 要求ulimit -n的值（即最大打开文件数）要大于maxconn * 2 + 18。 默认 defaults 配置： defaults mode http # 默认模式，tcp为4层，http为7层，health只返回ok option httplog # 采用http日志格式 option httpclose # 防止多余的cookie信息影响到客户端请求的处理结果 retries 3 # 尝试连接的次数，若连接失败则认为服务器不可用 option http_proxy # 开启代理（仅是基本的代理功能） option dontlognull # 不记录空连接 option http-server-close # 开启connection closing option forwardfor except 127.0.0.0/8 # 服务器能获取客户端的真实IP地址 option redispatch # 当客户端将请求发往了故障的服务器，则会自动将请求发往其他正常的机器 # option abortonclose # 当服务器负载很高的时候，自动结束掉当前队列处理比较久的链接 # 时间单位可以是us（微秒）|ms（毫秒）|s（秒）|m（分）|h（时）|d（天） timeout http-request 10s # http请求超时时间 timeout queue 1m # 队列超时时间 timeout connect 10s # 连接超时时间 timeout client 1m # 客户端响应超时时间 timeout server 1m # 服务器端响应超时时间 timeout http-keep-alive 10s # keepalive持久连接超时时间 timeout check 10s # 检查时间间隔 前端服务 frontend 配置： frontend main *:5000 # 在1.8版本中不能这么写，而是用bind # 设置acl规则 acl url_static path_beg -i /static /images /javascript /stylesheets acl url_static path_end -i .jpg .gif .png .css .js use_backend static if url_static # 调用后端服务器并检查是否匹配acl规则 default_backend app # 客户端访问时默认调用后端服务器地址池，与backend关联 stats uri /stats # 开启HAProxy的状态查看网页，通过/stats查看 后端服务 backend 配置： backend static # 定义后端服务器，static是配置存放静态资源的后端服务器 balance roundrobin # 算法 server static 127.0.0.1:4331 check # 对后端进行健康检查backend app # 运行应用的动态服务器 balance roundrobin server app1 127.0.0.1:5001 check server app2 127.0.0.1:5002 check 例： backend static balance roundrobin server static 172.16.246.135:80 check server static 172.16.246.136:80 checkbackend app balance roundrobin server tomcat1 172.16.246.151:8080 check server tomcat2 172.16.246.136:8080 check 访问 HAproxy 服务器的 5000 端口，就能访问后端服务器 web 查看状态可以配置专门的 frontend 设置 haproxy 自带的 stats，监控 haproxy 状态。 frontend stats bind *:8080 stats enable # 开启stats stats refresh 3s # 刷新间隔 stats uri /stats # 访问uri stats auth admin:redhat # 认证用户密码 stats admin if TRUE # 开启认证 ACL 配置ACL 操作通常包括阻止请求，选择后端或添加报文头。 从数据流，表或环境中提取数据样本 可选地对提取的样本进行一些格式转换 在此样本上应用一个或多个模式匹配方法 仅在模式与样本匹配时执行操作 ACL 的数量没有强制限制。 未使用的不会影响性能，只消耗少量内存。 acl 格式： acl acl名 acl方法（也称测试标准） [flags] 匹配路径或文件 常用 acl 方法： hdr_reg(host)：正则匹配 # 匹配URL是www.exam.com和www1.exam.com的请求acl www hdr_reg(host) -i ^(www.exam.com|www1.exam.com) hdr_dom(host)： hdr_beg(host)：测试请求报文的指定首部的开头部分是否符合指定的模式 # 匹配提供静态请求的主机img\\video\\ftpacl host_static hdr_beg(host) -i img. video. ftp. url_sub： url_dir： path_beg：测试请求的 URL 是否以后面指定的模式开头 acl url_static path_beg -i /static #匹配url以/static开头 path_end：测试请求的 URL 是否以后面指定的模式结尾 acl url_static path_end -i .jpg .js #匹配url以.jpg或.js结尾 frontend main bind *:5000 acl host_1 hdr_dom(name) -i host1.example.com acl host_2 hdr_dom(name) -i host2.example.com use_backend host1 if host_1 use_backend host2 if host_2backend host1 balance roundrobin server host1 192.168.60.130:80 checkbackend host2 balance roundrobin server host2 192.168.60.131:80 check 然后通过host1.example.com可访问 host1 的后端主机池，host2.example.com访问 host2 的后端主机池。 HAProxy+Keepalived 搭建安装 keepalived，可以直接 yum 安装，也可以源码安装。源码安装版本 2.0.10。 ./configure --prefix=/usr/local/keepalived \\ --bindir=/usr/bin \\ --sbindir=/usr/sbin \\ --sysconfdir=/etc 然后直接make &amp;&amp; make install即可。 修改配置/etc/keepalived/keepalived.conf # Master配置global_defs &#123; # 可以不用改 notification_email &#123; acassen@firewall.loc failover@firewall.loc sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 192.168.60.1 smtp_connect_timeout 30 router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state MASTER # master这边要改 interface ens32 # 改为面向集群的网卡（内网网卡） virtual_router_id 51 priority 120 # master的优先级一定要比backup高 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.60.200 # VIP &#125;&#125;# backup配置vrrp_instance VI_1 &#123; state BACKUP #backup这里要改 interface ens32 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.60.200 &#125;&#125; 修改完后直接systemctl restart keepalived，查看 Master 的网卡 2: ens32: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:38:e9:f3 brd ff:ff:ff:ff:ff:ff inet 192.168.60.134/24 brd 192.168.60.255 scope global noprefixroute ens32 valid_lft forever preferred_lft forever inet 192.168.60.200/32 scope global ens32 #VIP在Master这，Backup上是没有该VIP的 valid_lft forever preferred_lft forever.... 关闭 Master，再查看 Backup，发现 VIP 已转移到此主机上，且已变为 Master。 参考文章 HAProxy 用法详解 全网最详细中文文档 HAproxy 负载均衡-特性篇 转 笔记 1. HAProxy 介绍 http 反向代理之 haproxy 详解 Haproxy 的负载均衡、动静分离、状态监控、近期网络架构 CentOS7 haproxy+keepalived 实现高可用集群搭建 Haproxy 原理(1) HAProxy 从零开始到掌握 HAProxy 骏马金龙 Linux 集群与自动化运维","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"HAproxy","slug":"HAproxy","permalink":"https://coconutmilktaro.top/tags/HAproxy/"},{"name":"代理","slug":"代理","permalink":"https://coconutmilktaro.top/tags/%E4%BB%A3%E7%90%86/"},{"name":"负载均衡","slug":"负载均衡","permalink":"https://coconutmilktaro.top/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"}]},{"title":"Varnish笔记","slug":"Varnish笔记","date":"2018-05-31T04:00:39.000Z","updated":"2022-05-30T02:51:53.877Z","comments":false,"path":"2018/Varnish笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Varnish%E7%AC%94%E8%AE%B0/","excerpt":"Varnish 概述 Squid 与 Varnish 对比 Varnish 基本配置","text":"Varnish 概述 Squid 与 Varnish 对比 Varnish 基本配置 Varnish 概述Varnish 是一款高性能、开源的反向代理服务器和缓存服务器。并且，Varnish 还能提供以下功能： web 应用防火墙 DDoS 防护 网站防盗链 负载均衡 单点登录（SSO）网关 认证与授权 后端主机快速修复 HTTP 路由 Varnish 缓存策略的实现是通过 VCL（Varnish Configuration Language）实现，VCL 的语法简单，继承了 C 语言的很多特性，使得 VCL 样式看起来很像 C 和 PELR 语言，运行 Varnish 时，此配置将转换为 C 代码，然后送入 C 编译器，加载并执行。 Squid 与 Varnish 对比 软件 存储模式 共享存储 性能 Squid 硬盘 可并联，但很复杂 较高 Varnish 硬盘/内存 不能 很高 Varnish 对比 Squid 的优点： Varnish 稳定性比 Squid 高，Squid 发生故障的几率要高于 Varnish Varnish 访问速度比 Squid 快，因为 Varnish 采用了Visual Page Cache，所有缓存之久从内存中读取，而 Squid 从硬盘读取，所以 Varnish 会更快。 Varnish 支持更高的并发连接，因为 Varnish 的 TCP 连接释放要比 Squid 快。 Varnish 可以通过管理端口，使用正则表达式批量清除缓存。 Varnish 可通过 fork 进行多进程处理，而 Squid 仅是单进程。 Varnish 对比 Squid 的缺点： 一旦 Varnish 宕机或重启，缓存数据会在内存中丢失，所有请求就又给了后端服务器，造成后端的压力。 Varnish 在高并发下，CPU、I/O 和内存资源开销会高于 Squid。 Varnish 基本配置最好直接从官网下载，不要通过 epel 库下载，因为 epel 库中的 varnish 版本过老。或者从 varnish 官方提供的 repo 中下载。官方 repo 配置（貌似已不可用）。或者下载源码编译安装。 编译安装可能需要的依赖python-docutils、libedit-devel 进入解压目录，直接./configure --prefix=/usr/local/varnish6编译之后make &amp;&amp; make install即可。 Varnish 提供以下工具： **varnishd**：是 Varnish 的核心进程，以守护进程方式运行，接收 http 请求并转发到后端，进行缓存并响应客户端。存放在安装目录的sbin中 **varnishtest**：Varnish 测试工具，可验证 Varnish 安装、可自定义 client 请求模型、可与 Varnish 交互 **varnishadm**：Varnish 实例命令行管理工具 **varnishstat**：可访问全局计算器，提供全面统计信息。 **varnishlog**：显示 Varnish 日志，显示的是实时日志，最好设置好过滤规则，支持精确日志匹配 **varnishncsa、varnishtop、varnishhist**是 Varnish 的性能及状态分析工具 varnishhist：读取 varnish 日志，生成连续的柱状图，若缓存命中则标记|，若没命中则标记# 15_ | | | | #+--------------+--------------+--------------+--------------+----|1e-6 |1e-5 |1e-4 |1e-3 |1e-2 varnishtop：读取日志，显示实时日志信息，类似varnishlog，支持过滤 varnishncsa：实时显示请求日志信息 安装目录中是没有配置文件的，需要将解压目录中etc/example.vcl复制到安装目录中，并改名为default.vcl。 修改配置文件 backend default &#123; # 默认的后端服务器配置，默认为本机 .host = &quot;172.16.246.135&quot;; # 后端服务器IP .port = &quot;80&quot;; # 本地的代理端口。当要从后端取数据时，就会访问本地的该指定端口&#125; 然后开启 varnish。varnishd -f /var/local/varnish6/default.vcl，通过-f指定配置文件，而且配置文件的路径一定要是绝对路径，否则会提示没有该文件。确保该指定端口在本地没有被占用。 启动成功后，可以看到默认 varnish 启动了两个进程 # ps -ef | grep varnishroot 62560 1 0 02:23 ? 00:00:00 sbin/varnishd -f /usr/local/varnish6/default.vclroot 62571 62560 0 02:23 ? 00:00:00 sbin/varnishd -f /usr/local/varnish6/default.vcl 通过浏览器访问 Varnish 服务器，使用开发者工具可看到响应头中的消息显示是 Varnish 返回的。 可以通过backend块定义多个后端服务器。 backend host2 &#123; .host=&quot;172.16.246.136&quot;; .port=&quot;81&quot;; .connect_timeout=3s;&#125; 参考文章 Varnish 从菜鸟到专家（一） 高性能网站构建实战","categories":[],"tags":[{"name":"代理","slug":"代理","permalink":"https://coconutmilktaro.top/tags/%E4%BB%A3%E7%90%86/"},{"name":"缓存","slug":"缓存","permalink":"https://coconutmilktaro.top/tags/%E7%BC%93%E5%AD%98/"},{"name":"varnish","slug":"varnish","permalink":"https://coconutmilktaro.top/tags/varnish/"}]},{"title":"Squid笔记","slug":"Squid笔记","date":"2018-05-31T04:00:16.000Z","updated":"2022-05-30T02:51:53.871Z","comments":true,"path":"2018/Squid笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Squid%E7%AC%94%E8%AE%B0/","excerpt":"Squid 介绍 代理服务器概念 Squid 安装 Squid 常规配置 Squid 访问控制 Squid 多级代理配置 Squid 实验 透明二级代理 反向二级代理 cachemgr.cgi 管理 Squid Squid 日志 日志轮询 Sarg 工具分析日志 Squid 调优 调整文件描述符 调整临时端口范围 参考文章","text":"Squid 介绍 代理服务器概念 Squid 安装 Squid 常规配置 Squid 访问控制 Squid 多级代理配置 Squid 实验 透明二级代理 反向二级代理 cachemgr.cgi 管理 Squid Squid 日志 日志轮询 Sarg 工具分析日志 Squid 调优 调整文件描述符 调整临时端口范围 参考文章 Squid 介绍Squid 是一个支持 HTTP、HTTPS、FTP 等服务的 Web 缓存软件，可通过缓存页面实现降低带宽占用并优化页面相应时间。 Squid 特点与功能： 不仅能缓存 Web 页面资源，还能对 DNS 查询结果进行缓存 强大的访问控制功能 保护内网，并加速内网对外网的连接 记录内网用户访问外网行为 提供用户认证 减少出口流量 工作在 TCP/IP 的应用层，TCP 端口 3128 Squid 支持的网络协议：HTTP、FTP、Gopher（信息查找协议）、WAIS（广域信息查询系统）、SSL Squid 支持的内部缓存和管理协议：HTTP、ICP（互联网缓存协议，用于从缓存中查找指定对象）、Cache Digests（用于生成缓存中对象的索引）、SNMP（用于为外部工具提供缓存信息）、HTCP（用于发现 HTTP 缓存区，存储管理 HTTP 数据） Squid 请求过程： 客户端访问 Squid 服务器，由代理服务器代表客户端向 web 服务器（后端 Real Server）请求资源 Web 服务器将相应数据返回给代理服务器 代理服务器将数据返回给客户端，并保留一份在本地 其他客户端向该代理服务器请求相同的资源 代理服务器直接将本地的该资源缓存返回给客户端 squid 的硬件环境：内存与磁盘是缓存性能的重要体现。所有对象都会尽可能缓存到内存中，更大的磁盘空间实现更多的缓存目标和更高的命中率，最好使用 SAS，尽量不用 SATA。磁盘与内存间也有关联，最好每 G 的磁盘空间有 32M 的内存对应。 代理服务器概念代理服务器一般构建于内网和 Internet 间，负责转发内网对 Internet 的访问，并进行访问控制与记录，可实现安全保护、缓存数据、内容过滤、访问控制等功能。 Web 代理维护着庞大的缓存数据，因此对内存和硬盘的要求很高，更大的内存和硬盘意味着更多的缓存和更高的缓存命中率。 Web 缓存类型： 客户端缓存：一般就存放在浏览器中。有两个缺点：1.缓存容量小，不能存储大的 Web 对象，因此命中率较低。2.缓存在本地，不能共享，因此存在大量重复数据。 代理服务器缓存：位于网络中间位置。容量大，缓存能与内网所有客户端共享。但若性能达不到要求，反而会造成网络瓶颈。代理缓存应具有健壮性、可扩展性、稳定性、负载均衡的特点 服务器缓存：是为了减轻 web 服务器的负载，并不是为了提高资源命中率。服务器缓存减少了 web 服务器的流量、并保护了 web 服务器的安全，因为 web 服务器仅向服务器缓存提供数据，并不直接面向客户机。并且提高了网站的可靠性，因为各个服务器缓存间可实现共享。 三种典型代理方式： 传统代理：在浏览器中设置，指出代理服务器的 IP 地址和网络端口。便于用户对访问管理控制，配置简单。 透明代理（正向代理）：为内网提供外网的访问，即普通的代理服务器。但增加了网络设备的负担，并需要做好更详细的配置，会有一定的延时。若程序的一系列请求是相关的并涉及多个目标对象，有可能会出问题。 反向代理：能代理外部网络访问内网服务器。主要为本地网站做缓存，加快 web 服务器的响应速度。相当于服务器缓存。反向代理结合智能 DNS 即可实现基本的 CDN Squid 安装Squid 版本：3.5 可直接通过yum install squid安装。然后systemctl start squid启动。 安装时会自动创建用户 squid，并且是系统用户，家目录为/var/spool/squid，且禁止登录。 注：一定要做到 squid 服务器的时间同步，否则无法进行缓存 Squid 的相关配置文件： /etc/httpd/conf.d/squid.conf：用于在 Apache 中添加运行cachemgr.cgi的配置 /etc/logrotate.d/squid：Squid 的日志轮替配置 /etc/squid/squid.conf：Squid 主配置文件 /etc/squid/cachemgr.conf：设置可通过cachemgr.cgi管理的主机 /etc/squid/mime.conf：定义 MIME 类型的文件 Squid 其他相关文件： /var/log/squid/：存放 squid 日志的目录 /var/spool/squid/：存放 squid 缓存的目录 /usr/share/squid/errors/：存放给客户端的报错信息 HTML，目录中包含各个语言的子目录 /usr/lib64/squid/cachemgr.cgi：squid cache manager，用于管理主机的动态网页 squid 提供两个命令： squid：用于管理 squid 守护进程 squidclinet：用于管理 squid 客户端 squid [options] -a port 指定HTTP端口，默认3128 -d level 将指定调试等级的信息发送到标准错误输出 -f file 指定配置文件启动 -k 向squid服务器发送指令 reconfigure 重载配置文件 rotate 轮替日志文件 shutdown 安全关闭 restart 重启服务 interrupt 中断服务 kill 杀死服务 debug 开启debug check 检查运行状态 parse 检查配置文件 -s 启用syslog -u port 指定ICP端口，默认3130，若要关闭，就指定0 -z 创建缓存目录，即初始化缓存 -C 不捕获fatal信号 -D 不进行DNS参数测试 -F 不响应任何请求直到存储重建 -N 不使用daemon模式 -S 在重建期间仔细检查swap分区 -X 强制进入完全调试模式 squidclient [Basic Options] [HTTP Options] -s | --quiet 静默模式，不打印输出 -v | --verbose 显示详细信息，最多-vv -v：显示向外发的请求信息 -vv：显示动作跟踪信息 -h | --host host 指定将信息发给的主机，默认为localhost -l | --local host 指定绑定的本地IP地址，默认为空 -p | --port port 指定服务端口，默认3128 -T timeout 指定读写操作的超时时间 --ping [options] 允许ping模式 -g count 指定ping包的个数，默认一直ping -I interval 指定ping包发送间隔，默认1sHTTP Options: -a 不包含“accept:header” -j hosthdr Host header content -k 保持长连接，默认只接收一个请求就关闭连接 -m method 指定请求方法，默认GET -n 代理协商认证（kerberos） -N www协商认证（kerberos） -P file 将指定文件作为请求载荷 -r 强制缓存重新加载URL -t count 跟踪计数缓存跳数 -u user 代理认证用户名 -U user www认证用户名 -w password 代理认证密码 -W password www认证密码 Squid 常规配置http_port 3128 [模式] [options] #Squid监听的端口# 若要添加多个端口，用空格隔开 常用模式： accel：加速或反向代理模式 intercept：支持IP层NAT拦截传输到该Squid端口的流量 从squid3开始就没有transparent透明模式了icp_port 3130 #ICP端口# ICP是专门运行在代理服务器间交换缓存数据的协议。ICP使用UDP端口3130cache_effective_user squid #运行squid进程的用户#squid进程是root启动的，但启动后会由指定的普通用户继续运行cache_effective_group squid #运行squid进程的用户组pid_filename /var/run/squid.pid #squid的PID文件位置# 此文件由root在启动squid时创建logformat squid %ts.%03tu %6tr %&gt;a %Ss/%03&gt;Hs %&lt;st %rm %ru %un %Sh/%&lt;A %mt #指定日志记录格式access_log /var/log/squid/access.log squid #日志路径，类型为squidcache_mem 8 MB #cache内存#设定squid能用多少额外的内存来缓存对象的限制值cache_dir ufs /var/spool/squid 100 16 256 #指定缓存类型为ufs，保存在/var/spool/squid，是默认值#大小限制为100MB，第1层子目录为16个，第2层子目录为256个cache_store_log /var/log/squid/store.log #数据缓存的日志，是默认值maximum_object_size_in_memory 8 KB #squid保存在内存中的对象最大为8KB#内存中的对象访问速度最快，但内存有限，需要根据内存大小设置maximum_object_size 4096 KB #最大的缓存对象的字节数4096KB#只有小于该值的对象才会被缓存，若硬盘足够大，可适度提高cache_swap_low 90 #设置Squid缓存空间的使用策略。cache_swap_high 95 #当缓存中数据占到整个缓存大小的95%时 #就会按算法删除缓存中的数据 #直到缓存数据占到整个缓存大小的90% #可以最大限度利用缓存空间，但也不会出现空间溢出coredump_dir /var/spool/squid #放置squid进程运行时coredump文件的目录cache_mgr root #Squid管理员用户的Emailvisible_hostname proxy.example.com #设置对外可见的主机名，会在错误信息中显示 logformat 日志格式设置 Squid 会设置在缓存目录下建立多个目录，每个目录又建立多个子目录，在最里层的目录存放缓存文件，缓存文件是通过对客户端请求的 URL 进行哈希运算生成的。Squid 会在内存建立一张哈希表，记录硬盘中缓存文件配置的情形。 使用squid -k parse检查配置文件语法，确认没有报错后squid -z初始化缓存目录，会显示Making directories in /var/spool/squid/00等信息，发现，第一层的目录数量是 16，第二层目录的数量是 256，目录名都是由十六进制标号，与配置文件cache_dir配置的一致。若无法创建，可能是该目录的权限问题。 再使用squid -N -d1测试，没有报错，则说明启动完成。 通过浏览器测试，设置代理服务器 Squid 访问控制acl name type value1 value2... #设置ACL名字和对象的值http_access &lt;allow|deny&gt; [!]ACL对象1 ... #将客户端请求与http_access的对象匹配，指定allow或deny。!为取反。#若一个请求与所有http_access都不匹配，则执行与最后一条http_access指定的动作相反的动作 常见的 ACL 类型： 类型 含义 src 源 IP 地址，可以单个 IP 地址，可以是地址范围 dst 目的 IP 地址，同上 myip 本地网络接口 IP 地址 srcdomain 客户所属的域，Squid 会根据客户 IP 地址进行反向 DNS 查询 dstdomain 服务器所属的域，与客户请求的 URL 匹配 time 时间段 port 指向其他计算机的网络端口，即是目标服务器上的端口 myport 指向 squid 服务器的端口，是 squid 服务器上的端口 proto 客户端请求所使用的协议，如 http、https、ftp、gopher method HTTP 请求方法 proxy_auth squid 认证的用户名 url_regex 关于 URL 的正则表达式（域名） urlpath_regex 关于 URL 资源的正则表达式（资源路径，不带有域名） ident 指定用户 acl 对象的值间的关系为”或“，只要满足一个就匹配了该 acl 规则。而 http_access 与其他规则的设置使用”与“逻辑。squid 默认配置拒绝每个请求，因此在使用代理前，必须先添加访问控制规则。 若 value 为文件名，对象的值实际上是文件的内容。 常见案例： Squid3已默认定义acl名：all、localhost、manager、to_localhost acl all src 0.0.0.0/0 acl localhost src 127.0.0.1/32 acl manager proto cache_object #cache_object是squid自定义的协议，用于访问squid的缓存管理接口 acl to_localhost dst 127.0.0.1/8因此以上四个acl名字不能再使用，且可以直接调用，无需再定义acl worktime time MTWHF 08:00-17:00#时间从周一到周五，早上8点到下午5点 S-Sun M-Mon T-Tue W-Wed H-Thu F-Fri A-Satacl mynet src 10.1.1.1/24#源10.1.1.1/24的子网命名为mynetacl aim dstdomain .baidu.com .google.com#匹配指定目的域名acl giffile url_regex -i \\.gif$#匹配以.gif结尾的URLacl other srcdomain &quot;/etc/squid/other&quot;#匹配文件中指定的源域名acl safe_port port 80#匹配指定的目标服务器端口acl Users ident tomhttp_access allow tom#只允许tom访问acl mynet src 10.1.1.1-10.1.1.10http_access allow mynethttp_access deny all#仅允许mynet子网访问acl user1 src 10.1.1.1acl user2 src 10.1.1.2acl user1_time time MTWHT 08:00-12:00acl user2_time time MTWHT 08:00-13:00http_access allow user1 user1_timehttp_access allow user2 user2_time#给两个用户分别指定上外网的时间acl ftpmp3 url_regex -i &quot;^ftp://.*\\.mp3$&quot;http_access deny ftpmp3#禁止从任何ftp上下载mp3文件acl cgi urlpath_regex -i &quot;^/cgi-bin&quot;http_access deny cgi#禁止访问cgi网页acl limit maxconn 16http_access deny limit#限制同一IP客户端的最大连接数 squid 默认 acl 配置： #所有内网acl localnet src 10.0.0.0/8acl localnet src 172.16.0.0/12acl localnet src 192.168.0.0/16acl localnet src fc00::/7acl localnet src fe80::/10acl SSL_ports port 443 #SSL443端口acl Safe_ports port 80 # 放行httpacl Safe_ports port 21 # 放行ftpacl Safe_ports port 443 # 放行https..... #放行的其他服务acl CONNECT method CONNECT #connect方法，是HTTP中用于代理的方法http_access deny !Safe_ports ##只允许本机转发客户机对非Safe_ports的请求http_access deny CONNECT !SSL_ports #拒绝所有非SSL_ports的CONNECT请求，只允许本机用connect连接非SSL_portshttp_access allow localhost manager #允许localhost使用cache_object协议http_access deny manager #拒绝所有其他网络使用cache_objecthttp_access allow localnet #允许内网访问http_access allow localhost #允许本地访问http_access deny all #剩下的都不允许http_port 3128coredump_dir /var/spool/squidrefresh_pattern ^ftp: 1440 20% 10080refresh_pattern ^gopher: 1440 0% 1440refresh_pattern -i (/cgi-bin/|\\?) 0 0% 0refresh_pattern . 0 20% 4320 Squid 多级代理配置在大型网络中一台 Squid 服务器的性能不能应对巨大的访问量，需要构建多级代理服务器，类似与计算机集群，使用 ICP 交换缓存，形成一个逻辑上的大型 Squid 服务器。 代理服务器间的结构可分为：同级结构、层次结构、网状结构。最常见是层次结构。 需要配置参数cache_peer hostname type http_port icp_port options hostname为另一台 Squid 服务器的域名或 IP 地址 type为 ICP 请求的类型：parent或sibling http_port为对端的 Squid 监听请求端口 icp_port为对端 ICP 的端口 ICP 的两种请求类型 type： parent：会把客户端的请求发送给对方，对方的缓存中若有请求的数据，则返回，若没有，则对方向 web 服务器读取数据，再返回。（类似 DNS 的递归查询）一般对象处于上一级时使用此类型，因为上一级会更加接近于 web 服务器。 sibling：不会把客户端请求发给对方，仅仅询问有没有缓存。如果没有，则对方仅仅告诉回复没有，并不会向 web 服务器请求数据。一般对象处于同等级别时用此类型。de 常见的 options 参数： 选项 含义 proxy-only 从对方得到的数据不做缓存，默认会做 weight=n 指定对方的权重，有多个 cache_peer 时会按权重选择，默认根据网络响应时间自动选择 no-query 不向对方发送 ICP 请求，只发送 HTTP 代理请求，一般用于对方不支持 ICP 或不可用的情况 default 与 no-query 一起用，当对方都不支持 ICP 时，就用该 peer no-digest 不使用内存摘要表查询，直接 ICP 通信 login=user:password 若对方需要认证，就提供用户名和密码 示例： cache_peer system3.example.com parent 3128 3130 proxy-only defaultcache_peer system4.example.com sibling 3128 3130 proxy-only 若要通过规则选择不同的上级代理服务器，达到负载均衡，还需要配置： cache_peer_domain cache-host domain ... cache_peer_access cache-host allow|deny [!]ACL对象... 示例： cache_peer system1.example.com parent 3128 3130cache_peer system2.example.com sibling 3128 3130 proxy-onlycache_peer_domain system1.example.com examplecache_peer_domain system2.example.com !example#system1为example域的代理服务器，system2为非example域的代理服务器 Squid 实验透明二级代理环境： client1：192.168.1.128 server1：192.168.1.129 server2：192.168.1.130,192.168.205.140 web1：192.168.205.139 server1 上的配置squid.conf添加或修改以下内容： http_port 3128 accel #配置为透明代理icp_port 3130cache_effective_user squidpid_filename /var/run/squid.pidlogformat squid %ts.%03tu %6tr %&gt;a %Ss/%03&gt;Hs %&lt;st %rm %ru %un %Sh/%&lt;A %mtaccess_log /var/log/squid/access.log squidcache_dir ufs /var/spool/squid 100 16 256cache_store_log /var/log/squid/store.loghosts_file /etc/hosts #用于解析IP地址visible_hostname system1.example.com #错误信息中显示的主机名cache_effective_user squid #若不设置user和group，会默认使用nobodycache_effective_group squidacl PURGE method PURGE #purge是squid自定义的方法，用于删除squid缓存中的对象（能让管理员强制删除） #squid是默认拒绝Purge请求的http_access allow localhost manager #只允许本机使用cache_object协议http_access allow localhost PURGE #只允许本机使用purge方法http_access deny manager PURGEicp_access allow all #允许所有客户机访问ICP端口http_reply_access allow all #允许对所有客户机进行请求的回复 并且需要进行端口转发，即重定向，因为代理服务器需要将客户端发往 80 端口的数据包改为发往自己的 3128 端口，实现代理。因此需要开启防火墙 firewalld 或 iptables 服务。 若是使用 firewalld，则先要确定是否开启了伪装 IP 功能（Masquerade） firewall-cmd --query-masquerade，若为 no，则需要开启firewall-cmd --add-masquerade --permanent。 设置端口转发：firewall-cmd --add-forward-port=port=80:proto=tcp:toport=3128 --permanent 若为 iptables，则添加两条规则： iptables -t nat -A PREROUTING -i ens33 -p tcp --dport 80 -j --REDIRECT --to-ports=3128iptables -t nat -A POSTROUTING -o ens36 -s 192.168.205.0/24 MASQUERADE# ens33为内网卡，ens36为外网卡 然后打开转发，无论是 firewalld 还是 iptables，都要打开。 echo &quot;net.ipv4.ip_forward=1&quot; &gt;&gt; /etc/sysctl.confsysctl -p 最后放行 80 和 443 端口，放行 http 和 https 服务 firewall-cmd --permanent --add-port=80/tcp --add-port=443/tcpfirewall-cmd --permanent --add-service=http --add-service=https 反向二级代理cachemgr.cgi 管理 Squid通过 web 界面管理 Squid，需要cachemgr.cgi动态网页文件，存放在/usr/lib64/squid/中。可以将该 cgi 文件复制到/var/www/cgi-bin中，也可不动，但要注意文件的权限问题，一定要改为apache。然后在 httpd 的主配置文件中添加，Location 配置 ScriptAlias &quot;/squidcgi&quot; &quot;/var/www/cgi-bin/cachemgr.cgi&quot;&lt;Location &quot;/squidcgi&quot;&gt; Order deny,allow #逗号后不能有空格 Allow from all&lt;/Location&gt; 然后浏览器通过主机IP/squidcgi访问。 默认不需要填用户名密码即可登录。若要设置登录用户名密码，只需要在主配置文件中添加cachemgr_password参数。 cachemgr 密码 行为action要禁用一个操作，就把密码设为disable，后面跟上要禁止的操作要允许不输入密码的操作，就把密码设为none，后面跟上允许的操作action为all表示为所有操作设置相同密码 Squid 日志Squid 日志不仅记录服务器进程的运行，还记录用户的访问情况、缓存存储状况、缓存访问状况等。 Squid 三个日志：access.log，cache.log，store.log 和日志文件相关的配置： cache_log /var/log/squid/cache.log：指定缓存信息日志的路径。包含了缓存的起始配置信息，分类的错误信息，性能警告。 cache_store_log /var/log/squid/store.log：指定对象存储记录日志的路径。包含被写入缓存空间的对象、被从缓存空间清除的对象等。可设为none禁止 cache_swap_log /var/spool/squid/cache_swap.log：指定每个交换日志的路径。包含存储在交换空间的对象元数据。这类日志最好不要删除，否则可能会导致 Squid 故障。 debug_options ALL,1：控制日志记录内容的多少，第一个参数决定对哪些行为做记录，ALL 表示对所有行为做记录，第二个参数表示详细程度，1 表示详细程度最低。 log_fqdn：控制 access.log 日志中客户机地址的记录方式。on 表示会记录客户机的域名，off 则记录 IP 地址。开启会增加系统负担 logformat squid %ts.%03tu %6tr %&gt;a %Ss/%03Hs %&lt;st %rm %ru %un %Sh/%&lt;A %mt是在配置文件中日志格式的参数配置。 %ts.03tu：记录请求完成时间。%ts为相对于 Unix 纪元（1970-1-1）的秒数，%03tu表示 3 个宽度的毫秒数，.为写入日志的固定符号。 %6tr：响应时间，表明了 Squid 处理请求的时间（接收到 HTTP 请求到响应报文发出），单位毫秒。ICP 响应时间一般为 0，非常快速。 %&gt;a：记录客户端地址，若开启了log_fqdn，则会记录客户端主机名。还可通过client_netmask隐藏客户端 IP 的一部分 %Ss/%03Hs：记录请求结果和状态码，%Ss是 Squid 特有的请求结果码，%03Hs是 HTTP 状态码 %&lt;st：记录传输的字节数。是整个数据包的大小，会比实际载荷信息大。 %rm：记录请求的方法。HTTP 的常见请求和 ICP 的 ICP_QUERY 请求 %ru：记录客户端请求的 URI。默认不会记录 URL 中第一个?后所有信息 %un：记录客户端用户身份。Squid 使用 RFC1413 或 HTTP 的验证头部确认用户身份 %Sh/%&lt;A：记录 peer 主机（其他代理服务器）信息 %mt：记录 MIME 类型。从响应的Content-type域获取信息，若没有就使用一个-代替。 logfile_rotate：轮询保存的文件数，超过限制就会从头开始覆盖 日志轮询Squid 并没有自动轮询的机制，只能使用squid -k rotate命令，并编写脚本通过 cron 周期执行。 #!/bin/bashcd /var/log/squid/[ -f access.log ] &amp;&amp; mv access.log access_$(date +%F).logsquid -k rotate Sarg 工具分析日志Sarg 是一个 Squid 的日志分析工具，输出为 html 文件。Sarg 下载-tar.gz 包 依赖 gd 库，pcre 库。在 sarg 安装完成后，进入安装目录的bin目录，执行sarg命令，sarg 会自动寻找 Squid 的日志文件，并分析。SARG: Records in file: 595935, reading: 100.00%，然后会生成一个目录，是自动存放在/var/www/html/squid-reports下，目录名为起始日期-结束日期，该目录下有index.html 通过浏览器访问该 index 文件，数据量相当庞大，并提供图表和日期时间的数据记录 sarg 命令： sarg [options] --convert 将access.log文件转换为易读的日期格式 -d DATE 指定报告的日志范围dd/mm/yyyy-dd/mm/yyyy -e MAIL 将报告发送给指定email -f FILE 指定配置文件，默认为安装目录的etc/sarg.conf --keeplogs 保留以前生成的每个报告 -l FILE 指定日志文件 -n 使用rDNS将IP地址解析成域名 -o DIR 报告存放目录 -p 使用IP地址而不是userid --split 按-d指定的日期切割日志文件 -t TIME 指定时间范围[HH:MM 或 HH:MM-HH:MM] -u USER 只报告指定用户的行为 -x 开始分析，且会先输出完整的配置信息 -z 显示完整的输出信息 Sarg 配置，存放在安装目录的etc/sarg.conf access_log /usr/local/squid/var/logs/access.log #squid日志路径output_dir /var/www/html/squid-reports #报告输出目录date_format u #日期格式，u为美国格式mm/dd/yy，e为欧洲格式dd/mm/yyoverwrite_report no #是否对已存在的日期的报告覆盖 Squid 调优调整文件描述符Squid 在高负载下，需要大量内核资源，又因为 Squid 是做缓存服务器，所以极度消耗文件描述符，而 unix 对文件描述符是有限制的（1024），这样会造成极大的性能影响，当 squid 用完所有文件描述符后，就不能接收新的请求了，并且 squid 发现文件描述符短缺后，就会发布警告。 因此，需要先查看文件描述符是否满足使用，大多数情况 1024 已经足够使用，当出现高负载情况时，则需要更多，因此最好将系统限制的文件描述符数量设为每个进程限制的两倍。 ulimit -a 查看当前的资源限制信息找到open files，就是同时能打开的文件数量，即文件描述符，默认为1024优化性能，将此值设为2048ulimit -Hn 2048# -H 设定资源的硬性限制，也就是管理员所设下的限制# -n 设置同时最多能打开的文件数 注：ulimit 仅仅作为临时设置，可以作用于通过使用其命令登录的 shell 会话，在会话终止时便结束限制，并不影响于其他 shell 会话。 **若要永久设置，可写入/etc/profile：echo &quot;ulimit -Hn 2048&quot;&gt;&gt;/etc/profile ** 或写入/etc/security/limits.conf：echo &quot;* - nofile 2048&quot;&gt;&gt;/etc/security/limits.conf 调整临时端口范围临时端口是 TCP/IP 栈分配出连接的本地端口，当 squid 发起一条连接到另一台服务器，内核给本地 socket 分配一个端口号。CentOS 默认的临时端口范围是 32768 到 60999 当 squid 高负载时，若临时端口号短缺，会造成很大的性能影响，因为一些 TCP 连接在关闭时会进入TIME_WAIT状态，此状态下临时端口不能重用。 可通过sysctl -a | grep net.ipv4.ip_local_port_range查看 设置范围 4000 到 65000，sysctl -w net.ipv4.ip_local_port_range=&quot;4000 65000&quot; 参考文章 Linux 服务器架设指南（第二版） Linux 系统管理与网络管理（第二版） Linux 运维之道（第二版） 高性能网站构建实战","categories":[],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"代理","slug":"代理","permalink":"https://coconutmilktaro.top/tags/%E4%BB%A3%E7%90%86/"},{"name":"缓存","slug":"缓存","permalink":"https://coconutmilktaro.top/tags/%E7%BC%93%E5%AD%98/"},{"name":"squid","slug":"squid","permalink":"https://coconutmilktaro.top/tags/squid/"}]},{"title":"Nagios监控搭建","slug":"Nagios监控搭建","date":"2018-05-31T03:56:48.000Z","updated":"2022-06-21T16:58:53.085Z","comments":true,"path":"2018/Nagios监控搭建/","link":"","permalink":"https://coconutmilktaro.top/2018/Nagios%E7%9B%91%E6%8E%A7%E6%90%AD%E5%BB%BA/","excerpt":"本篇包含以下内容 Nagios 概述 Nagios 搭建 服务器端 客户端安装 目录与配置文件概述 templates.cfg resource.cfg Nagios 宏 commands.cfg hosts.cfg services.cfg contacts.cfg timeperiods.cfg cgi.cfg nagios.cfg Nagios 监控界面解析 常用操作 Nagios 性能分析图表 邮件告警配置 参考资料","text":"本篇包含以下内容 Nagios 概述 Nagios 搭建 服务器端 客户端安装 目录与配置文件概述 templates.cfg resource.cfg Nagios 宏 commands.cfg hosts.cfg services.cfg contacts.cfg timeperiods.cfg cgi.cfg nagios.cfg Nagios 监控界面解析 常用操作 Nagios 性能分析图表 邮件告警配置 参考资料 基于 Nagios4.4.1，Nagios-Plugin2.2.1 Nagios 概述Nagios 是一款用于监控系统和网络的开源应用软件，能有效的监控 windows，linux 和 unix 的主机状态。采用 C/S 结构。Nagios 由 ANSI C 编写。 Nagios 结构分为 Nagios Core 核心主程序和 Nagios Plugins 插件。核心只提供很少的监控功能，用户需要给 Nagios 安装相应插件以搭建完善的监控系统。 Nagios 如何工作 监控：监控关键 IT 基础架构组件，包括系统指标，网络协议，应用程序，服务，服务器和网络基础架构。 告警：在关键基础架构组件发生故障和恢复时发送警报，为管理员提供重要事件的通知。警报可以通过电子邮件，短信或自定义脚本提供。 响应：IT 人员可以确认警报并开始解决中断并立即调查安全警报。如果未及时确认警报，则警报可以升级到不同的组。 报告：报告提供中断，事件，通知和警报响应的历史记录，供以后查看。 维护：计划停机可防止在计划维护和升级窗口期间发出警报。 计划：通过趋势和容量规划图表和报告，运维人员可以在发生故障之前识别必要的基础架构升级。 Nagios 特点 网络服务监控（SMTP、POP3、HTTP、NNTP、ICMP、SNMP、FTP、SSH、端口，URL，丢包，进程数，网络流量、交换机端口流量，路由器，打印机） 本地和远端主机资源监控（CPU、内存、磁盘、日志、负载 uptime、I/O，Raid 级别，温度，passwd 文件的变化，本地所有文件指纹识别），也包括 Windows 主机（使用 NSClient++ plugin） 业务数据监控（用户登陆失败次数，用户登陆网站次数，输入验证码失败次数，某个 API 接口流量并发，网站订单，支付交易数量） 可以指定自己编写的 Plugin 通过网络收集数据来监控任何情况（温度、警告……）。可以通过配置 Nagios 远程执行插件远程执行脚本 远程监控支持 SSH 或 SSL 加通道方式进行监控 简单的 plugin 设计允许用户很容易的开发自己需要的检查服务，支持很多开发语言（shell scripts、C++、Perl、ruby、Python、PHP、C#等） 包含很多图形化数据 Plugins（Nagiosgraph、Nagiosgrapher、PNP4Nagios 等） 可并行服务检查 能够定义网络主机的层次，允许逐级检查，就是从父主机开始向下检查 当服务或主机出现问题时发出通告，可通过 email, pager, sms 或任意用户自定义的 plugin 进行通知 能够自定义事件处理机制重新激活出问题的服务或主机 自动日志循环 支持冗余监控 包括 Web 界面可以查看当前网络状态，通知，问题历史，日志文件等 Nagios 插件概述默认搭建的 Nagios 服务器只能监控简单的几个项目，而其他服务之类的监控项目都是需要插件来实现，插件可用官方提供的，也可以自己编写。插件是 Nagios Core 的独立扩展，可以使用 Core 监控任何事情。插件处理命令行参数，执行特定检查，然后将结果返回给 Nagios Core。它们可以是编译的二进制文件（用 C，C ++等编写）或可执行的脚本（shell，Perl，PHP 等）。 NRPE 概述NRPE 总共由两部分组成: check_nrpe 插件，位于监控主机上 NRPE daemon，运行在远程被监控的 Linux 主机上 当监控远程主机服务或资源时，工作流程如下： nagios 会运行 check_nrpe 插件并指定检查项 check_nrpe 插件会通过 ssl 连接到远程的 NRPE daemon NRPE daemon 会运行相应的 Nagios 插件来执行检查动作 NPRE daemon 将检查的结果返回给 check_nrpe 插件，插件将其递交给 Nagios 做处理 NRPE 的检测类型分为两种: 直接检测：检测的对象是运行 NRPE 的那台 Linux 主机的本地资源，直接使用 NRPE 插件监控远程 Linux 主机的本地或者私有资源 间接检测：当运行 Nagios 的监控主机无法访问到某台被监控主机，但是运行 NRPE 的机器可以访问得到的时候，运行 NRPE 的主机就充当一个中间代理，将监控请求发送到被监控对象上 就如下图中check_disk和check_load是直接检测，check_http和check_ftp是间接检测。 Nagios 搭建服务器端需要安装的软件： LAMP：因为 Nagios 需要 web 端显示，所以需要 LAMP 的 web 环境，也可以是 LNMP。 Nagios-core：Nagios 的主程序 Nagios-plugins：Nagios 的插件 NRPE：Nagios 的一个功能扩展，可在远程主机上执行插件程序，是客户端程序。 客户端需要安装的软件 Nagios-plugins NRPE 服务器端 首先搭建 LAMP 环境yum install httpd php php-gd gd* gcc* glibc* openssl*注：gd 是图像处理库 创建用户 nagios，创建用户组 nagcmd，将 apache 和 nagios 都添加到 nagcmd 副组中。组 nagcmd 用于从 Web 接口执行外部命令 创建安装目录/usr/local/nagios，编译安装 进入nagios-4.4.1解压目录./configure \\ --prefix=/usr/local/nagios \\ --with-command-group=nagcmd General Options: ------------------------- Nagios executable: nagios Nagios user/group: nagios,nagios Command user/group: nagios,nagcmd Event Broker: yes Install $&#123;prefix&#125;: /usr/local/nagios Install $&#123;includedir&#125;: /usr/local/nagios/include/nagios Lock file: /run/nagios.lock Check result directory: /usr/local/nagios/var/spool/checkresults Init directory: /lib/systemd/system Apache conf.d directory: /etc/httpd/conf.d Mail program: /usr/bin/mail Host OS: linux-gnu IOBroker Method: epoll Web Interface Options: ------------------------ HTML URL: http://localhost/nagios/ CGI URL: http://localhost/nagios/cgi-bin/ Traceroute (used by WAP): /usr/bin/traceroutemake all \\ &amp;&amp; make install \\ # 安装Nagios主程序的CGI和HTML &amp;&amp; make install-init \\ # 在/lib/systemd/system创建Nagios启动脚本，即可通过systemctl操作 &amp;&amp; make install-commandmode \\ # 配置目录权限 &amp;&amp; make install-config \\ # 安装示例配置文件，在/usr/local/nagios/etc目录 &amp;&amp; make install-webconf # 生成配置文件/etc/httpd/conf.d/nagios.conf 至此，Nagios Core 安装完成。 下面安装 nagios plugins 进入nagios-plugins-2.2.1解压目录./configure \\ --prefix=/usr/local/nagiosmake &amp;&amp; make install查看/usr/local/nagios/libexec目录下是否有插件文件，若有则安装成功 nagios plugins 安装后，继续安装 NRPE。 进入nrpe-3.2.1解压目录./configuremake all \\&amp;&amp; make install \\&amp;&amp; make install-plugin \\&amp;&amp; make install-daemon \\&amp;&amp; make install-config General Options: ------------------------- NRPE port: 5666 NRPE user: nagios NRPE group: nagios Nagios user: nagios Nagios group: nagios 因为可能开启邮件告警功能，所以要启动 sendmail 服务，不需要做任何配置。yum install sendmail*systemctl enable sendmailsystemctl start sendmail 也可以使用 postfix 服务进行邮件告警，同样不需要任何配置yum install postfix*systemctl enable postfixsystemctl start postfix注：sendmail 和 postfix 同时只开启一个，一个开启则另一个会自动停止 至此，服务器端核心组件安装完成。 如果要进行汉化，则可以安装中文插件nagios-cn。下载后直接解压进入目录，然后./configure，make &amp;&amp; make install即可。 先检查下/usr/local/nagios/bin中是否有命令nagios和nagiostats，如果没有，就将 nagios-4.4.1 目录中base目录下的nagios命令和nagiostats命令复制到/usr/local/nagios/bin/中，因为通过systemctl启动 nagios 时，会调用该目录的nagios命令。若不存在会报错无法启动。 为配置文件还有 nagios 和 nagiostats 命令创软链接。ln -s /usr/local/nagios/etc/nagios.cfg /etc/nagios.cfgln -s /usr/local/nagios/etc/cgi.cfg /etc/cgi.cfg 为 nagios 默认登录用户 nagiosadmin 创建 http 验证密码。htpasswd -c /usr/local/nagios/etc/htpasswd.users nagiosadmin 将/usr/local/nagios的所有者和所属组都改为nagios 使用nagios -v /etc/nagios.cfg检查配置文件是否正确。 确保 Selinux 关闭，确保防火墙放行 80 端口或关闭。 启动 httpd 和 nagios 以及 nrpe 服务，并设为开机自启。systemctl start httpd nagios nrpesystemctl enable httpd nagios nrpe 在浏览器上输入IP地址/nagios，会先要求输入 htpasswd 设置的用户名密码登录验证。登录成功后跳转到 Nagios 主页面。 服务器端配置 NRPEln -s /usr/local/nagios/etc/nrpe.cfg /etc/nrpe.cfg 修改commands.cfg文件，添加check_nrpe命令 define command &#123; command_name check_nrpe command_line $USER1$/check_nrpe -H $HOSTADDRESS$ -c $ARG1$&#125; 修改service.cfg，添加服务 define service &#123; use local-service host_name system3 service_description users check_command check_nrpe!check_users # 在check_nrpe!后直接使用nrpe.cfg定义的command变量&#125; 完成后重启 nrpe 客户端安装 安装 nagios plugins 进入nagios-plugins-2.2.1解压目录./configure --prefix=/usr/local/nagiosmake &amp;&amp; make install 安装 NRPE 进入nrpe-3.2.1解压目录./configuremake all \\ 构建nrpe和check_nrpe&amp;&amp; make install \\ 安装nrpe和check_nrpe&amp;&amp; make install-plugin \\ 安装check_nrpe插件&amp;&amp; make install-daemon \\ 安装nrpe daemon&amp;&amp; make install-config 安装nrpe 配置文件&amp;&amp; make install-init 安装systemd文件 查看配置文件/usr/local/nagios/etc/nrpe.cfg，若要添加监控命令，便找到以下配置，按照格式进行添加 command command[check_users]=/usr/local/nagios/libexec/check_users -w 5 -c 10command[check_load]=/usr/local/nagios/libexec/check_load -r -w .15,.10,.05 -c .30,.25,.20command[check_hda1]=/usr/local/nagios/libexec/check_disk -w 20% -c 10% -p /dev/hda1command[check_zombie_procs]=/usr/local/nagios/libexec/check_procs -w 5 -c 10 -s Zcommand[check_total_procs]=/usr/local/nagios/libexec/check_procs -w 150 -c 200# command后中括号里内容就是定义的变量，名称可任意指定 （可选）修改/etc/services，添加nrpe 5666/tcp # Nagios_client。 启动 nrpe，systemctl start nrpe，并使用ps -ef查看 nrpe 是否启动。 使用插件/usr/local/nagios/libexec/check_nrpe -H localhost检验 nrpe 是否启动成功。若成功，会输出 NRPE 的版本号。 修改配置文件/usr/local/nagios/nrpe.cfg #server_address=127.0.0.1 # 客户端主机IP地址# 添加监控服务器地址，声明合法的NRPE服务对象allowed_hosts=127.0.0.1,监控服务器地址或域名# 若没有在这指定监控服务器地址，则监控服务器无法获取本机的服务信息 给命令nrpe和配置文件nrpe.cfg创软链接，并以守护进程启动。nrpe -c /etc/nrpe.cfg -d也可通过systemctl start nrpe启动。 测试 nrpe 是否成功启动/usr/local/nagios/libexec/check_nrpe -H 127.0.0.1，输出 NRPE 版本号则启动成功。 测试 nrpe 是否能与客户端通信，同样使用check_nrpe指定对端 IP 地址或域名，输出 NRPE 版本号则通信正常。若报错 目录与配置文件概述在 Nagios 服务器端安装完后，/usr/local/nagios中有以下目录：bin，etc，sbin，share，libexec，var，var/archives，var/rw。其中libexec是存放外部插件的。share是存放网页文件的。var/archives是存放归档日志的。var/rw是存放外部命令的。 配置文件或目录 说明 cgi.cfg 控制 cgi 访问的配置文件 nagios.cfg Nagios 主配置文件 resource.cfg 变量定义文件 objects 目录，存放配置文件模板，用于定义 Nagios 对象 objects/commands.cfg 命令定义配置文件，定义的命令可悲其他配置文件使用 objects/contacts.cfg 定义联系人和联系人组 objects/localhost.cfg 定义监控本地主机 objects/printer.cfg 定义监控打印机，默认未开启 objects/switch.cfg 定义监控路由器，默认未开启 objects/templates.cfg 定义主机、服务，默认未开启 objects/timeperiods.cfg 定义监控时间段 objects/windows.cfg 定义 windows 主机，默认未开启 Nagios 配置中涉及到的定义：（建议按以下顺序配置） 主机、主机组、服务、服务组 监控命令 监控时间 联系人、联系人组 Nagios 监控的主机资源和服务在配置文件中称为对象。建议将 Nagios 各个定义对象创建独立的配置文件，如： 创建hosts.cfg定义主机和主机组 创建services.cfg定义服务和服务组 其余的对象都用默认文件即可。 templates.cfg定义主机和服务的模板 define contact &#123; # 定义联系人 name generic-contact ; 联系人名 service_notification_period 24x7 ; 服务通知时间，默认24x7任何时间 host_notification_period 24x7 ; 主机通知时间，默认24x7任何时间 service_notification_options w,u,c,r,f,s ; 服务发送通知的状态条件 # w：警告warn u：未知unknown c：紧急criticle r：重新恢复recover host_notification_options d,u,r,f,s ; 主机发送通知的状态条件 # d：宕机down u：未知/不可达unreachable r：重新恢复 service_notification_commands notify-service-by-email ; 服务发送通知的方式 host_notification_commands notify-host-by-email ; 主机发送通知的方式&#125;define host &#123; # 定义主机 name generic-host ; 主机名（对于配置文件，并非系统主机名） notifications_enabled 1 ; 是否开启主机通知，1为开启，2为不开启 event_handler_enabled 1 ; 是否开启事件处理 flap_detection_enabled 1 ; 是否启用报警延时 process_perf_data 1 ; 是否启用数据输出 retain_status_information 1 ; 是否在程序重启期间保留状态信息 retain_nonstatus_information 1 ; 是否在程序重启期间保留非状态信息 notification_period 24x7 ; 主机通知时间，默认任何时间&#125;define host &#123; name linux-server ; 主机名 use generic-host ; 引用指定的主机，此处引用了上面定义的主机的配置 check_period 24x7 ; 检查主机的时间段，默认不间断 check_interval 5 ; 检查主机的时间间隔，默认5分钟 retry_interval 1 ; 重试检查时间间隔，默认1分钟 max_check_attempts 10 ; 对主机检查的最多次数。并不是检查一次就判断，而是多检查几次才判断是否故障。单位次 check_command check-host-alive ; 默认检查命令 notification_period workhours ; 发送通知的时间段 notification_interval 120 ; 发送通知的间隔，单位分钟 notification_options d,u,r ; 发送通知的状态条件 contact_groups admins ; 联系人组&#125;后面还有对打印机、交换机的主机定义然后就是对服务的定义define service &#123; # 定义服务 name generic-service ; 服务名 active_checks_enabled 1 ; 是否开启动态检查 passive_checks_enabled 1 ; 是否开启主动检查 parallelize_check 1 ; 应该并行化活动服务检查（禁用此功能可能会导致严重的性能问题） obsess_over_service 1 ; 默认为1 check_freshness 0 ; 默认为0 notifications_enabled 1 ; 是否启用服务通知 event_handler_enabled 1 ; 是否启用事件处理 flap_detection_enabled 1 ; 是否启用报警延时 process_perf_data 1 ; 是否启用数据输出 retain_status_information 1 ; 是否在程序重启期间保留状态信息 retain_nonstatus_information 1 ; 是否在程序重启期间保留非状态信息 is_volatile 0 ; 是否稳定，0为稳定，1为不稳定 check_period 24x7 ; 检查服务的时间段，默认不间断 max_check_attempts 3 ; 对服务检查的最多次数 check_interval 10 ; 检查时间间隔，默认10分钟 retry_interval 2 ; 重试检查时间间隔，默认2分钟 contact_groups admins ; 联系人组 notification_options w,u,c,r ; 发送通知的状态条件 notification_interval 60 ; 发送通知的间隔，单位分钟 notification_period 24x7 ; 发送通知的时间段&#125; resource.cfg定义变量的模板。变量需要先定义才能在别的配置文件中调用，否则 nagios 就会报错。 # 变量$USER1$指定了nagios插件的安装路径$USER1$=/usr/local/nagios/libexec# $USER2$定义了事件处理的安装路径$USER2$=/usr/local/nagios/libexec/eventhandlers Nagios 宏Nagios 配置有两个特征：继承与引用，在命令行定义中使用宏，通过宏，Nagios 可灵活获取主机、服务等对象的信息。在命令执行前，Nagios 会对命令进行宏替换。宏分为：默认宏、按需而成的宏、用户自定义宏 默认宏：主机 IP 地址宏 define host&#123; host_name linuxbox address 192.168.1.2 check_command check_ping...&#125;define command&#123; command_name check_ping command_line /usr/local/nagios/libexec/check_ping -H $HOSTADDRESS$ -w 100.0,90% -c 200.0,60%&#125;在执行时，就会把宏替换为IP地址 命令参数宏向命令传递参数，参数指定在对象中定义，用一个!分隔。 define service&#123; host_name linuxbox service_description PING check_command check_ping!200.0,80%!400.0,40%...&#125;define command&#123; command_name check_ping command_line /usr/local/nagios/libexec/check_ping -H $HOSTADDRESS$ -w $ARG1$ -c $ARG2$&#125;在执行时，会将分隔的两个参数替换到命令中的ARG1和ARG2 注：如果要在命令中使用!，\\都要使用反斜杠转义 Nagios 可用的所有宏： 主机宏$HOSTNAME$：主机名，取自主机定义中host_name$HOSTADDRESS$：主机 IP 地址，取自主机定义中address 服务宏$SERVICESTATE$：服务状态描述，三个可能：w，u，c$SERVICEDESC$：对当前服务的描述 联系人宏$CONTACTNAME$：联系人名，在联系人文件中定义 通知宏$NOTIFICATIONTYPE$：返回状态信息。 日期宏$LONGDATETIME$：当前日期，时间戳 文件宏$LOGFILE$：日志文件保存位置$MAINCONFIGFILE：主配置文件保存位置 其他宏$ADMINMAIL$：管理员 E-mail 地址$ARGn$：第 n 个命令参数，n 是数字。最多支持 32 个参数宏。 commands.cfg定义命令。 define command &#123; command_name check-host-alive # 命令名 # 命令执行 command_line $USER1$/check_ping -H $HOSTADDRESS$ -w 3000.0,80% -c 5000.0,100% -p 5 # 调用插件check_ping，-w为warning警告状态，-c为紧急，80%表示ping的临界值。-p 5表示每次发5个ping包&#125; hosts.cfg默认不存在，定义主机。 define host &#123; use linux-server # 引用templates.cfg中linux-server的配置 host_name web # 主机名 alias web-system5 # 别名 address 192.168.163.137 # IP地址&#125;define hostgroup &#123; hostgroup_name web-servers # 主机组名 alias web-servers # 别名 members web # 组成员（填host_name，逗号分隔）&#125; services.cfg默认不存在，定义服务。 define service &#123; use local-service # 引用templates.cfg中local-service的配置 host_name web # 主机名 service_description ping # 服务描述 check command check_ping..... #引用命令&#125;define servicegroup &#123; # 服务组，配置类似主机组 servicegroup_name alias members&#125; contacts.cfg定义联系人。 define contact &#123; contact_name # 联系人名 use # 使用templates.cfg中指定模板信息 alias email # 联系人邮箱&#125;define contactgroup &#123; contactgroup_name # 联系人组名 alias members&#125; timeperiods.cfg定义监控时段。 define timeperiod &#123; name 24x7 #定义时段名 # 之前host和service中的24x7就是引用这里定义的时段名 timeperiod_name 24x7 alias 24 Hours A Day, 7 Days A Week # 定义监控时间，若某天不监控则不要写那天 sunday 00:00-24:00 monday 00:00-24:00 tuesday 00:00-24:00 wednesday 00:00-24:00 thursday 00:00-24:00 friday 00:00-24:00 saturday 00:00-24:00&#125; cgi.cfg控制 cgi 脚本。用于在 web 界面执行 cgi 脚本，如重启 nagios 进程、关闭通知、停止检测等。以下为修改权限涉及的参数 default_user_name=guestauthorized_for_system_information=nagiosadmin # 验证系统信息authorized_for_configuration_information=nagiosadmin # 验证配置信息authorized_for_system_commands=nagiosadmin # 验证系统命令authorized_for_all_services=nagiosadmin # 验证所有服务authorized_for_all_hosts=nagiosadmin # 验证所有主机authorized_for_all_service_commands=nagiosadmin # 验证所有服务命令authorized_for_all_host_commands=nagiosadmin # 验证所有主机命令 nagios.cfgNagios 的核心配置文件，所有配置文件必须在此文件中引用才有作用。 log_file=/usr/local/nagios/var/nagios.log # 日志文件路径cfg_file=/usr/local/nagios/etc/objects/commands.cfgcfg_file=/usr/local/nagios/etc/objects/contacts.cfgcfg_file=/usr/local/nagios/etc/objects/timeperiods.cfgcfg_file=/usr/local/nagios/etc/objects/templates.cfgcfg_file=/usr/local/nagios/etc/objects/localhost.cfg# 继续添加配置文件即可启用指定配置文件。# 也可以直接将文件放入一个目录，然后通过cfg_dir=指定object_cache_file=/usr/local/nagios/var/objects.cache # 指定一个所有对象配置文件的副本文件，也称为对象缓冲文件。# nagios会将所有对象文件的内容都写入该文件resource_file=/usr/local/nagios/etc/resource.cfg # 指定nagios资源文件的路径，可在nagios.cfg定义多个资源文件status_file=/usr/local/nagios/var/status.dat # 指定状态文件，用于保存nagios当前状态、注释、宕机信息等status_update_interval=10 # 状态文件的更新周期，单位秒，最小1秒nagios_user=nagios # nagios进程所属用户nagios_group=nagios # nagios进程所属用户组check_external_commands=1 # 是否允许nagios在web界面执行cgi命令interval_length=60 # 指定nagios时间单位，默认60s，即nagios配置中所有 时间单位为分钟 Nagios 监控界面解析左侧菜单栏中Current Status目录如下 Tactical Overview 总览Map 拓扑图Hosts 主机Services 服务Host Groups 主机组- Summary 汇总- Grid 表格Service Groups 服务组，也分为汇总和表格Problems 问题故障- Service(Unhandled) 未解决的服务故障- Hosts(Unhandled) 未解决的主机故障- Network Outages 网络整体 Reports目录如下： Availability 可用性Trends 趋势Alerts 报警- History 历史- Summary 汇总- Histogram 历史图Notification 通知Event Log 事件日志 System目录如下： Comments 注释Downtime 停机计划Process Info 进程信息Performance Info 性能查询Scheduling Queue 定时查询Configuration 配置 常用操作Nagios 对主机和服务有几个描述的状态 Hosts Up 启动（绿） Down 未启动（红） Unreachable 不可达（黄） Pending 等待（灰色） Services Ok 正常（绿） Warning 警告（黄） Unknown 未知（橙） Critical 紧急（红） Pending 等待（灰） Nagios 性能分析图表 需要安装pnp软件包，基于 PHP 和 Perl，利用 rrdtool 工具将 nagios 收集的数据绘制成图表。pnp 官网首先，需要安装gd库、zlib库、jpeg库yum install gd gd-devel zlib zlib-devel jpeg*接着安装rrdtool工具yum install rrdtool*安装perl环境yum install perl最后去 pnp 官网下载源码包安装 ./configure \\ --with-nagios-user=nagios \\ --with-nagios-group=nagios \\ --with-rrdtool=/usr/bin/rrdtool \\ --with-perfdata-dir=/usr/local/nagios/share/perfdatamake all \\ &amp;&amp; make install \\ &amp;&amp; make install-config \\ &amp;&amp; make install-init \\ &amp;&amp; make install-webconf或者直接make fullinstall（包含以上所有make） General Options: ------------------------- ------------------- Nagios user/group: nagios nagios Install directory: /usr/local/pnp4nagios HTML Dir: /usr/local/pnp4nagios/share Config Dir: /usr/local/pnp4nagios/etc Location of rrdtool binary: /usr/bin/rrdtool Version 1.7.0 RRDs Perl Modules: FOUND (Version 1.6999) RRD Files stored in: /usr/local/nagios/share/perfdata process_perfdata.pl Logfile: /usr/local/pnp4nagios/var/perfdata.log Perfdata files (NPCD) stored in: /usr/local/pnp4nagios/var/spool Web Interface Options: ------------------------- ------------------- HTML URL: http://localhost/pnp4nagios Apache Config File: /etc/httpd/conf.d/pnp4nagios.conf 安装完后，将/usr/local/pnp4nagios的所有者和所属组都改为 nagios 配置 pnp将/usr/local/pnp4nagios/share目录下所有文件复制到/usr/local/nagios/share/pnp中。将/usr/local/pnp4nagios/etc中npcd.cfg、rra.cfg、process_perfdata.cfg后面的-sample去除（如果有的话）。 首先修改process_perfdata.cfg。 # 指定日志路径LOG_FILE = /usr/local/pnp4nagios/var/perfdata.log# 日志输出级别，默认为0，最好改为2，即debugLOG_LEVEL = 2# 三个等级：0==slient 1==normal 2==debug 然后将 pnp 与 nagios 进行整合，对 templcates.cfg 配置，添加以下定义。 define host &#123; name hosts-pnp register 0 action_url /nagios/pnp/index.php?host=$HOSTNAME$ process_perf_data 1&#125;define service &#123; name services-pnp register 0 action_url /nagios/pnp/index.php?host=$HOSTNAME$&amp;srv=$SERVICEDESC$ process_perf_data 1&#125;# 注：必须在不应处理其性能数据的每个主机或服务的定义中禁用数据处理（process_perf_data 设为0）。 然后在hosts.cfg和services.cfg和localhost.cfg中要进行数据分析的服务或主机的name参数后加上hosts-pnp或service-pnp，如下： define service &#123; use local-service,serviecs-pnp host_name web service_description ping check command check_ping.....&#125; 修改 nagios.cfg # 开启Nagios数据输出。会将收集到的数据写入文件process_performance_data=1# 取消注释，启用主机和服务的输出功能host_perfdata_command=process-host-perfdataservice_perfdata_command=process-service-perfdatahost_perfdata_file=/usr/local/pnp4nagios/var/host-perfdataservice_perfdata_file=/usr/local/pnp4nagios/var/service-perfdatahost_perfdata_file_template=[HOSTPERFDATA]\\t$TIMET$\\t$HOSTNAME$\\t$HOSTEXECUTIONTIME$\\t$HOSTOUTPUT$\\t$HOSTPERFDATA$service_perfdata_file_template=[SERVICEPERFDATA]\\t$TIMET$\\t$HOSTNAME$\\t$SERVICEDESC$\\t$SERVICEEXECUTIONTIME$\\t$SERVICELATENCY$\\t$SERVICEOUTPUT$\\t$SERVICEPERFDATA$host_perfdata_file_mode=aservice_perfdata_file_mode=ahost_perfdata_file_processing_interval=0service_perfdata_file_processing_interval=0host_perfdata_file_processing_command=process-host-perfdata-fileservice_perfdata_file_processing_command=process-service-perfdata-file 修改 commands.cfg define command &#123; command_name process-host-perfdata-file # 将原来的command_line注释，改为如下参数 command_line /usr/local/pnp4nagios/libexec/process_perfdata.pl --bulk=/usr/local/pnp4nagios/var/host-perfdata&#125;define command &#123; command_name process-service-perfdata-file # 将原来的command_line注释，改为如下参数 command_line /usr/local/pnp4nagios/libexec/process_perfdata.pl --bulk=/usr/local/pnp4nagios/var/service-perfdata&#125; 重启 nagios 和 httpd进入 web 端，点击左侧菜单services。进入如下页面 点击红框框出的图标，即可进入 pnp 测试界面 若全部通过，便会提示删除或重命名/usr/local/nagios/share/pnp/install.php。于是将该 php 文件删除。rm -f /usr/local/nagios/share/pnp/install.php 如果在点击 pnp 图标时，出现以下报错： 则需要检查nagios.cfg和commands.cfg配置文件，查看commands.cfg配置可知command_name为process-host-perfdata的默认存放路径bulk为/usr/local/pnp4nagios/var/host-perfdata，同理，process-service-perfdata的存放路径为/usr/local/pnp4nagios/var/service-perfdata。 而在nagios.cfg中host_perfdata_file默认路径为/usr/local/nagios/var/host-perfdata，service_perfdata_file默认路径为/usr/local/nagios/var/service-perfdata，两个文件不一致，导致 pnp4nagios 无法获取。 将nagios.cfg的两条配置，改为与commands.cfg的一致即可 host_perfdata_file=/usr/local/pnp4nagios/var/host-perfdataservice_perfdata_file=/usr/local/pnp4nagios/var/service-perfdata 邮件告警配置先安装 sendmail 或 postfix，安装完后开启。本篇使用 sendmail 服务。使用mail命令发送测试邮件。若没有 mail 命令，需要先下载mailx软件mail -s test XXXX@XX.com输入内容完后ctrl+d结束。 关于邮件告警主要涉及以下几个文件： templates.cfg 其中有关于contact的定义define contact &#123; name generic-contact service_notification_period 24x7 host_notification_period 24x7 service_notification_options w,u,c,r,f,s host_notification_options d,u,r,f,s service_notification_commands notify-service-by-email host_notification_commands notify-host-by-email register 0&#125;# 发送服务通知使用的是notify-service-by-email命令 发送主机同时使用的是notify-host-by-email命令 于是查找commands.cfg文件 如果不成功，一定要注意到此文件中发送通知的命令默认使用sendmail命令，若主机没有就不可能发送成功。需要替换为mail或mailx。还要注意这两个命令的目录，是/bin还是/sbindefine command &#123; command_name notify-host-by-email command_line /usr/bin/printf &quot;%b&quot; &quot;***** Nagios *****\\n\\nNotification Type: $NOTIFICATIONTYPE$\\nHost: $HOSTNAME$\\nState: $HOSTSTATE$\\nAddress: $HOSTADDRESS$\\nInfo: $HOSTOUTPUT$\\n\\nDate/Time: $LONGDATETIME$\\n&quot; | /usr/bin/mail -s &quot;** $NOTIFICATIONTYPE$ Host Alert: $HOSTNAME$ is $HOSTSTATE$ **&quot; $CONTACTEMAIL$&#125;define command &#123; command_name notify-service-by-email command_line /usr/bin/printf &quot;%b&quot; &quot;***** Nagios *****\\n\\nNotification Type: $NOTIFICATIONTYPE$\\n\\nService: $SERVICEDESC$\\nHost: $HOSTALIAS$\\nAddress: $HOSTADDRESS$\\nState: $SERVICESTATE$\\n\\nDate/Time: $LONGDATETIME$\\n\\nAdditional Info:\\n\\n$SERVICEOUTPUT$\\n&quot; | /usr/bin/mail -s &quot;** $NOTIFICATIONTYPE$ Service Alert: $HOSTALIAS$/$SERVICEDESC$ is $SERVICESTATE$ **&quot; $CONTACTEMAIL$&#125; 最后通过contacts.cfg设置联系人，在 email 中填写自己的邮箱 define contact &#123; contact_name nagiosadmin use generic-contact alias Nagios Admin email XXXX@XX.com&#125; 当服务出现重启或故障时，系统会自动发送邮件。 参考资料 Nagios 配置安装详解 &gt; 使用 Nagios 搭建监控服务器 &gt; Nagios 官方文档 &gt; Pnp 官方配置文档 &gt; Nagios 监控系统架设全攻略 &gt; CentOS7 安装 nagios 并配置出图详解 &gt; 2017 年 11 月最新 Nagios4.3.4 部署高性能 Linux 服务器构建实战：运维监控、性能调优与集群应用","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"运维","slug":"运维","permalink":"https://coconutmilktaro.top/tags/%E8%BF%90%E7%BB%B4/"},{"name":"监控","slug":"监控","permalink":"https://coconutmilktaro.top/tags/%E7%9B%91%E6%8E%A7/"},{"name":"Nagios","slug":"Nagios","permalink":"https://coconutmilktaro.top/tags/Nagios/"}]},{"title":"Ansible基础学习笔记","slug":"Ansible学习笔记","date":"2018-05-27T16:01:18.000Z","updated":"2022-05-30T02:51:53.770Z","comments":true,"path":"2018/Ansible学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Ansible%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"Ansible 是一个部署一群远程主机的工具，使用 SSH 实现管理节点和远程节点间的通信，实现批量自动化操作。","text":"Ansible 是一个部署一群远程主机的工具，使用 SSH 实现管理节点和远程节点间的通信，实现批量自动化操作。 Ansible 结构 Ansible 安装 Inventory Playbook 命令解析 变量引用 register 命令行传参 notify 与 handler 逻辑控制 条件判断 when changed_when、failed_when ignore_errors 迭代（循环） Block 块 任务间流程控制 交互式提示 模板 tags 标签 includes 和 roles includes roles ansible-galaxy 常用技巧 Ansible 插件类型 Ansible 变量 Lookup Ansible 加密 Jinja2 过滤器 Jinja 语法 过滤器 Ansible 实战 为新系统添加 SSHkey 部署 LAMP+Varnish+Memcached Ansible-Tower Ansible 常见模块 cron user group copy template file service command shell script yum dnf setup synchronize mount get_url lineinfile Ansible 结构Ansible 具有以下核心组件： ansible core：ansible 核心程序 Host Inventory：主机信息文件 Playbooks：剧本，用于简便管理主机 Core Modules：核心模块，Ansible 通过模块进行管理 Custom Modules：自定义模块，补充核心模块的功能 Connection Plugins：连接插件，用于 Ansible 和主机的通信 Plugins：其他各种插件，提供连接或功能接口 Ansible 特性： 基于 Python 实现，有三个关键模块：Paramiko（ssh 连接插件）、PyYAML（YAML 语言）、jinja2（定义模板，即 Playbook） 部署简单，轻量级，无需在客户端安装 agent，去中心化 默认使用 SSH。1.基于密钥 2.在 inventory 文件指定密码 支持自定义模块，支持各种编程语言 主从模式 master 和 slave 使用 playbook 进行主机管理 幂等性：一种操作重复多次结果相同，只需运行一次 playbook 就可将需要配置的机器都置为期望状态，同一台机器多次执行一个 playbook 是安全的 Ansible 是模块化的，通过调用模块来实现管理 支持多层部署，可通过 VM 和容器为多层应用程序的部署配置提供支持 为架构的多个层次带来一致性，借助 Ansible 可通过编程操作计算架构中从基础设施到应用程序的每一层 Ansible 支持异构 IT 环境，支持 Windows 和 Linux 及多个硬件平台和云平台 实验系统 CentOS-7主节点服务器：192.168.163.102从节点服务器：192.168.163.103 Ansible 安装首先安装 epel-release，能够获得更多的 Ansible 包资源。 yum install epel-release然后安装 Ansible yum install ansible Ansible 有以下配置文件： /etc/ansible/ansible.cfg 主配置文件 /etc/ansible/hosts Inventory 配置文件 Ansible 配置以 ini 格式存储数据，Ansible 几乎所有配置都可通过 Playbook 或环境变量重新赋值。当运行 Ansible 命令时，会按照以下顺序查找并读取配置文件。 ANSIBLE_CONFIG：环境变量指定的路径 ./ansible.cfg：当前目录的 ansible.cfg 配置文件 ~/ansible.cfg：家目录的 ansible.cfg 配置文件 /etc/ansible/ansible.cfg：ansible 主配置文件 Ansible 主配置文件中的几个重要参数 inventory = /root/ansible/hosts # inventory文件的位置library = /usr/share/my_modules/ # ansible模块位置forks = 5 # 默认情况下Ansible最多能有多少个进程同时工作，默认5个进程并行处理。 # 可以根据控制端性能和被管理节点的数量来确定sudo_user = root # 默认执行命令的用户remote_port = 22 # 指定连接被管理节点的管理端口，默认是22host_key_checking = False # 是否检查SSH主机的密钥timeout = 20 # SSH连接的超时间隔，单位：秒log_path = /var/log/ansible.log # Ansible默认不记录日志# 若开启了日志，则要通过该参数设置日志文件路径# 模块将会调用被管节点的rsyslog来记录，执行Ansible的用户需要有写入日志的权限remote_tmp = ~/.ansible/tmp # 远程主机的临时文件存放位置local_tmp = ~/.ansible/tmp # 本机的临时文件存放位置 Ansible 提供文档命令可查看指定的用法说明ansible-doc命令用于查看 Ansible 帮助文档 -h 查看帮助-l 列出所有Ansible模块-s &lt;module&gt; 查找指定模块的用法 公钥认证Ansible 默认开启公钥认证，Ansible 主节点应该与所有要管理的节点进行 ssh 验证。主要使用以下命令： ssh-keygen 创建密钥对ssh-copy-id -i ~/.ssh/id_rsa.pub root@&lt;节点IP地址&gt; 然后在/etc/ansible/hosts添加该节点的 IP 地址 如果有个主机重新安装并在/home/.ssh/known_hosts文件中中有了不同的 key，就会一直提示错误。若节点主机未进行公钥认证，即没有在该文件中初始化，则每次使用 ansible 命令时都会要求确认 key 信息。 若要禁用 ansible 确认密钥的行为，可在主配置文件中参数host_key_checking = False设置，也可以通过环境变量ANSIBLE_HOST_KEY_CHECKING=False设置。 ansible 主命令 ansible &lt;host-pattern&gt; [options] # host-pattern可填inventory的组名，ip地址，all（所有主机） # 一些简单常用参数 -f 设置一次处理的主机个数，即并行执行 -m 设置使用的模块 -a 模块的参数 -i 指定inventory文件 InventoryAnsible 可同时操作属于一个组的多台主机,组和主机之间的关系通过 inventory 文件配置，默认的文件路径为/etc/ansible/hosts。inventory 文件遵循 INI 文件风格，方括号[]中是组名,用于对系统进行分类,便于对不同系统进行个别的管理。一个系统可以属于不同的组，属于两个组的变量都可以为这台主机所用。组名可自定义。 # 可以直接写要管理的主机IP地址或域名192.168.163.103#也可设置管理组[test]192.168.163.103system[1:5].example.com# 数字简写模式，表示system1--sysetm5# 也支持字母简写，system[a:f].example.com# 可对每个服务器设置连接类型和连接用户名localhost ansible_connection=localsystem3.example.com ansible_connection=ssh ansible_ssh_user=ansible# 可定义变量，可使用变量定义的配置主机变量host1 http_port=80 maxRequestsPerChild=808# 也可定义组变量[test:vars]ntp_server=ntp.example.com# 还可组嵌套，即在其他组中引用一个组# 这些变量可以给ansible-playbook使用,但不能给ansible使用。[team1]host1[team2]host2[test:hosts]team1team2 当 Inventory 中存在有效主机时，ansible 就默认隐式地可以使用localhost作为本机，但 inventory 中没有任何主机时是不允许使用它的，且all或*所代表的所有主机也不会包含 localhost。 一些常见的 Inventory 参数： ansible_ssh_host # ansible使用ssh要连接的主机ansible_ssh_port # ssh的端口。默认为22ansible_ssh_user # ssh登录的用户名。默认为rootansible_ssh_pass # ssh登录远程用户时的认证密码 # 不安全，最好使用 --ask-passansible_sudo_pass # sudo命令输入的root密码（当使用非root用户操作时） # 不安全，最好使用 --ask-sudo-passansible_connection # 连接远程主机使用的模式，默认为smart智能模式 # smart：若本地ssh支持持久连接时采用ssh连接，否则采用python的paramiko ssh连接 # paramiko：python的ssh连接库ansible_ssh_private_key_file # ssh登录远程用户时的认证私钥文件ansible_ssh_common_args # 指定ssh、scp等命令的参数ansible_shell_type # 指定远程主机执行命令时的shell解析器，默认为shansible_python_interpreter # 远程主机上的python解释器路径。默认为/usr/bin/python Inventory 配置文件可以有多个，且可以通过 Dynamic Inventory 来动态生成只需在 ansible 的主配置文件中将inventory参数设置为对应的文件或目录即可，如果是目录，那么此目录下的所有文件都是 inventory 文件。 可创建多个独立文件用于保存变量，然后在主文件中引用注：这些独立文件的格式为 YAML在独立文件/etc/ansible/group_vars/servers中添加 ---ntp_server: ntp.example.comdatabase_server: system2.example.com 然后在 inventory 文件中指定该文件/etc/ansible/group_vars/servers可以为一个主机，或一个组，创建一个目录，目录名就是主机名或组名。目录中的可以创建多个文件，文件中的变量都会被读取为主机或组的变量。 Playbook一个简单的配置管理和多主机部署系统。Playbook 是由一个或多个“Plays”组成的列表。将事先归为一组的主机装扮为通过 Ansible 的任务 Task 定义好的角色。任务也就是调用 Ansible 的模块将多个“play”组织到一个 playbook 中。playbook 的模板使用 Python 的 jinja2 模块处理。 Playbook 的组成： Inventory Modules Ad Hoc Commands Playbooks，包含以下部分 Tasks：任务，即调用模块完成的某操作。这是Playbook的核心，定义顺序执行的Action，每个Action调用一个Ansible模块Variables：变量Template：模板Handlers：处理器，由某事件触发执行的操作Roles：角色 playbook 基本组件play 的主体部分是 task list，task list 中各个任务按次序逐个在 hosts 指定的主机上运行，即在所有主机上完成第一个任务后再按顺序完成第二个，若中途某个主机出现错误，则所有执行的任务都可能回滚。 建议每个任务都定义一个 name 标签，且每个 task 执行一个模块 - hosts: test # 指定主机组，也可指定单个主机 remote_user: root # 指定远程主机上执行任务的用户（也可用于各个task中） sudo: yes # sudo执行命令，也可在task中添加 sudo_user: # sudo身份 tasks: # 任务列表 - name: install latest apache yum: name=httpd state=latest - name: run apache service: name=httpd state=started # 运行service模块，后面跟上参数选项 命令解析ansible-playbook对 yaml 文件进行执行 ansible-playbook [选项] yml文件 -f # 指定并行进程数，默认为5 -i 或 --inventory # 指定Inventory文件 -e 或 --extra-vars= # 设置额外的环境变量 --flush-cache # 清空收集到的facts缓存 --list-tasks # 列出所有将被执行的tasks --list-tags # 列出所有可获得的tags --step # 每执行一步都进行交互式确认 --syntax-check # 检查playbook语法 --list-hosts # 列出执行该playbook会影响的主机 -v 或 --verbose # 查看详细输出 ansible-pull拉取指定主机的配置 变量引用- hosts: test vars: service: httpd package: httpd # 或直接在本地创建变量文件，然后在playbook中通过vars_files调用 vars_files: - XXX/XXX.yml tasks: - name: install latest apache # 若要通过上面定义的变量引用，则需要两对大括号调用 yum: name=&#123;&#123; package &#125;&#125; state=latest - name: run apache service: name=&#123;&#123; service &#125;&#125; state=started # 若定义的是一个对象，可直接用中括号或点调用子属性---- hosts: group1 remote_user: root vars: foo: field1: one field2: two tasks: - name: echo foo.field1 # debug: msg=&quot;echo &#123;&#123;foo.field1&#125;&#125;&quot; debug: msg=&quot;echo &#123;&#123;foo[&#x27;field1&#x27;]&#125;&#125;&quot; - name: echo foo.field2 debug: msg=&quot;echo &#123;&#123;foo.field2&#125;&#125;&quot;结果：TASK [echo foo.field1] *********************************************************ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;echo one&quot;&#125;TASK [echo foo.field2] *********************************************************ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;echo two&quot;&#125; # 若定义的是个列表，也通过序号直接调用---- hosts: test vars: foo_list: - python - java - go tasks: - name: print foo_list debug: msg=&quot;&#123;&#123; foo_list &#125;&#125;&quot; - name: print foo_list[1] debug: msg=&quot;&#123;&#123; foo_list[1] &#125;&#125;&quot;结果：TASK [print foo_list] *****************************************************************************ok: [192.168.60.129] =&gt; &#123; &quot;msg&quot;: [ &quot;python&quot;, &quot;java&quot;, &quot;go&quot; ]&#125;TASK [print foo_list[0]] **************************************************************************ok: [192.168.60.129] =&gt; &#123; &quot;msg&quot;: &quot;python&quot;&#125; registerregister 为注册变量，即：将任务执行的结果当做一个变量的值，待后面的任务使用。 ---- hosts: group1 remote_user: root tasks: - shell: ls register: result - debug: msg=&quot;&#123;&#123; result.stdout &#125;&#125;&quot;# 就能输出在对端主机ls得到的结果结果：TASK [debug] *******************************************************************ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;anaconda-ks.cfg&quot;&#125; 命令行传参- hosts: &#x27;&#123;&#123;group&#125;&#125;&#x27; # 一定要加引号（无论是单引号还是双引号），否则会产生YAML陷阱，报错 remote_user: &#x27;&#123;&#123;user&#125;&#125;&#x27; ..... 在执行 ansible-playbook 时添加参数--extra-vars &quot;group=group1 user=root&quot; notify 与 handler当远端发生改动时，playbooks 本身可以识别这种改动,并且有一个基本的事件系统,可以响应这种改动。notify 会在 playbook 的每一个 task 结束时触发,即使有多个不同的 task 通知发生了改动（changed），notify 只会被触发一次。Handlers 也是 task 的列表，若 notify 有定义，则 handlers 一定要有对应的处理方法。handlers 主要用在重启服务，或系统重启。 一个 handler 最多只执行一次，并且在所有 task 都执行完后再执行，即 handler 是按照定义的顺序执行的，并不是按照 task 的调用顺序执行的。如果有多个任务调用同一个 handler，则也只执行一次。 - hosts: test tasks: - name: install apache yum: name=httpd state=latest notify: yum error # 关注可能发生的错误（不一定是错误），类似抛出异常 # 若notify有多个，可通过列表定义 - yum error - httpd error # 定义了多个，则handler也要有对应处理 handlers: # 当关注的资源发生变化时采取的措施 - name: yum error # 当有notify抛出，也要有对应的解决方案，name要与对应的notify的名字一致。 service: name=httpd state=restarted 若要再一个 playbook 的中间执行 handlers，需要使用 meta 模块实现。 - meta: XXXXX 一般情况若 play 在执行到 handlers 前就失败了，则 handlers 不会执行，但可以通过 mega 模块的--force-handlers选项强制执行 handlers。 逻辑控制三种逻辑控制语句： when：条件判断，类似 if loop：（迭代）循环，类似 while block：将几个任务组成一个代码块，以便针对一组操作的异常进行处理 条件判断当需要根据变量等信息判断是否需要执行某 task 时，则需要条件判断 whentasks: - name: echo hello command: echo hello when: ansible_fqdn == &#x27;system3.example.com&#x27;# 在task后添加when子句即可进行条件测试，when语句支持jinja2语法# when语句中还能使用Jinja2的很多&#x27;filter&#x27;执行结果：TASK [echo hello] **************************************************************skipping: [192.168.163.104]changed: [192.168.163.103]PLAY RECAP *********************************************************************192.168.163.103 : ok=2 changed=1 unreachable=0 failed=0192.168.163.104 : ok=1 changed=0 unreachable=0 failed=0# 经过判断system4不满足when条件，所以skipping跳过，而system3满足，所以changed 可使用 and、or、not 进行逻辑连接或判断，==、!=、&gt;、&lt;、&gt;=、&lt;=进行算数比较 可使用 is exists 或 is not exists 判断指定的文件是否存在，且可通过 not 取反即 not XXX is exists 等于 XXX is not exists 可使用 defined、undefined、none 判断变量是否已定义，以及变量是否为空 可使用 success 或 succeeded、failure 或 failed、change 或 changed、skip 或 skipped 分别判断任务返回状态是否为成功、任务返回状态是否为失败、任务返回状态是否为改变、任务返回状态是否为跳过 可使用 file、directory、link、mount 判断路径是否为一个文件、目录、链接、挂载点 可使用 lower、upper 判断字符串是否为纯小写、纯大写 可使用 even、odd 判断数值是否为偶数、奇数，可用 divisibleby(num)判断是否可以整除数值 num 可用 version(version 值,’算数比较符’)比较版本与指定值的大小 可用 string、number 分别判断值是否为字符串或数字 可用 subset、superset（版本 2.5 及以上）|issubset、issuperset（版本 2.5 以下）判断一个 list 是否是另一个 list 的子集或父集 changed_when、failed_when对命令结果进行判断。 可以使用changed_when对结果返回信息进行重写，而不是单纯的changed。 示例： PHP composer安装依赖项若返回信息包含&#x27;Nothing to install update&#x27;，说明并没有changed，只有在显示其他安装成功信息时才changed---- hosts: test gather_facts: no tasks: - command: composer global require phpinit/phpinit --prefer-dist register: composer_result changed_when: &quot;&#x27;Nothing to install or update&#x27; not in composer_result.stdout&quot; 可使用failed_when对错误进行判断。failed_when能匹配程序写入 stderr 的报错信息 示例： 判断jenkins是否运行失败---- hosts: test gather_facts: no tasks: - shell: java -jar ~/jenkins-cli.jar -s http://localhost:8000 create-job &#x27;myjob&#x27; &lt; /usr/local/myjob.xml register: import failed_when: &quot;import.stderr and &#x27;already exists&#x27; not in import.stderr&quot; ignore_errors某些任务在运行中会报出错误，但是不会影响运行，而此时报错却会导致 playbook 执行中断。需要在会出现这类报错的任务中添加ignore_errors来屏蔽所有错误消息，但是屏蔽错误消息也会影响排错。 迭代（循环）重复同类的 task 时使用。item 定义迭代，with_items 定义循环列表。with_items 中的列表值可以使字典，若是字典，引用时要使用 item.键名 列表形式- apache- php字典形式- &#123;name: apache, conf: /etc/httpd.conf&#125;- &#123;name: php, conf: /etc/php.ini&#125;试验：- name: create user user: name=&#123;&#123;item&#125;&#125; state=present with_items: - zhangsan - lisi就相当于 user: name=zhangsan state=present user: name=lisi state=present或者在vars中定义列表，然后使用with_items调用vars: user_list=[&#x27;zhangsan&#x27;, &#x27;lisi&#x27;]tasks: - user: name=&#123;&#123;item&#125;&#125; state=present with_items: &quot;&#123;&#123; user_list &#125;&#125;&quot;嵌套循环tasks: - debug: msg=&quot;&#123;&#123; item.0 &#125;&#125; &#123;&#123; item.1 &#125;&#125;&quot;#或 -debug: msg=&quot;layer1: &#123;&#123;item[0]&#125;&#125; layer2: &#123;&#123;item[2]&#125;&#125;&quot; with_nested: # 使用with_nested进行嵌套循环 - [&#x27;1&#x27;, &#x27;2&#x27;] # 这个循环用item.0表示 - [&#x27;4&#x27;, &#x27;5&#x27;] # 这个循环用item.1表示结果：TASK [debug] *******************************************************************ok: [172.16.246.133] =&gt; (item=[u&#x27;1&#x27;, u&#x27;4&#x27;]) =&gt; &#123;... &quot;msg&quot;: &quot;1 4&quot;&#125;ok: [172.16.246.133] =&gt; (item=[u&#x27;1&#x27;, u&#x27;5&#x27;]) =&gt; &#123;... &quot;msg&quot;: &quot;1 5&quot;&#125;ok: [172.16.246.133] =&gt; (item=[u&#x27;2&#x27;, u&#x27;4&#x27;]) =&gt; &#123;... &quot;msg&quot;: &quot;2 4&quot;&#125;ok: [172.16.246.133] =&gt; (item=[u&#x27;2&#x27;, u&#x27;5&#x27;]) =&gt; &#123;... &quot;msg&quot;: &quot;2 5&quot;&#125;文件列表循环在当前目录下创建demo目录，并在其中创建httpd_1.conf和httpd_2.conf，编写Playbook tasks: - debug: msg=&quot;&#123;&#123; item &#125;&#125;&quot; with_fileglob: # 使用with_fileglob进行文件列表循环 - ./demo/* # 要循环的文件路径结果：TASK [debug] *******************************************************************ok: [172.16.246.133] =&gt; (item=/root/./demo/httpd_1.conf) =&gt; &#123; &quot;item&quot;: &quot;/root/./demo/httpd_1.conf&quot;, &quot;msg&quot;: &quot;/root/./demo/httpd_1.conf&quot;&#125;ok: [172.16.246.133] =&gt; (item=/root/./demo/httpd_2.conf) =&gt; &#123; &quot;item&quot;: &quot;/root/./demo/httpd_2.conf&quot;, &quot;msg&quot;: &quot;/root/./demo/httpd_2.conf&quot;&#125; Block 块多个Action组成block块，可进行一个块的执行 tasks: - debug: msg: &quot;task1 not in block&quot; - block: - debug: msg: &quot;task2 in block&quot; - debug: msg: &quot;task3 in block&quot; when: 2 &gt; 1 # block块中的when是用于判断block块是否执行的条件 但 Block 块更常见的用法是“错误处理”。当某任务出错时，能够执行指定的其他任务。作用与when XXX is failed一致。 tasks: - block: - shell: &quot;ls ./aaa&quot; # 该目录不存在，会出错 rescue: # 一旦出错就会调用rescue任务，类似except，处理异常 - debug: msg: &quot;caught an error&quot; always: # 总是会执行的语句，类似finally - debug: msg: &quot;this always executes&quot;结果：TASK [command]fatal: ......TASK [debug]ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;caught an error&quot;&#125;TASK [debug]ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;this always executes&quot;&#125; 任务间流程控制默认情况下，任务是在指定的机器上执行的，但有时一些任务需要在特定的机器上执行（例如向某台服务器发送通知），所以需要 ansible 的任务委托功能。delegate_to可指定某项任务在特定服务器上运行，其他任务照旧。 示例： 操作对象全体为webservers，但添加监控对象需要在监控服务的主机上运行---- hosts: webservers gather_facts: no tasks: - command: monitor-server webservers &#123;&#123; inventory_hostname &#125;&#125; delegate_to: &quot;&#123;&#123; monitoring_master &#125;&#125;&quot; 若需要一个任务在 ansible 服务器本机运行，有两种方法 将该任务委托给 127.0.0.1 使用模块local_actiontasks: - local_action: xxxx 某些情况下，需要等待主机上的某个条件达成或某个状态的恢复（如等待主机上的服务重启或端口打开），此时需要暂停 playbook 的执行，知道该主机状态达到要求。 示例： - name: 等待webserver启动 local_action: module: wait_for host: webserver-1 port: 80 delay: 10 timeout: 300 state: started # 会每10s检查一次是否webserver-1的80端口开启，若超过300s仍未开启则返回错误 wait_for 模块常用于： 使用 host、port、timeout 来判断一段时间内主机的端口是否可用 使用 path 和 timeout 判断某个路径下文件是否存在 使用 host、port、stat 判断活动连接数是否被耗尽 使用 delay 指定 timeout 时间内检查的间隔 交互式提示在执行过程中若需要用户输入数据，则可用到vars_prompt实现交互。 示例： ---- hosts: all vars_prompt: - name: share_user prompt: &quot;Input your network username?&quot; - name: share_pass prompt: &quot;Inpur your network password?&quot; private: yes vars_prompt 常用选项： default：默认值 private：若为 yes 则输入不可见 confirm：要求输入两次，常用于密码 尽量避免使用交互式，会降低运维的自动化。 模板通过配置模板，可将配置文件中的参数按照 inventory 文件中变量以及 ansible facts 中变量动态赋值，使得每个指定的主机的配置都是定制的。首先要创建一个 templates 目录。mkdir /etc/ansible/templates将配置文件放入该目录，并最好改名为xxx.conf.j2 以httpd为例，修改配置文件/etc/ansible/templates/httpd.conf.j2Listen &#123;&#123; http_port &#125;&#125; # 使用inventory定义变量User &#123;&#123; username &#125;&#125; # 同上Group &#123;&#123; groupname &#125;&#125; # 同上ServerName &#123;&#123; ansible_fqdn &#125;&#125; # 使用facts变量然后修改/etc/ansible/hosts文件[test]192.168.163.103 http_port=8081 username=system3 groupname=system3192.168.163.104 http_port=8082 username=system4 groupname=system4然后在playbook中将本地的配置文件复制到远端，以下是完整试验- hosts: test vars: service: httpd tasks: - name: alter config template: src=/etc/ansible/templates/httpd.conf.j2 dest=/etc/httpd/conf/httpd.conf notify: - restart httpd - name: start httpd service: name=&#123;&#123; service &#125;&#125; enabled=true state=started handlers: - name: restart httpd service: name=&#123;&#123; service &#125;&#125; state=restarted执行ansible-playbook test.yml。之后查看103和104主机的httpd配置文件system3和system4上，httpd配置文件都更改成功。以下为system3上的配置Listen 8081Include conf.modules.d/*.confUser system3Group system3ServerAdmin root@localhostServerName system3.example.com tags 标签可以为 playbook 中的每个任务都打上标签，标签的主要作用是可以在 ansible-playbook 中设置只执行被打上 tag 的任务或忽略被打上 tag 的任务。 tasks:- name: install apache yum: name=httpd state=present tags: apache- name: install mysql yum: name=mysql-server state=present tags: mysql当执行playbook时，可通过--tags= 运行打上指定tag的taskansible-playbook test.yml --tags=&quot;apache&quot; 则只运行安装打上apache标签的taskansible-playbook test.yml --skip-tags=&quot;apache&quot;则会跳过执行apache标签的task tags: always打上always标签的task总会被执行，不管是否指定了--tags=&quot;XXX&quot;--tags tagged # 会执行所有打上tag的task，不管打上的是什么标签--tags untagged # 会执行所有没有打标签的task--tags all # 执行所有任务，无论是否打标签 includes 和 roles如果把所有 play 都写在一个 playbook 中，会导致文件不易阅读。ansible 可以将多个不同任务分别写在不同的 playbook 中，然后使用 include 将其包含进去，实现 Playbook 的重用。roles 也是一种整合 playbook 的方式。include 的维护成本较高，重用能力有限，而 role 更加灵活，且可以重用一组文件。 includes使用 include 语句引用 task 文件的方法，可允许你将一个配置策略分解到更小的文件中，将 tasks 从其他文件拉取过来（handlers 也是 tasks）。即 include 可以导入两种文件：导入 task、导入 playbook。 导入task：创建一个单独的yml配置文件，a.yml--- - name: echo hello command: echo &#x27;hello&#x27; - name: echo value command: echo &#123;&#123;value&#125;&#125;在主playbook中便可通过include引用该文件- hosts: test tasks: - include: a.yml value=&#x27;hello&#x27; # 可以直接在文件名后传参数 # 也可以通过vars传参 tasks: - include: a.yml vars: value: hello导入playbook：并不在task中通过include调用yml了，而是直接在最外层导入playbook- hosts: test.....- include: test1.yml- include: apache.yml http_port=8000 # 传参方式与上面一样在include中使用tags- include: test1.yml tags: [aaa, bbb] 可在 includes 下添加 when 语句，实现动态的 includes，仅在满足条件的时候导入 tasks。 - name: 检查文件是否存在 stat: path=extra/extra-tasks.yml # 检查文件是否存在 register: extra_file # 状态返回值- include: tasks/extra-tasks.yml when: extra_file.stat.exists # 当上面的文件存在时再导入task文件 roles角色，封装 playbook，实现复用性，能够根据层次型结构自动加载变量文件、tasks 以及 handlers 等。roles 就是通过分别将变量、文件、任务、模板以及处理器放置于单独的目录中，然后通过 include 调用的一种机制。roles 一般用于基于主机构建服务的场景中，也可以使用于构建守护进程的场景中。 相较于 includes，roles 更适用于大项目 playbook 的编排。 创建 role 的步骤： 在 playbooks 目录中创建 roles 目录 在 roles 目录中创建角色名的目录 在每个角色命令的目录中创建 files、handlers、meta、tasks、templates、vars 目录。若用不到的目录也可不创 在 playbook 中调用各角色 如果定义了环境变量ANSIBLE_ROLES_PATH，则也会查找该目录下的 role role 有默认的存放目录/etc/ansible/roles，若既没有定义环境变量，也没有在 playbook 中定义变量roles_path，则会在该默认目录中查找 role。如果既定义了环境变量，又定义了roles_path，则后者失效。 roles 中各目录： tasks目录：至少包含一个 main.yml，其定义了此角色的任务列表，此文件可用 include 包含其他 task 目录。main.yml是所有任务的入口 files目录：存放有 copy 或 script 等模块调用的文件 templates目录：template 模块会自动在此文件中寻找 jinja2 模板 handlers目录：此目录中应包含一个 main.yml，定义此角色用到的 handler yml文件：用于定义此角色用到的个 handler， vars目录：应包含一个 main.yml，定义此角色用到的变量 meta目录：应包含一个 main.yml，定义此角色的特殊设定和依赖关系 defaults目录：应包含一个 main.yml，为当前角色设定默认变量时使用此目录 可使用命令ansible-galaxy init role-name在当前目录中自动生成指定的 role 目录 案例目录结构 roles├── test1│ ├── files│ ├── handlers│ ├── meta│ ├── tasks│ ├── templates│ └── vars└── test2 ├── files ├── handlers ├── meta ├── tasks ├── templates └── vars 将要编写的 task 文件存放在 tasks 目录中，编写 main.yml。将 httpd 的配置文件复制到 files 目录中。 - name: install httpd yum: name=httpd state=present- name: install config copy: src=httpd.conf dest=/etc/httpd/conf/httpd.conf # 这里copy的源可直接写文件名，会自动定位到files目录中 tags: - conf notify: - restart httpd- name: start httpd service: name=httpd state=started 然后在 handlers 中添加 handler 文件，在目录中创建 main.yml - name: restart httpd service: name=httpd state=restarted 在于 roles 平级的目录中创建 site.yml 文件（名字可自定义），就是主配置文件。roles 后面也可跟上参数，也可跟上条件判断。 - hosts: system1 remote_port: root roles: - test1- hosts: system3 roles: - test2- hosts: system4 roles: - test1 - test2 带参数的 role a.yml和roles的目录结构a.ymlroles/└── myrole └── tasks └── main.yml在main.yml中指定task--- - debug: msg=&quot;&#123;&#123; param &#125;&#125;&quot;在a.yml中的配置如下：---- hosts: group1 remote_user: root roles: - role: myrole param: &quot; task first&quot; - role: myrole param: &quot; task second&quot;执行结果：TASK [myrole : debug]ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot; task first&quot;&#125;TASK [myrole : debug]ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot; task second&quot;&#125;会循环遍历a.yml中设置指定参数执行 role 的默认参数 defaults，在 myrole 中创建 defaults 目录，并创建 main.yml roles/└── myrole ├── defaults │ └── main.yml └── tasks └── main.yml在defaults中的main.yml中只要配置---param: &quot;default param&quot;将a.yml中的myrole下的参数配置删除roles: - myrole再执行，就会调用defaults中的指定参数TASK [myrole : debug]ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;default param&quot;&#125;但执行速度会变慢 role 与 when 的结合：当满足条件时才采用指定值 roles: - role: myrole param: &quot;myrole param&quot; - role: myrole when: 2&gt;1结果：TASK [myrole : debug]ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;myrole param&quot;&#125;TASK [myrole : debug]ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;default param&quot;&#125; 在 role 中使用 tags roles: - role: myrole tags: [&#x27;aaa&#x27;, &#x27;bbb&#x27;] role 和 tasks 的执行顺序 pre_tasks：是在最先执行的 task roles：roles 会在 tasks 前执行 tasks post_tasks：最后执行的 task ansible-galaxy init：创建空的 roles info：显示 roles 的版本号、作者等信息 install：安装 roles list：列出本地的 roles remove：移除指定 roles galaxy 上的 roles 命名规范遵循 username.rolename 若要一次安装多个 roles，可以通过设置 txt 文件或 yml 文件指定，类似 pip 的 requirement 文件。并且 ansible 支持从多个源下载，除了官方的 galaxy 网站，也支持 github、web 服务器、bitbucket 网站等。 # roles.txtuserA.role1,v1.0.0userB.role2# roles.yml# 从galaxy下载- src: user1.role1# 从github下载- src: https://github.com/user1/nginx# 从github下载到本地相对路径- src: https://github.com/user1/nginx path: lnmp/roles/# github上指定源码版本- src: https://github.com/user1/nginx version: master name: nginx_role# 从web服务器下载打包好的源码- src: https://files.example.com/ansible/master.tar.gz name: http_role 然后通过 install 的-r指定文件 ansible-galaxy install -r roles.yml 常用技巧 若要使 command 或 shell 的成功返回值不为 0，有以下两种方式 tasks: - name: run this command and ignore the result shell: /usr/bin/somecommand || /bin/true或tasks: - name: run this command and ignore the result shell: /usr/bin/somecommand ignore_errors: True Ansible 插件类型当前 ansible 已有十几种插件类型。几个重点常用的插件类型： Connection 类：用于与远端主机通信。默认提供 paramiko、native ssh、local 等方式。 Lookup 类：循环体功能。如 with_items、with_fileglob 等遍历功能插件 Vars 类：变量。如 通过 host_vars、group_vars 或 inventory 生成的 Filter 类：Jinja2 的 Filter，常见实现为 to_yaml、to_json Callback 类：捕捉响应事件，允许进行自定义响应。是最常用的插件类型 Ansible 变量Ansible 有三个组成部分： Global：作用域为全局。在以下方面定义： Ansible 配置文件中定义 环境变量 ansible 及 ansible-playbook 命令行传入的变量 Play：作用域为 Play（一个 Playbook 由多个 Play 组成）。在以下方面定义： Play 中 vars 关键词定义的变量 通过模块 include_vars 定义的变量 role 在文件 default/main.yml 和 vars/main.yml 中定义的变量 Host：作用域为某个主机。在以下方面定义： 定义在主机 Inventory 中的变量 主机的系统变量 注册变量 Ansible 所有变量的优先级（从高到低）： extra vars：通过命令传入的变量 # 指定字段ansible-playbook a.yml --extra-vars &quot;foo=bar&quot;# 引入单独的变量文件，如json、yamlansible-playbook a.yml --extra-vars &quot;@vars.json&quot;ansible-playbook a.yml --extra-vars &quot;@vars.yaml&quot; task vars：仅在该任务中使用的变量 tasks: - XXXX vars: XXX: XXX block vars：只在 Playbook 的任务中某个 block 定义的变量 tasks:.... - block: - XXX: XXXX vars: XXX: XXX role include vars：在tasks/main.yml中，通过 include 加载的变量 - name: xxx include_vars: &quot;XXX.yml&quot; role and include vars：role 的变量。在vars/main.yml中定义的变量 set_facts：这是一个模块，通过该模块加入一些 Facts 变量 - set_fact: XXX: XXX registered vars：注册变量 play vars_files：将变量单独放在一个文件中，通过关键字var_files从文件中加载的变量 var_files: - XXX.yml play vars_prompt：需要用户在执行 Playbook 时输入的变量 vars_prompt: - name: &quot;yourname&quot;tasks: - debug: msg=&quot;your name is &#123;&#123;yourname&#125;&#125;&quot;在执行Playbook时传参ansible-playbook a.yml -e &#x27;yourname=zhangsan&#x27; play vars：Playbook 中的vars关键字下定义的参数 host facts：Ansible 在执行 Playbook 时，收集到的远程主机的信息两种获取 facts 的方法。一种是 playbook 中开启gather_facts（默认开启），一种是自定义 facts，在远端主机的/etc/ansible/facts.d/创建一个.fact 文件，例如settings.fact。 [systeminfo]Manufacturer=VMware, Inc.ProductName=VMware Virtual PlatformSerialNumber=VMware-56 4d 07 14 97 13 ce 07-df 17 d7 3d 71 1b e1 a8UUID=14074d56-1397-07ce-df17-d73d711be1a8 $ ansible test -m setup -a &quot;filter=ansible_local&quot;192.168.60.129 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;ansible_local&quot;: &#123; &quot;settings&quot;: &#123; &quot;systeminfo&quot;: &#123; &quot;manufacturer&quot;: &quot;VMware, Inc.&quot;, &quot;productname&quot;: &quot;VMware Virtual Platform&quot;, &quot;serialnumber&quot;: &quot;VMware-56 4d 07 14 97 13 ce 07-df 17 d7 3d 71 1b e1 a8&quot;, &quot;uuid&quot;: &quot;14074d56-1397-07ce-df17-d73d711be1a8&quot; &#125; &#125; &#125;, &quot;discovered_interpreter_python&quot;: &quot;/usr/libexec/platform-python&quot; &#125;, &quot;changed&quot;: false&#125; playbook host_vars：Playbook 同级子目录host_vars中文件内定义的变量 playbook group_vars：Playbook 同级子目录group_vars中文件内定义的变量 inventory host_vars：可在两个地方定义。一是在 inventory 文件中直接定义，二是在 Inventory 文件的同级子目录host_vars中与 host 同名的文件中定义 在/etc/ansible/host_vars目录中创建host同名文件，如host1.example.com，以yaml语法在其中配置变量 inventory group_vars：可在两个地方定义。一是在 inventory 文件中直接定义，二是在 Inventory 文件的同级子目录group_vars中与 group 同名的文件中定义。操作同上 inventory vars：Inventory 文件中定义的变量 role defaults：role 的默认变量，在defaults/main.yml中定义 Lookuplookup 既能读取 Ansible 管理节点上文件系统的文件内容，还能读取数据库内容。 lookup 读取文件 vars: contents: &quot;&#123;&#123; lookup(&#x27;file&#x27;, &#x27;data/test.txt&#x27;) &#125;&#125;&quot;将data/test.txt中的内容赋给变量contents，file指定读取的对象类型是文件 lookup 生成随机密码，若文件不存在，会自动创建，并将生成的密码存入。若文件存在，则直接读取作为密码 vars: password: &quot;&#123;&#123; lookup(&#x27;password&#x27;, &#x27;/etc/password/zhangsan length=6&#x27;) &#125;&#125;&quot;tasks: - debug: msg=&quot;password &#123;&#123;password&#125;&#125;&quot;执行结果：TASK [debug]ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;password BWNcQ2 &quot;&#125; lookup 读取环境变量 tasks: - debug: msg=&quot;&#123;&#123; lookup(&#x27;env&#x27;, &#x27;HOME&#x27;) &#125;&#125;&quot;结果：ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;/root&quot;&#125; lookup 读取 Linux 命令执行结果 tasks: - debug: msg=&quot;&#123;&#123; lookup(&#x27;pipe&#x27;, &#x27;uname -r&#x27;) &#125;&#125;&quot;结果：ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;4.8.6-300.fc25.x86_64&quot;&#125; lookup 读取 template 变量替换后的文件 tasks: - debug: msg=&quot;&#123;&#123; lookup(&#x27;template&#x27;, &#x27;./httpd.conf.j2&#x27;) &#125;&#125;&quot; lookup 读取配置文件 demo.ini配置文件：[global]port = 873.....[rsync_test]comment = rsync testpath = /root/rsync_test.....tasks: - debug: msg=&quot;global-port &#123;&#123; lookup(&#x27;ini&#x27;, &#x27;port section=global file=./demo.ini&#x27;) &#125;&#125;&quot; - debug: msg=&quot;rsync_test-path &#123;&#123; lookup(&#x27;ini&#x27;, &#x27;path section=rsync_test file=./demo.ini&#x27;) &#125;&#125;&quot;# lookup的第二个参数分为几个部分：要查的字段 section=节的名称 file=文件名若是properties文件，则需要添加参数type=properties完整的几个参数：参数名 默认值 含义type ini 文件类型file ansible.ini 文件名section global 节re False 字段的正则表达式default &quot;&quot; 字段不存在时的返回值执行结果：ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;global-port 873&quot;&#125;ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;rsync_test-path /root/rsync_test&quot;&#125; lookup 读取 CSV 文件的指定单元 csv文件：name age sexzhangsan 22 malelisi 23 maletasks: - debug: msg=&quot;&#123;&#123;lookup(&#x27;csvfile&#x27;, &#x27;lisi file=./demo.csv delimiter=, col=0&#x27;)&#125;&#125;&quot;# 获取lisi的第0列，即名字lisiok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;lisi&quot;&#125;支持的参数：参数名 默认值 含义file ansible.csv 文件名col 1 列号（从0开始计数）delimiter TAB CSV文件的分隔符default &quot;&quot; 元素不存在时的返回值encoding utf-8 CSV文件的编码 lookup 读取 DNS 解析的值。可以向 DNS 服务器查询指定域的 DNS 记录，可查询任何 DNS 记录（包括正向和反向） tasks: - debug: msg=&quot;ipv4 address of baidu.com &#123;&#123; lookup(&#x27;dig&#x27;, &#x27;baidu.com&#x27;) &#125;&#125;&quot; - debug: msg=&quot;txt record of baidu.com &#123;&#123; lookup(&#x27;dig&#x27;, &#x27;baidu.com&#x27;, &#x27;qtype=TXT&#x27;) &#125;&#125;&quot; - debug: msg=&quot;txt record of baidu.com &#123;&#123; lookup(&#x27;dig&#x27;, &#x27;baidu.com./TXT&#x27;) &#125;&#125;&quot; - debug: msg=&quot;MX record of 163.com &#123;&#123; lookup(&#x27;dig&#x27;, &#x27;163.com./MX&#x27;, &#x27;wantlist=True&#x27;) &#125;&#125;&quot;需要安装dnspython模块，直接pip install 即可执行结果：ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;ipv4 address of &#x27;baidu.com&#x27; 220.181.57.216,123.125.115.110&quot;&#125;ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;txt record of &#x27;baidu.com&#x27; v=spf1 include:spf1.baidu.com include:spf2.baidu.com include:spf3.baidu.com a mx ptr -all,google-site-verification=GHb98-6msqyx_qqjGl5eRatD3QTHyVB6-xQ3gJB5UwM&quot;&#125;ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;txt record of &#x27;baidu.com&#x27; v=spf1 include:spf1.baidu.com include:spf2.baidu.com include:spf3.baidu.com a mx ptr -all,google-site-verification=GHb98-6msqyx_qqjGl5eRatD3QTHyVB6-xQ3gJB5UwM&quot;&#125;ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;MX record of &#x27;163.com&#x27; 50 163mx00.mxmail.netease.com.,10 163mx01.mxmail.netease.com.,10 163mx03.mxmail.netease.com.,10 163mx02.mxmail.netease.com.&quot;&#125;反向解析：- debug: msg=&quot;fqdn of &#x27;8.8.8.8&#x27; &#123;&#123; lookup(&#x27;dig&#x27;, &#x27;8.8.8.8/PTR&#x27;) &#125;&#125;&quot;结果：ok: [172.16.246.133] =&gt; &#123; &quot;msg&quot;: &quot;fqdn of &#x27;8.8.8.8&#x27; google-public-dns-a.google.com.&quot;&#125; Ansible 加密ansible 在执行任务时会接触到各种敏感数据，可能是管理员密码、ssh 私钥等。可以使用 ansible 自带的 vault 加密功能，将经过加密的密码和敏感数据同 playbook 存储在一起。 ansible vault 采用 AES-256 加密算法。 ansible-vault create 创建新的加密文件 encrypt 加密一个yaml文件 encrypt_string 加密一个字符串 decrypt 解密一个文件 rekey 修改加密文件的密码 view 查看加密文件 edit 编辑加密文件 加密一个 yaml 文件： $ ansible-vault encrypt play.ymlNew Vault password:Confirm New Vault password:Encryption successful打开该文件$ cat play.yml$ANSIBLE_VAULT;1.1;AES256373930326235663730343434626561356161373464376538303337303539313961653464626563316530393866663037343237303130363265666463336536350a303465613835653334316161616166626562383763323761383031326533376363303964373439643437306133383532373839633561363363373032306636310a65343830383239393262643662663135633961393266393662346432313830383561633435353833323766306637613165306530373139643839623838346265626133393866...... 加密之后若要查看修改文件，就只能通过ansible-vault edit和view进行。 ansible 还提供密码文件形式进行解密的认证方式，类似 ssh 密钥认证。ansible-vault 将密码文件放在~/.ansible中，该目录的权限为 600，该目录下创建文件vault_pass.txt，写入密码。就可以通过参数运行加密后的 playbook $ ansible-playbook play.yml --vault-password-file ~/.ansible/vault_pass.txt 可通过安装 python 的 cryptography 模块加速 vault 速度 pip install cryptography Jinja2 过滤器Jinja2 是 Python 的 web 开发中常用的模板语言，也被用于管理配置文件。Jinja2 是 Flask 作者仿 Django 模板开发的模板引擎。但 Jinja2 具有更好的性能，更加灵活，具有很好的可读性。 格式化数据 强制定义变量对于未定义变量，Ansible 默认行为是 fail，也可关闭。 未定义变量默认值Jinja2 提供一个有用 default 过滤器，设置默认变量值。比强制定义变量更好。 忽略未定义变量和参数可使用 default 过滤器忽略未定义的变量和模块参数 Jinja2 的三种语法： 控制结构 &#123;% %&#125; 变量取值 &#123;&#123; &#125;&#125; 注释 &#123;# #&#125; Jinja 语法Jinja2 控制结构： &#123;% if ... %&#125;&#123;% elif ... %&#125;&#123;% else %&#125;&#123;% endif %&#125; Jinja2 的 for 循环： &#123;% for .. in .. %&#125;&#123;% endfor %&#125; for 循环中的特殊变量： 变量 描述 loop.index 当前循环的次数（从 1 开始计数） loop.index0 当前循环的次数（从 0 开始计数） loop.revindex 到循环结束的次数（从 1 开始计数） loop.revindex0 到循环结束的计数（从 0 开始计数） loop.first 如果是第一次迭代，为 True loop.last 如果是最后一次迭代，为 True loop.length 序列中的项目数 loop.cycle 在一串序列间取值的辅助函数 Jinja2 的宏。类似函数，将行为抽象成可重复调用的代码块 &#123;% macro input(name, type=&#x27;text&#x27;, value=&#x27;&#x27;) %&#125; &lt;input type=&#x27;&#123;&#123; type &#125;&#125;&#x27; name=&#x27;&#123;&#123; name &#125;&#125;&#x27; value=&#x27;&#123;&#123; value &#125;&#125;&#x27;&gt;&#123;% endmacro %&#125; 宏的调用： &lt;p&gt;&#123;&#123; input(&#x27;username&#x27;, value=&#x27;user&#x27;) &#125;&#125;&lt;/p&gt;&lt;p&gt;&#123;&#123; input(&#x27;password&#x27;, &#x27;password&#x27;) &#125;&#125;&lt;/p&gt;相当于&lt;p&gt;&lt;input type=&#x27;text&#x27;, name=&#x27;username&#x27;, value=&#x27;user&#x27;&gt;&lt;/p&gt;&lt;p&gt;&lt;input type=&#x27;password&#x27;, name=&#x27;password&#x27;&gt;&lt;/p&gt; Jinja2 继承。若 Jinja2 仅用于配置文件，则基本用不到继承功能，而在网页开发中，继承相当强大，常用于配置模板文件，在 Django 和 Flask 中会被大量使用，减少重复代码的开发编写，使 html 文件更加简洁易读。 &#123;% block block块名 %&#125;&#123;% endblock block块名 %&#125;在endblock中block块名可以不加，但为了阅读性最好加上在html文件的最前面应该添加要继承的模板文件&#123;% extends &quot;xxx.html&quot; %&#125;继承模板中指定块的内容&#123;% block xxx %&#125;&#123;&#123; super() &#125;&#125;&#123;% endblock %&#125; 过滤器 quote：给字符串加上引号 &#123;&#123; str | quote &#125;&#125; default：为没有定义的变量提供默认值 &#123;&#123; variable | default(&#x27;xxxx&#x27;) &#125;&#125; omit：忽略变量的占位符。常与 dafault 合用，当定义了参数时则会调用该参数，而若没有该参数时，则不会传入任何值 &#123;&#123; variable | default(omit) &#125;&#125;例：- file: dest=&#123;&#123; item.path &#125;&#125; state=touch mode=&#123;&#123; item.mode|default(omit) &#125;&#125; with_items: - path: /tmp/demo1 # demo1没有设置mode，因此mode不会传入任何值。omit起到为有值的item项占位 - path: /tmp/demo2 # demo2的path和mode都有 mode: &quot;0664&quot; mandatory：强制变量必须定义，否则报错。Ansible 默认若变量没有定义，则使用未定义的变量会报错。也可以在 Ansible 配置文件中修改参数error_on_undefined_vars = False，即使遇到未定义变量，也不会报错。若要强制约束一个变量必须定义，则可以使用mandatory。 &#123;&#123; undefined_variable | mandatory &#125;&#125; bool：判断变量是否为布尔类型 &#123;&#123; variable | bool &#125;&#125; ternary：Playbook 的条件表达式。类似(A?B:C) &#123;&#123; 条件判断 | ternary(&quot;满足时采用的值&quot;, &quot;不满足时采用的值&quot;) &#125;&#125; basename、dirname、expanduser、realpath、relpath、splitext &#123;&#123; path | basename &#125;&#125; 获取路径中的文件名&#123;&#123; path | dirname &#125;&#125; 获取文件的目录&#123;&#123; path | expanduser &#125;&#125; 当前用户目录&#123;&#123; path | realpath &#125;&#125; 获取链接文件所指文件的真实路径&#123;&#123; path | relpath &#125;&#125; 获取相对于某一根目录的相对路径&#123;&#123; path | splitext &#125;&#125; 把文件名用点号分割成多个部分 vars: conf_path: &quot;/etc/httpd.conf&quot; yml_path: &quot;~/a.yml&quot; tasks: - debug: msg=&quot;&#123;&#123; conf_path | basename &#125;&#125;&quot; - debug: msg=&quot;&#123;&#123; conf_path | dirname &#125;&#125;&quot; - debug: msg=&quot;&#123;&#123; yml_path | expanduser &#125;&#125;&quot; - debug: msg=&quot;&#123;&#123; yml_path | realpath &#125;&#125;&quot; - debug: msg=&quot;&#123;&#123; yml_path | relpath(&#x27;/home&#x27;) &#125;&#125;&quot; - debug: msg=&quot;&#123;&#123; conf_path | splitext &#125;&#125;&quot;执行结果： &quot;msg&quot;: &quot;httpd.conf&quot; #/etc/httpd.conf的文件名 &quot;msg&quot;: &quot;/etc&quot; #/etc/httpd.conf文件所在目录名 &quot;msg&quot;: &quot;/root/a.yml&quot; #~/a.yml，用实际用户替代~ &quot;msg&quot;: &quot;/root/~/a.yml&quot; #仅对链接文件有效，指向真实文件的路径 &quot;msg&quot;: &quot;../root/~/a.yml&quot; #~/a.yml相对于指定路径的相对路径 &quot;msg&quot;: &quot;(u&#x27;/etc/httpd&#x27;, u&#x27;.conf&#x27;)&quot; #分隔文件与所在目录 若是 Windows 系统，Ansible 提供的路径过滤器： &#123;&#123; path | win_basename &#125;&#125; # 获取文件名&#123;&#123; path | win_dirname &#125;&#125; # 获取文件所在目录路径&#123;&#123; path | win_splitdrive &#125;&#125; # 将路径分隔成多个部分 b64encode、b64decode、to_uuid、hash &#123;&#123; string | b64encode &#125;&#125; # 将字符串转化为base64格式&#123;&#123; string | b64decode &#125;&#125; # 将字符串（base64格式）解码&#123;&#123; string | to_uuid &#125;&#125; # 将字符串转变为UUID&#123;&#123; string | hash(&#x27;sha1&#x27;) &#125;&#125; # 使用sha1求出字符串的哈希，还可用md5、blowfish求&#123;&#123; string | checksum &#125;&#125; # 使用checksum求哈希&#123;&#123; string | password_hash(&#x27;&#x27;) &#125;&#125; # 使用哈希算法求密码的hash 判断是否是合法 IP 地址。 &#123;&#123; ip_addr_str | ipaddr &#125;&#125; # 判断IP地址是否合法&#123;&#123; ip_addr_str | ipv4 &#125;&#125; # 返回ipv4地址&#123;&#123; ip_addr_str | ipv6 &#125;&#125; # 返回ipv6地址&#123;&#123; ip_addr_str | ipaddr(&#x27;address&#x27;) &#125;&#125; # 返回纯ip地址（不带掩码） to_datetime：字符串类型时间转换为时间戳 &#123;&#123; date_str | to_datetime &#125;&#125; Json 操作 vars: value: - key1: &quot;value1&quot; - key2: &quot;value2&quot; tasks: - name: outputfile /tmp/b.txt blockinfile: dest: /tmp/b.txt block: | &#123;&#123; value | to_json &#125;&#125; ------------------------ &#123;&#123; value | to_yaml &#125;&#125; ------------------------ &#123;&#123; value | to_nice_json &#125;&#125; ------------------------ &#123;&#123; value | to_nice_yaml &#125;&#125;执行结果：在指定的主机上查看/tmp/b.txt[&#123;&quot;key1&quot;: &quot;value1&quot;&#125;, &#123;&quot;key2&quot;: &quot;value2&quot;&#125;]------------------------- &#123;key1: value1&#125;- &#123;key2: value2&#125;------------------------[ &#123; &quot;key1&quot;: &quot;value1&quot; &#125;, &#123; &quot;key2&quot;: &quot;value2&quot; &#125;]------------------------- key1: value1- key2: value2 在 Json 对象中搜索符合条件的属性 vars: host_group: cluster1: - name: &quot;host1&quot; - name: &quot;host2&quot; tasks: - debug: var=item with_items: &quot;&#123;&#123; host_group | json_query(&#x27;cluster1[*].name&#x27;) &#125;&#125;&quot;执行结果：ok: [172.16.109.132] =&gt; (item=host1) =&gt; &#123; &quot;item&quot;: &quot;host1&quot;&#125;ok: [172.16.109.132] =&gt; (item=host2) =&gt; &#123; &quot;item&quot;: &quot;host2&quot;&#125; 测试变量，只返回 true 或 false variable | match(&quot;正则表达式&quot;) 完全匹配，从头匹配到最后表达式中字段即可，并不是字段后还会匹配variable | search(&quot;正则表达式&quot;) 部分匹配，只要匹配字符串中包含该匹配字段即可 vars: url: &quot;https://example.com/user/foo/resources/bar&quot; tasks: - debug: msg=&quot;match 1 &#123;&#123;url | match(&quot;https://example.com/.*/resources/.*&quot;)&#125;&#125; &quot; - debug: msg=&quot;match 2 &#123;&#123;url | match(&quot;/user/&quot;)&#125;&#125;&quot; - debug: msg=&quot;match 3 &#123;&#123;url | match(&quot;.*/user&quot;)&#125;&#125;&quot; - debug: msg=&quot;match 4 &#123;&#123;url | match(&quot;user/.*&quot;)&#125;&#125;&quot; - debug: msg=&quot;match 5 &#123;&#123;url | search(&quot;foo&quot;)&#125;&#125;&quot; 结果： &quot;msg&quot;: &quot;match 1 True &quot; &quot;msg&quot;: &quot;match 2 False&quot; &quot;msg&quot;: &quot;match 3 True&quot; &quot;msg&quot;: &quot;match 4 False&quot; &quot;msg&quot;: &quot;match 5 True&quot; 比较版本 version | version_compare(&quot;要比较的版本号&quot;, &quot;比较运算符&quot;) vars: version: &quot;18.06&quot; tasks: - debug: msg=&quot;does 12.04 &gt; version(18.06)? &#123;&#123;version | version_compare(&quot;12.04&quot;, &quot;&gt;&quot;)&#125;&#125;&quot; - debug: msg=&quot;does 19.04 &gt; version(18.06)? &#123;&#123;version | version_compare(&quot;19.04&quot;, &quot;&gt;&quot;)&#125;&#125;&quot;结果： &quot;msg&quot;: &quot;does 12.04 &gt; version(18.06)? True&quot; &quot;msg&quot;: &quot;does 19.04 &gt; version(18.06)? False&quot; 测试 List 包含关系，返回 true 或 false list_1 | issuperset(list_2) list_1是否包含list_2list_2 | insubset(list_1) list_2是否是list_1的子列表 vars: list_1: [&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;,&#x27;e&#x27;] list_2: [&#x27;b&#x27;,&#x27;c&#x27;] tasks: - debug: msg=&quot;list_1 included list_2 &#123;&#123; list_1 | issuperset(list_2) &#125;&#125;&quot; - debug: msg=&quot;list_2 is included in list_1 &#123;&#123; list_2 | issubset(list_1) &#125;&#125;&quot;结果： &quot;msg&quot;: &quot;list_1 included list_2 True&quot; &quot;msg&quot;: &quot;list_2 is included in list_1 True&quot; 测试文件路径，返回 true 或 false path | is_file 是否是文件path | is_dir 是否是目录path | is_link 是否是链接path | exists 是否存在 vars: path_1: /root/a.yml path_2: /etc tasks: - debug: msg=&quot;a.yml is file &#123;&#123;path_1|is_file&#125;&#125;&quot; - debug: msg=&quot;/etc is dir &#123;&#123;path_2|is_dir&#125;&#125;&quot; - debug: msg=&quot;a.yml is link file &#123;&#123;path_1|is_link&#125;&#125;&quot; - debug: msg=&quot;/etc exists &#123;&#123;path_2|exists&#125;&#125;&quot;结果： &quot;msg&quot;: &quot;a.yml is file True&quot; &quot;msg&quot;: &quot;/etc is dir True&quot; &quot;msg&quot;: &quot;a.yml is link file False&quot; &quot;msg&quot;: &quot;/etc exists True&quot; 测试命令执行结果，返回 true 或 false result | failed # 是否失败result | changed # 是否改变result | success # 是否成功result | skipped # 是否跳过 tasks: - shell: ls register: result ignore_errors: true - debug: msg=&quot;execute failed? &#123;&#123; result|failed &#125;&#125;&quot; - debug: msg=&quot;execute changed? &#123;&#123; result|changed &#125;&#125;&quot; - debug: msg=&quot;execute success? &#123;&#123; result|success &#125;&#125;&quot; - debug: msg=&quot;execute skipped? &#123;&#123; result|skipped &#125;&#125;&quot;结果： &quot;msg&quot;: &quot;execute failed? False&quot; &quot;msg&quot;: &quot;execute changed? True&quot; &quot;msg&quot;: &quot;execute success? True&quot; &quot;msg&quot;: &quot;execute skipped? False&quot; Ansible 实战为新系统添加 SSHkey先通过在 inventory 中配置ansible_ssh_user和ansible_ssh_pass，然后使用 ansible adhoc 命令测试是否能成功执行。接着调用 authorized_key 模块，添加认证到远端。 ansible xxx -m authorized_key -a &quot;user=远端用户 key=&#x27;&#123;&#123; lookup(&#x27;file&#x27;, &#x27;/本端用户家目录/.ssh/id_rsa.pub&#x27;) &#125;&#125;&#x27; path=/远端用户家目录/.ssh/authorized_keys manage_dir=no&quot; 部署 LAMP+Varnish+Memcached 一台 Varnish 作为前端接入，做负载均衡 两台 Apache 做 web 服务器，上面部署 php 两台 Mysql 做主从 一台 Memcached，做 Mysql 的缓存 创建 ansible 项目目录lamp，包含以下目录或文件： inventory: playbooks db memcached varnish www provisioners configure.yml provision.yml requirement.yml 编辑playbooks/varnish/main.yml ---- hosts: lamp-varnish become: yes vars_files: - vars.yml roles: - geerlingguy.firewall - geerlingguy.repo-epel - geerlingguy.varnish tasks: - name: 生成varnish配置模板，并传到远端 template: src: &quot;templates/defaults.vcl.j2&quot; dest: &quot;/etc/varnish/default.vcl&quot; notify: restart varnish 编辑playbooks/varnish/vars.yml ---firwall_allowed_tcp_ports: - &quot;22&quot; - &quot;80&quot;varnish_use_default_vcl: false 编辑 varnish 模板playbooks/varnish/templates/default.vcl.j2 vcl 4.0;import directors;&#123;% for host in groups[&#x27;lamp-www&#x27;] %&#125;backend www&#123;&#123; loop.index &#125;&#125; &#123; .back = &quot;&#123;&#123; host &#125;&#125;&quot;; .port = &quot;80&quot;;&#125;&#123;% endfor %&#125;sub vcl_init &#123; # 初始化 # 采用random负载策略 new vdir = directors.random(); &#123;% for host in groups[&#x27;lamp-www&#x27;] %&#125; vdir.add_backend(www&#123;&#123; loop.index &#125;&#125;, 1); &#123;% endfor %&#125;&#125;sub vcl_recv &#123; # 将请求发给vdir定义的后端 set req.backend_hint = vdir.backend(); return (pass);&#125; 添加 apache 和 php 的 playbookplaybooks/www/main.yml ---- hosts: lamp-www become: yes vars_files: - vars.yml roles: - geerlingguy.firewall - geerlingguy.repo-epel - geerlingguy.apache - geerlingguy.php - geerlingguy.php-mysql - geerlingguy.php-memcached tasks: - name: 去除apache的测试页 file: path: /var/www/html/index.html state: absent - name: 复制index模板 template: src: templates/index.php.j2 dest: /var/www/html/index.php 添加变量文件playbooks/www/vars.yml ---firewall_allowed_tcp_ports: - &quot;22&quot; - &quot;80&quot; 创建 memcached 的 playbook，playbooks/memcached/main.yml ---- host: lamp-memcached become: yes vars_files: - vars.yml roles: - geerlingguy.firewall - geerlingguy.memcached 创建变量文件playbooks/memcached/vars.yml ---firewall_allowed_tcp_ports: - &quot;22&quot;firewall_additional_rules: - &quot;iptables -A INPUT -p tcp --dport 11211 -s &#123;&#123; groups[&#x27;lamp-www&#x27;][0] &#125;&#125; -j ACCEPT&quot; - &quot;iptables -A INPUT -p tcp --dport 11211 -s &#123;&#123; groups[&#x27;lamp-www&#x27;][1] &#125;&#125; -j ACCEPT&quot;memcached_listen_ip: &quot;&#123;&#123; groups[&#x27;lamp-memcached&#x27;][0] &#125;&#125;&quot; 配置 mysql 的 playbook playbooks/db/main.yml ---- hosts: lamp-db become: yes vars_files: - vars.yml pre_tasks: - name: Mysql变量 set_fact: mysql_user: - name: test host: &quot;&#123;&#123; groups[&#x27;lamp-www&#x27;][0] &#125;&#125;&quot; password: secret priv: &quot;*.*:SELECT&quot; - name: test host: &quot;&#123;&#123; groups[&#x27;lamp-www&#x27;][1] &#125;&#125;&quot; password: secret priv: &quot;*.*:SELECT&quot; mysql_replication_master: &quot;&#123;&#123; groups[&#x27;a4d.lamp.db.1&#x27;][0] &#125;&#125;&quot; roles: - geerlingguy.firewall - geerlingguy.mysql 配置 mysql 的变量文件playbooks/db/vars.yml ---firewall_allowed_tcp_ports: - &quot;22&quot; - &quot;3306&quot;mysql_replication_user: (name: &#x27;replication&#x27;, password: &#x27;secret&#x27;)mysql_databases: - name: mydatabase collection: utf8_general_ci encoding: utf8 配置最后的configure.yml，该文件位于 playbooks 的同级目录 ---- include: playbooks/varnish/main.yml- include: playbooks/db/main.yml- include: playbooks/www/main.yml- include: playbooks/memcached/main.yml 部署时执行ansible-playbook configure.yml Ansible-Tower中心化的 Ansible 管理节点，向管理员提供 web 接口。实现：1. 管理员在 Ansible Tower 上使用分享主机的 SSH 密钥，但不能查看和复制私钥 2. Ansible 的 web 上的所有管理员都可共享 Playbook 脚本 3. Ansible Tower 可收集展示所有主机的 Playbook 执行情况。 Ansible Tower 提供了一个数据库来存储 inventory 配置信息，这个数据库可以通过 web 访问，或通过 REST 访问。Tower 与所有使用的 Ansible 动态 inventory 源保持同步，并提供了一个图形化的 inventory 编辑器。 在 Tower 中还能实现权限管理、Playbook 执行状态统计、REST API。ansible-tower 下载。解压后查看其中的inventory文件 [tower]localhost ansible_connection=local[database][all:vars]admin_password=&#x27;&#x27; # Tower管理员的密码pg_host=&#x27;&#x27;pg_port=&#x27;&#x27;pg_database=&#x27;awx&#x27;pg_username=&#x27;awx&#x27;pg_password=&#x27;redhat&#x27;rabbitmq_username=towerrabbitmq_password=&#x27;&#x27;rabbitmq_cookie=cookiemonster Ansible 常见模块croncron 计划任务模块 month # 指定月份 minute # 指定分钟 job # 指定任务（需要state=present） day # 指定小时 hour # 指定小时 weekday # 周几 name # 指定名称（默认为*） user # 使用的用户身份去执行 special_time # 指定执行的时间 reboot # 重启时 yearly # 每年 # 还有annually monthly weekly daily hourly state # 添加或删除 present # 安装 absent # 移除 backup # 对远程主机上原有任务计划做备份 cron_file # 使用指定文件替换远程主机上/etc/cron.d/中的任务计划 例：ansible webserver -m cron -a &#x27; minute=&quot;*/10&quot; job=&quot;/bin/echo hello&quot; name=&quot;test&quot; state=present &#x27; useruser 用户账号管理 name # 用户名 uid # UID state # 状态 present # 添加 absent # 移除 password # 设置密码 group # 所属组 groups # 附加组（用逗号分隔） home # 家目录 createhome # 是否创建家目录 comment # 注释 system # 是否设为系统用户 generate_ssh_key=yes # 是否加密密码 ssh_key_bits=2048 # 加密密钥长度 ssh_key_file=.ssh/id_rsa # 密码文件 注：指定password参数时，不能使用后面这一串密码会被直接传送到被管理主机的/etc/shadow文件中，所以需要先将密码字符串进行加密处理。然后将得到的字符串放到password中即可。 默认加密方式是根据/etc/login.defs的ENCRYPT_METHOD指定，默认为SHA512 groupgroup 组管理 gid # GID name # 组名 state # 状态 system # 是否是系统组 copycopy 复制文件，类似scp，需要关闭所有机器的selinux，否则会报错 src # 本地源路径 dest # 远程主机目标路径 owner # 指定拥有者 group # 指定所属组 mode # 设置权限 content # 取代src=，表示直接用此处信息生成文件内容 backup # 在覆盖前备份原文件，两个选项（yes | no） directory_mode # 递归设置目录权限，默认为系统默认权限 force # 用于设置当目标主机包含该文件，但内容不同时的操作 # 若设置为yes，则强制覆盖，若为no，则只有当目标主机的目标位置不存在该文件时，才复制。 # 默认为yes# 所有的file模块里的选项都可以在这里使用# 若出现了有关selinux的报错，可在被控机上安装libselinux-python解决# ansible all -m yum template用法和 copy 模块用法基本一致，主要用于复制模板。 template backup # 拷贝的同时也创建一个包含时间戳信息的备份文件，默认为no dest= # 目标路径 force # 设置为yes (默认)时，将覆盖远程同名文件。设置为no时，忽略同名文件的拷贝 group # 设置远程文件的所属组 owner # 设置远程文件的所有者 mode # 设置远程文件的权限。使用数值表示时不能省略第一位，如0644。 # 也可以使用&#x27;u+rwx&#x27;或&#x27;u=rw,g=r,o=r&#x27;等方式设置 src= # ansible控制器上Jinja2格式的模板所在位置，可以是相对或绝对路径 validate # 在复制到目标主机后但放到目标位置之前，执行此选项指定的命令。 # 一般用于检查配置文件语法，语法正确则保存到目标位置。 # 如果要引用目标文件名，则使用%s，下面的示例中的s%即表示目标机器上的/etc/nginx/nginx.conf。 filefile 设置文件属性 path # 设置文件路径（必填） dest # 设置目的路径 name # 设置文件名 owner # 指定拥有者 group # 指定所属组 mode # 设置权限 recurse # 递归设置目录属性 state # 文件状态 file # 文件不存在就不会被创建 dictionary # 若目录不存在，就自动创建 link # 创建软连接 hard # 创建硬链接 touch # 若不存在就自动创建 absent # 删除文件或目录 src # 指定源文件，只应用于state=link的情况 force # 强制创建软链接。 # 两种情况：1.当源文件不存在，但之后会建立 2.要先取消已创建的软链接，再重新创 serviceservice 管理服务运行状态 enabled # 是否开机自启（yes| no） name # 指定服务名（必填） state # 指定服务状态 started # 启动 stoped # 停止 restarted # 重启 reloaded # 重新加载 arguments # 服务参数 pattern # 设置模式 # 通过status指令来查看服务的状态时 # 若没有响应，就会通过ps指令在进程中根据该模式进行查找 # 如果匹配到，则认为该服务依然在运行 runlevel # 运行级别 sleep # 若执行restarted，则在stop和start键沉睡几秒 command若不指定模块，则默认使用 command 模块。command 模块不能解析变量(如$HOME)和某些操作符(“&lt;”, “&gt;”, “|“, “;”以及”&amp;”)，若需要使用以上符号，就要用 shell 模块。 command chdir # 在执行定义的命令前进入指定目录 creates # 创建文件，参数为一个文件或一个glob表达式，若已经存在就不会执行 removes: # 删除文件，参数为一个文件或一个glob表达式，若不存在就不会执行 stdin: # 可要求输入读取指定值 shellshell 在远程主机上运行命令，一般要使用管道符语法时，会使用shell模块。与raw模块类似 例：ansible all -m shell -a &#x27;echo hello&#x27; scriptscript 将本地脚本复制到远程主机并运行 例：ansible all -m script -a &#x27;/tmp/a.sh&#x27; yumyum 安装程序包 config_file # yum的配置文件 disable_gpg_check # 关闭gpg_check disablerepo # 不启用某个源 enablerepo # 启用某个源 name # 程序包名 state # 设置状态 present # 安装 latest # 安装 absent # 卸载 注：yum 模块是基于 python2，若要基于 python3 安装，需要模块 dnf。否则会以下报错： 192.168.163.103 | FAILED! =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;The Python 2 bindings for rpm are needed for this module. If you require Python 3 support use the `dnf` Ansible module instead.. The Python 2 yum module is needed for this module. If you require Python 3 support use the `dnf` Ansible module instead.&quot;&#125; 也可通过 command 模块直接安装：ansible 主机 -m command -a &#39;yum -y install 软件&#39; dnf类似 yum，但由于 yum 基于 python2，若有依赖于 python3 的软件包则会报错，因此可用 dnf 代替，并且 dnf 的安装速度都有提升。常用参数与 yum 一致。 setupsetup 收集远程主机的facts，获取主机信息 # 每个被管理节点在接受并运行管理命令前，会将自己主机相关信息（如操作系统信息，IP地址等报告给ansible） filter # 过滤器（正则表达式） 例：ansible 192.168.163.103 -m setup -a &#x27;filter=ansible_eth[0-2]&#x27; ---- hosts: group1 remote_user: root tasks: - name: get system info debug: msg=&quot;system = &#123;&#123; ansible_os_family &#125;&#125; kernel = &#123;&#123; ansible_kernel &#125;&#125; ip_addr = &#123;&#123; ansible_all_ipv4_addresses &#125;&#125;&quot;# 用ansible XXX -m setup就能看到所有变量名 收集 Facts 会消耗额外的时间，若不需要，可以在 playbook 中关闭 - hosts: group1 gather_facts: no synchronizesynchronize 使用rsync同步文件 archive # 归档，相当于同时开启recursive(递归)、links、perms、times、owner、group、-D选项都为yes ，默认该项为开启 checksum # 跳过检测sum值，默认关闭 compress # 是否开启压缩，默认yes copy_links # 复制链接文件，默认为no ，注意后面还有一个links参数 delete # 删除不存在的文件，默认no dest # 目录路径 dest_port # 默认目录主机上的端口 ，默认是22，走的ssh协议 dirs # 传速目录不进行递归，默认为no，即进行目录递归 rsync_opts # rsync参数部分 set_remote_user # 主要用于/etc/ansible/hosts中定义或默认使用的用户-与rsync使用的用户不同的情况 mode # push或pull 模块 # push模式一般用于从本机向远程主机上传文件 # pull 模式用于从远程主机上取文件 mountmount 设置挂载点 dump fstype # 必选项，挂载文件的类型 name # 必选项，挂载点 opts # 传递给mount命令的参数 src # 必选项，要挂载的文件 state # 必选项present：只处理fstab中的配置 present # 只处理fstab中的配置 absent # 删除挂载点 mounted # 自动创建挂载点并挂载 umounted # 卸载 get_urlget_url 用于从http、ftp、https服务器上下载文件（类似于wget） sha256sum # 下载完成后进行sha256 check； timeout # 下载超时时间，默认10s url # 下载的URL dest # 本地存放路径 url_password/url_username # 主要用于需要用户名密码进行验证的情况 use_proxy # 使用代理，代理需事先在环境变更中定义 lineinfile修改文件。主要用于修改量小的情况。 示例：用来配置环境变量 ---- hosts: test tasks: - name: 修改环境变量 lineinfile: dest=~/.bash_profile regexp=^TEST_ENV= line=TEST_ENV=hello - name: 刷新配置文件并获取该环境变量 shell: &#x27;source ~/.bash_profile &amp;&amp; echo $TEST_ENV&#x27; register: test_env - name: 打印该环境变量 debug: msg=&quot;TEST_ENV &#123;&#123; test_env.stdout &#125;&#125;&quot;执行后，到目标主机上查看# tail -n1 .bash_profileTEST_ENV=hello 查看模块用法信息ansible-doc 模块名 参考资料 Ansible 中文权威指南 Ansible ：一个配置管理和 IT 自动化工具 Ansible 系列 大神带你 20 分钟学会 Ansible！ Ansible 详解（一） Ansible 详解（二） 朱双印个人日志-Ansible Linux 集群与自动化运维 Ansible 快速入门技术原理与实战 Ansible 权威指南","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"ansible","slug":"ansible","permalink":"https://coconutmilktaro.top/tags/ansible/"},{"name":"运维","slug":"运维","permalink":"https://coconutmilktaro.top/tags/%E8%BF%90%E7%BB%B4/"},{"name":"监控","slug":"监控","permalink":"https://coconutmilktaro.top/tags/%E7%9B%91%E6%8E%A7/"},{"name":"自动化","slug":"自动化","permalink":"https://coconutmilktaro.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"}]},{"title":"LVS负载均衡学习笔记","slug":"LVS负载均衡笔记","date":"2018-05-27T15:21:51.000Z","updated":"2022-06-21T17:03:51.982Z","comments":true,"path":"2018/LVS负载均衡笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/LVS%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%AC%94%E8%AE%B0/","excerpt":"","text":"LVS 原理LVS（Linux Virtual Server）Linux 虚拟服务器是由章文嵩于 1998 年开发的负载均衡软件，提供传输层和应用层的负载均衡，传输层的负载均衡工具为 IPVS，应用层的负载均衡工具为 KTCPVS。 LVS 集群的通用体系结构LVS 集群采用三层结构： 负载调度器（load balancer）：整个集群的前端机，将网络请求无缝调度到真实服务器上，使服务器集群的结构对客户透明。因为所有的操作都是在 Linux 内核完成的，调度开销很小，所以具有很高的吞吐率。 服务器池（server pool）：是一组真正执行客户请求的服务器。服务器池的结点数目是可变的，可以在服务器池中增加服务器来满足不断增长的请求负载。对大多数网络服务来说，请求间不存在很强的相关性，请求可以在不同的结点上并行执行。 共享存储（shared storage）：为服务器池提供一个共享的存储区，通常是数据库、网络文件系统或者分布式文件系统，这样很容易使得服务器池拥有相同的内容，提供相同的服务。需要一个分布式锁管理器（Distributed Lock Manager）来保证应用程序在不同结点上并发访问的一致性。 前端负载均衡器称为 Director Server（DR），后端的实际服务器称为 Real Server(RS)，IP 虚拟服务器软件称为 IP Virtual Server（IPVS），IPVS 工作在 Linux 内核中。在调度器的实现技术中，IP 负载均衡技术的效率是最高的。 LVS 的几种 IP 地址 VIP：virtual IP，DR 上接收外网数据包的网卡 IP 地址 DIP：director IP，DR 上转发数据包到 RS 的网卡 IP 地址 RIP：real IP，RS 的 IP 地址 CIP：client IP，客户端 IP 地址 为什么要用共享存储？共享存储是可选的，但若网络服务需要相同的内容，应该使用共享存储，否则无共享结构的代价会很大，每台服务器需要一样大的存储空间，任何更新需要涉及每一台服务器，系统的维护代价会非常高。分布式文件系统提供良好的伸缩性和可用性，分布式文件系统在每台服务器使用本地硬盘作 Cache，可以使得访问分布式文件系统的速度接近于访问本地硬盘。 如何实现高可用性？调度器上有资源监测进程时刻监视各个服务器结点的健康状况，当服务器对ICMP ping 不可达时或者网络服务在指定的时间内没有响应时，资源监测进程会通知内核将该服务器从调度列表中删除。一旦监测到服务器恢复工作，通知调度器将其加入调度列表，管理员也可通过命令随时向调度列表添加或移除服务器。调度器存在单点故障问题，因此需要对调度器进行主从备份，并用 HeartBeat 机制进行主从故障监测。当从调度器不能听得主调度器的心跳时，从调度器通过 ARP 欺骗 （Gratuitous ARP）来接管集群对外的 VIP，同时接管主调度器的工作来提供负载调度服务。当主调度器恢复后，有两种恢复机制。第一种为主调度器自动变成从调度器（类似抢占），另一种为从调度器释放 VIP，主调度器收回 VIP 继续提供负载调度服务。当主调度器失效时，主调度器上所有已建立连接的状态信息将丢失，已有连接会中断。 客户需要重新连接从调度器，从调度器才会将新连接调度到各个服务器上。因此，调度器在内核中实现了一种高效同步机制，将主调度器的状态信息及时同步到从调度器。当从调度器接管时，绝大部分已建立的连接会持续下去。 三种 IP 负载均衡技术 VS/NAT：调度器重写请求报文的目标地址，根据预设算法，将请求分派给实际服务器，实际服务器在响应报文通过调度器时，报文的源地址被重写，再返回给客户。优点： 节约 IP 地址，能对内部进行伪装缺点： 效率低，返回给请求方的流量需经过 DR 且请求和响应报文都要 DR 进行地址的重写，当客户端请求增多时，DR 的处理能力会成为瓶颈完整过程： PC 向调度器发送请求报文，调度器收到后根据调度算法选择后端的实际服务器，将报文中目的 IP 与目的端口改写为实际服务器的 IP 地址与端口，并进行转发。 实际服务器收到后，进行处理，将结果返回给调度器 调度器再将源 IP 地址与源端口改回为调度器的 IP 和端口，回复给 PC。 数据包流向：客户端–&gt;调度器–&gt;实际服务器–&gt;调度器–&gt;客户端 VS/TUN（IP Tunneling）：调度器将请求报文通过 IP 隧道转发到实际服务器，实际服务器将响应报文直接回复给客户，调度器仅需处理请求报文，将请求报文的地址重写，无需重写响应报文的地址，极大解放了调度器，集群系统的最大吞吐量能提高 10 倍。 IP 隧道技术：又称为 IP 封装技术，可以将带有源和目标 IP 地址的数据报文使用新的源和目标 IP 进行第二次封装，这样这个报文就可以发送到一个指定的目标主机上 由于多个 RS 都共享一个隧道 IP（为 VIP），所以需要通过 ARP 进行 IP 地址解析出 MAC，而为了不让 RS 响应 ARP 请求导致出现错误，必须对 RS 进行抑制操作，这样只有 DR 进行 ARP 响应，也就让 PC 认为 DR 就是实际服务器。 注： 由于调度器不会对 IP 报文进行修改，所以 TCP 报文中的目的端口也不会修改，因此要求 RS 与 DR 的端口号必须一致 完整过程： PC 发送请求给调度器，调度器进行调度算法选择后端的实际服务器，将原报文进行第二次封装，源地址变为 DIP，目的地址变为 RIP，然后通过 IP 隧道发给指定实际服务器。 实际服务器处理完数据后直接回复给 PC 实际服务器的 RIP 和 DR 的 DIP 可以不处于同一物理网络中，且 RIP 必须可以和公网通信，即集群节点可以跨互联网实现。实际服务器的隧道接口上需要配置 VIP 地址，以便接收 DR 转发的数据包，以及作为响应报文的源 IP。DR 给 RS 时需要借助隧道，隧道外层的 IP 头部的源 IP 是 DIP，目标 IP 是 RIP。而 RS 响应给客户端的 IP 头部是根据隧道内层的 IP 头分析得到的，源 IP 是 VIP，目标 IP 是 CIP。这样客户端就无法区分这个 VIP 到底是 DR 的还是服务器组中的。 VS/TUN 模式一般会用来负载调度缓存服务器组，这些缓存服务器一般放置在不同网络环境，可以就近返回数据给客户端。在请求对象不能在缓存服务器本地找到的情况下，缓存服务器要向源服务器发请求，将结果取回，最后将结果返回给客户。 数据包流向：客户端–&gt;调度器–&gt;实际服务器–&gt;客户端 VS/DR（Direct Routing）：与 VS/TUN 类似，但调度器改写的是数据包的目的 MAC 地址，通过链路层进行负载分担。此法没有 IP 隧道的开销，但要求调度器与实际服务器必须在同一网段，也就是说 RIP 可用公网地址。 完整过程： PC 向调度器发送请求，调度器根据调度算法选择后端实际服务器，将数据帧的目的 MAC 改写为该实际服务器的 MAC 地址，并转发。 实际服务器收到后处理完数据后直接将结果回复给 PC 注：因为与 VS/TUN 类似，直接修改以太网帧，所以对于 IP 报文不会做修改，因此RS 的端口号必须与 DR 一致。且RS 上必须配置 VIP（通过配置环回口 IP 地址），VIP 为网卡别名的 IP 地址，仅用于回复数据包时使用作为源地址，不能用于通信。由于流出接口为 RIP 所在网卡接口，因此源 MAC 地址为 RIP 所在接口的 MAC 地址。且并不支持端口映射。 数据包流向：客户端–&gt;调度器–&gt;实际服务器–&gt;客户端 三种模式的比较DR 和 TUN 模式的性能高于 NAT，因为不需要 DR 对响应报文的操作DR 性能高于 TUN，因为不需要维护 IP 隧道DR 中调度器和实际服务器必须在同一个网段中，TUN 可实现跨网段负载均衡。 只有 NAT 支持端口映射，DR 与 TUN 都不支持。 为什么 VS/TUN 与 VS/DR 要在环回口 L0 上配置 VIP，能不能在出口网卡上配置 VIP？在环回口上配置 VIP 使得 RS 能通过路由收到请求数据包，并将结果返回给客户。不可以将 VIP 配置在出口网卡上，否则真实服务器会响应客户端的 ARP 请求，客户端上的 ARP 表就会记录真实服务器的 MAC，造成混乱，LB 就失效了。必须保证路由器只保存 DR 上的 VIP 对应的 MAC，即只允许 DR 进行 ARP 响应。在环回口配置 VIP 后，还需要设置arp_ignore=1和arp_announce=2来隐藏 RS 上的 VIP。应该在配置 VIP 之前就设置 arp 参数，防止配置 VIP 后、设置 arp 抑制之前被外界主机发现。 arp_ignore：接收到 ARP 请求时的响应级别。默认为 0。 0：响应目的地址是本地任意网卡上的所有 IP 地址的包 1：只响应目的地址恰好是入网卡的 IP 地址的包 arp_announce：将自己的地址向外通告时的通告级别。默认为 0。 0：使用本地任意接口上的任意地址向外通告 1：尽量避免使用本地属于对方子网的 IP 地址向外通告 2：总是使用最佳本地地址向外通告 arp_announce 为 2 的含义：在此模式下将忽略这个 IP 数据包的源地址并尝试选择能与该地址通信的本地地址。首要是选择所有网络接口的子网中包含该数据包目标 IP 地址的本地地址，如果没有发现合适的地址，将选择当前的发送网络接口或其他有可能接收到该 ARP 回应的网络接口来进行发送。 且这两项对所有参与集群调度的网卡都要设置 sysctl -w net.ipv4.conf.all.arp_ignore=1sysctl -w net.ipv4.conf.ens33.arp_ignore=1sysctl -w net.ipv4.conf.lo.arp_ignore=1sysctl -w net.ipv4.conf.all.arp_announce=2sysctl -w net.ipv4.conf.ens33.arp_announce=2sysctl -w net.ipv4.conf.lo.arp_announce=2 IPVS 如何解决 HTTPS 连接问题？当客户访问 HTTPS 服务时，会先建立一个 SSL 连接，来交换对称公钥加密的证书并协商一个 SSL Key，来加密以后的会话。在 SSL Key 的生命周期内，后续的所有 HTTPS 连接都使用这个 SSL Key，所以同一客户的不同 HTTPS 连接也存在相关性。IPVS 调度器提供了持久服务的功能，使得在设定的时间内，来自同一 IP 地址的不同连接会被发送到集群中同一个服务器结点，可以很好地解决客户连接的相关性问题。 可伸缩的缓存服务调度器一般使用 IP 隧道方法（VS/TUN）来架构缓存集群，因为缓存服务器可能在不同地方，而调度器与缓存服务器池可能不在同一个物理网段。若请求对象不能在本地找到，缓存服务器会向源服务器发请求，将结果取回并返回给客户。使用此方法，调度器只调度网页缓存服务器，而缓存服务器将响应数据直接返回给客户，调度器只需要调度一次请求，其余三次都由缓存服务器直接访问 Web 服务器完成。缓存服务器间有专用的组播通道，通过 ICP（Internet Cache Protocol）协议交互信息。当一台 Cache 服务器在本地硬盘中未命中当前请求时，它可以通过 ICP 查询其他 Cache 服务器是否有请求对象的副本，若存在，则从邻近的 Cache 服务器取该对象的副本，这样可以进一步提高 Cache 服务的命中率。 可伸缩邮件服务服务器池有 LDAP 服务器和一组邮件服务器，调度器将 SMTP、POP3、IMAP4 和 HTTP/HTTPS 请求流负载较均衡地分发到各邮件服务器上。系统中可能的瓶颈是 LDAP 服务器，可对 LDAP 服务中 B+树的参数进行优化。若分布式文件系统没有多个存储结点间的负载均衡机制，则需要相应的邮件迁移机制来避免邮件访问的倾斜。 LVS 两种调度方式与八种算法两种调度方式 静态调度：仅根据调度算法进行调度，不管实际服务器的系统负载 动态反馈调度：会根据实际服务器的系统负载及性能，计算出可以调度的服务器对象 八种算法 静态调度 轮询（Round Robin）：调度器将请求根据调度算法按顺序轮流分配到实际服务器。调度器均等地对待每一台服务器，不管服务器上实际的连接数和系统负载。 加权轮询（Weighted Round Robin）：根据实际服务器的不同处理能力调度访问请求。使处理能力强的服务器处理更多访问流量，调度器自动询问实际服务器负载情况，并动态调整权值。 目标地址散列（Destination Hashing）：将请求的目标地址作为散列键，从静态分配的散列表中找出对应的服务器。 源地址散列（Source Hashing）：将请求的源地址作为散列键，从静态分配的散列表中找出对应服务器。 动态反馈调度 最少连接（Least Connections）：动态将网络请求调度到已建立的连接数最少的服务器上。计算方法：活跃连接数 active*256+非活跃连接数 inactive 加权最少连接（Weighted Least Connections）：当集群中服务器性能差异较大的情况下使用。具有较高权值的服务器将承受较大比例的活动连接负载。调度器可以自动问询真实服务器的负载情况并动态调整权值。此算法为默认调度算法。 计算方法：(active*256+inactive)/weight 基于局部性最少连接（Locality-Based Least Connections）：针对 IP 地址的负载均衡，用于缓存集群系统。根据请求的 IP 地址找出该目标 IP 地址最近使用的服务器，若该服务器不可用，则用最少连接原则选出一个可用的服务器。该算法维护的是从一个目标 IP 地址到一台服务器的映射。 带复制的基于局部性最少连接（Locality-Based Least Connections with Replication）：针对 IP 地址的负载均衡，根据请求的目标 IP 地址找出与之对应的服务器组，按最小连接原则选出一台服务器。若该服务器超载，就在集群中按最小连接原则选出一台服务器，添加到服务器组中。该算法维护的是从一个目标 IP 地址到一组服务器的映射。 LVS 持久连接无论使用哪种算法，LVS 持久化都能实现在一定时间内，将来自 统一客户端请求派发到此前访问过的 RS。 需要持久连接的原因：若连接是基于 SSL 的，则在建立连接时需要交换密钥，认证 CA 等操作，若每次刷新就又分配别的 RS，则会造成资源浪费，速度变慢，因此需要持久连接。 每一次建立连接后，DR 会在内存缓冲区中为每一个客户端与 RS 建立映射关系（该记录也称 “持久连接模板” ），并且能做到对同一客户端的所有请求（不局限于一个服务）都定位到一台 RS。 持久连接分类： PPC（Persistent Port Connections）持久端口连接：将来自同一客户端对同一个集群的请求（同一端口）都定向到先前访问的 RS 上。 PCC（Persistent Client Connections）持久客户端连接：将来自同一客户端对同一个集群所有端口（即所有服务）的请求都定向到先前访问的 RS 上。 PNMPP（Persitent Netfilter Marked Packet Persistence）持久防火墙标记连接：通过防火墙策略，将对某类服务几个不同端口的访问定义成一类。 先对某一特定类型的数据包打上标记，然后再将基于某一类标记的服务送到后台的 RS 上去，后台的 RS 并不识别这些标记。将持久和防火墙标记结合起来就能够实现端口姻亲功能，只要是来自某一客户端的对某一特定服务（可以是不同端口）的访问都定向到同一台 RS 上。 KeepAlived 原理KeepAlived 用于 RS 的健康状态检查与 LB 主从之间的故障转移（Failover）实现。 Keepalived 实现了一组健康检查器，根据其健康状况动态自适应地维护和管理负载平衡的服务器池，支持4、5、7 层协议的健康检查。使用VRRP 实现高可用性，VRRP 是路由器故障转移的基础实现方法。此外，keepalived 实现了一组到 VRRP 有限状态机的挂钩，提供低级别的高速协议交互。每个 Keepalived 框架可以独立使用或一起使用，以提供弹性基础设施。 Keepalived 采用纯 ANSI/ISO C 编写，围绕一个中央 I/O 多路复用器提供实时网络设计（Realtime Networking Design）。设计重点是在所有元素之间提供均匀的模块化功能。 为了确保鲁棒性和稳定性，守护进程 keepalived 分为 3 个不同的进程： 一个精简的父进程负责分支子进程的监控 两个子进程，一个负责VRRP 框架，另一个负责健康检查 每个子进程都有自己的调度 I/O 多路复用器，这样 VRRP 调度抖动得到了优化，因为 VRRP 调度比健康检查更关键。这种拆分设计可最小化健康检查外部库的使用情况，并将其自身行为降至最低，使主机闲置，从而避免由其本身造成的故障。 父进程监视框架称为Watchdog，每个子进程打开一个套接字，当守护进程引导时，父进程连接到套接字并向子进程周期（5s）发送 hello 包。若父进程无法向子进程套接字发送 hello，则只要重启子进程即可。 Watchdog 设计的优点：从父进程发送到子进程的 hello 数据包通过 I/O 多路复用器调度程序完成，这样可以检测到子进程调度框架中的死循环并能通过使用 sysV 信号来检测死亡的子进程。 Keepalived 使用四个 Linux 内核组件： LVS 框架：使用 getsockopt 和 setsockopt 调用来获取和设置套接字上的选项。 Netfilter 框架：支持 NAT 和伪装（Masquerading）的 IPVS 代码。 Netlink 接口：设置和删除网络接口上的 VRRP 虚拟 IP。 组播：通过组播地址224.0.0.18发送 VRRP 通告。 LVS 与 KeepAlived 搭建首先在 DR 上安装依赖工具包libnl3-devel、popt-static，然后安装ipvsadm。ipvsadm是 ipvs 的命令行管理工具，可以定义、删除、查看 virtual service 和 Real Server 的属性。 可通过grep -i &#39;ip_vs&#39; /boot/config-内核版本号查看是否内核中编译了 IPVS 功能 ipvsadm 的下载地址也可以通过 yum 安装，安装完成后启动并设置开机自启systemctl enable ipvsadm,systemctl start ipvsadm ipvsadm 命令 ipvsadm选项中，大写选项管理虚拟服务virtual service，小写选项管理关联了虚拟服务的真实服务器RealServer1. 管理virtual service -A --add-service # 添加virtual service -t --tcp-service 服务器IP[:端口] # TCP服务 -u --udp-service 服务器IP[:端口] # UDP服务 -f --fwmark-service 防火墙标记 # 防火墙标记 -s --scheduler 算法 # 指定算法 -E # 修改，参数与-A一致 -D # 删除，参数与-A一致 -t|-u|-f -C # 清空所有虚拟服务（IPVS规则） -L # 查看所有虚拟服务 -n 数字格式显示主机地址和端口 --stats 显示更详细的统计信息（连接数、入站包、出站包量等） --rate 显示速率（每秒连接数CPS、每秒入站包个数InPPS、出站包个数OutPPS等）且是实时的 --timeout 显示会话超时时间（tcp、tcpfin、udp） -p 设置持久化连接时长 --persistent-conn 查看持久化连接情况 -c 显示当前IPVS的连接状况，实时的 --sort 排序，是实时的 -S # 保存IPVS规则，并输出到屏幕。可通过 &gt;文件，导入到文件 -R #载入之前的规则（要指定规则文件）。一般通过 &lt;文件，导入规则2. 管理RealServer -a # 添加real server -r 指定RS的IP地址和端口 -g DR模式 -i TUN模式 -m NAT模式 -t|-u|-f -w 权重 -e # 编辑real server -d # 删除real server NAT 模式搭建实验环境： Client：192.168.205.151 VIP：192.168.205.152 DIP：172.16.184.130 RIP1：172.16.184.131 RIP2：172.16.184.132 Client 和 RS 都采用单网卡，但非同一网段。DR 采用双网卡，一张连接 Client，一张连接 RS。且此实验 RS 要用 host-only 网卡，需要设置网关 route add default gw 172.16.184.130 确保 Server3 和 Server4 的网关配置生效，否则无法给 Client 连接。 注：一定要将网卡配置为静态 IP 地址，不能使用 DHCP 获取，否则配置的网关会自动消失。 # route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.16.184.130 0.0.0.0 UG 0 0 0 ens36172.16.184.0 0.0.0.0 255.255.255.0 U 100 0 0 ens36# ip routedefault via 172.16.184.130 dev ens36172.16.184.0/24 dev ens36 proto kernel scope link src 172.16.184.131 metric 100 Client 请求过程：Client 向 DR 发请求包，VIP 接收，经过 ip_forward 转发到 DIP，然后根据算法选择 RS，将数据包发往 RS。RS 响应过程：RS 向 DR 发响应包，DR 的 DIP 接收响应包，经过 ip_forward 转发到 VIP，最后将包回复给 Client。因为 VIP 与 DIP 不是一个网段，所以 DR 上要开启 ip_forward，并且要注意 iptables 与 ipvs 不可同时配置。 echo &quot;net.ipv4.ip_forward=1&quot; &gt;&gt; /etc/sysctl.conf &amp;&amp; sysctl -p 在 Server2 上配置： ipvsadm -A -t 192.168.205.152:80 -s rripvsadm -a -t 192.168.205.152:80 -r 172.16.184.131 -mipvsadm -a -t 192.168.205.152:80 -r 172.16.184.131 -m 通过ipvsadm -nL查看 LVS 服务 # ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 192.168.205.152:80 rr -&gt; 172.16.184.131:80 Masq 1 0 0 -&gt; 172.16.184.132:80 Masq 1 0 0 LVS 需要服务器间的时间同步，因此需要在 Server2 上配置 chronyd 服务。修改/etc/chronyd.conf，添加更新源。然后chronyc sources -v自动同步。 然后在 Server3 和 Server4 的 chronyd 配置文件中修改更新源server 192.168.205.152 iburst并自动更新。 在 Client 上多次访问192.168.205.152，因为选择的算法是轮询，所以会有以下现象。 # curl 192.168.205.152Server 3# curl 192.168.205.152Server 4# curl 192.168.205.152Server 3# curl 192.168.205.152Server 4 在 Server2 上查看ipvsadm -L --stats # ipvsadm -L --statsIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Conns InPkts OutPkts InBytes OutBytes -&gt; RemoteAddress:PortTCP server2:http 7 34 20 2235 2240 -&gt; server3:http 3 14 8 918 896 -&gt; server4:http 4 20 12 1317 1344 修改为 wrr 算法。在 Server2 上修改 IPVS 规则： ipvsadm -E -t 192.168.205.152:80 -s wrripvsadm -e -t 192.168.205.152:80 -r 172.16.184.131:80 -m -w 5ipvsadm -e -t 192.168.205.152:80 -r 172.16.184.132:80 -m -w 3 Client 上访问几次，再在 Server2 上查看，可发现访问 Server3 和 Server4 的包数量比例大约为 5:3 # ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 192.168.205.152:80 wrr -&gt; 172.16.184.131:80 Masq 5 0 22 -&gt; 172.16.184.132:80 Masq 3 0 13 DR 模式搭建环境： DR 的 VIP：172.16.246.140 DIP：172.16.246.134 RIP1：172.16.246.135 RS1 的 VIP：172.16.246.140 RIP2：172.16.246.136 RS2 的 VIP：172.16.246.140 一定要确保 DR 和 RS 在同一个交换机上，即都在同一个网段，以及 VIP 都要在同一个网段。 首先在 DR 上配置，创建网卡别名ens33:0 # ifconfig ens33:0 172.16.246.140 netmask 255.255.255.0 broadcast 172.16.246.255 up # 这是临时的，切要确保地址都是静态的，否则过一段时间配的地址会自动删除# ifconfigens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.16.246.134 netmask 255.255.255.0 broadcast 172.16.246.255......ens33:0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.16.246.140 netmask 255.255.255.0 broadcast 172.16.246.255 ether 00:0c:29:bf:f9:0c txqueuelen 1000 (Ethernet) 在两个后端 RS 服务器上pingDR 上的这两个地址，测试能够联通 然后在 RS 上配置 IP 地址，也确保为静态 IP。并且需要将 RS 的内核参数arp_ignore和arp_announce分别调整。 sysctl -w net.ipv4.conf.all.arp_ignore=1sysctl -w net.ipv4.conf.ens33.arp_ignore=1sysctl -w net.ipv4.conf.lo.arp_ignore=1sysctl -w net.ipv4.conf.all.arp_announce=2sysctl -w net.ipv4.conf.ens33.arp_announce=2sysctl -w net.ipv4.conf.lo.arp_announce=2 然后在环回口上配置 VIP，保证 DR、RS 的 VIP 相同。 ifconfig lo:0 172.16.246.140 netmask 255.255.255.255 boardcast 172.16.246.140 up# 一定要设置netmask为255.255.255.255，否则连接可能出问题 并且配置路由 route add -host 172.16.246.140 dev lo:0# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface172.16.246.0 0.0.0.0 255.255.255.0 U 100 0 0 ens33s3 0.0.0.0 255.255.255.255 UH 0 0 0 lo 在 DR 上篇配置路由route add -host 172.16.246.140 dev ens33:0 确保 RS 与 DR 的防火墙都放行了 http 以及对应端口。 在 DR 上配置 LVS ipvsadm -A -t 172.16.246.140:80 -s wlcipvsadm -a -t 172.16.246.140:80 -r 172.16.246.135 -g -w 3ipvsadm -a -t 172.16.246.140:80 -r 172.16.246.136 -g -w 4 在宿主机上测试 # curl 172.16.246.140# ipvsadm -L.... -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP s1:http wlc -&gt; rs1:http Route 3 0 13 -&gt; rs2:http Route 4 0 17 持久化配置仍使用 DR 配置的实验环境 只需要配置一条ipvsadm -E -t 172.16.246.140:80 -p 600 ipvsadm -L -n..... -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 172.16.246.140:80 wlc persistent 600 -&gt; 172.16.246.135:80 Route 3 0 0 -&gt; 172.16.246.136:80 Route 4 0 0 在宿主机上访问172.16.246.140，访问到的是 RS2（即172.16.246.136），在查看 DR 上信息 ipvsadm -L -n --persistent-connProt LocalAddress:Port Weight PersistConn ActiveConn InActConn -&gt; RemoteAddress:PortTCP 172.16.246.140:80 wlc persistent 600 -&gt; 172.16.246.135:80 3 0 0 0 -&gt; 172.16.246.136:80 4 1 0 4 # 可知RS2已有一个持久化连接 Keepalived 配置实验环境： DR：172.16.246.134 DR-Backup：172.16.246.133 RS1：172.16.246.135 RS2：172.16.246.136 需要在 DR 和 DR-backup 上安装 keepalived，可直接通过 yum 安装。 设置独立的 keepalived 日志，因为默认 keepalived 日志是记录在/var/log/messages中的。修改/etc/sysconfig/keepalived # 有以下配置项# --vrrp -P 只运行vrrp子进程# --check -C 只运行健康检查子进程# --dont-release-vrrp -V 不会在守护进程停止时删除VIP和路由# --dont-release-ipvs -I 不会在守护进程停止时删除IPVS拓扑# --dump-conf -d 备份日志配置文件# --log-detail -D 记录详细日志信息# --log-facility -S syslog的号码0-7# 默认配置是只有-DKEEPALIVED_OPTIONS=&quot;-D&quot;# 添加-S 0KEEPALIVED_OPTIONS=&quot;-D -S 0&quot; 然后在/etc/rsyslog.conf中添加local0的配置local0.* /var/log/keepalived.log，重启 rsyslog 服务即可。 keepalived 的配置文件/etc/keepalived/keepalived.conf，修改前最好备份。配置文件分为是三个部分：全局配置、VRRPd 配置、LVS 配置 # 此配置截取自默认配置，仅用于说明参数# 全局配置global_defs &#123; # 邮件报警功能，可以不要 notification_email &#123; acassen@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc # 告警邮箱地址 smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id LVS_DEVEL # 标识keepalived服务器的字符串 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;# VRRP实例配置vrrp_instance VI_1 &#123; state MASTER # 角色状态，master或backup interface eth0 # 定义vrrp绑定的接口，此网卡是面向集群的网卡 virtual_router_id 51 # VRID，同实例的该值必须相同 priority 100 # 优先级，值越大越高 advert_int 1 # 心跳信息发送间隔，单位秒 authentication &#123; # 认证方式 auth_type PASS # 密码认证 auth_pass 1111 # 密码，最多8个字符 &#125; virtual_ipaddress &#123; # VIP地址设置，只要master节点设置 192.168.200.16 &#125;&#125;# LVS虚拟服务配置virtual_server 10.10.10.2 1358 &#123; # VIP与端口 delay_loop 6 # 健康检查时间间隔 lb_algo rr # LB算法 lb_kind NAT # LB类型 persistence_timeout 50 # 持久化时长 protocol TCP sorry_server 192.168.200.200 1358 # 当所有RS都宕机时，请求发送到的服务器，一般就设为DR或本节点 real_server 192.168.200.2 1358 &#123; # RS的配置 weight 1 # 权重 HTTP_GET &#123; # 健康状况检查的检查方式 # 常见的有HTTP_GET|SSL_GET|TCP_CHECK|DNS_CHECK|MISC_CHECK url &#123; path /testurl/test.jsp # 状态检查url路径的是否健康 digest 640205b7b0fc66c1ea91c463fac6334d #健康状况需要状态码，可以是status_code、digest或digest+status_code #digest值用keepalived的genhash命令生成，一般使用status_code即可 status_code 200 &#125; connect_timeout 3 # 连接超时时间，若超时则认为RS可能宕机 nb_get_retry 3 # 重试次数，防止误判 delay_before_retry 3 # 重试时间间隔 &#125; &#125;&#125; 在 DR 上的配置 global_defs &#123; notification_email &#123; sysadmin@firewall.loc &#125; notification_email_from Alexandre.Cassen@firewall.loc smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state MASTER interface ens33 virtual_router_id 51 priority 120 advert_int 1 authentication &#123; # 不同实例的认证密码最好不同，以确保接管正常 auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 172.16.246.141 # 虚拟IP，不能和真实IP一致，随便写就行 172.16.246.142 &#125;&#125;virtual_server 172.16.246.140 80 &#123; # 虚拟主机 delay_loop 6 lb_algo wlc lb_kind DR # LVS类型为DR persistence_timeout 50 protocol TCP sorry_server 172.16.246.134 80 # 配置RS全部挂掉后访问的服务器 real_server 172.16.246.135 80 &#123; # 后端RS weight 1 HTTP_GET &#123; url &#123; path / status_code 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125; real_server 172.16.246.136 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status_code 200 &#125; connect_timeout 3 nb_get_retry 3 delay_before_retry 3 &#125; &#125;&#125; 可以将 DR 上的该配置文件传到 Backup 上，修改配置。还要修改router_id的配置，因为router_id用于标识不同服务器 # 只要修改以下配置vrrp_instance VI_1 &#123; state BACKUP priority 100&#125; 都启动 keepalived，使用命令keepalived或者systemctl start keepalived即可 可查看日志文件/var/log/keepalived.log s1 Keepalived[42451]: Starting Healthcheck child process, pid=42452s1 Keepalived[42451]: Starting VRRP child process, pid=42453s1 Keepalived_healthcheckers[42452]: Initializing ipvss1 Keepalived_healthcheckers[42452]: Opening file &#x27;/etc/keepalived/keepalived.conf&#x27;.s1 Keepalived_vrrp[42453]: Registering Kernel netlink reflectors1 Keepalived_vrrp[42453]: Registering Kernel netlink command channels1 Keepalived_vrrp[42453]: Registering gratuitous ARP shared channels1 Keepalived_vrrp[42453]: Opening file &#x27;/etc/keepalived/keepalived.conf&#x27;.s1 Keepalived_healthcheckers[42452]: Activating healthchecker for service [172.16.246.140]:80s1 Keepalived_healthcheckers[42452]: Activating healthchecker for service [172.16.246.140]:80 Keepalived 配置会自动创建 ipvs 策略，此时看ipvsadm -L已是 keepalived 的配置了 # ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 172.16.246.140:80 wlc persistent 50 -&gt; 172.16.246.135:80 Route 1 0 0 -&gt; 172.16.246.136:80 Route 1 0 0 宿主机上访问服务 &gt; curl 172.16.246.140RS2&gt; curl 172.16.246.140RS2&gt; curl 172.16.246.140RS2# 因为设置了持久化，一直访问RS2 此时停止 RS2 的 httpd 服务，再访问服务 &gt; curl 172.16.246.140RS1&gt; curl 172.16.246.140RS1# 成功切换到RS1 恢复 RS2 服务，在 DR 上停止 keepalived。再访问服务，仍能访问，keepalived 配置成功。 将后端 RS 的 httpd 全部关闭，然后再次访问，就会访问到 DR 的页面。 几种时间间隔： advert_int：vrrp 主备间发送和接收心跳信息的时间间隔 delay_loop：健康状态检查的时间间隔 connect_timeout：连接 RS 的超时时间 nb_get_retry：一个节点不健康的判定重试次数 delay_before_retry：判定某节点可能宕机后等待的时间，之后重试连接 几种健康检查： TCP_CHECK：TCP 连接来检查后端 RS 是否健康 HTTP_GET：获取指定页面检查 RS 是否健康（通过匹配 digest、status_code） SSL_GET：类似 HTTP_GET，但使用的 HTTPS MISC_CHECK：加载自定义健康状态检查脚本来检查对象是否健康（脚本的返回值需要是 0 或 1） DNS_CHECK：通过 DNS 检查 RS 是否健康 若出现两台服务器争抢同一 IP 资源时，要先排查两个地方： 主备两台服务器之间是否正常通信，如果不正常是否有 iptables 防火墙阻挡 主备两台服务器对应的 keepalived.conf 配置是否出错 解决裂脑的常见方案： 如果开启防火墙，一定要放行心跳信息，一般通过允许 IP 段解决 拉一条以太网网线作心跳线冗余 通过监控软件监测裂脑 若备节点出现 VIP，且主节点完好正常，则说明发生裂脑了。 Keepalived 双实例双主模式配置多实例，即业务 A 在 hostA 上是主模式，在 hostB 上是备模式，而业务 B 在 hostA 上是备模式，在 hostB 上是主模式。 实验环境： 172.17.1.128：业务 A 的 master，业务 B 的 backup 172.17.1.129：业务 A 的 backup，业务 B 的 master 业务 A 的 VIP：172.17.1.100 业务 B 的 VIP：172.17.1.200 172,17.1.128 的配置： vrrp_instance BusinessA &#123; state MASTER interface ens32 virtual_router_id 100 priority 120 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 172.16.1.100/24 dev ens32 label ens32:1 &#125;&#125;vrrp_instance BusinessB &#123; state BACKUP interface ens32 virtual_router_id 200 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 172.16.1.200/24 dev ens32 label ens32:2 &#125;&#125; 172.17.1.129 的配置： vrrp_instance BusinessA &#123; state BACKUP interface ens32 virtual_router_id 100 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 172.16.1.100/24 dev ens32 label ens32:1 &#125;&#125;vrrp_instance BusinessB &#123; state MASTER interface ens32 virtual_router_id 200 priority 120 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 172.16.1.200/24 dev ens32 label ens32:2 &#125;&#125; 重新启动 keepalived，查看ip addr ens32: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:05:bd:c3 brd ff:ff:ff:ff:ff:ff inet 172.16.1.128/24 brd 172.16.1.255 scope global dynamic noprefixroute ens32 valid_lft 1800sec preferred_lft 1800sec inet 172.16.1.100/24 scope global secondary ens32:1 valid_lft forever preferred_lft forever inet 172.16.1.200/24 scope global secondary ens32:2 valid_lft forever preferred_lft foreverens32: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:74:d9:f9 brd ff:ff:ff:ff:ff:ff inet 172.16.1.129/24 brd 172.16.1.255 scope global dynamic noprefixroute ens32 valid_lft 1722sec preferred_lft 1722sec inet 172.16.1.200/24 scope global secondary ens32:2 valid_lft forever preferred_lft forever inet 172.16.1.100/24 scope global secondary ens32:1 valid_lft forever preferred_lft forever Nginx 负载均衡配合 Keepalived实验环境： LB 1: 172.16.1.128 LB 2: 172.16.1.129 业务 A VIP：172.16.1.100 业务 B VIP：172.16.1.200 web RS1：172.17.0.2 业务 A：80 端口 业务 B：81 端口 web RS2：172.17.0.3 业务 A：80 端口 业务 B：81 端口 RS1 上的 nginx 配置 server 块： server &#123; listen 80; server_name business-a-2; root /usr/share/nginx/html; include /etc/nginx/default.d/*.conf; location / &#123; index index.html; &#125;&#125;server &#123; listen 81; server_name business-b-2; root /usr/share/nginx/html-1; include /etc/nginx/default.d/*.conf; location / &#123; index index.html; &#125;&#125; RS2 上的 nginx 配置 server 块： server &#123; listen 80; server_name business-a-3; root /usr/share/nginx/html; include /etc/nginx/default.d/*.conf; location / &#123; index index.html; &#125;&#125;server &#123; listen 81; server_name business-b-3; root /usr/share/nginx/html-1; include /etc/nginx/default.d/*.conf; location / &#123; index index.html; &#125;&#125; LB1 和 LB2 的 nginx 上配置负载均衡： upstream business-a-pool &#123; server 172.17.0.2:80 weight=1; server 172.17.0.3:80 weight=1;&#125;upstream business-b-pool &#123; server 172.17.0.2:81 weight=1; server 172.17.0.3:81 weight=1;&#125; LB 上的 keepalived 配置沿用上面双实例的配置。 LB1 和 LB2 的 nginx 的 server 块一致，配置如下： server &#123; listen 172.16.1.100:80; server_name business-a; include /etc/nginx/default.d/*.conf; location / &#123; proxy_pass http://business-a-pool; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $remote_addr; &#125;&#125;server &#123; listen 172.16.1.200:80; server_name business-b; include /etc/nginx/default.d/*.conf; location / &#123; proxy_pass http://business-b-pool; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $remote_addr; &#125;&#125; 依次访问两个业务，实现了多实例的负载均衡与高可用： $ curl 172.16.1.100business-a 172.17.0.2:80$ curl 172.16.1.100business-a 172.17.0.3:80$ curl 172.16.1.200business-b 172.17.0.3:81$ curl 172.16.1.200business-b 172.17.0.2:81 解决服务监听网卡上不存在 IP 地址的问题若在 nginx 配置中 server 块的 listen 配置一个本机没有的 ip 地址，则会报错解决方法：在/etc/sysctl.conf添加配置net.ipv4.ip_nonlocal_bind = 1或通过命令sysctl -w net.ipv4.ip_nonlocal_bind=1然后sysctl -p使配置生效。 解决高可用服务只针对物理服务器的问题若出现机器未宕机且 keepalived 正常工作，然而是服务挂了，keepalived 则无法进行切换。有两种方法解决当服务挂掉的时候也能实现 keepalived 的 IP 漂移切换。 写守护进程脚本处理。当本地 nginx 业务出现问题，就强制停掉 keepalived 以实现 IP 漂移。#!/bin/bashwhile truedo if [ `netstat -lntup | grep nginx | wc -l` ne 1 ]; then systemctl stop keepalived fi sleep 5done 使用监测脚本，然后配置在 keepalived 的配置文件中。脚本与上面类似，但是去掉sleep 5，然后在配置文件中添加：vrrp_script chk_nginx_proxy &#123; script &quot;脚本路径&quot; interval 2 //当nginx挂掉时，keepalived在2秒内就会按照脚本停止&#125; 并在 vrrp 实例中添加track_script &#123; chk_nginx_proxy&#125; 解决多组 keepalived 服务器在一个局域网的冲突问题若同一个局域网内存在多组 keepalived 服务器对，就会造成 IP 多播地址冲突问题，导致接管错乱，不同组 keepalived 都会默认使用224.0.0.18作为多播地址。解决方法：每个 keepalived 对指定唯一多播地址。 global_defs &#123; ...... vrrp_mcast_group4 224.0.0.19 //指定多播地址&#125; 配置指定文件接受 Keepalived 日志keepalived 默认日志输出到/var/log/messages，所以最好单独记录该日志。修改/etc/sysconfig/keepalived， KEEPALIVED_OPTIONS=&quot;-D -d -S 0&quot; 选项含义： # --vrrp -P 只与vrrp子系统运行# --check -C 只与健康检查子系统运行# --dont-release-vrrp -V 在keepalived进程停止时不移除VIP和VIP的路由# --dont-release-ipvs -I 在keepalived进程停止时不移除ipvs策略# --dump-conf -d 导出配置数据# --log-detail -D 输出详细日志# --log-facility -S 本地syslog设备（0-7，默认为LOG_DAEMON） 然后在/etc/rsyslog.conf中添加 keepalived 配置 local0.* /var/log/keepalived.log 并在*.info;mail.none;authpriv.none;cron.none后添加local0.none *.info;mail.none;authpriv.none;cron.none;local0.none /var/log/messages 重启 rsyslog 服务 systemctl restart rsyslog.service 开发监测 Keepalived 裂脑脚本在备服务器上执行脚本，若能 ping 通主节点且备节点有 VIP，就报警。 #!/bin/bashVIP=172.16.1.100LB1_IP=172.16.1.128while truedo ping -c 2 -W 3 $LB1_IP &amp;&gt;/dev/null if [ $? -eq 0 -a `ip addr | grep &quot;$LB1_IP&quot;|wc -l` -eq 1 ];then echo &quot;split brain ....&quot; sleep 5done 参考文档 LVS 中文官方文档 &gt; 骏马金龙 LVS 系列文章 &gt; 负载均衡的原理高性能网站构建实战Linux 之虚拟服务器 LVS 搭建 lvs arp 问题配置误区 LVS 集群中持久连接详解（PPC+PCC+PNMPP）","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"负载均衡","slug":"负载均衡","permalink":"https://coconutmilktaro.top/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"name":"LVS","slug":"LVS","permalink":"https://coconutmilktaro.top/tags/LVS/"},{"name":"keepalived","slug":"keepalived","permalink":"https://coconutmilktaro.top/tags/keepalived/"}]},{"title":"Wireshark学习笔记","slug":"wireshark学习笔记","date":"2018-05-18T13:21:35.000Z","updated":"2022-05-30T02:51:53.879Z","comments":true,"path":"2018/wireshark学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/wireshark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"基于 wireshark v2.4.5本篇包含以下内容 基本操作 捕获过滤器 tshark 命令使用 常用操作 参考文章","text":"基于 wireshark v2.4.5本篇包含以下内容 基本操作 捕获过滤器 tshark 命令使用 常用操作 参考文章 基本操作两种过滤器： 捕获过滤器 Capture Filter：也称抓包过滤器，使用伯克利包过滤语言（BPF），依赖于 BPF 的库（libpcap，Winpcap），用于限制抓的包，即抓包前的设定 显示过滤器 Display Capture：用于限制已经抓的包的显示，即抓包后的设定 捕获过滤器 type（类型）限定词host、net、port、portrange dir（方向）限定词src、dst proto（协议）限定词ether、arp、icmp、ip、tcp、udp、http、ftp 逻辑运算：&amp;&amp;或and（与）、||或or（或）、!或not（非）过滤器基本语法[protocol] [direction] [host] [value] [logical operations] [other expression] 捕获–&gt;捕获过滤器 有常用的语法案例 常用过滤表达式举例：ether tshark 命令使用tshark 是一个网络协议分析器。 它允许从实时网络捕获数据包数据，或从先前保存的捕获文件中读取数据包，将这些数据包的解码形式打印到标准输出或将数据包写入文件。TShark 的本机捕获文件格式是 pcapng 格式，也是 wireshark 和各种其他工具使用的格式。 tshark 参数选项： 捕获接口: -i: -i &lt;interface&gt; 指定捕获接口，默认是第一个非本地循环接口 -f: -f &lt;capture filter&gt; 设置抓包过滤表达式，遵循libpcap过滤语法，在抓包的过程中过滤，如果是分析本地文件则用不到 -s: -s &lt;snaplen&gt; 设置快照长度，用来读取完整的数据包，因为网络中传输有65535的限制，值0代表快照长度65535，默认也是这个值 -p: 以非混合模式工作，只关心和本机有关的流量 -I: 在监控模式（monitor）抓包 -B: -B &lt;buffer size&gt; 设置缓冲区的大小，只对windows生效，默认是2M -y: -y&lt;link type&gt; 设置抓包的数据链路层协议，不设置则默认为-L找到的第一个协议，局域网一般是EN10MB等 -D: 打印接口的列表 -L: 列出本机支持的数据链路层协议，供-y参数使用捕获停止选项: -c: -c &lt;packet count&gt; 捕获n个包之后结束，默认捕获无限个 -a: -a &lt;autostop cond.&gt; ... duration:NUM 在num秒之后停止捕获 filesize:NUM 在numKB之后停止捕获 files:NUM 在捕获num个文件之后停止捕获捕获输出选项: -b &lt;ringbuffer opt.&gt; ... ring buffer的文件名由-w参数决定,-b参数采用test:value的形式书写 duration:NUM - 在NUM秒之后切换到下一个文件 filesize:NUM - 在NUM KB之后切换到下一个文件 files:NUM - 形成环形缓冲，在NUM文件达到之后RPCAP选项: remote packet capture protocol，远程抓包协议进行抓包； -A: -A &lt;user&gt;:&lt;password&gt;,使用RPCAP密码进行认证;输入文件: -r: -r &lt;infile&gt; 设置读取本地文件处理选项: -2: 执行两次分析 -R: -R &lt;read filter&gt;,包的读取过滤器，可以在wireshark的filter语法上查看；在wireshark的视图-&gt;过滤器视图，在这一栏点击表达式，就会列出来对所有协议的支持。 -Y: -Y &lt;display filter&gt;,使用读取过滤器的语法，在单次分析中可以代替-R选项; -n: 禁止所有地址名字解析（默认为允许所有） -N: 启用某一层的地址名字解析。“m”代表MAC层，“n”代表网络层，“t”代表传输层，“C”代表当前异步DNS查找。如果-n和-N参数同时存在，-n将被忽略。如果-n和-N参数都不写，则默认打开所有地址名字解析。 -d: 将指定的数据按有关协议解包输出,如要将tcp 8888端口的流量按http解包，应该写为“-d tcp.port==8888,http”;tshark -d. 可以列出所有支持的有效选择器。 输出选项: -w: -w &lt;outfile|-&gt; 设置raw数据的输出文件。这个参数不设置，tshark将会把解码结果输出到stdout,“-w -”表示把raw输出到stdout。如果要把解码结果输出到文件，使用重定向“&gt;”而不要-w参数。 -F: -F &lt;output file type&gt;,设置输出的文件格式，默认是.pcapng,使用tshark -F可列出所有支持的输出文件类型。 -V: 增加细节输出; -O: -O &lt;protocols&gt;,只显示此选项指定的协议的详细信息。 -P: 即使将解码结果写入文件中，也打印包的概要信息； -S: -S &lt;separator&gt; 行分割符 -x: 设置在解码输出结果中，每个packet后面以HEX dump的方式显示具体数据。 -T: -T pdml|ps|text|fields|psml,设置解码结果输出的格式，包括text,ps,psml和pdml，默认为text -e: 如果-T fields选项指定，-e用来指定输出哪些字段; -E: -E &lt;fieldsoption&gt;=&lt;value&gt;如果-T fields选项指定，使用-E来设置一些属性，比如 header=y|n separator=/t|/s|&lt;char&gt; occurrence=f|l|a aggregator=,|/s|&lt;char&gt; -t: -t a|ad|d|dd|e|r|u|ud 设置解码结果的时间格式。“ad”表示带日期的绝对时间，“a”表示不带日期的绝对时间，“r”表示从第一个包到现在的相对时间，“d”表示两个相邻包之间的增量时间（delta）。 -u: s|hms 格式化输出秒； -l: 在输出每个包之后flush标准输出 -q: 结合-z选项进行使用，来进行统计分析； -X: &lt;key&gt;:&lt;value&gt; 扩展项，lua_script、read_format，具体参见 man pages； -z：统计选项，具体的参考文档;tshark -z help,可以列出，-z选项支持的统计方式。 常用操作若不使用任何选项，会抓取第一个非回环网卡的所有网络包。若指定-i网卡，则只监听该网卡的流量 root@kali:~# tshark -i eth0Capturing on &#x27;eth0&#x27; 1 0.000000000 192.168.80.139 → 192.168.80.2 DNS 69 Standard query 0xfe4e A baidu.com 2 0.000103735 192.168.80.139 → 192.168.80.2 DNS 69 Standard query 0x7c57 AAAA baidu.com 3 0.005903526 192.168.80.2 → 192.168.80.139 DNS 101 Standard query response 0xfe4e A baidu.com A 123.125.115.110 A 220.181.57.216 输出信息从左到右依次为： 抓包开始时间 源IP 目的IP 协议 包长度 包信息 参考文章 [Wireshark 命令行工具 tshark 详解(含例子)-01]","categories":[],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"wireshark","slug":"wireshark","permalink":"https://coconutmilktaro.top/tags/wireshark/"}]},{"title":"Redis基础学习笔记","slug":"Redis学习笔记","date":"2018-05-17T11:01:43.000Z","updated":"2022-06-21T15:16:21.218Z","comments":true,"path":"2018/Redis学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"本篇笔记主要包含以下内容","text":"本篇笔记主要包含以下内容 Redis 介绍 Redis 安装 Redis 数据类型 排序 发布与订阅 事务 过期时间 持久化 Redis 优化技术 管道 内部编码优化 集群 复制（主从） 无硬盘复制 增量复制 哨兵 集群 管理 安全 通信协议 一些管理命令 Redis 配置文件常用参数 在 Docker 上搭建 Redis FAQ Redis 报错问题 参考资料 Redis 介绍Redis（Remote Dictionary Server）是由 C 语言写成的高性能 key-value 非关系型数据库。为了保证效率，Redis 的数据都缓存在内存中，并周期性地将更新的数据写入磁盘，或将修改写入记录文件，在此基础上实现了主从同步。 Redis 安装 因为 Redis 是由 C 语言写的，所以要装 gcc。yum install gcc 安装 jemalloc，用于动态内存分配，是 malloc 的一个优化版本yum install jemalloc 安装工具命令语言 TCLyum install tcl 解压 redis 包到/usr/local，进入后make &amp;&amp; make install即可编译安装，可先make test检查是否出错 建立软连接，redis 的命令都存放在/usr/local/redis/src/目录下 ln -s /usr/local/redis/src/redis-server /bin/redis-serverln -s /usr/local/redis/src/redis-cli /bin/redis-cliln -s /usr/local/redis/src/redis-benchmark /bin/redis-benchmarkln -s /usr/local/redis/redis.conf /etc/redis.conf 如果可用内存十分小，最好要设置内核参数vm.overcommit_memory为 1。 overcommit_memory指定了内核针对内存分配的策略： 0：内核将检查是否有足够可用内存供应用进程使用。若不足，会报错 1：内核允许分配所有物理内存，不管当前内存状态。 2：内核允许分配超过所有物理内存和 swap 之和的内存大小。 echo &quot;vm.overcommit_memory=1&quot; &gt;&gt; /etc/sysctl.conf并且sysctl -p redis 命令 redis-server用于开启 redis 服务器端 [redis.conf文件路径] 设置配置文件路径，即可按照指定配置启动redis-server--[配置参数] 设置指定参数例：--port=6378-v 查看redis版本 redis-cli开启客户端命令行 -h [hostname] 指定要连接的redis服务器主机名，默认127.0.0.1-p [port] 指定服务器端口，默认6379-s [socket] 指定服务器socket，会覆盖主机名和端口-a [password] 设置连接redis服务器时要用的密码-u [uri] 设置服务器的URIshutdown 关闭redis 注： 若要让 redis 默认在后台启动，可修改配置文件中daemonize 参数，若为 no，则是前台启动，若为 yes，则是后台启动 redis-check-aof与redis-check-rdb：用于检测持久化状态或进行修复 redis-cli 基本操作exists [key] 查看键是否存在，存在返回 1，否则返回 0del [key] 删除键type [key] 返回键的数据类型keys [pattern] 返回符合指定匹配规则的键，支持 glob 风格通配符格式。rename [old-key] [new-key] 重命名键dbsize 返回当前数据库的键数量expire [key] [time] 指定键的生存时间（单位秒），返回 1 说明设置成功。未设置默认键的生存时间是无穷，会一直占用空间。ttl [key] 返回键的剩余生存时间，-1 表示永久，-2 表示不存在（已删除）select [db-num] 选择数据库编号 0 为默认，从 1 开始会在端口后显示，最大为 15，即最多有 16 个数据库。若超出范围，虽会显示该编号，但是仅会对 15 号数据库操作。 move [key] [db-num] 将指定键移动到指定数据库（不是复制）flushdb 删除当前数据库中所有键flushall 删除所有数据库的所有键 glob 风格通配符 符号 含义 ? 匹配一个字符 * 匹配任意个字符 [] 匹配括号建的任一字符 Redis 数据类型 字符串 String：可包含任意数据，包括图片和序列化对象，单个值上限 512MB 列表 List：双向链表，通过 push 和 pop 从链表头部或尾部添加删除元素，因此即可用作栈也可用作队列，且都是双向的 哈希 Hash：也称散列，字符串类型的键值对的映射表，适合存储对象，每个 Hash 可存储 2^32-1 个键值对。 新建的 hash 对象以 zipmap 来存储，zipmap 本身不是 hash table，但相比正常的 hash，可以节省 hash 自身需要的元数据存储开销。 zipmap 的增删改的复杂度都是 O(n)，但是一般对象的 field 都不多，所以速度也较快。若 field 或 value 的大小超出一定限制，则 redis 会自动将 zipmap 替换为正常的 hash 实现。 可通过配置文件的hash-max-ziplist-entries和hash-max-ziplist-value设置限制大小，单位字节。默认 entries 设为 512，value 设为 64 集合 Set：字符串类型的无序集合，通过 Hash 表实现，所有操作的复杂度都为 O(1)，最多可包含 2^32-1 个键值对 有序集合 Sorted Set：也称 ZSet，每个元素都会关联一个 double 类型的分数，称为权。redis 正是通过权来为集合中的成员进行从小到大的排序。有序集合的成员是唯一的,但权却可以重复。 redis-cli 操作 字符串set [key] [value] 设置键值 setnx [key] [value]：设置键值，若 key 已存在，则不会修改该值，并返回 0 setex [key] [expire time][value]：设置键值以及有效时间 mset [key1] [value1] [key2] [value2] ... 同时设置多个键值 msetnx：设置多个键值，但若 key 存在，则不会修改值 get [key] 获取键值mget [key1] [key2] ...同时获取多个键的值getrange [key] [start] [end] 返回键中字符串值的子字符串setrange [key] [start] [end] 设置字符串值的子串值getset [key] [value] 设置键的值并返回旧值strlen [key] 返回该键的字符串值的长度incr [key] 设置键值自增，返回新值decr [key] 设置键值自减，返回新值incrby [key] [value] 设置键值自增指定 value，返回新值descby [key] [value] 设置键值自减指定 value，返回新值append [key] [value] 指定键追加值 value，返回新值长度substr [key] [start] [end] 取字符串（字符编号从 0 开始） 列表lpush [list] [member1] [member2]... 在 list 头部添加元素rpush [list] [member1] [member2]... 在 list 尾部添加元素lpop [list] 在 list 头部删除元素，返回删除元素rpop [list] 在 list 尾部删除元素，返回删除元素 linsert [list] before|after [指定值][value]：在指定 list 的特定位置前或后添加字符串 value lset [list][index][value]：设置指定下标的值 lrem [list][num][value]：删除 list 中 num 个和 value 值一致的值。若 num&gt;0，则从头开始删，若 num&lt;0，则从尾开始删，若 num 为 0，删除全部 llen [list] 返回 list 的长度lindex [list] [index] 获取列表中对应索引的元素lrange [list] [start] [end] 返回 list 指定区间的元素（编号从 0 开始）ltrim [list] [start] [end] 截取 list，只保留截取区间的元素 集合sadd [set] [member1] [member2]... 添加集合元素srem [set] [member1] [member2]... 删除集合元素 spop [set] [value]：删除指定 value scard [set] 返回集合元素个数smove [set1] [set2] [value] 将指定元素从 set1 移到 set2sismember [set] [value] 判断该元素是否属于指定集合smembers [set] 返回集合中所有元素 srandmember [set]：随机返回 set 中的一个值 sinter [set1] [set2] 返回集合的交集sinterstore [set3] [set1] [set2] 将集合的交集存储到 set3 集合中sunion [set1] [set2] 返回集合的并集sunionstore [set3] [set1] [set2] 将集合的并集存储到 key3 集合中sdiff [set1] [set2] 返回集合的差集sdiffstore [key3] [key1] [key2] 将集合的差集存储到 key3 集合中 有序集合zadd [key] [score1] [member1]... 向有序集合添加成员并设置权值zrem [key] [member] 删除集合元素zincrby [key] [incr] [member] 设置元素的增加值zrank [key] [member] 返回指定元素的下标（从小到大）zrevrank [key] [member] 返回指定元素的下标（从大到小）zrange [key] [start] [end] 返回集合的指定区间元素zrevrange [key] [start] [end] 返回集合的指定区间元素（逆序） zrangebyscore [key][start][end]：返回 score 在指定范围间的元素。可在最后添加参数withscores返回该元素的 score zcount [key][start][end]：返回 score 在指定范围内的元素个数 zcard [key] 返回集合元素个数zscore [key] [member] 返回元素的权值zremrangebyrank [key] [start] [end] 删除集合中给定排名区间的元素 zremrangebyscore [key][start][end]：删除集合中 score 在指定范围内的元素 哈希hset [table] [column] [value] 设置字段 column 的值 hsetnx [table][column][value]：设置字段值，若字段存在则不会修改值，并返回 0 hget [table] [column] 获取字段的值hmset [table] [column1] [value1]... 设置多个字段的值hmget [table] [column1] [column2]... 获取多个字段的值hincrby [table] [column] [incr] 字段增加指定值hexists [table] [column] 字段是否存在 hlen [table]：返回指定 hash 的字段数 hdel [table] [column]... 删除表中字段hdel [table] 删除表hkeys [table] 返回表的所有字段hvals [table] 返回表的所有值hgetall [table] 返回表的所有字段与值 排序Redis 支持对 list、set、sorted set 的排序。sort [key] [BY partten][LIMIT offset count][GET pattern][ASC|DESC][ALPHA][STORE dstkey] sort 默认排序为从小到大，若要按照字母顺序排可选择ALPHA选项，ALPHA可以和ASC DESC一起用。在对有序集合类型排序时会忽略元素的分数，只针对元素自身的值进行排序。 &gt; lpush list1 1 2 3 4&gt; sort list11) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;4) &quot;4&quot;&gt; lpush list2 a b c A B C&gt; sort list2 alpha1) &quot;a&quot;2) &quot;A&quot;3) &quot;b&quot;4) &quot;B&quot;5) &quot;c&quot;6) &quot;C&quot; BY partten 设置条件进行排序 &gt; sort list1 by a* LIMIT offset count 表示跳过前 offset 个元素，并获取之后的 countge 元素 &gt; sort list2 alpha limit 2 31) &quot;b&quot;2) &quot;B&quot;3) &quot;c&quot; GET pattern 发布与订阅发布/订阅是一种消息通信模式，主要目的是解除消息发布者与消息订阅者的耦合。 订阅者可以通过subscribe和psubscribe命令向 redis server 订阅自己感兴趣的消息类型，redis 将消息类型称为频道(channel)。当发布者通过publish命令向 redis server 发送特定类型的消息时，该频道的全部订阅者都会收到此消息。这里消息的传递是多对多的。一个订阅者可以订阅多个频道,也可以向多个频道发送消息。publish [channel] [message]向指定频道发布信息subscribe [channel] 订阅频道实验：开启两个终端 # 终端2&gt; SUBSCRIBE chan1Reading messages... (press Ctrl-C to quit)1) &quot;subscribe&quot;2) &quot;chan1&quot;3) (integer) 1 此时该终端处于订阅状态，该状态下客户端不可使用除subscribe、unsubscribe、psubscribe、punsubscribe以外的命令。在订阅频道后客户端会收到三种类型的回复，每种回复都包含三个值。第一个值为消息类型。有以下三种消息类型 subscribe：表示订阅成功的反馈信息。此时第二个值为订阅的频道名，第三个值为当前客户端订阅的频道数 # 终端21) &quot;subscribe&quot;2) &quot;chan1&quot;3) (integer) 1 message：表示接收到的消息。此时第二个值为产生消息的频道，第三个值为消息的内容另一个终端在该频道发布消息 # 终端1&gt; PUBLISH chan1 hello# 终端21) &quot;message&quot;2) &quot;chan1&quot;3) &quot;hello&quot; unsubscribe：表示成功取消订阅某个频道。此时第二个值为对应频道名，第三个值为当前频道数量。 # 频道2&gt; UNSUBSCRIBE chan11) &quot;unsubscribe&quot;2) &quot;chan1&quot;3) (integer) 0 unsubscribe命令可退订频道，若不指定频道则退订所有频道 按照规则订阅使用psubscribe订阅符合指定规则的频道。规则支持 glob 风格的通配符。 # 频道2&gt; PSUBSCRIBE chan*Reading messages... (press Ctrl-C to quit)1) &quot;psubscribe&quot;2) &quot;chan*&quot;3) (integer) 1# 频道1&gt; PUBLISH chan2 hello&gt; PUBLISH chan100 hello# 频道21) &quot;pmessage&quot;2) &quot;chan*&quot;3) &quot;chan2&quot;4) &quot;hello&quot;1) &quot;pmessage&quot;2) &quot;chan*&quot;3) &quot;chan100&quot;4) &quot;hello&quot; 频道 2 收到了任意以chan开头的频道的信息第一个值：表示该信息的通过psubscribe命令订阅得到的第二个值：订阅使用的通配符第三个值：收到消息的具体频道名第四个值：收到的消息内容 punsubscribe命令可退订规则，若不指定频道则退订所有规则，且只会退订由规则加入的频道，并不会退订subscribe加入的频道。退订的规则必须严格匹配，与订阅时的一致。 发布订阅存在的问题： 如果订阅者读取消息的速度很慢，会使得消息不断积压在发布者的输出缓存区中，造成内存占用过多； 如果订阅者在执行订阅的过程中网络出现问题，那么就会丢失断线期间发送的所有消息。 事务事务的原理是先将属于一个事务的命令发送给 Redis，使 Redis 依次执行这些命令。 使用multi开启事务，之后的所有操作都属于该事务，直到提交exec，在事务中若有失误，可通过discard回滚，取消事务中所有操作。使用事务可保证一个事务内不会有其他的客户端的命令的插入。 &gt; set num1 11OK&gt; set num2 abcOK&gt; set num3 111OK&gt; MULTIOK&gt; incr num1QUEUED&gt; incr num2QUEUED&gt; incr num3QUEUED&gt; exec1) (integer) 122) (error) ERR value is not an integer or out of range3) (integer) 112 可见，事务中若有错误命令，仅会影响该命令，不会影响接下来的命令的执行。 事务的所有操作都是在事务提交时操作并一起返回值的，而有时需要先获得一条命令的返回值，再根据这个值执行下一条命令，即前一条命令的返回值需要作为后一条命令的参数。于是需要另一条命令watch，用于监视一个或多个键，一旦其中一个键被修改了，之后的事务都不会执行，监控一直持续到事务提交 &gt; set key 1OK&gt; watch keyOK&gt; set key 2 # 由于这里key被修改，于是之后的事务不会执行OK&gt; multiOK&gt; incr keyQUEUED&gt; exec(nil)&gt; get key&quot;2&quot;# 若在开启监视后，事务开启前，该键未被修改，则事务中对该键的操作仍有效&gt; set key 1OK&gt; watch keyOK&gt; multiOK&gt; incr keyQUEUED&gt; exec1) (integer) 2 在执行exec后会取消对所有键的监视，若不想在执行事务中的命令也可使用unwatch命令取消监控。 过期时间在实际开发中会遇到有时效的数据，过了一定时间就应该清除，在 Redis 中可使用expire设置一个键的过期时间，到达该时间后 Redis 会自动删除该键。expire &lt;key&gt; &lt;time&gt; （时间单位：秒，且必须是整数）返回值为 1 表示设置成功，0 表示未成功或键不存在。 若要设置更加精确的时间，可用命令pexpire，单位：毫秒。可使用ttl命令查看指定键的剩余时间。若该键被删除了，则返回值为-2，若未设置该键的过期时间，，则返回-1。可使用persist &lt;key&gt;命令取消设置指定键的过期时间，成功则返回 1，否则（键不存在，或键原来就没有过期时间设置）返回 0。 注：set或getset命令对键重新赋值也会清除过期时间。 若watch监视一个没有过期时间的键，该键到期自动删除后并不会被watch认为该键被修改。 命令expireat &lt;key&gt; &lt;time&gt; 用 Unix 时间作为过期时间（1970 年 1 月 1 日到现在的秒数）命令pexpireat &lt;key&gt; &lt;time&gt;同上，但单位为毫秒 当 Redis 用作缓存系统时，可以限制 Redis 能够使用的最大内存，并让 Redis 按照一定规则淘汰不需要的缓存键。修改配置文件maxmemory参数，限制最大可用内存大小（单位：字节）。当超出限制后，Redis 会根据maxmemory-policy参数指定的策略删除键直到 Redis 占用的内存小于指定内存。 以下为 Redis 提供的策略规则： 规则名 作用 volatile-lru 使用 LRU 算法删除一个键（只对设置了过期时间的键起作用） allkeys-lru 使用 LRU 算法删除一个键（会不断删除） volatile-random 随机删除一个键（只对设置了过期时间的键起作用） allkeys-random 随机删除一个键 volatile-ttl 删除过期时间最近的一个键 noeviction 不开启策略 LRU 算法：Least Recently Used 最近最少使用。该算法认为最近最少使用的键在未来一段时间内也不会被用到，即当需要空间时这些键是可以被删除的。 注：实际上，Redis 不会准确将整个数据库中最久未使用的键删除，而是每次从数据库中随机取 5 个键（可修改）并删除其中最久未被使用的键。随机取的键个数可通过配置文件的maxmemory-samples参数设置。默认为 5 个能产生最优的结果。10 个最接近 LRU 算法的要求，但会消耗更多的 CPU 资源。3 个会更快，但并不准确。 持久化redis 为了内部数据安全考虑，会把数据以文件形式保存一份在硬盘中，服务器重启后会自动将数据还原到内存中，将数据保存到硬盘称为持久化。持久化分为以下两种： 快照持久化（snap shotting），也称 RDB AOF 持久化（append only file） RDB默认开启，一次性将所有数据保存在硬盘中，整个数据库只保存为一个文件，便于数据迁移，也便于数据库毁坏后的恢复。在开始初始化时，唯一要做到的只是 fork 出子进程，再由子进程完成持久化工作，极大避免了服务进程执行 IO 操作，而父进程仍然处理客户端的请求，实现性能最大化。相较于 AOF，若数据集很大，RDB 的启动效率会很高。 若要保证数据的高可用性，最大限度避免数据丢失，则不宜选择 RDB。因为依靠子进程完成持久化，所以当数据集较大时，可能会导致整个服务器延时增大。 写时复制策略保证了在 fork 的时刻虽然生成了两份内存副本，但内存的占用量并不会增加一倍，因此需要确保 linux 系统允许应用申请超过可用内存的空间。可通过/etc/sysctl.conf中修改vm.overcommit_memory参数为 1。 当快照时，若写入操作交到，造成 fork 前后差异较大，是会使内存使用量显著超过实际数据大小的，因为内存不仅保存了当前数据库数据，还保存了快照时的内存数据。 快照方式 根据配置规则自动进行快照redis 目录中的 dump.rdb 就是快照持久化的数据备份文件。配置文件/etc/redis.conf的 RDB 参数 # 设置备份频率save 900 1 # 900秒中有一个键发生变化就触发RDB备份save 300 10 # 300秒中有10个键发生变化就触发RDB备份save 60 10000 # 60秒中有10000个键发生变化就触发RDB备份dbfilename dump.rdb # 备份数据库文件名dir ./ # 备份数据库文件存放位置 在 RDB 中可实现精细持续化，将每个修改的键保存，频率可达到秒级。 只有当快照结束时，新的 rdb 文件才会覆盖旧的文件，而在备份过程中，redis 是不会修改原 rdb 文件的，即任何时刻 rdb 文件都是完整的。于是可通过定时备份 rdb 文件实现 redis 数据库备份。rdb 文件是经过压缩的二进制格式，可通过配置文件的rdbcompression yes参数禁用来压缩节省 CPU 占用。 Redis 启动后会读取 RDB 文件，将数据从硬盘载入内存。通常一个记录 1000 万个字符串类型键、大小为 1GB 的快照文件载入到内存中需要花费 20-30 秒。 使用save或bgsave命令快照save命令：Redis 会同步进行快照，快照时会自动阻塞所有来自客户端的请求。若数据量大会导致 Redis 长时间无法访问，在生产环境中尽量不要用。bgsave命令：推荐使用，可在后台异步快照，快照时 Redis 仍能响应客户端请求。可通过lastsave命令查看快照是否完成，返回 unix 时间戳。 执行flushall命令Redis 会清空数据库中所有数据。无论清空数据库过程中是否触发了自动快照条件，只要自动快照条件不为空，redis 就会执行一次快照。若未指定自动快照条件，则flushall并不会执行快照。 执行复制（replication）时当设置了主从模式时，Redis 会在复制初始化时自动执行快照，并生成 RDB 文件，并不需要定义快照条件或手动执行。 AOF将用户执行的写指令都备份到日志文件中，还原数据就是执行写指令。AOF 可带来更高的数据安全性，即持久性。注：开启 AOF 持久化会清空 redis 数据库所有数据，所以若要选择 AOF 持久化，应该在安装完 redis 服务器后就要立刻开启。 Redis 有三种同步策略：每秒同步，每修改同步，不同步。 每秒同步为异步持久化，效率高。若服务器突然宕机，则在这一秒中的数据会丢失。 每修改同步为同步持久化，每次数据发生变化就会立刻记录到磁盘中，效率低。 该机制对日志文件的写入操作采用 append 模式，即使在写入过程中出现宕机，也不会破坏日志中已写入的数据，在 Redis 重启后可通过命令redis-check-aof解决数据一致性问题。当日志过大时，Redis 会自动启用重写 rewrite 机制，以 append 模式不断将修改数据写入老磁盘文件，并会创建一个新文件用于记录期间哪些修改命令被执行，保证了数据持久性。 对于相同数量的数据集，AOF 文件通常比 RDB 文件大。AOF 的运行效率通常慢于 RDB，但其中每秒同步的效率较高。 配置文件/etc/redis.conf的 AOF 参数 # 若要开启AOF，将此项改为yesappendonly no# 还可设置AOF的备份文件位置appendfilename appendonly.aof# 设置同步机制，三种机制always，everysec，no。# always：每修改同步# everysec：每秒同步，默认# no：同步禁用appendfsync everysec AOF 是将 Redis 客户端向 Redis 发送的所有命令全部记录下来，这就造成了有很多冗余无用的命令，如SELECT等也会记录，随着执行命令的增多，AOF 文件的大小也会逐渐增大。因此，Redis 提供了优化策略，可在配置文件中修改以下两个参数：auto-aof-rewrite-percentage 100：设置当目前 AOF 文件的大小超过上一次 AOF 文件大小的指定百分比时就会再次进行重写，若之前未重写过，则会根据启动时的 AOF 文件大小作依据。auto-aof-rewrite-min-size 64mb：限制允许重写的最小 AOF 文件大小。 若不满足重写条件，可通过命令bgrewriteaof手动重写。 Redis 优化技术管道客户端使用 TCP 与服务器建立连接，若执行较多的命令，每个命令的往返时延累加起来对性能有一定的影响。在执行多个命令时每条命令都需要等待上一条命令执行完（即收到 Redis 的返回结果）才能执行。Redis 支持管道（pipelining），可一次性发送多条命令并在执行后一次性返回结果。通过减少客户端与服务器的通信次数来实现降低往返时累计值。注：每一组中的命令都不能依赖之前命令的执行结果。 内部编码优化Redis 为每种数据类型都提供了两种内部编码的方式，并且会自动根据实际情况进行编码的转变，对于开发者而言是透明的。其中一种为复杂度是 O(1)的编码，而当键的元素个数大时，变会采用复杂度为 O(n)的编码。可通过object encoding &lt;key&gt;查看指定键的编码方式。 Redis 的每个键值都是使用一个 redisObject 的结构体保存的。 typedef struct redisObject &#123; unsigned type:4; unsigned notused:2; # not used unsigned encoding:4; unsigned lru:22; # lru时间 int refcount; # 存储某个键值被引用的数量 void *ptr;&#125;robj; 每个数据类型的编码 数据类型 内部编码方式 object encoding 命令结果 字符串 REDIS_ENCODING_RAW “raw” REDIS_ENCODING_INT “int” REDIS_ENCODING_EMBSTR “embstr” 散列 REDIS_ENCODING_HT “hashtable” REDIS_ENCODING_ZIPLIST “ziplist” 列表 REDIS_ENCODING_LINKEDLIST “linkedlist” REDIS_ENCODING_ZIPLIST “ziplist” 集合 REDIS_ENCODING_HT “hashtable” REDIS_ENCODING_INTSET “intset” 有序集合 REDIS_ENCODING_SKIPLIST “skiplist” REDIS_ENCODING_ZIPLIST “ziplist” EMBSTR 字符串编码方式与 RAW 类似，都是基于 sdshdr 实现。 字符串类型Redis 使用sdshdr类型变量存储一个键值能被多个键引用，Redis 会预先建立 10000 个分别存储从 0 到 9999 的 redisObject 型的变量对象，若设置的键值在 10000 以内，则该键就会直接引用这个共享对象，并不会再建立一个 redisObject 对象了。 当配置文件设定了macmemoryRedis 可用最大空间后，就不会使用共享对象，因为对于每个键值都需要使用一个 redisObject 记录 LRU 信息 集群Redis 集群主要用于防止单点故障，以及解决存储、性能瓶颈问题。 复制（主从）Redis 提供的复制功能，可实现当一台数据库中的数据更新后，自动将更新的数据同步到其他数据库上。因此，将数据库分为主数据库（master）与从数据库（slave）。主数据库可进行读写操作，当数据更新时将更新的数据同步到从数据库。而从数据库一般为只读操作，接收主数据库的同步数据。一个从数据库只能有一个主数据库。从数据库默认只读，若创建键会报错。 在一台主机上模拟主从数据库首先启动主数据库redis-server /etc/redis.conf然后可直接通过命令redis-server --port 6666 --slaveof 127.0.0.1 6379再打开一个数据库，并作为从数据库连接到从数据库redis-cli -p 6666并执行INFO replication，查看从数据库的信息 &gt; INFO replication# Replicationrole:slavemaster_host:127.0.0.1master_port:6379master_link_status:up...... 在主数据库上创建键，在从数据库上就能得到该键了。 127.0.0.1:6379&gt; set key1 123127.0.0.1:6666&gt; get key1&quot;123&quot; 也可通过修改数据库的配置文件将该数据库设为从数据库在 redis从服务器上修改配置文件参数bind &lt;主服务器IP&gt;slaveof &lt;主服务器IP&gt; &lt;主服务器端口&gt;主数据库上不需要任何配置。 还可在一个已开启的数据库中输入命令slaveof &lt;ip-addr&gt; &lt;port&gt;将本数据库设为指定主数据库的从数据库。若该数据库已是其他数据库的从数据库了，则这条命令会取消与原主数据库的同步，而与新指定的主数据库同步。还可通过命令slaveof no one使当前数据库停止接收主数据库同步，并转变为主数据库。 主从同步流程： 当一个从数据库启动后，会向主数据库发送 SYNC 命令。 主数据库收到 SYNC 后，会在后台保存 RDB 快照，并将快照与缓存的命令都发送给从数据库。 从数据库收到后载入快照，并将执行缓存命令。1 到 3 步称为复制初始化 复制初始化完成后，主数据库每当收到写命令后就会将命令同步到从数据库。 当主从数据库断开连接并重连后，Redis 提供有条件的增量数据传输，主数据库只需将断线期间执行的命令传送给从数据库即可。 试验同步：先使用 telnet 伪装成一个从数据库与主数据库通信 # telnet 127.0.0.1 6379Trying 127.0.0.1...Connected to 127.0.0.1. 然后在从数据库中使用ping确认与主数据库的连接，若正常则主数据库会返回PONG。再输入REPLCONF listening-port 6666说明自己端口号。开始同步SYNC，此时 telnet 的界面会出现以下信息 * Slave 127.0.0.1:6666 asks for synchronization* Starting BGSAVE for SYNC with target: disk* Background saving started by pid 4079* DB saved on disk* RDB: 6 MB of memory used by copy-on-write* Background saving terminated with success* Synchronization with slave 127.0.0.1:6666 succeeded 默认从数据库会使用同步前的数据响应客户端请求，可以在从数据库上修改配置文件参数slave-serve-stale-data为 no，使从数据库再同步完成前对所有命令都返回错误（除INFO和SLAVEOF）。复制同步阶段会贯穿整个主从同步始终，直到主从关系终止。 乐观复制：允许一定时间内主从数据库的内容不一致，但最终是会同步的。主从数据库的数据同步是异步的，会产生主从数据库数据不一致的时间窗口（即网络传输的时间加上命令执行的时间），因此，主数据库是不知道命令最终同步给多少个数据库的。Redis 提供配置文件参数限制至少同步给的从数据库的数量时，主数据库才是可写的。min-slaves-to-write 3 表示有 3 个以上的从数据库连接到主数据库时，主数据库才是可写的。min-slaves-max-lag 10 表示允许从数据库失去与主数据库连接的最长时间。若从数据库最后一次与主数据库的联系（即发送replconf ack命令）的时间小于该值，则认为从数据库仍与主数据库连接，否则就断开主从连接。这一特性默认关闭。 图结构：从数据库不仅能从主数据库接收同步数据，还能再以自身作为主数据库，将数据再同步给下属的从数据库。 通过复制可实现读写分离，提高负载能力。往往读的频率大于写的频率，当单机的 Redis 无法应对大量读请求时，可通过复制建立多个从数据库节点，主数据库只进行写操作，从数据库负责读操作。 从数据库的持久化可通过复制建立一个或多个从数据库，并在从数据库启动持久化，在主数据库禁用持久化。当从数据库崩溃重启后主数据库会自动同步数据。当主数据库崩溃后则需要按照以下步骤进行恢复。 1. 在从数据库中使用命令slaveof no one将从数据库提升为主数据库继续对外提供服务2. 启动崩溃的主数据库，再使用slaveof将其设置为新的主数据库的从数据库，再将数据进行同步。 当开启复制且主数据库关闭持久化功能时，不要使用 supervisor 等进程管理工具使主数据库崩溃后自动重启。同样当主数据库所在服务器因故关闭时，也要避免直接重启。因为主数据库重启后没有开启持久化功能，所以主数据库中所有数据都会被清空，而从数据库又会与从主数据库中同步数据，导致从数据库所有数据也被清空。 无硬盘复制Redis 的复制是基于 RDB 方式持久化实现的，即主数据库端在后台保存 RDB 快照，从数据库接收并载入快照文件。缺点： 当主数据库禁用 RDB 快照后，如果执行复制初始化，Redis 依然会生成 RDB 快照，所以下次启动后主数据库会以该快照恢复数据。因为复制发生的时间不能确定，这使得恢复的数据可能是任何时间点的。 因为复制初始化时需要在硬盘中创建 RDB 快照文件，所以如果硬盘性能很慢（如网络硬盘）时这一过程会对性能产生影响。举例来说，当使用 Redis 做缓存系统时，因为不需要持久化，所以服务器的硬盘读写速度可能较差。但是当该缓存系统使用一主多从的集群架构时，每次和从数据库同步，Redis 都会执行一次快照，同时对硬盘进行读写，导致性能降低。 因此 Redis 引入了无硬盘复制选项，开启该选项时，Redis 在与从数据库进行复制初始化时将不会将快照内容存储到硬盘上，而是直接通过网络发送给从数据库，避免了硬盘的性能瓶颈。 可修改配置文件中repl-diskless-sync参数为 yes 开启。 增量复制场景：当主从数据库连接断开后，从数据库会发送 SYNC 命令来重新进行一次完整复制操作。虽然断开期间数据库的变化很小，但也需要将数据库中的所有数据重新快照并传送一次。因此 Redis 实现了主从断线重连的情况下的增量复制。 增量复制是基于如下 3 点实现的。 从数据库会存储主数据库的运行 ID（run id）。每个 Redis 运行实例均会拥有一个唯一的运行 ID，每当实例重启后，就会自动生成一个新的运行 ID。 在复制同步阶段，主数据库每将一个命令传送给从数据库时，都会同时把该命令存放到一个积压队列（backlog）中，并记录下当前积压队列中存放的命令的偏移量范围。 同时，从数据库接收到主数据库传来的命令时，会记录下该命令的偏移量。 当主从连接准备就绪后，从数据库会发送一条 PSYNC 命令来告诉主数据库可以开始把所有数据同步过夹了，格式为PSYNC 主数据库的运行ID断开前最新的命令偏移量。主数据库收到 PSYNC 命令后，会执行以下判断来决定此次重连是否可以执行增量复制。 首先主数据库会判断从数据库传送来的运行 ID 是否和自己的运行 ID 相同，确保从数据库之前确实是和自己同步的。 然后判断从数据库最后同步成功的备今信移景是否在积压队列中，如果在则可以执行增量复制，并将积压队列中相应的命令发送给从数据库。如果此次重连不满足增量复制的条件，主数据库会进行一次全部同步。 增量复制的过程对开发者来说是完全透明的，唯一需要开发者设置的就是积压队列的大小了。主数据库可以正常地和旧版本的从数据库同步（通过接收 SYNC 命令），从数据库也可以与旧版本的主数据库同步（通过发送 SYNC 命令）。积压队列在本质上是一个固定长度的循环队列，默认情况下积压队列的大小为 1MB，可以通过配置文件的rep1-backlog-size选项来调整。积压队列越大，其允许的主从数据库断线的时间就越长。根据主从数据库之间的网络状态，设置一个合理的积压队列很重要。因为积压队列存储的内容是命令本身，所以估算积压队列的大小只需估计主数据库可能执行的命令的大小即可。另一个配置参数是rep1-backlog-ttl，当所有从数据库与主数据库断开连接后，经过多久时间可以释放积压队列的内存空间，默认为 1 小时。 哨兵Redis 提供哨兵实现自动化的系统监控和故障恢复功能，哨兵是一个独立的进程。哨兵有以下功能： 监控主数据库和从数据库是否正常运行。 主数据库出现故障时自动将从数据库转换为主数据库。 在一个主从 Redis 系统中，可使用多个哨兵进行监控任务以保证系统足够稳健。哨兵不仅能监控主从数据库，还能与其他哨兵互相监控。 哨兵实验 实验环境：system2 192.168.163.102主服务器：127.0.0.1 6379redis-server /etc/redis.conf从服务器：127.0.0.1 6380 6381redis-server --port 6380 --slaveof 127.0.0.1 6379redis-server --port 6381 --slaveof 127.0.0.1 6379127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:2slave0:ip=127.0.0.1,port=6380,state=online,offset=266,lag=0slave1:ip=127.0.0.1,port=6381,state=online,offset=266,lag=0设置哨兵创建配置文件/etc/sentinel.conf，并添加以下内容：sentinel monitor MyMaster_1 127.0.0.1 6379 1sentinel monitor master-name ip port quoram# MyMaster_1为要监视的主数据库的名字，可自定义# 后面跟上主数据库的IP地址和端口号# 最后一个数字quoram表示最低通过票数# 配置哨兵时，只需要配置监视的主数据库即可，哨兵会自动发现主数据库下的所有从数据库。然后启动Sentinel进程redis-sentinel /etc/sentinel.conf启动哨兵后会报如下信息Sentinel ID is 33766bbd6ec93b3574240b6a4ac5c8ea498207d4+monitor master MyMaster_1 127.0.0.1 6379 quorum 1* +slave slave 127.0.0.1:6380 127.0.0.1 6380 @ MyMaster_1 127.0.0.1 6379* +slave slave 127.0.0.1:6381 127.0.0.1 6381 @ MyMaster_1 127.0.0.1 6379# +slave表示发现了从数据库然后在另一个终端中关闭主数据库在原终端中会出现一连串的以下信息* Connecting to MASTER 127.0.0.1:6379* MASTER &lt;-&gt; SLAVE sync startedError condition on socket for SYNC: Connection refused在过了一段时间（默认30s，可配置修改）后，会出现以下信息+sdown master MyMaster_1 127.0.0.1 6379+odown master MyMaster_1 127.0.0.1 6379 #quorum 1/1# +sdown 表示哨兵主观认为主数据库停止服务了# +odown 表示哨兵客观认为主数据库停止服务了# 此时哨兵执行故障恢复，挑选一个从数据库提升为主数据库然后会报出许多信息，下面列举出几条重要的信息+try-failover master MyMaster_1 127.0.0.1 6379+vote-for-leader 33766bbd6ec93b3574240b6a4ac5c8ea498207d4 1......+failover-end master MyMaster_1 127.0.0.1 6379+switch-master MyMaster_1 127.0.0.1 6379 127.0.0.1 6380+slave slave 127.0.0.1:6381 127.0.0.1 6381 @ MyMaster_1 127.0.0.1 6380+slave slave 127.0.0.1:6379 127.0.0.1 6379 @ MyMaster_1 127.0.0.1 6380# +try-failover 表示哨兵开始故障恢复# +failover-end 表示哨兵完成故障恢复，故障恢复步骤包括领头哨兵选举、备份从数据库的选择等# +switch-master 表示主数据库从6379端口迁移到6380，即6380端口的从数据库提升为主数据库# 两个+slave，原主数据库变为了现主数据库的从数据库，但此时6379的数据库并未启动，说明哨兵并不会清除已停止服务的实例的信息再次登录上127.0.0.1:6380127.0.0.1:6380&gt; info replication# Replicationrole:masterconnected_slaves:1slave0:ip=127.0.0.1,port=6381,state=online,offset=99027,lag=1# 因为6379端口数据库未启动，所以此时只有一个从数据库然后重启6379端口数据库-sdown slave 127.0.0.1:6379 127.0.0.1 6379 @ MyMaster_1 127.0.0.1 6380+convert-to-slave slave 127.0.0.1:6379 127.0.0.1 6379 @ MyMaster_1 127.0.0.1 6380# -sdown 表示实例6379已恢复服务（与+sdown相反）# +convert-to-slave 表示将6379端口实例设置为6380端口实例的从数据库。在6379端会报以下信息：SLAVE OF 127.0.0.1:6380 enabled再在6380端查看127.0.0.1:6380&gt; info replication# Replicationrole:masterconnected_slaves:2slave0:ip=127.0.0.1,port=6381,state=online,offset=541686,lag=0slave1:ip=127.0.0.1,port=6379,state=online,offset=541686,lag=0 一个哨兵节点可同时监控多个 Redis 主从系统，只要提供多个 sentinel monitor 配置即可。多个哨兵节点也可监控一个主从系统。 sentinel.conf 配置文件的其他配置 # sentinel down-after-milliseconds 主数据库名 值# 这条参数用于设置哨兵发送ping命令检测数据库节点状态的周期时间，单位为毫秒 当超过该参数时间而未收到回复后，则哨兵认为该节点主观下线。若该节点为主数据库，则哨兵会进一步判断是否需要故障恢复。哨兵会向其他节点发送sentinel is-master-down-by-addr命令询问其他节点是否他们也认为主数据库主观下线，当赞同的节点达到指定数目后，哨兵会认为主数据库客观下线，并进行故障恢复。这个指定数目就是sentinel.conf中sentinel monitor最后一项数值 quoram。 哨兵启动后，会与要监控的主数据库建立两条连接。一条连接用来订阅该主数据的_sentinel_:he11o频道以获取其他同样监控该数据库的哨兵节点的信息，另外哨兵也需要定期向主数据库发送 INFO 等命令来获取主数据库本身的信息。 哨兵创建后与立刻做的事情：发送 INFO 命令获得当前数据库的相关信息（包括运行 ID、复制信息等）从而实现新节点的自动发现。哨兵向主数据库发送 INFO 命令，通过解析返回结果来得知从数据库列表，而后对每个从数据库同样建立两个连接。至此，与主数据库的连接建立成功。 和主数据库的连接建立完成后，哨兵会定时执行下面 3 个操作。 每 10 秒哨兵会向主数据库和从数据库发送 INFO 命令。 每 2 秒哨兵会向主数据库和从数据库的_sentinel_:hel1o频道发送自己的信息。发送的消息内容为：&lt;哨兵IP&gt;，&lt;哨兵port&gt;，&lt;哨兵运行ID&gt;，&lt;哨兵配置版本&gt;，&lt;主数据库名&gt;，&lt;主数据库IP&gt;，&lt;主数据库port&gt;，&lt;主数据库配置版本&gt; 每 1 秒哨兵会向主数据库、从数据库和其他哨兵节点发送 PING 命令。 选举领头哨兵的过程使用了 Raft 算法，过程如下： 发现主数据库客观下线的哨兵节点向每个哨兵节点发送命令，要求对方选自己为领头哨兵。 如果目标哨兵节点没有选过其他人，则会同意将该节点设为领头哨兵。 如果该节点发现有超过半数且超过该数值的哨兵节点同意选自己为领头哨兵，则此节点会成功成为领头哨兵。 当有多个哨兵节点同时参选领头哨兵，会出现没有一个节点当选的情况。此时参选节点会等待一个随机时间重新发起参选请求，进行下一轮直到选举成功。 故障恢复过程： 领头哨兵会从从数据库中挑选一个作为新的主数据库。挑选的依据分为三点： 所有在线的从数据库中选择优先级最高的，优先级可通过配置文件slave-priority参数设置 若有多个最高优先级的从数据库，则复制的命令偏移量大的优先 若上述都相同，则运行 ID 小的优先 选出符合的从数据库后，会向该数据库发送slaveof no one使其提升为主数据库，然后再向其他数据库发送slaveof使其成为新的主数据库的从数据库。最后再更新数据记录，原的主数据库变为从数据库。 哨兵的部署方案： 每个节点部署一个哨兵 每个哨兵与其对应节点网络环境相同或相近 这样可以保证哨兵的视角具有代表性和可靠性。最好将 quoram 的值设为 N/2+1（N 为哨兵节点个数）。若每个节点都部署一个哨兵的话，可能会因为 Redis 不支持连接复用而造成产生大量冗余连接。 集群集群往往用于水平扩容。若要开启集群，只要将配置文件的cluster-enabled参数设为 yes 即可，默认开启。每个集群至少需要三个主数据库。 集群实验 实验环境6个数据库，3个主数据库，3个从数据库三个主数据库端口分别为6000，6001，6002三个从数据库端口分别为6003，6004，6005 集群会将当前节点记录的集群状态持久化到指定文件，默认为当前目录下的nodes.conf，这里还是存放在/etc/nodes.conf，每个节点对应的文件必须不同，否则会启动失败，因此启动节点时要注意最后为每个节点使用不同的工作目录，或通过配置文件cluster-config-file 节点文件路径修改。最好给每个节点都创建一个目录，然后每个节点都复制一份配置文件，并修改port参数，cluster-config-file参数。然后通过redis-server 配置文件启动。使用ps查看，每个节点都是显示类似redis-server *:6000 [cluster]。然后进入节点 127.0.0.1:6000&gt; info cluster# Clustercluster_enabled:1 # 1说明集群启动正常 目前仅仅节点运行正常，但并未加入集群。需要使用 redis 的 ruby 插件。首先需要安装 ruby，最好不要 yum 安装，应该下最新版本的源码包编译安装。安装完后可以在/usr/local/redis/src/目录下找到redis-trib.rb命令，创软链接。然后使用该命令初始化集群 redis-trib.rb create --replicas 1 \\ 127.0.0.1:6000 \\ 127.0.0.1:6001 \\ 127.0.0.1:6002 \\ 127.0.0.1:6003 \\ 127.0.0.1:6004 \\ 127.0.0.1:6005# --replicas 1 表示每个主数据库拥有的从数据库个数为1会出现以下信息：&gt;&gt;&gt; Creating cluster&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Using 3 masters:127.0.0.1:6000127.0.0.1:6001127.0.0.1:6002Adding replica 127.0.0.1:6004 to 127.0.0.1:6000Adding replica 127.0.0.1:6005 to 127.0.0.1:6001Adding replica 127.0.0.1:6003 to 127.0.0.1:6002......Can I set the above configuration? (type &#x27;yes&#x27; to accept):确认输入yes创建集群 通过redis-trib.rb创建集群的过程： 首先该命令会以客户端形式尝试连接所有节点，并发送 ping 确定节点正常，同时发送info获取节点运行 ID、验证是否开启了集群 集群会向每个节点发送cluster meet IP地址 端口告诉当前节点指定的节点也是集群成员。 redis-trib.rb会分配主从数据库节点，分配原则为尽量保证每个主数据库运行在不同 IP 地址上，同时每个从数据库和主数据库都不运行在同一 IP 地址。 分配完成后，会为主数据库分配插槽，即分配哪些键归哪些节点复制。对每个要成为子数据库的节点发送cluster replicate 主数据库运行ID将当前节点转换为从数据库并复制指定主数据库。 127.0.0.1:6000&gt; CLUSTER nodes5c15caa067f96e557d73704e961ee08504fe3ac1 127.0.0.1:6004@16004 slave b098c2ddc169cc6e5411e8c42fb5afa96fa91764 0 1531150828178 5 connected6808731e0d12e8ec740509ef060c81306d5cd9bd 127.0.0.1:6000@16000 myself,master - 0 1531150825000 1 connected 0-54608e885e28b530491468b76bb8084cf7c22a8166f4 127.0.0.1:6005@16005 slave 9de0340daaf29f81b32c96d8010e9e443d66be0b 0 1531150827000 6 connectedb098c2ddc169cc6e5411e8c42fb5afa96fa91764 127.0.0.1:6001@16001 master - 0 1531150825153 2 connected 5461-10922e219bc21f1ab59f4cf00ca1b35da84e955424556 127.0.0.1:6003@16003 slave 6808731e0d12e8ec740509ef060c81306d5cd9bd 0 1531150827000 4 connected9de0340daaf29f81b32c96d8010e9e443d66be0b 127.0.0.1:6002@16002 master - 0 1531150827169 3 connected 10923-16383 可通过cluster meet IP地址 端口向新节点发送使新节点加入集群当新节点收到该命令后，会根据命令中的 IP 地址和端口与目标建立握手连接，然后目标会认为此节点为集群中的一员，并使用 Gossip 协议（一种分布式系统通信协议）向集群中所有节点发送此节点的信息。 新节点加入集群后可进行以下操作： 使用cluster replicate复制每个主数据库，以从数据库运行 向集群申请分配插槽（slot）以主数据库运行 在一个集群中，所有键会被分配给 16384 个插槽，每个主数据库会负责处理其中一部分插槽。 确认创建集群后的报出的信息&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:6000)M: 6808731e...... 127.0.0.1:6000 slots:0-5460 (5461 slots) master 1 additional replica(s)S: 5c15caa0...... 127.0.0.1:6004 slots: (0 slots) slave replicates b098c2d......S: 8e885e28...... 127.0.0.1:6005 slots: (0 slots) slave replicates 9de0340......M: b098c2dd...... 127.0.0.1:6001 slots:5461-10922 (5462 slots) master 1 additional replica(s)......由此也可看出，只有主数据库才会分配插槽，从数据库无插槽。 初始化集群时分配给每个节点的插槽是连续的，但实际上 Redis 没有限制，可将任意几个插槽分配给任意节点。 键与插槽的关系键名的有效部分通过算法计算出散列值并取 16384 的余数。使得每个键都可以分配到 16384 个插槽中，进而分配的指定的一个节点中处理。 有效部分：若键名包含大括号，则有效部分为大括号内的内容，若不包含大括号，则整个键名都是有效部分 可使用命令cluster slots查看插槽分配情况127.0.0.1:6000&gt; CLUSTER SLOTS1) 1) (integer) 0 2) (integer) 5460 3) 1) &quot;127.0.0.1&quot; 2) (integer) 6000 3) &quot;6808731e0d12e8ec740509ef060c81306d5cd9bd&quot; 4) 1) &quot;127.0.0.1&quot; 2) (integer) 6003 3) &quot;e219bc21f1ab59f4cf00ca1b35da84e955424556&quot;2) 1) (integer) 5461 2) (integer) 10922 3) 1) &quot;127.0.0.1&quot; 2) (integer) 6001 3) &quot;b098c2ddc169cc6e5411e8c42fb5afa96fa91764&quot; 4) 1) &quot;127.0.0.1&quot; 2) (integer) 6004 3) &quot;5c15caa067f96e557d73704e961ee08504fe3ac1&quot;3) 1) (integer) 10923 2) (integer) 16383 3) 1) &quot;127.0.0.1&quot; 2) (integer) 6002 3) &quot;9de0340daaf29f81b32c96d8010e9e443d66be0b&quot; 4) 1) &quot;127.0.0.1&quot; 2) (integer) 6005 3) &quot;8e885e28b530491468b76bb8084cf7c22a8166f4&quot;因为有3个master，所以有三条记录，每条记录中包含四个值：1) 插槽的开始号2) 插槽的结束号3) 所有负责该插槽的节点（第一个是主数据库，后面都是从数据库）。包含以下内容： 1) 节点的IP地址 2) 节点端口号 3) 节点运行ID 插槽的分配的情况 插槽之前没被分配过，现在想分配给指定节点 插槽之前被分配过，现在想移动到指定节点 将插槽分配给节点的过程 若是上述的第一种情况，即插槽未被分配过。使用cluster addslots [插槽号]....可分配多个插槽。若被分配过则会报错(error) ERR Slot 100 is already busy 若是第二种情况，即插槽被分配过。redis-trib.rb 提供简便迁移方法redis-trib.rb reshard 目标IP地址:端口其中reshard表示需要重新分片。 目标：将6000端口的插槽分1000个到6001端口redis-trib.rb reshard 127.0.0.1:6000# 然后会询问要迁移的插槽个数How many slots do you want to move (from 1 to 16384)? 1000# 询问要迁移到的节点ID（redis-trib.rb会给出，也可以进入数据库cluster nodes查看）What is the receiving node ID? b098c2ddc169cc6e5411e8c42fb5afa96fa91764# 询问从哪个节点开始移出插槽，输入6000端口节点的ID# 在结束输入后回车，并输入donePlease enter all the source node IDs. Type &#x27;all&#x27; to use all the nodes as source nodes for the hash slots. Type &#x27;done&#x27; once you entered all the source nodes IDs.Source node #1:6808731e0d12e8ec740509ef060c81306d5cd9bdSource node #2:done# 然后会要求再次确认，输入yesDo you want to proceed with the proposed reshard plan (yes/no)? yes# 再进redis查看cluster slots127.0.0.1:6000&gt; CLUSTER SLOTS1) 1) (integer) 1000 2) (integer) 5460 3) 1) &quot;127.0.0.1&quot; 2) (integer) 6000 3) &quot;6808731e0d12e8ec740509ef060c81306d5cd9bd&quot; 4) 1) &quot;127.0.0.1&quot; 2) (integer) 6003 3) &quot;e219bc21f1ab59f4cf00ca1b35da84e955424556&quot;2) 1) (integer) 0 2) (integer) 999 3) 1) &quot;127.0.0.1&quot; 2) (integer) 6001 3) &quot;b098c2ddc169cc6e5411e8c42fb5afa96fa91764&quot; 4) 1) &quot;127.0.0.1&quot; 2) (integer) 6004 3) &quot;5c15caa067f96e557d73704e961ee08504fe3ac1&quot;3) 1) (integer) 5461 2) (integer) 10922 3) 1) &quot;127.0.0.1&quot; 2) (integer) 6001 3) &quot;b098c2ddc169cc6e5411e8c42fb5afa96fa91764&quot; 4) 1) &quot;127.0.0.1&quot; 2) (integer) 6004 3) &quot;5c15caa067f96e557d73704e961ee08504fe3ac1&quot; 若不使用redis-trib.rb命令。也可通过cluster setslot命令分片。 cluster setslot 插槽号 node 新节点运行ID例：若要将上面分好的1000个插槽迁移回到6000管理前提：插槽中没有任何键。因为这样迁移时并不会连同相应键一起迁移，会造成键的丢失。 可通过cluster getkeysinslot 插槽号 要返回的键的数量获取指定插槽中的键，以查看要迁移的插槽中是否存在键。然后把每个键迁移迁移到目标节点： migrate 目标节点地址 目标节点端口 键名 数据库号 超时时间 [copy] [replace]`其中copy和replace可选，copy表示不会将键从当前数据库删除，只是复制。replace表示目标节点若存在同名键则覆盖。因为集群模式数据库只能使用0号数据库，所以数据库号始终是0 Redis 还提供以下命令实现集群不下线的数据迁移： clsuter setslot 插槽号 migrating 新节点运行IDcluster setslot 插槽号 importing 原节点运行ID迁移时若要把N号插槽从A迁移到B，需要如下操作在B执行cluster setslot N importing A在A执行clsuter setslot N migrating B在A执行cluster getkeysinslot N 获取N号插槽的键列表对列表的每个键都执行migrate命令执行cluster setslot 0 B 完成迁移 当客户端向集群中任一节点发送命令后，该节点都会判断相应键是否在当前节点，若在则立刻处理，若不在则返回一个 MOVE 重定向请求，告诉客户端目前负责该键的节点。返回的错误信息格式为：(error) MOVED 键所在的插槽号 IP地址:端口redis-cli也提供集群模式支持自动重定向，通过-c参数启动客户端。 集群中每个节点都会每隔 1 秒随机选 5 个节点，并选择其中最久无响应的节点发送一个 ping，若超时无回复，则变为主观下线，进行判断，与哨兵类似。选择主数据库的过程也与哨兵一致，都使用 Raft 算法。若一个至少负责一个插槽的主数据库下线且无相应从数据库可进行故障恢复，则整个集群默认会进入下线状态无法工作。也可修改配置文件的cluster-require-full-coverage设为 no，使集群在这种情况下继续工作。 管理安全可通过配置文件的requirepass参数设置密码，于是客户端每次连接数据库时必须发送密码验证，否则 Redis 会拒绝执行客户端发来的命令，会报错：(error)NOAUTH Authentication required。使用auth 密码验证。也可在 redis 中通过命令config set requirepass 密码设置密码。也可通过命令config get requirepass获取密码（已验证后才能看） 攻击者会通过穷举法破解 Redis 密码（1 秒内可尝试十几万个密码）。 配置 Redis 复制时，若主数据库设置了密码，需要在从数据库的配置文件中通过masterauth参数设置验证密码，在从数据库连接主数据库时会自动auth验证。 Redis 支持对命令的重命名，可在配置文件中的rename-command进行设置。格式为rename-command 命令 重命名后的命令。若要禁用某命令可直接将该命令重命名为空字符串即可。 通信协议Redis 支持两种通信协议： 统一请求协议：二进制安全 简单协议：便于在 telnet 中输入（已废弃） 简单协议中提供五种 telnet 返回值表示形式已被封装到 redis-cli 中，而就成为了 redis 的返回形式。 错误回复 error reply以-开头，并跟上错误信息，以\\r\\n结尾 状态回复 status reply以+开头，跟上状态信息，以\\r\\n结尾 整数回复 integer reply以:开头，跟上数值，以\\r\\n结尾 字符串回复 bulk reply以$开头，跟上字符串长度，\\r\\n分隔，再跟上字符串内容，再以\\r\\n结尾。若返回值为空，会返回$-1 多行字符串回复 multi-bulk reply以*开头，跟上字符串个数，\\r\\n分隔，再跟上字符串内容，再以\\r\\n结尾。 统一请求协议命令格式类似于多行字符串回复的格式，每个命令都可以包含二进制字符。Redis 的 AOF 文件和主从复制时发送的内容都使用了统一请求协议。若发送命令set foo bar，则在传输中的写法为*3\\r\\n$3\\r\\nSET\\r\\n*3\\r\\n$3\\r\\nFOO\\r\\n*3\\r\\n$3\\r\\nBAR\\r\\n。 一些管理命令 耗时命令日志当一条命令执行时间超时后，Redis 会将该命令的执行时间等信息加入耗时命令日志（slow log）。可通过配置文件的slowlog-log-slower-than参数设置该限制时间，单位为微秒（1s=10^6μs），默认为 10000μs。耗时命令日志会存储在内存中，也可通过配置文件slowlog-max-len设置记录的最多条数，默认 128。 可在 rediscli 中使用slowlog get获取当前耗时命令日志。每条日志由四个部分组成： 该日志的唯一 ID 该命令执行的 Unix 时间 该命令的耗时时间，单位微秒 命令和参数 命令监控Redis 提供monitor命令监控 Redis 执行的所有命令。在一个终端中输入monitor，便开始监视任何执行操作（该终端被挂起，不能执行命令）。 在第一个终端中输入127.0.0.1:6379&gt; MONITOROK在另一个终端中执行一条命令set foo bar于是在第一个终端中就会打印出以下内容1531104811.876088 [0 127.0.0.1:41664] &quot;COMMAND&quot;1531104820.923283 [0 127.0.0.1:41664] &quot;set&quot; &quot;foo&quot; &quot;bar&quot; monitor命令十分影响 redis 的性能，会降低近一半的负载能力，因此只适合进行排错和调试。 config get &lt;指定配置&gt;：获取服务器配置信息（就是配置文件中的参数）。 Redis 配置文件常用参数按照配置文件中的出现顺序 bind &lt;IPaddr&gt;： 指定 Redis 只接收该地址的请求。默认接收所有 IP 地址的请求，这样会造成安全隐患，最好填写需要调用 redis 的服务器的 IP 地址，或者直接写127.0.0.1仅允许本地用户调用。 daemonize yes|no：是否在后台运行。默认在前台运行 pidfile：PID 文件路径。若运行多个 Redis，则需要在各自的配置文件中指定不同的 PID 文件路径和端口 port：Redis 监听的端口，默认为 6379 timeout：客户端连接超时时间，单位秒。若客户端在超时前没发出任何指令，则会关闭连接。 logfile：日志文件路径 loglevel：日志等级，分为四个：debug、verbose、notice、warning，默认为 notice databases：数据库个数，默认设为 16 个 save &lt;seconds&gt; &lt;changes&gt;：redis 进行备份的频率。在多少秒内进行几次更新操作，就会触发备份，将数据同步到 RDB 文件。详见持久化 rdbcompression yes|no：是否开启 rdb 备份时压缩，默认开启 dbfilename：rdb 备份的文件名，默认为dump.rdb dir：rdb 备份文件存放路径。路径和文件名要分开配置，因为 Redis 备份时会先将当前数据库的状态写入一个临时文件，等备份完成后再把该文件替换为指定文件。 slaveof &lt;masterip&gt; &lt;masteport&gt;：在从数据库上设置，指定主数据库 masterauth &lt;master-password&gt; ：当主数据库连接需要密码验证时，在此指定密码 requirepass：设置客户端连接后进行任何其他操作前需要的密码。 因为 Redis 速度快，所以外部用户可以每秒进行 150000 次密码尝试，若密码简单，很容易被破解。 maxmemory &lt;bytes&gt;：设置 Redis 最大能使用的内存，单位字节。当分配的内存完全被占用后，若再接收到 set 命令，则 Redis 会先删除设置了 expire 的键，无论是否到期。若所有 expire 的键都被删除了，则 redis 不再进行 set 操作，只允许 get 操作。此参数适合把 redis 当做类似 Memcached 缓存来使用 maxclients：限制同时连接的客户数。当连接数超过该值，则不再接收，并返回 error。 appendonly yes|no ：默认情况下，redis 会在后台异步把数据库镜像备份到此磁盘，但这样备份非常耗时，且不能很频繁，若断电会造成大量数据丢失。若开启 appendonly，则 redis 会将接收到的每一次写操作追加到appendonly.aof中，当 redis 重启时，会从该文件恢复到之前的状态。但这样容易造成该文件过大。可通过指令BGREWRITEAOF对该文件整理。 appendfsync always|everysec|no：详见持久化 vm-enabled：是否开启虚拟内存支持。当内存不够时，会把 value 存放到交换区（swap）中。性能基本不受影响。同时要将vm-max-memory设置足够大以存放所有 key vm-max-memory：开启虚拟内存后 redis 可用的最大内存大小，默认为 0。在生产环境中，最好不要设为 0，根据实际情况调整。 在 Docker 上搭建 RedisFAQ Redis 内存用完怎么办？ redis 的性能会下降，并可能出错。可通过 info 查看 redis 内存使用情况，并写脚本进行监控。可通过配置文件maxmemory调整 redis 可用内存大小。若 redis 的内存使用达到上限，即给出错误写入的命令，仅能接受只读命令。 使 redis 内存使用率降低的方法？ 最好使用 redis 的哈希、列表、排序集、整数集 如果没有数据集较大的内存，怎么进行高层次的操作？ 使用客户端哈希，并部署 redis 集群，自动分发，以及 redis 子集的热备 单线程 Redis 如何利用多个 CPU？ 只需在同一台机器上启动多个 redis 实例，当做不同服务器即可。 Redis 报错问题 Redis 被配置为保存数据库快照，但它目前不能持久化到硬盘。用来修改集合数据的命令不能用 报错：(error) MISCONF Redis is configured to save RDB snapshots, but it is currently not able to persist on disk. Commands that may modify the data set are disabled, because this instance is configured to report errors during writes if RDB snapshotting fails (stop-writes-on-bgsave-error option). Please check the Redis logs for details about the RDB error. 原因：强制关闭 Redis 快照导致不能持久化解决：将配置文件stop-writes-on-bgsave-error设置为 no 参考资料 Redis 入门指南（第二版）Linux_基于 Docker 搭建 Redis 集群 &gt; Docker Redis 的官方镜像简单使用 &gt; 【Redis】基于 Redis3.2.4 集群搭建说明 高性能网站构建实战","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://coconutmilktaro.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"Redis","permalink":"https://coconutmilktaro.top/tags/Redis/"}]},{"title":"运维小技巧整理","slug":"运维小技巧整理","date":"2018-05-09T07:15:30.000Z","updated":"2022-05-30T02:51:54.005Z","comments":true,"path":"2018/运维小技巧整理/","link":"","permalink":"https://coconutmilktaro.top/2018/%E8%BF%90%E7%BB%B4%E5%B0%8F%E6%8A%80%E5%B7%A7%E6%95%B4%E7%90%86/","excerpt":"网上、书上的运维技巧整理","text":"网上、书上的运维技巧整理 删除脚本-1为防止rm -rf失误造成破坏，可将删除写成一个脚本remove.sh 首先在指定目录创建.trash目录，作为回收站 创建脚本，放在一个固定位置/root/shell/vim /root/shell/remove.sh #!/bin/bashTRASH_DIR=&quot;/root/.trash&quot;for i in $*;do TIME_STAMP=$(date +%F) filename=$i mv $i $TRASH_DIR/$TIME_STAMP.$filename done 设置rm别名vim /root/.bashrc修改alias rm=&quot;sh /root/shell/remove.sh&quot; 设置定时任务echo &quot;0 0 * * * rm -rf /root/.trash/*&quot; &gt;&gt; /etc/crontab这样删除的文件也能尽快恢复","categories":[],"tags":[{"name":"运维","slug":"运维","permalink":"https://coconutmilktaro.top/tags/%E8%BF%90%E7%BB%B4/"},{"name":"linux","slug":"linux","permalink":"https://coconutmilktaro.top/tags/linux/"}]},{"title":"Nginx笔记","slug":"Nginx笔记","date":"2018-05-02T09:20:03.000Z","updated":"2022-06-21T15:42:26.602Z","comments":true,"path":"2018/Nginx笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Nginx%E7%AC%94%E8%AE%B0/","excerpt":"基于 Nginx1.14-1.16","text":"基于 Nginx1.14-1.16 Nginx 介绍Nginx 有以下功能： 负载均衡 HTTP web 服务器 http 协议的反向代理服务器 pop3\\smtp\\imap4 等邮件协议的反向代理 能缓存打开的文件（元数据） ，支持 FastCGI、uWSGI 协议，作为缓存服务器 模块化（非 DSO 机制），过滤器 zip，SSI，SSL 特性： 模块化设计，较好扩展性 高可靠性：master/worker 支持多进程，也支持多线程 支持热备份：不停机更新配置文件，更换日志，更新服务器程序版本 低内存消耗：10000 个 keep-alive 连接模式下非活动连接仅消耗 2.5M 内存 热部署（平滑升级）：旧的配置维持现状，新的配置立刻使用，并在使用中逐步自动将旧配置替换为新配置。 Nginx 架构：nginx 会以 daemon 方式启动进程，后台进程包含一个 master 进程和多个 worker 进程。master 进程用于启动管理多个 worker 进程，若取消 master 进程，则 nginx 会以单进程运行一个请求只能在一个 worker 进程中处理，一个 worker 进程只能处理一个请求。worker 进程个数一般设置为 cpu 核数 master：加载配置文件、管理 worker 进程、平滑升级 worker：http 服务、http 代理、fastcgi 代理 事件驱动：epoll 消息通知：select、poll、rt signals 模块类型：核心模块、标准 http 模块、可选 http 模块、邮件模块、第三方模块 Master 进程完成的工作： 读取并验证配置文件 创建、绑定及关闭套接字 启动、终止及维护 worker 进程 无需终止服务而重新配置 控制非中断式程序升级（平滑升级），启用新的二进制程序，并且能在需要时回滚到老版本 重新打开日志文件 可编译嵌入式 perl 脚本 Worker 进程完成的工作： 接收、传入并处理来自客户端的请求 提供反向代理及过滤功能 请求过程： 在 master 进程里面，先建立好需要 listen 的 socket（listenfd）之后，然后再 fork 出多个 worker 进程 所有 worker 进程的 listenfd 会在新连接到来时变得可读，为保证只有一个进程处理该连接，所有 worker 进程在注册 listenfd 读事件前抢 accept_mutex，抢到互斥锁的那个进程注册 listenfd 读事件，在读事件里调用 accept 接受该连接。 当一个 worker 进程在 accept 这个连接之后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接 IO 复用： ​ 单进程模型：阻塞，一次仅处理一个请求，无法应对高并发场景 ​ 多进程模型：每个进程响应一个请求。缺陷：当请求量庞大时，会占用大量 CPU 及内存资源，因为每个请求访问的页面都会单独缓存，若访问的页面相同，会造成内存使用效率低，且进程间切换会消耗大量 CPU 资源 ​ Linux 原生并不支持线程，Windows 和 SunOS 都支持。Linux 将线程当做进程来处理，但称作 LWP 轻量级进程（Light Weight Process），Linux 进行线程管理是通过调用各种线程库实现的。 ​ 多线程单请求模型：每个线程响应一个请求。线程和进程类似，也需要切换，但线程是轻量级的进程，切换消耗的资源很小。且同一进程内的线程共享一片存储空间，若有访问同一资源的请求，也可直接从存储空间调用该资源。线程对内存资源的需求量也比进程要小。而如果 CPU 数量只有 1 个，则多线程的优势基本无法体现，只有在多颗 CPU 的情况下，多线程才能发挥出极高的效率。 ​ 多线程多请求模型：一个线程响应多个请求。 Nginx 安装在安装 nginx 前需要安装以下环境： gcc 与 gcc-c++等，可直接组安装 Development Tools prce-devel(perl 的正则表达库) zlib 与 zlib-devel(资料压缩的函数库) openssl 与 openssl-devel(安全套接字密码库) 注：带 devel 的包是一定要安装的。 创建系统用户组groupadd -r nginx创建系统useradd -r nginx -M -g nginx -r创建系统用户（组），-M不创建该用户家目录 创建目录/var/tmp/nginx/client，否则在后面运行时可能报错。 初始化文件模块配置 ./configure --prefix=/usr/local/nginx \\ # 设置nginx安装目录 --sbin-path=/usr/sbin/nginx \\ # 命令放在/sbin下 --conf-path=/etc/nginx/nginx.conf \\ # 配置文件位置 --pid-path=/var/run/nginx/nginx.pid \\ # nginx进程号文件 --lock-path=/var/lock/nginx.lock \\ # nginx锁文件 --user=nginx \\ # 指定用户 --group=nginx \\ # 指定用户组 --http-log-path=/var/log/nginx/access.log \\ # 运行日志位置 --error-log-path=/var/log/nginx/error.log \\ # 报错日志 --http-client-body-temp-path=/var/tmp/nginx/client \\ # 指定客户端post上传的$_FILES上传的文件地址，该目录需要自己创 --with-http_ssl_module \\ # 加载ssl模块，默认没加载 --with-http_stub_status_module \\ # 加载监控模块 --with-http_gzip_static_module \\ # 加载gzip压缩模块 --with-debug # 允许debug 之后make &amp;&amp; make install 也可以通过配置的 yum 源安装，此例是 centos7 的 repo 配置。然后直接yum install最新的版本。 [nginx]name=nginx.repobaseurl=https://nginx.org/packages/centos/7/x86_64/gpgcheck=0enabled=1 nginx 命令 nginx -t #检查配置文件语法 -c #指定配置文件 -V #查看编译信息 -s #指定对nginx的操作 stop #停止（快速关闭） quit #退出（优雅关闭，所有worker进程会停止接收新连接，然后将所有未处理完的请求处理完后关闭） reopen #重新打开日志文件 reload #重新读取配置文件（平滑重启） -q #在配置测试期间不显示非错误消息 Nginx 配置文件解析配置文件/etc/nginx.conf配置文件组织结构： 全局配置：main 模块配置：events，http，server，location 注：配置的每条指令的结尾必须加上分号; 全局配置main 块 正常运行配置： user Username [Groupname] # 指定运行worker进程的用户pid PATH # 指定nginx进程的pid文件worker_rlimit_nofile # 指定一个worker进程所能打开的最大文件描述符数量worker_rlimit_sigpending # 指定每个用户能发给worker进程的最大的信号数量 性能优化配置： worker_processes # worker进程个数，通常为物理CPU核心数量-1可填auto，自动使用所有CPU。worker_cpu_affinity CPUMask # 指定使用的cpu，cpumask为cpu掩码。# cpumask：0001,0010,0100,1000work_priority NICE # 指定nice值，即优先级 worker_processes和work_connections决定了最大并发数量。最大并发量即为worker_processes × work_connections 而在反向代理场景，因为 nginx 既要维持和客户端的连接，又要维持和后端服务器的连接，因此处理一次连接要占用 2 个连接，所以最大并发数为：worker_processes × worker_connections/2 Nginx 可能还会打开其他的连接，这些都会占用文件描述符，影响并发数量 最大并发数量还受”允许打开的最大文件描述符数量”限制，可以使用worker_rlimit_nofile指令修改。或直接通过ulimit -n修改 调试定位配置： daemon &#123;on|off&#125; # 是否以守护进程方式启动master_process &#123;on|off&#125; # 是否以master模型运行error_log PATH [level] # 错误日志文件路径及日志等级。#日志等级：debug|info|notice|warn|error|crit|alert|emerg，默认为error# 可设置为debug，但需要在编译时指定--with-debug才能用# 若要禁用error_log 则可设置为error_log /dev/null; 模块配置events 块events &#123; &#125;事件驱动，并发响应连接。控制 Nginx 处理连接的方式包含以下配置： worker_connections # 每个worker进程能响应的最大并发请求数量use &#123;epoll | select | poll &#125; # 选择使用事件类型，最好让nginx自动选择accept_mutex &#123;on|off&#125; # 是否开启负载均衡锁，启用时，表示让多个worker进程轮流响应请求。默认开启lock_file PATH # 锁文件路径 http 块http &#123; &#125;处理 http 响应的配置 注：http 块下的文件相对路径是相对于 Nginx 的配置文件目录 包含以下配置： #include 用于引入配置文件#设置mime.types文件路径，若是相对地址则是相对于nginx.conf配置文件include mime.types;#default_type用于设置默认文件类型#若在编译时设置了配置文件目录，则此项路径为/usr/share/mime/application/octet-stream.xmldefault_type application/octet-stream;#日志格式#log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;#日志文件路径，由于已设置了/var/log/nginx/access.log，所以不需要改#access_log logs/access.log main;#开启高效传输文件模式，默认为on。#可以让Nginx在传输文件时直接在磁盘和tcp socket之间传输数据，而不用经过任何buffersendfile on;#在sendfile为on时，确定是否启用tcp_nopush(FreeBSD)或tcp_cork(linux)#会将多个http首部打包为单个报文。用于防止网络阻塞，但会额外占用资源#tcp_nopush on;tcp_nodelay &#123;on| off&#125;#tcp 有一个nagle算法，将多个小的数据块打包传输以提高带宽利用率。#有时只需要访问一个较小页面，但由于nagle算法，要等待其他响应以打包为大数据块，所以会等较长时间才会返回页面。对keepalive模式下连接是否使用tcp_nodelay。通常关闭。#设置keepalive长连接的超时时间。默认为65。若为0则保持长连接#keepalive_timeout 0;#设置keepalive连接上最大请求资源数，默认100keepalive_requests 数量#指明禁止指定浏览器使用keepalive功能keepalive_disable &#123;none|browser&#125;#设置发送响应报文的超时时长，默认60ssend_timeout 时长#接收客户端报文body的缓冲区大小，默认8k（32位|16k--64位）超出该大小时，会被存储于磁盘client_body_buffer_size 数值#指定存储客户端报文的路径及子目录的结构数量#就是编译时--http-client-body-temp-path指定的路径client_body_temp_path PATH &#123;level1 | level2 | level3&#125;#例：client_body_temp_path /var/tmp/client_body 2 2 表示该目录下有2层目录，每层有两个子目录#是否开启gzip压缩响应报文#gzip on;#指定错误页面#error_page 404 /404.html;error_page 500 502 503 504 /50x.html;#更改响应状态码，隐藏服务器的真实状态码信息#error_page 404=200 /404.html;#设置由重定向后实际的处理结果来决定状态码#error_page 404=/404.html;server &#123; &#125; # 虚拟主机模块 server 块#虚拟主机监听的本地端口和主机名listen 80;server_name localhost;#设置编码类型，为防止网页出现乱码，需要将charset设置gb2312或utf-8。#charset koi8-r;#Location，URI的根location / &#123; root html; index index.html index.htm;&#125;location /XXX &#123;&#125; #location块#在响应首部中添加字段add_header 字段名 值try_files [页面] [uri | =code]#设置尝试打开的文件，若都找不到再返回最后一个uri#最后一个uri必须存在，且必须由别的location定义，不能在当前的location中，否则会死循环#例：try_files $uri xxx.html 先找用户需要的uri，若找不到返回xxx.html#显示访问连接信息。#stub_status on; #注：要放入server块# 然后设置location#location /status &#123;# stub_status;#&#125;#便可通过域名/status 查看。Active connections: 164server accepts handled requests 7595 7595 7601Reading: 0 Writing: 164 Waiting: 0# 当前活动连接数已接收的连接个数，已处理的连接个数，已经处理的请求个数正在读取客户端的Header个数（Reading），正在返回给客户端的Header个数（Writing），长连接或等待状态的请求个数（Waiting）（当开启Keepalive时，该值为Active-Reading-Writing）。 location 块目录映射。允许根据用户请求的 URI 匹配定义的 location，匹配到就按该 location 中的配置处理 location 的第一个/为 root 中指定路径的最后的/ alias 设置别名，用于定义路径别名，只用于 location 配置段。 location /images &#123; root /data/www/imgs/ ; # 即表示在URL中访问/images即为访问服务器中/data/www/imgs/images/目录 # root为将location接在root指定的路径后&#125;location /images &#123; alias /data/www/imgs/ ; # 即表示在URL中访问/images即为访问服务器的/data/www/imgs/ 目录 # alias为整段替换&#125; 与 root 和 alias 指令相关的变量为$document_root、$realpath_root。 $document_root的值即是 root 指令、alias 指令的值 $realpath_root的值是对 root、alias 指令进行绝对路径换算后的值 location匹配 不带符号 URI：匹配该资源目录下的所有资源 = URL 精确匹配，只匹配该资源 ~ 正则表达式匹配，区分大小写 ~* 正则表达式匹配，不区分大小写（例如 location ~* \\.(gif|jpg)$ 匹配.gif或jpg结尾的文件） ^~ URI 左半部分匹配，非正则匹配，不区分大小写 优先级：= &gt; ^~&gt;~或~*&gt; 不带符号的 URI 若要将 Nginx 作为文件存放服务器，在 location 块中添加autoindex on;即可，并且要确保文件存放的目录中没有首页文件。 访问控制、身份认证与 SSLNginx 的两个配置访问控制的指令allow和deny，由模块ngx_http_access_module提供。 allow|deny IP地址[/mask]allow|deny all 若同一个块中同时配置了多条deny或allow，则先出现的访问权限生效。若多个块中都配置了权限指令，则内层的优先级高于外层的。 被访问控制 deny 的 IP 地址访问时会返回403状态码。 Nginx 的 basic 身份认证指令auth_basic和auth_basic_user_file由模块ngx_http_auth_basic_module提供。可配置在 http、server、location 块中 auth_basic 描述 | off; 是否开启http_basic对用户认证auth_basic_user_file FILE; 指定用户认证的账号文件 可通过 Apache 的工具htpasswd创建用户认证文件 htpasswd -c -m -b /etc/nginx/secret mike 123456 使用 SSL 将网站配置为 HTTPS 注：如果是编译安装，需要在构建时添加--with_http_ssl_module支持 SSL。 首先进入目录/etc/pki/CA/private下创建服务器 RSA 私钥，叫server.key。 openssl genrsa -out server.key 2048 生成服务器的 CSR 证书请求文件。CSR 证书请求文件是服务器的公钥，用于提交给 CA 机构进行签名。 openssl req -new -key server.key -out server.csr按要求填写信息即可 Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:JiangsuLocality Name (eg, city) [Default City]:Yangzhou....Common Name (eg, your name or your server&#x27;s hostname) []:system3.example.com CSR 参数含义： 参数 说明 Country Name 符合 ISO 的两个字母的国家代码，中国 CN State or Province Name 省份 Locality Name 城市 Organization Name 公司名 Organizational Unit Name 组织名/部门名 Common Name 使用 SSL 加密的域名，不能填错 Email Address 邮箱，可省略 A challenge password 有的 CA 机构需要此密码，通常省略即可 An optional company name 可选的公司名，可省略 CA 为服务器认证证书 openssl x509 -req -days 30 -in server.csr -signkey server.key -out server.crt 在 CA 用私钥签名了证书后，该证书将用于浏览器验证请求的网站是否真实，防止网络通信过程中被篡改。 浏览器保存了受信任的 CA 机构的公钥，在请求 HTTPS 网站时，会利用 CA 公钥验证服务器证书，并检查域名是否吻合、证书是否过期等。 在 Nginx 配置文件中设置 https 配置。Nginx 配置文件中已默认存在 SSL 配置，只是注释了，取消注释，并修改 crt 文件路径即可。 server &#123; listen 443 ssl; server_name system3.example.com; ssl on; # 这条配置文件可能没有，要加上 ssl_certificate /etc/pki/CA/private/server.crt; # SSL认证证书路径 ssl_certificate_key /etc/pki/CA/private/server.key; # SSL认证密钥路径 ssl_session_cache shared:SSL:1m; # 用于存储SSL会话的高速缓存的类型和大小 ssl_session_timeout 5m; # 客户端可以重复使用存储在缓存中的会话参数的时间 ssl_ciphers HIGH:!aNULL:!MD5; # openssl允许的加密类型 ssl_prefer_server_ciphers on; location / &#123; root html; index index.html index.htm; &#125; &#125; location / &#123; proxy_set_header X-FORWARDED-PROTO https; #将http协议头换为https proxy_pass http://upstream; &#125;&#125; 在/etc/hosts中确定配置了 system3 的条目。在浏览器上输入https://system3.example.com会说明此连接不安全。是因为当前的证书是服务器自己作为 CA 签名的，所以浏览器无法信任。点添加例外，可查看该证书，与配置的一致。 添加安全例外后，即可访问网页 Nginx 日志关于 Nginx 日志的指令如下： log_format：日志格式。格式：log_format 格式名 格式（由变量组成），默认格式如下： &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39; &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39; &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#39; $remote_addr：客户端的地址。如果 Nginx 服务器在后端，则该变量只能获取到前端（代理或负载均衡服务器）的 IP 地址 $remote_user：远程客户端用户名称 $time_local：记录访问时间和时区信息 $request：记录用户访问时的 url 和 http 协议信息 $status：记录客户端请求时返回的状态码 $body_bytes_sent：记录服务器响应给客户端的主体大小 $http_referer：记录此次请求是从哪个链接过来的，需要模块ngx_http_referer_module支持，默认已装，可以防止盗链问题 $http_user_agent：记录客户端的浏览器信息。如使用什么浏览器发起的请求，是否是压力测试工具在测试等 $http_x_forward_for：若要在$remote_addr中获取的是真正客户端的 IP，则要添加此项，并且在前端的服务器要开启x_forward_for access_log：访问日志的路径和采用的日志格式名。格式：access_log path [format [buffer=size] [gzip[=level]] [flush=time] [if=condition]];。若不指定格式名，默认为 combined。buffer 为日志缓冲区的大小，默认 64K。flush 为日志刷盘时间，即日志保存在缓冲区的最大时间。gzip 为日志刷盘前是否压缩以及压缩级别。if 指定条件。 open_log_file_cache：设置日志文件缓存。默认关闭，即每一条日志都是打开文件，写入后关闭，这样会消耗一定量的 IO。而设置了缓存后，会等日志数量达到一个指标后一下写入日志文件。 格式：open_log_file_cache max=N [inactive=time] [min_uses=N] [valid=time];。其中 max 为缓存中的最大文件描述符数量。inactive 为存活时间，默认是 10s。min_uses 为在 inactive 时间段内，日志文件最少使用多少次后，该日志文件描述符记入缓存中，默认是 1 次。valid 为检查频率，默认 60s。 log_subrequest：是否在 access.log 中记录子请求的访问日志。默认不记录 rewrite_log：是否在 error.log 中记录 notice 级别的重写日志。默认关闭 日志切割默认 Nginx 不会自动切割日志。 手动切割： mv access.log access-XXXX.lognginx -s reopen 自动切割： 创建 shell 脚本 #!/bin/bashACCESS_LOG=/var/log/nginx/access.logNEW_ACCESS_LOG=/var/log/nginx/access-$(date -d yesterday +%Y%m%d).log[ -f $ACCESS_LOG ] || exit 1 mv $ACCESS_LOG $NEW_ACCESS_LOG /sbin/nginx -s reopen 添加定时任务 0 0 * * * /bin/sh XXX.sh &amp;&gt; /dev/null #每天晚上12点切割日志 Nginx 的其他常见 HTTP 功能文件查找规则Nginx 为了响应一个请求，它将请求传递给一个内容处理程序，由配置文件中的location指令决定处理。 无条件内容处理程序首先被尝试 ：perl 、proxy_pass、flv、mp4等。如果这些处理程序都不匹配，那么 Nginx 会将该请求按顺序传递给下列操作之一，顺序依次是 ：random index 、 index 、 autoindex 、gzip_static、 static 。 处理以斜线结束请求的是 index 处理程序。如果 gzip 没有被激活，那么 static 模块会处理该请求。 这些模块如何在文件系统上找到适当的文件或者目录则由某些指令组合来决定。**root 指令最好定义在一个默认的 server 指令内 ， 或者至少在一个特定的 location 指令之外定义，以便它的有效界限为整个 server。** 文件路径指令： 指令 说明 root 设置文档根目录，URI 后设置，可在文件系统中找到具体文件 try_files 测试文件的存在性，若前面的文件没找到，则最后的条目作为备用，需要确保最后一个路径或 location 存在 例： location / &#123; try_files $uri $uri/ /40x.html;&#125; 域名解析 指令 说明 resolver 设置一个或多个域名解析服务器。可选 valid 参数，会覆盖域名记录的 TTL resolver_timeout 当解析服务器不可达时的等待时间，尽量设置的小。目前只适用于商业订阅 该指令需要配置在 server 块中 server &#123; resolver 8.8.8.8 valid=300s; resolver_timeout 2s;&#125; 客户端交互Nginx 与客户端交互的方式有多种，这些方式可以从连接本身 （IP 地址、超时、存活时间等）到内容协商头的属性。 指令 说明 default_type 设置响应的默认 MIME 类型。只有文件的 MIME 类型不匹配任何 types，才会用此项 types 设置 MIME 类型到文件扩展名的映射，一般不用设置。只需要载入conf/mime.types即可 ignore_invalid_headers 禁止忽略无效名字的头。一个有效的名字由 ASCII 字母、数字、连字符号组成 recursie_error_pages 启用 error_page 实现多个重定向 Nginx 缓存Nginx 的模块ngx_http_proxy_module自带缓存功能，Nginx 可实现几种缓存：网页内容缓存，日志缓存，打开文件缓存，fastcgi 缓存。Nginx 的缓存功能主要由proxy_chache相关的命令集和fastcgi_cache相关命令集构成，前者用于反向代理，后者用于对 FastCGI 缓存。 需要下载第三方的 Nginx 模块ngx_cache_purge，用于清除指定的 URL 缓存，下载地址 下载后解压，然后重新编译 Nginx，通过nginx -V查看编译选项，复制过去然后加上解压后的模块目录（不是里面的.c 文件） ./configure........ --add-module=/root/ngx_cache_purge-2.3 编译后能看到新的模块添加成功的信息 configuring additional modulesadding module in /root/ngx_cache_purge-2.3 + ngx_http_cache_purge_module was configured 然后只要make就行，不能make install，不然就是重新安装 Nginx 了 最后还要替换 nginx 启动文件，在重新编译后，nginx 的下载目录中有个objs目录，将里面的nginx启动脚本复制到/usr/sbin/下，替换原来的nginx rm -f /usr/sbin/nginxcp /root/nginx-1.14.0/objs/nginx /usr/sbin/ 通过nginx -V查看，--add-module=/root/ngx_cache_purge-2.3已成功编译 Nginx 自带的缓存相关指令有三个： proxy_cache_path：配置缓存存放路径。常用格式：proxy_cache_path path [levels=levels] keys_zone=name:size [max_size=size] [inactive=time];。 levels：缓存目录级别以及缓存目录名的字符数，数字间用冒号分隔。最多可有三层目录，目录名最多两个字符。例：levels=1:2:2表示一共三层目录，第一层目录名 1 字符，第二和第三层目录名为 2 字符。 keys_zone：缓存标识名和内存中缓存的最大空间。名字必须唯一。可在keys_zone的值后加上:内存缓存空间指定内存中的缓存空间大小 max_size：磁盘中缓存目录的最大空间 inactive：缓存的默认时长，到达指定时间却没被访问的缓存会被自动删除 proxy_cache：指定要使用的缓存方法，使用proxy_cache_path中keys_zone指定的名称。格式：proxy_cache 缓存标识名; proxy_cache_valid：根据状态码指定缓存有效期。格式：proxy_cache_valid 状态码（可指定多个，或直接指定any） 时间（支持m|h等时间单位）; 在配置文件添加缓存配置。首先在 http 块下（server 块外）添加缓存路径，路径可根据需要自定义 proxy_temp_path /data/ngxcache/proxy_temp_dir;proxy_cache_path /data/ngxcache/proxy_cache_dir levels=1:2 keys_zone=cache_one:1000m max_size=1g; Nginx 启动后，缓存加载程序只进行加载一次，加载时会将缓存的元数据加载到共享内存区域，但是如果一次加载整个缓存全部内容可能会使 Nginx 刚启动的前几分钟性能消耗严重，大幅度降低 Nginx 的性能。所以可以在 proxy_cache_path 命令中配置缓存迭代加载。缓存迭代加载一共可以设置三个参数： loader_threshold - 迭代的持续时间，以毫秒为单位(默认为200)loader_files - 在一次迭代期间加载的最大项目数(默认为100)loader_sleeps - 迭代之间的延迟(以毫秒为单位)(默认为50) 例：proxy_cache_path /data/ngxcache/proxy_cache_dir keys_zone=cache_one:10m loader_threshold=300 loader_files=200; proxy_cache_methods[GET HEAD POST]; 在虚拟服务器下配置proxy_cache_methods命令可以指定该虚拟服务器下什么类型的 HTTP 方法可以被缓存。默认情况下 GET 请求及 HEAD 请求会被缓存，而 POST 请求不会被缓存。 proxy_cache_bypass $cookie_nocache $arg_nocache$arg_comment; proxy_cache_bypass命令可以配置不会向客户端响应缓存，而是直接将请求转发给后端服务进行请求数据。可以通过上述命令配置需要绕过缓存的请求 URL，也就是说 URL 中包含该配置的值，则这次请求会直接跳过缓存直接请求后端服务去获取数据。 proxy_cache_min_uses 2; proxy_cache_min_uses命令可以设置当某请求最少响应几次后会被缓存。若设置为 2 则表示每个请求最少被请求 2 次后会加入到缓存中。 例：简单添加 location 缓存配置 location / &#123; proxy_cache cache_one; proxy_cache_valid 200 1h; # 状态码为200的缓存1小时 proxy_cache_valid 404 1m; proxy_cache_valid any 5m; # 剩余的所有状态都缓存5分钟&#125; 当开启缓存后，Nginx 会生成进程cache_manager对缓存进行管理。 nginx 1359 1356 0 11:23 ? 00:00:00 nginx: cache manager processnginx 1360 1356 0 11:23 ? 00:00:00 nginx: cache loader process 可在server块中添加add_header X-Cache $upstream_cache_status;，该参数用于显示缓存状态返回值，一共有七种： 返回值 说明 HIT 缓存 MISS 未命中，请求被传送到后端 EXPIRED 缓存已过期，请求被传到后端 UPDATING 正在更新缓存，将使用旧的应答 STALE 无法从后端服务器更新缓存时，返回了旧的缓存 BYPASS 缓存被绕过了 REVALIDATED 在启用 proxy_cache_revalidate 后，当缓存内容过期后时，Nginx 通过一次 If-Modified-Since 的请求头去验证缓存内容是否过期，此时会返回该状态 add_header X-Cache &quot;$upstream_cache_status from $server_addr&quot;; 完整的配置： proxy_temp_path /data/ngxcache/proxy_temp_dir;proxy_cache_path /data/ngxcache/proxy_cache_dir levels=1:2 keys_zone=cache_one:1000m max_size=1g;upstream web &#123; server 172.16.246.134; server 172.16.246.135;&#125;server &#123; listen 80; server_name web; root /usr/share/nginx/html; add_header X-Cache &quot;$upstream_cache_status from $server_addr&quot;; location / &#123; proxy_cache cache_one; proxy_cache_valid 200 1h; proxy_pass http://web;&#125; 通过浏览器访问http://web。如果是第一次访问，会发现状态是 MISS 若再次访问，则状态会变为 HIT。 并且可以看到缓存目录下已建立缓存 /data/ngxcache/proxy_cache_dir/├── 5│ └── a4│ └── 383e066904ed363cfafab47f9f53fa45└── 6 └── 01 └── abca5f5d9926bd7fbfc52390d3d35016 另外常用的三种缓存： open_log_cache：日志缓存 open_file_cache：文件缓存 fastcgi_cache：fastcgi 缓存 清除缓存如果缓存过期则需要从缓存中删除过期的缓存文件，防止新旧缓存出现交错出错，当 Nginx 接收到自定义 HTTP 头或者 PURGE 请求时，缓存将会被清除。 在 HTTP 节点下创建一个新变量$purge_method 来标识使用 PURGE 方法的请求并删除匹配的 URL。 http &#123; map $request_method $purge_method &#123; PURGE 1; default 0; &#125;&#125; 进入虚拟服务器配置，在 location 中配置高速缓存，并且指定缓存清除请求命令proxy_cache_purge。 server &#123; listen 80; server_name web; location / &#123; proxy_cache cache_one; proxy_cache_purge $purge_method; &#125;&#125; 发送清除命令 配置proxy_cache_purge指令后需要发送 PURGE 请求来清除缓存。例如使用 PURGE 方式请求 url: PURGE web/app 则 app 对应的缓存中的数据将被删除。但是，这些高速缓存数据不会从缓存中完全删除，它们将保留在磁盘上，直到它们被删除为非活动状态，或由缓存清除进程处理。 限制 IP 访问清除命令：清除缓存这种命令一般需要权限才可进行操作，所以一般需要配置允许发送缓存清除请求的 IP 地址： geo $purge_allowed &#123; default 0; 192.168.0.1/24 1;&#125;map $request_method $purge_method &#123; PURGE $purge_allowed; default 0;&#125; 当 Nginx 接收到清除缓存请求时，Nginx 检查客户端 IP 地址，若 IP 地址已经获得清除缓存权限，则$purge_method设置为$purge_allowed，值为 1 表示允许清除缓存，值为 0 表示表示 IP 地址未获得权限。 从缓存中完全删除文件：高速缓存数据不会从缓存中完全删除，它们将保留在磁盘上，直到它们被删除为非活动状态，或由缓存清除进程处理。要完全删除与 getArticle 相匹配的缓存数据，需要在proxy_cache_path添加参数 purger，该参数表示永久的遍历所有缓存条目，并删除与通配符相匹配的条目。 proxy_cache_path /data/ngx_cache/proxy_cache_dir keys_zone=cache_one:10m purger=on; Nginx 负载均衡Nginx 提供负载均衡模块ngx_http_upstream_module，且 Nginx 有四种典型的负载均衡配置方式。 配置方式 说明 轮询 round-robin 默认配置方式，每个请求按照时间顺序逐一分配到不同后端服务器 权重 利用 weight 指定轮询的权重比率，weight 与访问比率成正比。用于后端服务器性能不均的情况 ip_hash 每个请求按访问 IP 的 hash 结果分配，使每个访客固定访问一个后端服务器，可解决 Session 问题 fair（第三方） 按后端服务器响应时间来分配请求，响应时间短的优先 url_hash（第三方） 按访问 URL 的哈希结果来分配请求，使每个 URL 固定访问一个后端服务器。适用于后端服务器做缓存的情况 一致性 Hash（Tengine） 将每个服务器虚拟成 N 个节点，均匀分布在哈希环上，每次请求时会根据配置参数计算出一个 Hash，在哈希环上查找离这个哈希值最近的虚拟节点，对应服务器作为该次请求的后端服务器。好处：若增加或减少了机器，对整个集群的影响会最小 Nginx 作负载均衡的优点： 配置简单 成本低 支持 Rewrite 重写规则 有内置的健康检查功能 节省带宽：支持 Gzip，可添加浏览器本地缓存的 Header 稳定性高：高并发的情况下也基本不会宕机 轮询实验环境： 负载均衡器：172.16.246.133 后端服务器 1：172.16.246.134 后端服务器 2：172.16.246.135 仅需在负载均衡服务器上配置 server&#123; listen 80; server_name sys1.example.com; location / &#123; proxy_pass http://sys1.example.com; &#125;&#125;upstream sys1.example.com &#123; server 172.16.246.134; server 172.16.246.135;&#125;# 实际上，轮询也是有weight的，只是所有的server的weight都为默认的1。 在本机上进行访问 &gt; curl sys1.example.comWEB1&gt; curl sys1.example.comWEB2&gt; curl sys1.example.comWEB1&gt; curl sys1.example.comWEB2 权重仍在负载均衡器上配置，仅修改 upstream 块 upstream sys1.example.com &#123; server 172.16.246.134 weight=1 max_fails=1 fail_timeout=2; server 172.16.246.135 weight=3 max_fails=2 fail_timeout=2;&#125; 参数 说明 weight 权重 max_fails 允许请求失败次数，默认为 1。当超过最大次数时，返回 proxy_next_upstream 指令定义的错误 fail_timeout 在经历 max_fails 次失败后，暂停服务的时间 backup 预备备份机器，当所有其他主机都 down 后，就会启用该主机 down 指定的 server 暂不参与负载均衡 查看现象 &gt; curl sys1.example.com # 一共四次访问，访问web1和web2的比例正好为1:3WEB2&gt; curl sys1.example.comWEB1&gt; curl sys1.example.comWEB2&gt; curl sys1.example.comWEB2 ip_hashupstream sys1.example.com &#123; ip_hash; server 172.16.246.134; server 172.16.246.135;&#125; 测试： &gt; curl sys1.example.comWEB2&gt; curl sys1.example.comWEB2&gt; curl sys1.example.comWEB2&gt; curl sys1.example.comWEB2 每个 IP 地址绑定一个 web 服务器，可能会导致某些 web 服务器负载很大，有的很少，反而无法保证负载均衡。 upstream 块补充指令： keepalive：指定每个 worker 进程缓存到上游服务器的连接数。需要先将proxy_http_version设为 1.1，proxy_set_header设为Connection &quot;&quot; upstream apache &#123; server 127.0.0.1:8080; keepalive 32;&#125;location / &#123; proxy_http_version 1.1; proxy_set_header Connection &quot;&quot;; proxy_pass http://apache;&#125; least_conn：直接激活最少连接算法，将请求发送至活跃连接数最少的服务器。 Nginx 反向代理在模块ngx_http_proxy_module中与 Nginx 反向代理有关的指令：proxy_pass，用于设置后端服务器的地址，仅用于location块。可以设置代理服务器的协议和地址以及映射位置的可选 URI。 实验环境： 代理服务器：172.16.246.133 后端服务器 1：172.16.246.134 后端服务器 2：172.16.246.135 只需要在代理服务器上配置即可 server&#123; listen 80; server_name web1; location / &#123; proxy_pass http://172.16.246.134; &#125;&#125;server&#123; listen 80; server_name web2; location / &#123; proxy_pass http://172.16.246.135; &#125;&#125; 然后在本机上配置 hosts 文件 172.16.246.133 web1 web2 确保代理服务器和后端服务器的端口都打开。然后在本机测试 &gt; curl web1WEB1&gt; curl web2WEB2 如果代理的是动态内容服务器，要使用fastcgi_pass指令进行代理 location ~* \\.php$ &#123; fastcgi_pass http://phpserver;&#125; 补充指令： **proxy_set_header**：在将客户端请求转交给后端服务器前，更改请求头信息。默认只重新定义了两个字段 proxy_set_header Host $proxy_host;# 最好使用$host变量，它的值等于“Host”请求头字段中的服务器名称，如果此字段不存在则等于主服务器名称proxy_set_header Host $host;# 服务器名称可以与代理服务器的端口一起传递proxy_set_header Host $host:$proxy_port;# 请求host为代理主机proxy_set_header Host $proxy_host;# 添加客户端IPproxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_set_header Connection close; 若设置了X-Real-IP，且后端服务器是 httpd，则可以修改 httpd 配置文件中的LogFormat，添加一个%&#123;X-Real-IP&#125;i。而若后端是 Nginx，则不用修改了，因为 Nginx 的log_format中已默认配置了$remote_addr，即客户端 IP。 如果启用了缓存，则标题字段为“If-Modified-Since”，“If-Unmodified-Since”，“If-None-Match”，“If-Match”，“Range”和“If-Range”来自原始请求不会传递给代理服务器。 **proxy_connect_timeout**：配置 Nginx 与后端代理服务器尝试连接的超时时间，默认 60s，且通常不会大于 75 秒。 proxy_connect_timeout 30; proxy_read_timeout：配置 Nginx 向后端服务器组发出 read 请求后，等待响应的超时时间。仅在两个连续的读操作之间设置超时，而不是整个响应。如果代理服务器在此时间内未传输任何内容，则关闭连接。默认为 60 秒。 proxy_read_timeout 15; proxy_send_timeout：配置 Nginx 向后端服务器组发出 write 请求后，等待响应的超时时间。仅在两个连续的写操作之间设置超时，而不是整个请求。如果代理服务器在此时间内未收到任何内容，则关闭连接。默认为 60 秒。 proxy_send_timeout 15; **proxy_redirect**：用于修改后端服务器返回的响应头中的 Location 和 Refresh proxy_redirect off; proxy_buffer_size：用于读取从代理服务器接收的响应的第一部分的缓冲区的大小，这部分通常包含一个小的响应头。默认情况下，缓冲区大小等于一个内存页面，默认为 4K 或 8K。 proxy_buffer_size 4k; **proxy_buffers**：用于从代理服务器读取响应的缓冲区的数量(number)和大小(size)，用于单个连接。默认情况下，缓冲区大小等于一个内存页面。默认为 4K 或 8K。语法：proxy_buffers number size; proxy_buffers 32 4k;# 那么最多支持的并发活动连接数 = 分配给nginx的内存量（字节）/ (number×size×1024) **proxy_temp_file_write_size**：当启用从代理服务器到临时文件的响应缓冲时，限制一次写入临时文件的数据大小。默认情况下，size 由proxy_buffer_size和proxy_buffers指令设置的两个缓冲区限制。默认为 8k 或 16k。 proxy_temp_file_write_size 64k; 可以讲这些代理指令单独存放为一个配置文件，然后在主配置文件的代理块中include应用。 非 HTTP 型上游服务器Memcached 服务器Nginx 提供 memcached 模块并默认开启，用于与 memcached 守护进程通信，此时 Nginx 并不是充当反向代理，而是使 Nginx 使用 memcached 协议会话，因此 key 的查询能够在请求传递到应用服务器之前完成。 upstream memcaches &#123; server xxxx:11211; server xxxx:11211;&#125;server &#123; location / &#123; set $memcached_key &quot;$uri?args&quot;; memcached_pass memcaches; &#125;&#125; memcached_pass将使用$memcached_key变量实现 key 查找。 FastCGI 服务器fastcgi 模块也是默认开启。开启后 Nginx 可使用 FastCGI 协议与多个上游服务器会话。 upstream fastcgis &#123; server xxxx:9000; server xxxx:9000;&#125;location / &#123; fastcgi_pass fastcgis;&#125; SCGI 与 uWSGI 服务器与 FastCGI 类似，分别使用scgi_pass与uwsgi_pass指定上游服务器。 Nginx 邮件服务Nginx 提供邮件代理服务，能代理 POP3、IMAP、SMTP。 每一个进入的请求都会基于相应的协议处理，对于每一个协议，Nginx 都需要一个用户名密码认证服务，如果认证成功，则连接被代理到邮件服务器。 Nginx 邮件服务，若是编译安装，则需要在构建时指定--with-mail。 如果代理 pop3 服务，在配置文件中添加以下配置 mail &#123; auth_http localhost:9000/auth; # 查询认证服务 server &#123; listen 110; # pop3的TCP端口110，监听POP3服务 protocol pop3; proxy on; &#125;&#125; 如果是代理 imap 服务，则同理，而可以不用指定protocol imap;，因为 imap 是默认值。而端口号改为 143。 如果代理 smtp 服务，则修改protocol后，再修改端口号为 25。 Nginx 还为三种服务提供了可选指令，能做到根据服务器的情况灵活代理。 最好在mail块中，指定server_name，为邮件代理提供统一入口。 重写与重定向Nginx 提供模块ngx_http_rewrite_module实现 URL 重写与重定向。rewrite指令能用于server、location、if块中。Nginx 的 rewrite 需要 pcre 的支持。 rewrite regex replacement [flag]; 其中 regex 为正则表达式，replacement 为符合正则的替换算法。flag 为进一步处理的标识。以下为 flag 的可选值： 参数值 说明 last 终止 rewrite，继续匹配 break 终止 rewrite，不再继续匹配 redirect 临时重定向，返回的 HTTP 状态码为 302 permanent 永久重定向，返回的 HTTP 状态码为 301 例： rewrite ^/(.*) http://www1.example.com/$1 permanent;# ^/(.*) 正则表达式，表示匹配全部# 匹配成功就跳转到www1.example.com/$1# $1对应的是前面正则表达式的()部分 rewrite 有以下的作用： 使 URL 更加规范，方便开发 使搜索引擎收录网站内容 网站换域名后，让旧域名的访问跳转到新域名上 根据特殊变量、目录、客户端信息进行 URL 跳转 应用实验：使 www1.example.com 和 example.com 访问同一个地址 server &#123; listen 80; server_name example.com; rewrite ^/(.*) http://www1.example.com/$1 permanent;&#125;server &#123; listen 80; server_name www1.example.com; location / &#123; root html/www; index index.html; &#125;&#125; 重写两种匹配规则： break：本条规则匹配完成后，不再继续匹配后续任何规则 last：本条规则匹配完成后，继续向下匹配新的 location URI 规则 在 server 块中添加以下内容： # !-e判断请求指定的资源是否不存在，若不存在就执行if块中的指令。$request_filename表示当前请求的文件路径if (!-e $request_filename) &#123; rewrite &quot;^/.*&quot; /40x.html break; # 重写符合规则的请求地址。&quot;^/.*&quot;表示匹配当前网站下的所有请求&#125; 试验： &gt; curl http://web/asdasd400 ERROR 若使用 last 标记： 常用的测试： 双目测试： ~，!~：正则匹配与不匹配（区分大小写） =，!=：精确匹配与不匹配 ~*，!~*：正则匹配与不匹配（不区分大小写） 使用案例： if ($request_method=&quot;POST&quot;)&#123;&#125;if ($request_uri ~* &quot;/forum&quot;)&#123;&#125; 单目测试： -f，!-f：文件存在与不存在 -d，!-d：目录存在与不存在 -e，!-e：文件或目录存在与不存在 -x，!-x：判断是否可执行与不可执行 Nginx 优化安全优化隐藏 Nginx 版本号# curl -I localhostHTTP/1.1 200 OKServer: nginx/1.14.1 # nginx的版本号会被轻松获取Content-Type: text/htmlContent-Length: 4057...... 通过修改配置，在 http 块中添加 server_tokens off; 再次访问 # curl -I localhostHTTP/1.1 200 OKServer: nginxContent-Type: text/htmlContent-Length: 4057..... 这样配置，在 nginx 的报错页面中也不会出现版本号。 文件解析漏洞CRLF注入漏洞目录遍历漏洞属于配置不当的漏洞，错误的配置会使目录被遍历，导致源码或重要信息泄露。解决：对某location路径添加autoindex off;，默认就是off的。 服务器请求伪造漏洞整数溢出漏洞性能优化nginx 单个进程允许的客户端最大连接数worker_connections为 nginx 单个进程允许的客户端最大连接数，这个连接数包含了所有连接，如代理连接、客户端连接等，需要根据服务器性能和内存消耗来指定，同时还与最大打开文件数（worker_rlimit_nofile）有关，在配置ulimit -HSn 65535后，该配置才能生效。该参数配置在event块。 nginx 的总并发连接 = worker 数量 * worker_connections worker 进程最大打开文件数worker_rlimit_nofile为 nginx worker 进程最大打开文件数，可设置为ulimit -n的大小。配置在主标签段，不属于任何块。 优化服务器域名的散列表散列表和监听端口关联，每个端口都最多关联到三张表： 确切名字的散列表 以星号起始的通配符名字的散列表 以星号结束的通配符名字的散列表 Nginx 会按以上的顺序依次查找域名，为了尽快找到域名，尽量使用确切名字。若定义了大量名字，或定义了非常长的名字，则需要在 HTTP 配置块中调整server_names_hash_max_size和server_names_hash_bucket_size的值。nginx 默认的值取决于 cpu 缓存行长度，可能是 32 或 64 或 128。配置在主标签段。 先尝试设置server_names_hash_max_size，差不多为名字列表的名字总量，若不能解决问题再设置server_names_hash_bucket_size。 server_names_hash_max_size默认为 512kb，一般是 cpu L1 的 4-5 倍，不要加单位，默认单位为 kb。 nginx 连接参数优化keepalive_timeout设置连接会话保持时间，默认 65。client_header_timeout设置读取客户端请求头数据的超时时间，若超过这个时间，客户端还没发送完整的 header 数据，nginx 会返回 408（request time out）错误，默认 60，推荐为 15，有单位。设置在 http 或 server 块。client_body_timeout同上，是客户端请求体的超时时间，默认为 60s。设置在 http 或 server 块或 location 块。send_timeout指定响应客户端的超时时间，仅限于两个连接活动之间的时间，若超过这个时间客户端没有活动，则 nginx 连接关闭，默认 60s，推荐 25s，有单位。设置在 http 或 server 块或 location 块。 上传文件大小限制client_max_body_size限制上传文件大小，具体根据业务调整，默认为 1m，推荐先设为 8m。设置在 http 或 server 块或 location 块。 FastCGI 优化gzip 压缩expires 缓存放盗链防爬虫CDN 加速Nginx 降权Nginx 与 PHPNginx 自带的 FastCGI 模块，不仅能接受 php-fpm，还能与任何兼容 FastCGI 的服务器通信，该模块默认启用。 常用指令： FastCGI 指令 说明 fastcgi_buffer_size 来自 FastCGI 服务器响应头的第一部分设置缓冲大小 fastcgi_buffers 设置来自 FastCGI 服务器响应的缓冲数量和大小，用于单个连接 fastcgi_cache 定义一个共享的内存 zone，用于缓存 fastcgi_no_cache 指定一个或多个字符串，Nginx 不会将来自 FastCGI 服务器的响应保存在缓存中 fastcgi_keep_conn 服务器不立即关闭连接以保持到 FastCGI 服务器的连接 fastcgi_pass 指定 FastCGI 服务器如何传递请求，格式：address:port fastcgi_cache_path 该指令用于设置存储缓存响应的目录和存储活跃的 key 和响应元数据的共享内存 zone fastcgi_connect_timeout 指定 Nignx 将等待它接受连接的最长时间 fastcgi_hide_header 指定不应该传递到客户端的头列表 Nginx 常见模块gzip 压缩gzip 模块默认启用。 http &#123; gzip on; gzip_comp_level 2; # 指定gzip压缩等级（1-9） gzip_types text/plain text/css application/xml application/json; #指定除了text/html，其他需要压缩的MIME类型 gzip_min_length 1024; # 在启用压缩前先确定响应的长度，若高于此处设定的值，则启用压缩 gzip_buffers 40 4k; # 用于压缩响应使用的缓存数量和大小&#125; LNMP 分布式集群方案搭建 Nginx+PHP 环境编译安装 PHP，php 版本 7.2.11。 确保zlib及其库zlib-devel，gd及其库gd-devel，curl及其库libcurl、libcurl-devel，openssl及其库openssl-devel，libxml2和libxml2-devel（php 编译必须的依赖包），libjpeg及其库libjpeg-devel，libpng及其库libpng-devel，freetype-devel ./configure --prefix=/usr/local/php7.2 \\ --enable-fpm \\ # 开启PHP的FPM功能 --with-zlib \\ --enable-zip \\ --enable-mbstring \\ # 用于多字节字符串处理 --with-mysqli \\ # 增强版Mysql数据库访问支持 --with-pdo-mysql \\ # 基于PDO（php data object）的MySQL数据库访问支持 --with-gd \\ # gd库支持，用于php图像处理 --with-jpeg-dir \\ # jpeg图像处理库 --with-png-dir \\ # png图像处理库 --with-freetype-dir \\ # freetype字体图像处理库 --with-curl \\ # curl支持 --with-openssl \\ --with-mhash \\ # mhash加密支持 --enable-bcmath \\ # 精确计算功能 --enable-opcache # 开启opcache，一种php代码优化器make &amp;&amp; make install 安装完成后，php-fpm 是无法通过systemctl管理的。需要以下配置 php 配置文件目录 /usr/local/php7.2/etc/├── pear.conf├── php-fpm.conf.default└── php-fpm.d └── www.conf.default 需要将php-fpm.conf.default改名为php-fpm.conf。然后进入解压包的目录中sapi/fpm目录，里面有一个php-fpm.service，将该文件的权限改为754，然后复制到/usr/lib/systemd/system/中。 如果是 CentOS/Redhat 6 系列，则将init.d.php-fpm复制到/etc/init.d/中并改名为php-fpm，添加执行权限，然后chkconfig --add php-fpm。 查看php-fpm.service，确保 PID 文件、配置文件路径正确。 [Unit]Description=The PHP FastCGI Process ManagerAfter=network.target[Service]Type=simplePIDFile=/usr/local/php7.2/var/run/php-fpm.pidExecStart=/usr/local/php7.2/sbin/php-fpm --nodaemonize --fpm-config /usr/local/php7.2/etc/php-fpm.confExecReload=/bin/kill -USR2 $MAINPIDPrivateTmp=true[Install]WantedBy=multi-user.target 其中配置文件的路径为安装目录的etc/php-fpm.conf，所以该配置文件原来的.default文件一定要改名为php-fpm.conf。并且还需要将php-fpm.d/下的www.conf.default改名为www.conf，否则还会报错。 /usr/local/php7.2/sbin/php-fpm -t检查配置文件是否正确。 再启动 php-fpm，systemctl start php-fpm.service ps -ef | grep php-fpmroot 47197 1 0 22:53 ? 00:00:00 php-fpm: master process (/usr/local/php7.2/etc/php-fpm.conf)nobody 47198 47197 0 22:53 ? 00:00:00 php-fpm: pool wwwnobody 47199 47197 0 22:53 ? 00:00:00 php-fpm: pool wwwss -anpt | grep php-fpmLISTEN 0 128 127.0.0.1:9000 *:* users:((&quot;php-fpm&quot;,pid=47199,fd=5),(&quot;php-fpm&quot;,pid=47198,fd=5),(&quot;php-fpm&quot;,pid=47197,fd=7)) 可以看出，php-fpm 监听了 9000 端口，主进程用户为 root，子进程用户为 nobody。 可以将bin和sbin目录添加到环境变量中，将配置文件软连接到/etc/php/中，方便操作。 php-fpm 主配置文件php-fpm.conf，该配置文件采用的是 INI 的格式 [global] # 全局配置pid = run/php-fpm.pid # pid文件路径error_log = log/php-fpm.log # 日志路径syslog.ident = php-fpm # syslog标识名称log_level = notice # 日志等级。alert, error, warning, notice, debugdaemonize = yes # 是否在后台运行events.mechanism = epoll # 事件机制。include=/usr/local/php7.2/etc/php-fpm.d/*.conf # 进程池配置存放目录; process.max = 128 php-fpm 进程池配置文件www.conf，采用的也是 INI 格式 [www] # 进程池配置user = nobody # 工作用户group = nobody # 工作组listen = 127.0.0.1:9000 # 监听IP地址与端口;listen.owner = nobody # socket文件所属用户;listen.group = nobody # socket文件所属组;listen.allowed_clients = 127.0.0.1 # 指定允许连接的客户端IP，默认为环回口pm = dynamic # 控制子进程的数量，默认为dynamic（动态控制）pm.max_children = 5 # 最多子进程数pm.start_servers = 2 # 启动进程数，对应了php-fpm:pool www的个数pm.min_spare_servers = 1 # 最少空闲进程数，若少于该数，则会自动创建空闲进程pm.max_spare_servers = 3 # 最多空闲进程数，若多于该数，则会自动删除空闲进程access.log = log/$pool.access.log # 日志文件，默认不记录日志;php_admin_value[memory_limit] = 32M # 以php_admin_value的方式替换php.ini中memory_limit的值;php_flag[display_errors] = off # 以php_flag的方式替换php.ini中display_errors的值 在 php 的解压包中，有两个关于php.ini的配置文件：php.ini-production和php.ini-development，其中 production 适用于实际上线环境（安全性高），development 适合于开发环境（便于调试）。选择其中一个复制到 php 安装目录下lib/php/中，并改名为php.ini。 [PHP] # PHP核心配置output_buffering = 4096 # 输出缓冲，单位字节;open_basedir = # 限制php脚本可访问的路径disable_functions = # 禁用的函数列表max_execution_time = 30 # 每个PHP脚本最长时间限制，单位秒memory_limit = 128M # 每个PHP���本最大内存使用限制display_errors = On # 是否输出错误信息log_errors = On # 是否开启错误日志;error_log = php_errors.log # 错误日志路径post_max_size = 8M # 通过POST提交的最大限制default_mimetype = &quot;text/html&quot; # 默认MIME类型default_charset = &quot;UTF-8&quot; # 默认编码file_uploads = On # 是否开启文件上传;upload_tmp_dir = # 上传文件临时保存目录upload_max_filesize = 2M # 上传文件最大限制allow_url_fopen = On # 是否允许打开远程文件;cgi.fix_pathinfo=1 # 开启在CGI模式下自动识别PATHINFO# PATHINFO用于在某个脚本后添加自定义内容# 开启后，若要访问的资源不存在，则会执行上一级的文件，若上一级的文件仍不存在，则返回“File not found”# 若不开启，要访问的文件不存在，则直接返回错误“No input file specified”[Date] # 时间与日期配置;date.timezone = Asia/Shanghai # 时区[Session] # 会话配置session.save_handler = files # 将会话以文件形式保存;session.save_path = &quot;/tmp&quot; # 会话保存目录 修改 Nginx 配置文件，取消以下行的注释： location ~ \\.php$ &#123; root html; fastcgi_pass 127.0.0.1:9000; # 将动态请求交给该端口处理 fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # 额外参数文件 include fastcgi.conf; # 包含了当前目录下的fastcgi_params配置 # 一定要设置为fastcgi.conf，不能设为fastcgi_params # 因为.conf文件不_params文件多了一条 # fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; # 这是指定了脚本的路径&#125; 查看/etc/nginx/fasstcgi_params文件 # 请求字符串（/xxx.php?xxx=xxxx&amp;xxx=xxxx）fastcgi_param QUERY_STRING $query_string;# 请求方法（get、post）fastcgi_param REQUEST_METHOD $request_method;# 内容的类型（即MIME类型）fastcgi_param CONTENT_TYPE $content_type;# 内容长度fastcgi_param CONTENT_LENGTH $content_length;# CGI脚本名fastcgi_param SCRIPT_NAME $fastcgi_script_name;# 请求URIfastcgi_param REQUEST_URI $request_uri;# 文件URIfastcgi_param DOCUMENT_URI $document_uri;# 文件根目录fastcgi_param DOCUMENT_ROOT $document_root;...... 在index.php中写入&lt;? phpinfo(); ?&gt;。刷新配置，浏览器访问。 Nginx+Apache 动静分离Nginx 提供外部访问，静态请求直接由 Nginx 处理，动态请求转交给 Apache 处理，实现动静分离。 首先要确保 Apache 已支持 PHP。确保 Apache 是--enable-so的，会在 Apache 的bin/下有一个apxs命令，是 Apache 的一个扩展工具（Apache extension tools），用于编译模块。php 的 configure 可用 apxs 编译用于 Apache 访问 PHP 的模块。 使用php -i | grep configure查看 php 的编译参数 Configure Command =&gt; &#x27;./configure&#x27; &#x27;--prefix=/usr/local/php7.2&#x27; &#x27;--enable-fpm&#x27; &#x27;--with-zlib&#x27; &#x27;--enable-zip&#x27; &#x27;--enable-mbstring&#x27; &#x27;--with-mcrypt&#x27; &#x27;--with-mysql&#x27; &#x27;--with-mysqli&#x27; &#x27;--with-pdo-mysql&#x27; &#x27;--with-gd&#x27; &#x27;--with-jpeg-dir&#x27; &#x27;--with-png-dir&#x27; &#x27;--with-freetype-dir&#x27; &#x27;--with-curl&#x27; &#x27;--with-openssl&#x27; &#x27;--with-mhash&#x27; &#x27;--enable-bcmath&#x27; &#x27;--enable-opcache&#x27; 重新整理并添加--with-apxs2=/usr/local/httpd-2.4/bin/apxs。 注：如果此时直接编译仍然会报错，错误来自于apxs命令。报错信息如下： ./configure: /usr/local/httpd-2.4/bin/apxs: /replace/with/path/to/perl/interpreter:bad interpreter: No such file or directory 查看apxs命令。第一行是#!/replace/with/path/to/perl/interpreter -w肯定是不存在的，需要替换为#!/usr/bin/perl -w。重试./configure，然后make（不要make install）。 修改 httpd 的配置文件，以下为要修改的内容 Listen 81 # 防止与Nginx冲突，要修改端口号# 修改httpd虚拟主机&lt;VirtualHost *:81&gt; Servername &quot;system3.example.com&quot; DocumentRoot &quot;htdocs/php&quot;&lt;/VirtualHost&gt; 修改 Nginx 配置文件，将以下行取消注释并修改 location ~ \\.php$ &#123; proxy_pass http://127.0.0.1:81; # 代理客户端浏览器请求Apache服务器 proxy_set_header Host $host; # 发送Host消息头 # 因为Nginx在代理时能传递客户端的请求头，但无法传递Host消息头 # $host保存了请求的主机名（system3.example.com）&#125; 参考文章 nginx 基础及提供 web 服务(nginx.conf 详解) nginx 日志配置 Nginx 缓存原理及机制 Nginx 高性能 Web 服务器实战教程 高性能网站构建实战 跟老男孩学 Linux 运维 Web 集群实战 精通 Nginx（第二版）","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"代理","slug":"代理","permalink":"https://coconutmilktaro.top/tags/%E4%BB%A3%E7%90%86/"},{"name":"负载均衡","slug":"负载均衡","permalink":"https://coconutmilktaro.top/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"name":"集群","slug":"集群","permalink":"https://coconutmilktaro.top/tags/%E9%9B%86%E7%BE%A4/"},{"name":"缓存","slug":"缓存","permalink":"https://coconutmilktaro.top/tags/%E7%BC%93%E5%AD%98/"},{"name":"Nginx","slug":"Nginx","permalink":"https://coconutmilktaro.top/tags/Nginx/"},{"name":"LNMP","slug":"LNMP","permalink":"https://coconutmilktaro.top/tags/LNMP/"}]},{"title":"Apache-Server笔记","slug":"Apache-Server笔记","date":"2018-05-02T09:19:50.000Z","updated":"2022-06-21T15:39:11.612Z","comments":true,"path":"2018/Apache-Server笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Apache-Server%E7%AC%94%E8%AE%B0/","excerpt":"","text":"Apache-httpd 服务器介绍Apache 服务器全称 Apache-HTTP-Server，而 httpd 就是 Apache 服务器端运行的软件，提供 WWW 服务器平台。 Apache 特性 简单强大的配置文件 支持虚拟主机 支持多种 HTTP 认证 集成 Perl 脚本，代理服务器模块 支持通用网关接口、FastCGI 支持实时监视服务器状态和定制服务器日志 支持 SSL、服务器端包含指令 SSI 可通过第三方模块支持 Java Servlets 提供对用户会话的跟踪 采用模块化设计模型 Apache 最重要的特性就是：采用模块化设计模型。模块分为： 静态模块：是 Apache 最基本的模块，无法随时添加和卸载，在编译安装时设定 动态模块：是可以随时添加和卸载的模块，使得部署有最大的灵活性 Apache 的模块会被编译为动态共享对象 DSO，这些 DSO 独立于 httpd，可以在编译时就添加，也可以后期通过 Apache Extension Tool 工具编译添加模块，可使用httpd -M查看模块加载清单。 httpd 服务器安装httpd 版本 2.4.34。可以使用源码安装，可进行更精细的设定。 ./configure --enable-so # 开启模块化功能，支持DSO --enable-mods-shared # 指明以DSO方式编译的模块，all表示所有模块，most表示大部分模块 --enable-ssl # 支持ssl加密 --enable-rewrite # 支持地址重写 --with-mpm # 设置httpd工作模式 --with-suexec-bin # suexec库的路径，用于支持SUID、SGID --with-apr # 指定apr程序的绝对路径 apr：Apache Portable Runtime（APR）项目的任务是创建和维护软件库，为底层平台特定的实现提供可预测且一致的接口。主要目标是提供一个 API，软件开发人员可以对其进行编码，并确保可预测的行为，如果不是相同的行为，无论他们的软件构建在哪个平台上，都可以减轻他们编写特殊情况条件以解决问题的需要。 源码安装前需要安装 apr 和 apr-util，仍然需要源码安装 下载 apr 和 apr-util 的源码包，先安装 apr，先要确保安装了gcc ./configure --prefix=/usr/local/apr-1.6然后make &amp;&amp; make install 再安装 apr-util，需要--with-apr指定 apr 的安装路径 ./configure --prefix=/usr/local/apr-util-1.6 --with-apr=/usr/local/apr-1.6然后make &amp;&amp; make install 可能会报一个错xml/apr_xml.c:35:19: 致命错误：expat.h：No such file or directory，需要安装expat-devel，yum install expat expat-devel即可。 安装与 Apache 相关的依赖库或软件，直接yum|dnf安装即可： pcre-devel # perl正则表达式库libxml2-devel # xml文件库，很重要libpng-devel # png图像库libjpeg-devel # jpeg图像库libmcrypt-devel # mcrypt加密库zlib-devel # zlib压缩库freetype-devel # freetype字体库autoconf # 生成配置脚本的软件，不一定要gd-devel # gd图像库libcurl-devel # curl库openssl-devel # openssl库。注：php5不支持openssl1.1.0版本以上的 然后 --prefix./configure --prefix=/usr/local/httpd-2.4 \\ --with-apr=/usr/local/apr-1.6 \\ --with-apr-util=/usr/local/apr-util-1.6 # 若不指定--prefix，会默认安装在/usr/local/apache2中 并make &amp;&amp; make install即可。源码安装后可能不能直接使用 Apache 的管理命令，需要添加环境变量，export PATH=/usr/local/httpd-2.4/bin:$PATH，也可以永久添加。 httpd 提供两种编译方式：静态编译和动态编译 静态编译：把模块直接编译进 httpd 核心，httpd 启动时所有静态编译的模块都会启动 动态编译：将模块编译好，但不编译进 httpd 核心，httpd 启动动态模块并不会启动，而是需要在配置文件中LoadModule加载才能启动，实现了模块热插拔。 可在编译时指定--enable-模块名=shared|static指定模块是动态或静态编译。 httpd 能够实现动态编译的原因在于 httpd 默认会安装mod_so，此模块提供了配置文件的LoadModule和LoadFile指令，在编译时指定--enable-so即可，而此模块只能使用静态编译，若指定为shared则编译时会出错。 动静态编译的优先级： 不指定模块编译选项，则默认值为--enable-mods-shared=most即动态编译大部分模块 显式指定的优先级高，如果某个模块既指定了静态，又指定了动态，则静态优先。 静态关键字规则优先于动态关键字规则，即若--enable-mods-static=few和--enable-mod-shared=all同时配置，静态优于动态，静态生效。 也可通过各种源安装。yum|dnf install httpd。安装完成后，Apache 会提供apachectl脚本命令，可进行 httpd 的启动、关闭和测试，若没有修改配置文件下使用start启动 httpd，会报以下错误信息： Could not reliably determine the server&#x27;s fully qualified domain name 报错说明 httpd 无法确定服务器域的名称，可通过修改主配置文件的ServerName解决。修改后再通过apachectl starts启动 httpd，再直接打命令apachectl能看到 httpd 已在运行的消息httpd (pid 2012) already running。也可以通过systemctl start httpd启动 httpd。 若开启了firewalld服务，需要放行80/tcp端口，放行http服务。 httpd 的主要目录（yum 源安装）： /etc/httpd：httpd 服务根目录 /etc/httpd/conf和/etc/httpd/conf.d：httpd 服务配置文件目录 /var/www/html：网站数据目录 /var/log/httpd：httpd 日志目录，里面存放有access_log访问日志和error_log错误日志 若是通过 yum 源安装的 httpd，则在/etc/httpd中的logs，modules和run目录都是软连接。 /etc/httpd/logs --&gt; /var/log/httpd/etc/httpd/modules --&gt; /usr/lib64/httpd/modules/etc/httpd/run --&gt; /run/httpd apachectl命令： apachectl -V # 查看apache版本信息，以及模块、编译信息 -l # 查看已被编译的模块 start # 启动httpd，如果已在运行就会返回错误 stop # 停止httpd restart # 重启httpd fullstatus # 显示mod_status的完整状态报告。需要在服务器上启用mod_status，并在系统上使用基于文本的浏览器 status # 显示简要状态报告，信息与systemctl status httpd一致 graceful # 优雅地重启Apache httpd守护进程。如果守护程序未运行，则启动它。这与正常重启不同，因为当前打开的连接不会中止。副作用是旧的日志文件不会立即关闭。这意味着，如果在日志轮换脚本中使用，则可能需要大量延迟才能确保在处理旧日志文件之前将其关闭。 graceful-stop # 让已运行的httpd进程不再接受新请求，并给他们足够的时间处理当前正在处理的事情，处理完成后才退出。所以在进程退出前，日志文件暂时不会关闭，正在进行的连接暂时不会断开。 configtest # 配置文件语法测试，相当于apachectl -t httpd命令：与apachectl一致 httpd # 可直接启动httpd -D name # 定义一个在&lt; IfDefine name &gt;中使用的name，以此容器中的指令 -d directory # 指定ServerRoot -f file # 指定配置文件 -C &quot;directive&quot; # 指定在加载配置文件前要处理的指令(directive) -c &quot;directive&quot; # 指定在加载配置文件后要处理的指令 -e level # 显示httpd启动时的日志调试级别 -E file # 将启动信息记录到指定文件中 -v # 显示版本号 -V # 显示编译配置选项 -h # 显示帮助信息 -l # 显示已编译但非动态编译的模块，即静态编译的模块 -L # 显示静态模块可用的指令列表 -t -D DUMP_VHOSTS # 显示虚拟主机的设置信息 -t -D DUMP_RUN_CFG # 显示运行参数 -S # 等价于-t -D DUMP_VHOSTS -D DUMP_RUN_CFG。在调试如何解析配置文件时非常有用 -t -D DUMP_MODULES # 显示所有已被加载的模块，包括静态和动态编译的模块 -M # 等价于-t -D DUMP_MODULES -t # 检查配置文件语法 -T # 不检查DocumentRoot，直接启动 -X # 调试模式，此模式下httpd进程依赖于终端 -k # 管理httpd进程，接受start|restart|graceful|graceful-stop|stop httpd 配置文件httpd 主配置文件主要由指令和容器构成，容器使用&lt;容器名&gt;&lt;/容器名&gt;作为开始和结束，容器的指令一般只在容器内生效，每个指令都是某个模块提供的，指令生效方式是从上往下读取，所以不要变更指令位置。 主配置文件重点指令： ServerRoot：设置 Apache 的安装主目录，若采用源码安装，默认路径为/usr/local/apache2 Listen：设置服务器监听端口 IP 及端口号，默认监听服务器本机所有 IP 地址的 80 端口。格式：Listen [IP地址:]端口 [协议]，默认监听所有 IP，使用 TCP 协议。可多次使用 Listen 以开启多个端口 LoadModule：加载模块。格式 LoadModule 模块名 模块文件名，模块文件一般存放在 ServerRoot 的 module 目录中 ServerAdmin：主服务器返回给客户端的错误消息中的管理员邮箱地址 ServerName：设置服务器本机的主机名和端口，用于 URL 重定向 User：apache 在本地系统上运行的用户名 Group：apache 在本地系统上运行的组名 DocumentRoot：网络路径相对路径的根，是文档的根目录，使用 rpm 包安装则默认值为/var/www，使用源码安装则默认为$ServerRoot/htdocs ErrorLog：服务器错误日志存放位置，默认使用相对路径logs/error_log ErrorLogFormat：错误日志格式 CustomLog：客户端访问日志文件路径及日志格式，格式：CustomLog 文件名 格式，默认相对路径为logs/access_log LogFormat：用户日志文件格式，一般用这里指定的格式创建别名，然后通过CustomLog调用该格式 LogLevel：日志消息等级，分为debug/info/notice/warm/error/crit/alert/emerg AllowOverride：支持从.htaccess文件中重写前面的指令，若值为None，表示不支持 Require：给所有用户或特定用户/组授予或拒绝对目录的访问 Include：允许 Apache 在主配置目录加载其他配置文件，默认为conf.d/*.conf Options：为特殊目录设置选项，语法格式为Options [+|-]选项。 All：开启除MultiViews之外的所有选项 None：不启用额外功能 FollowSymlinks：允许 Options 指定目录下的文件链接到目录外的文件或目录 Indexes：若与 URL 对应的 Options 目录下找不到DirectoryIndex指定的首页文件，则会将当前目录的所有文件索引出来 Order：控制默认访问状态以及Allow和Deny的顺序 若为Order deny,allow则先检查拒绝，当拒绝与允许冲突时，allow优先，默认规则allow，即只要是deny排在前面，就只要写拒绝的 IP 地址即可，使用Deny from [IP地址]|all 若为Order allow,deny则先检查允许，当拒绝与允许冲突时，deny优先，默认规则deny，即只要是allow排在前面，就只要写拒绝的 IP 地址即可，使用Allow from [IP地址]|all Alias：用于将 URL 路径映射到本地文件系统的路径，且本地路径不受 DocumentRoot 的限制，该目录中的脚本不允许执行。格式：Alias URL路径 &quot;本地资源的文件系统路径&quot;。Alias不支持正则，而AliasMatch支持，格式一致。 ScriptAlias：类似于 Alias，并且能将 Web 路径映射到 DocumentRoot 之外的文件系统位置，还告诉 Apache 指定的目录存在 CGI 脚本，可以执行脚本 DirectoryIndex：作为索引的文件名，默认找index.html。若 url 中未指定网页文件，则会返回该目录下DirectoryIndex定义的文件，可指定多个文件，若都不存在，会生成所有文件列表，此时Option Indexes必须打开。 UserDir：定义和本地用户的主目录相对的目录，可将公共的 html 文件放入该目录，即每个用户的个人站点。默认设置为public_html，每个用户都可在自己的主目录下创建名为public_html的目录，该目录下的 html 文件可通过域名/~用户名访问。若值为disabled表示禁止使用个人站点。 Timeout：客户端与服务器连接的超时间隔 KeepAlive：开启长连接。HTTP/1.1 中支持一次连接多次传输，可在一次连接中传递多个 HTTP 请求 KeepAliveTimeout：一次连接中多次请求间的超时间隔 MaxKeepAliveRequests：一个 HTTP 连接中最多可请求的次数，若为 0，表示无限制 指令文档 常用容器： IfDefine：使管理员能采用多种配置方式启动 Apache，当启动 httpd 时使用命令httpd -D 自定义名便会匹配，若测试条件为真，就会加载该容器中定义的参数。格式：&lt;IfDefine [!]自定义名&gt; IfModule：可以封装仅在条件满足时才会处理的命令，根据模块是否加载决定条件是否满足。语法：&lt;IfModule [!] 模块&gt;指令&lt;/IfModule&gt; Directory：仅用于特定的文件系统目录、子目录及目录下内容，通常用绝对路径，即使是相对路径，也是相对于文件系统的根目录。语法：&lt;Directory 路径&gt;指令&lt;/Directory&gt;，路径可使用~匹配正则表达式。 DirectoryMatch：类似 Directory，可直接用正则表达式匹配 Files：类似 Directory，但 Files 内指令仅能应用与特定文件，匹配的范围是它所在的上下文。语法：&lt;Files 文件名&gt;指令&lt;/Files&gt;，可使用~匹配正则表达式 FilesMatch：与 Files 类似，可直接用正则表达式匹配 Location：该容器内的指令仅对特定 URL 有效。格式&lt;Location URL&gt;指令&lt;/Location&gt;，可使用~匹配正则表达式。Location支持三种匹配模式： 精确匹配：精确到资源的 URL 路径 加尾随斜线：匹配目录内容，如&lt;Location &quot;/myapp/&quot;&gt; 无尾随斜线：匹配目录和目录内容，如&lt;Location &quot;/myapp&quot;&gt; LocationMatch：类似 Location，可直接用正则表达式匹配 在 DirectoryMatch，Files，FilesMatch，Location，LocationMatch 中，若出现包含关系，如一个目录同时匹配到了两个相同类型容器，则会选择匹配先定义的容器 VirtualHost：虚拟主机，可直接用正则表达式匹配。语法：VirtualHost IP地址:[端口号]，IP 地址为监听的本地网卡 IP，若为*则表示监听本地所有网卡 EnableSendfile：使用 sendfile 系统调用，把静态文件发送给客户端，获得更好的性能 httpd 虚拟主机基于 IP 的虚拟主机可根据不同 IP 地址及端口号定位不同的网站请求，但需要独立的公网 IP 地址。基于域名的虚拟主机能实现在一台公网服务器上部署多个网站，服务器根据客户端访问 HTTP 头部信息实现网站的分离解析。 客户端请求到达后，服务器根据&lt;VirtualHost IP地址:[端口号]&gt;匹配主机，若 IP 地址 为*，表示匹配本地所有 IP 地址 匹配顺序： 匹配虚拟主机。匹配虚拟主机的规则为最佳匹配法，IP 地址越精确，匹配就越优先。 如果基于名称的虚拟主机无法匹配上，则采用虚拟主机列表中的第一个虚拟主机作为响应主机。 如果所有虚拟主机都无法匹配上，则采用从主配置段落中的主机。 首先配置虚拟主机配置文件，将/usr/share/doc/httpd/httpd-vhosts.conf复制到/etc/httpd/conf.d/目录下，可改名，以此为模板，创建一台虚拟主机。配置完成后重启 httpd。 &lt;VirtualHost *:80&gt; DocumentRoot &quot;/var/www/virhost1&quot; ServerName virhost1.example.com ErrorLog &quot;/var/log/httpd/virhost1-error_log&quot; CustomLog &quot;/var/log/httpd/virhost1-access_log&quot; common&lt;/VirtualHost&gt;&lt;Directory &quot;/var/www/virhost1&quot;&gt; Require all granted Options Indexes AllowOverride None&lt;/Directory&gt; 注：在实验机上，需要将该地址解析出来，所以要修改/etc/hosts，在环回口后添加virhost1.example.com，再访问即可。 注：物理站点与虚拟站点不能同时存在，如果启动虚拟站点，物理站点立刻失效。若要让之前的物理站点恢复访问，就将该站点按虚拟站点的格式重新搭建 httpd 认证授权httpd 提供各种认证模块，名称以mod_auth开头。基础的 http 认证模块为mod_auth_basic。 可以通过命令htpasswd生成用于网页认证的用户信息文件，该命令在httpd-tools包中，支持 3 种加密算法：MD5、SHA 和系统上的 crypt()函数，默认为 md5。 htpasswd [-cimBdpsDv] [-C cost] passwordfile usernamehtpasswd -b[cmBdpsDv] [-C cost] passwordfile username password -c 创建一个新密码文件 -n 不会更新密码文件，仅仅在输出显示，因此不用指定密码文件。而若不指定此项就必须指定密码文件，不能与-c一起用 -b 在命令行中读取密码，若不指定，系统会提示输入 -i 从输入读取密码（类似echo XXX | htpasswd -i），常用于脚本 -m 使用md5加密（默认） -B 使用bcrypt函数加密，很安全 -C 使用bcrypt函数加密的次数，默认为5，范围是4到31，次数越多越安全，但会更慢 -d 使用crypt函数加密，不安全 -s 使用SHA加密密码，不安全 -p 不加密密码，不安全 -D 删除指定认证用户 主配置文件的认证指令： AuthType 指定web身份认证的类型，有四种类型： none 不认证 basic 文件认证（默认），需要mod_auth_basic模块 digest md5摘要认证，需要mod_auth_digest模块 form 表单认证，需要mod_auth_form模块AuthName 设置身份认证时的提示信息AuthUserFile 指定web用户认证列表，即htpasswd命令生成的密码文件AuthGroupFile 指定组认证文件，文件中分组格式为&quot;组名: 组成员....&quot; Require指令：只能放在Directory容器中，用于控制对目录的访问权限，功能由mod_authz_core模块提供。有以下配置： Require all granted | denied：允许|拒绝所有人访问该目录 Require method http方法 ...：只有指定的 http 方法（如 get,post）才能访问该目录 Require expr 正则表达式：只要满足指定正则表达式才能访问 Require user 用户...：只有指定用户能访问 Require valid-user 用户...：认证列表中所有用户都可访问 关于用户的认证需要mod_authz_user模块 Require group 组...：指定组内的用户才能访问 Require file-owner：web 用户名必须与请求文件的 UID 对应用户名一致才能访问 Require file-group：web 用户名必须为请求文件的 gid 组中的一员才能访问 组认证需要mod_authz_groupfile模块 Require ip IP地址[/Mask]...：指定 IP 能访问该目录 Require host 域名...：指定域名能访问该目录 关于 ip 和 host 的认证需要mod_authz_host模块 若Require后加上not则是取反。 认证实验： 首先创建密码认证文件：htpasswd -cb /etc/httpd/secret mike 123456，认证用户并不需要在系统中存在。 配置文件中的认证配置： &lt;Directory &quot;/var/www/virhost1&quot;&gt; Options Indexes AllowOverride None #若通过.htaccess文件配置了以下认证信息，则需要将AllowOverride的值设为AuthConig AuthType Basic AuthName &quot;Enter Auth Username and Password:&quot; AuthUserFile /etc/httpd/secret Require user mike&lt;/Directory&gt; 重启 httpd，通过浏览器访问，会提示输入用户名密码。 创建组认证文件echo &quot;group1: mike&quot; &gt; /etc/httpd/auth_group，修改配置文件： &lt;Directory &quot;/var/www/virhost1&quot;&gt; Options Indexes AllowOverride None AuthType Basic AuthName &quot;Enter Auth Username and Password:&quot; AuthUserFile /etc/httpd/secret AuthGroupFile /etc/httpd/auth_group Require user mike Require group group1&lt;/Directory&gt; .htaccess 文件.htaccess文件提供了一种基于每个目录进行配置更改的方法，该文件包含一个或多个配置，而该文件存放在某个Directory下，则该文件中的配置都应用于这个Directory。 如果有权限访问 httpd 主服务器配置文件，则应该完全避免使用.htaccess文件，使用.htaccess文件会降低 Apache http 服务器的速度。 如果想给.htaccess文件改名，则需要在配置文件中用指令AccessFileName &quot;文件名&quot;说明。 是否启用.htaccess文件取决于AllowOverride指令，该指令决定是否启用文件中定义的指令。 通常，只有在无法访问主服务器配置文件时才应使用.htaccess 文件。.htaccess 文件主要面向于没有 root 访问权限而无法改动主配置文件的用户，允许他们通过配置各自网站的.htaccess 文件自行进行配置修改。 应该避免使用.htaccess文件的两点原因： 当 AllowOverride 设置为允许使用.htaccess 文件时，httpd 将在每个目录中查找.htaccess 文件。因此，允许.htaccess 文件会导致性能下降，且每次请求文档时都会加载.htaccess 文件。 httpd 必须在所有更高级别的目录中查找.htaccess 文件，以便拥有必须应用的完整指令。 例如，如果从目录/www/htdocs/example中请求文件，httpd 必须查找以下文件 /.htaccess/www/.htaccess/www/htdocs/.htaccess/www/htdocs/example/.htaccess 这样会查找四个文件，即使不存在。 若指定重定向的指令，则在.htaccess 上下文中，必须重新编译每个对目录的请求的正则表达式 允许用户修改服务器配置可能导致无法控制的更改，必须对用户的权限进行精细的控制，准确地设置 AllowOverride 的内容。 由于会从最上级目录迭代向下查找.htaccess 文件，所以，若不同的.htaccess 文件中有相同指令，则最下层的.htaccess 文件中的该指令生效，下层的文件中的指令会覆盖上层文件中相同的指令。 .htaccess 文件的常用示例认证（Authentication）：需要在&lt;Directory&gt;中配置AllowOverride AuthConfig AuthType BasicAuthName &quot;Password Required&quot;AuthUserFile &quot;/www/passwords/password.file&quot;AuthGroupFile &quot;/www/passwords/group.file&quot;Require group admins 服务器端包括（Server Side Includes，SSI）：提供了向现有 HTML 文档添加动态内容的方法，而无需通过 CGI 程序或其他动态技术。 SSI 适用于在大部分内容都是静态的网页中添加小块动态信息，例如当前时间。若网页大部分内容都是动态生成的，则并不适用。 **若要使能 SSI，则需要在配置文件中或.htaccess 文件中添加Options +Includes**，表示允许为 SSI 指令解析文件。 还需要告诉 Apache 需要解析的文件，例如： AddType text/html .shtmlAddOutputFilter INCLUDES .shtml 缺点：如果想将 SSI 指令添加到现有页面，则必须更改该页面的名称以及该页面的所有链接。 重写规则（Rewrite Rules）：在.htaccess 文件中使用 RewriteRule 时，每个目录的上下文会稍微改变一下，规则被认为是相对于当前目录，而不是原始请求的 URI。 在根目录中的.htaccess文件RewriteRule &quot;^images/(.+)\\.jpg&quot; &quot;images/$1.png&quot;在images中的.htaccess文件RewriteRule &quot;^(.+)\\.jpg&quot; &quot;$1.png&quot; CGI 配置：允许指定目录中的 CGI 程序运行 Options +ExecCGIAddHandler cgi-script cgi pl若要将目录中的所有文件都看做CGI程序，则将AddHandler替换为SetHandler cgi-script 页面重定向CGICGI（common gateway interface，通用网关接口）是 Web 服务器运行时外部程序的规范，按 CGI 编写的程序可以扩展服务器功能，处理动态内容。CGI 应用程序能与浏览器进行交互，还可通过数据库 API 与数据库服务器等外部数据源进行通信,从数据库服务器中获取数据。对于 HTTP，只有 get 和 post 方法允许执行 cgi 脚本。 常见的 CGI 术语： fastcgi：是 cgi 协议的优化版本 php-cgi：php-cgi 实现了 fastcgi，但性能不佳，单进程处理请求。 php-fpm：全称：php-fastcgi process manager，是 php-cgi 的改进版，管理多个 php-cgi 的进程及线程 cgi 进程或线程：用于接收 web 服务器的动态请求，调用并初始化 zend 虚拟机 zend 虚拟机：对 php 文件的语法分析、编译并执行，执行完成后关闭 CGI 的三种交互模式： cgi 模式：httpd 每收到一个动态请求就 fork 一个 cgi 进程，该进程返回结果后就自动销毁 动态模块模式：将 php-cgi 模块编译进 httpd php-fpm 模式：使用 php-fpm 管理 php-cgi，httpd 不再控制 php-cgi 进程的启动，可将 php-fpm 独立运行在其他非 web 服务器上，实现动静分离 动态 httpd安装 Apache 后，会在存放网页的目录中生成一个目录cgi-bin，关于 CGI 的配置在主配置文件中。 指令ScriptAlias：使 Apache 允许执行一个特定目录中的 CGI 程序，当客户端请求此特定目录中的资源时，Apache 假定其中所有的文件都是 CGI 程序并试图运行它。格式：ScriptAlias /cgi-bin/ &quot;CGI存放目录&quot; &lt;IfModule alias_module&gt; ScriptAlias /cgi-bin/ &quot;/var/www/cgi-bin/&quot;&lt;/IfModule&gt;&lt;Directory &quot;/var/www/cgi-bin&quot;&gt; AllowOverride None Options None Require all granted&lt;/Directory&gt; 关于 CGI 模块加载的配置在conf.modules.d/01-cgi.conf中。 &lt;IfModule mpm_worker_module&gt; LoadModule cgid_module modules/mod_cgid.so&lt;/IfModule&gt;&lt;IfModule mpm_event_module&gt; LoadModule cgid_module modules/mod_cgid.so&lt;/IfModule&gt;# worker和event使用mod_cgid，而prefork使用mod_cgi&lt;IfModule mpm_prefork_module&gt; LoadModule cgi_module modules/mod_cgi.so&lt;/IfModule&gt; 若开启了 Selinux，则还需要修改cgi-bin的上下文chcon -R -t httpd_sys_script_exec_t /var/www/cgi-bin httpd 与 SSLSSL 对 Apache 能提供的功能： 认证用户与服务器 提供数据保密性和完整性 SSL 协议的工作流程包括服务器认证阶段和用户认证阶段 客户端向服务器发送一个 hello 开始消息，发起一个会话连接 服务器根据客户端信息确定是否生成新的主密钥，如果需要就会在响应 hello 信息中添加生成主密钥需要的信息 客户端收到响应信息，根据信息生成一个主密钥，用服务器的公钥加密发给服务器 服务器收到后返回客户一个用主密钥认证的信息，让客户端认证服务器 服务器通过客户端认证后，进入用户认证阶段，由服务器开始对客户端的认证 服务器向客户端发起提问（封装在数字签名中） 客户端返回答案和公钥，提供认证信息 HTTPS 安全超文本传输协议，内置在浏览器中，对数据压缩和解密。HTTPS 就是用 SSL 作为 HTTP 应用层的子层，使用 TCP443 端口。 Apache 通过mod_ssl模块实现对 TLS/SSL 的支持，该模块存放在/usr/lib64/httpd/modules/mod_ssl.so，并有配置文件/etc/httpd/conf.modules.d/00-ssl.conf。还有相关模块mod_socache_shmcb，是一个共享对象缓存提供程序，提供对共享内存段内高性能循环缓冲区支持的缓存的创建和访问，已默认加载。 httpd 服务器配置自签名证书： openssl genrsa -out /etc/pki/tls/private/server.key 2048 #生成私钥openssl req -new -x509 -key /etc/pki/tls/private/server.key -out /etc/pki/tls/certs/server.crt #根据私钥生成根证书 Country Name (2 letter code) [XX]:CN #国家名 State or Province Name (full name) []:jiangsu #省名 Locality Name (eg, city) [Default City]:Yangzhou #地名 Organization Name (eg, company) [Default Company Ltd]:NJUPT #公司名 Organizational Unit Name (eg, section) []:Tech #部门名 Common Name (eg, your name or your server&#x27;s hostname) []:system1 #主机名 Email Address []:system1@example.com #邮箱 也可以通过进入/etc/pki/tls/certs并使用命令make server.key创建私钥。 配置 SSL 虚拟主机，需要引用/etc/httpd/conf.d/ssl.conf中的配置，并做修改。 &lt;VirtualHost *:443&gt; SSLEngine on SSLProtocol all -SSLv2 -SSLv3 SSLCipherSuite HIGH:3DES:!aNULL:!MD5:!SEED:!IDEA SSLHonorCipherOrder on SSLCertificateFile /etc/pki/tls/certs/server.crt SSLCertificateKeyFile /etc/pki/tls/private/server.key DocumentRoot &quot;/var/www/html&quot; ServerName system1.example.com ErrorLog &quot;/var/log/httpd/error_log&quot; CustomLog &quot;/var/log/httpd/access_log&quot; common&lt;/VirtualHost&gt; 由于是对已存在的虚拟主机进行 SSL 封装，所以，需要在原虚拟主机中添加两条指令进行重定向，使任何访问原 http 地址的客户端都跳转到 https 地址。 &lt;VirtualHost *:80&gt; DocumentRoot &quot;/var/www/html&quot; ServerName system1.example.com ErrorLog &quot;/var/log/httpd/error_log&quot; CustomLog &quot;/var/log/httpd/access_log&quot; common RewriteEngine on RewriteRule ^(/.*)$ https://%&#123;HTTP_HOST&#125;$1 [redirect=301]&lt;/VirtualHost&gt; 通过浏览器访问http://system1.example.com，会出现不安全提示 确认添加例外后，就能自动跳转到https://system1.example.com。 httpd 日志当 Apache 开始运行后，会生成 4 种标准日志文件： 错误日志 error_log 访问日志 access_log 传输日志 cookie 日志 若使用 SSL 加密，还会生成ssl_access_log、ssl_error_log、ssl_request_log。当日志文件过大时，Apache 会自动生成新的日志文件，文件的名称以配置文件中指定。 LogFormat指定的日志记录格式变量： 变量 含义 %b 发送字节（不含 HTTP 标题） %f 文件名 %h 远程主机 %a 远程 IP 地址 %{HEADER}i HEADER 内容：发送给服务器的请求 %p 服务器的服务端口 %r 请求的第一行，类似 GET / HTTP/1.0 %s 状态（起始请求），最后请求状态为%&gt;s %t 时间，格式是 common 日志格式中的时间格式 %{format}t 时间，格式由 format 给出 %T 服务器请求花费的时间（单位秒） %u 来自 auth 的远程用户，若返回码为 401 则可能是假的 %U 请求的 URL 路径 %v 服务器的提供服务 ServerName 错误日志记录的等级： 等级 解释 Emerg 紧急，系统不可用 Alert 需要立刻注意 Crit 危险警告 Error 除上述三种外的其他情况 Warm 警告 Notice 需要引起注意 Info 一般消息 Debug Debug 模式产生的消息 访问日志的种类： 普通日志：在 LogFormat 定义的名字为 common 参考日志：记录客户访问站点的用户身份，名字为 referer 代理日志：记录请求的用户代理，名字为 agent 综合日志：结合了上面三种，名字为 combined 日志切割Apache 提供了命令rotatelogs，对日志进行切割，将庞大的日志文件切割为相对小的文件。 以轮替时间做切割：rotatelogs [options] 日志文件 [轮替时间（单位秒）][偏移量]以日志大小做切割：rotatelogs [options] 日志文件 [日志文件大小]偏移量为相对于UTC的分钟数，若省略，默认为0，即使用UTC时间。东八区即为8x60=480可以把以下配置添加到主配置文件：TransferLog &quot;|rotatelogs 日志文件 86400&quot;TransferLog &quot;|rotatelogs 日志文件 5M&quot;默认生成的日志名为&quot;日志名.日志开始记录的时间&quot;，如果使用轮替时间，则该值就是轮替时间的倍数，可通过cron服务设置。如果日志文件包含了strftime转换格式，则使用该格式。当轮替时间结束或日志文件大小达到指定值，就会生成一个新日志文件 -v 详细的操作信息会被错误输出(strerr) -l 使用本地时间。不要在改变了GMT偏移量的环境中使用该选项，若设置了此选项，也就不用设置偏移量了 -f 在程序启动时强制开启日志 -t 截断日志 -e 输出日志到标准输出 -c 创建日志，无论是否为空 若要按时间轮替日志文件： ErrorLog &quot;|rotatelogs 日志存放目录/%Y%m%d_error.log 86400 480&quot; CustomLog &quot;|rotatelogs 日志存放目录/%Y%m%d_access.log 86400 480&quot; common 若要按日志大小轮替日志文件： ErrorLog &quot;|rotatelogs -l 目录/%Y%m%d_error.log 5M&quot; CustomLog &quot;|rotatelogs -l 目录/%Y%m%d_access.log 5M&quot; common Webalizer 分析统计日志Webalizer 是一个高效的 web 服务器日志分析程序。webalizer 基本支持所有的日志文件格式，包括 common，combined。目前还支持 ftp 日志、squid 日志分析。 直接yum install webalizer即可。webalizer 的配置主要通过配置文件webalizer.conf实现。 LogFile /var/log/httpd/access_log #日志文件的路径，也可通过命令行选项指定OutputDir /var/www/usage #统计报表的输出位置HistoryName /var/lib/webalizer/webalizer.hist #webalizer生成的历史文件名Incremental yes #设置是否增量IncrementalName /var/lib/webalizer/webalizer.current #保存当前数据的文件名PageType htm* #定义哪些类型的URL属于页面访问UseHTTPS no #若在一台安全服务器上运行，需要开启DNSCache /var/lib/webalizer/dns_cache.db #反向DNS解析的缓存文件DNSChildren 10 #设置用于DNS解析的子进程，值要在5到20间Quiet yes #不显示输出信息FoldSeqErr yes #强制忽略次序错误，因为Apache HTTP服务器可能会生成无序日志条目HideURL *.gif #设置需要隐藏的内容SearchEngine yahoo.com p= #设置搜索引擎和URL查询格式 一般只要配置LogFile和OutputDir即可。 webalizer [options] [log file] -v 显示日志详细信息 -d 显示额外的debug信息 -F type 设置日志类型（clf | ftp | squid | w3c） -f 忽略次序错误 -i 忽略历史文件 -p 保留状态（递增） -b 忽视状态（递增） -q 忽略消息信息 -Q 忽略所有信息 -T 显示时间信息 -c file 指定配置文件 -n name 指定服务器主机名 -o dir 指定存放结果的文件 -t name 指定报告题目的主机名 --ip 查看指定IP地址的访问情况 --start 指定开始时间 --end 指定结束时间 在/var/www/usage下生成了几张图片和两个 html 文件，其中index.html是简要信息，usage_日期.html是详细的分析文件 index.html usage_日期.html httpd 代理httpd 通过ProxyRequests指令配置正向代理的功能 ProxyRequests onProxyVia on&lt;Proxy &quot;*&quot;&gt; #访问任意外网URL Require host 允许通过代理访问外网的内网服务器 #也可以是Require all granted&lt;/Proxy&gt; Apache-MPM 模式MPM(Multi-Processing Modules)，Apache 的多路处理模块，有三种模式：prefork、worker、event。 编译时可通过--with-mpm指定模式，也可以通过--enable-mpms-shared=all支持全部三种。httpd2.4 以上默认采用event模式。并可通过apachectl -l看到编译了event.c模块。 httpd2.4 通过 rpm 安装会发现仍采用 prefork 模式，而源码安装则已使用 event 模式 可以修改httpd.conf中添加以下模块（源码安装）或/etc/httpd/conf.modules.d/00-mpm.conf（rpm 安装）改变模式 LoadModule mpm_prefork_module modules/mod_mpm_prefork.so 或LoadModule mpm_worker_module modules/mod_mpm_worker.so 或LoadModule mpm_event_module modules/mod_mpm_event.so prefork：实现了一个非线程、预派生的工作模式。在 Apache 启动之初，就会预派生一些子进程，然后等待连接。可以减少频繁创建和销毁进程的开销，每个子进程只有一个线程。成熟稳定，可以兼容新老模块，也不需要担心线程安全问题。效率比worker略高 缺点：一个进程相对地占用更多的资源，消耗大量内存，不擅长处理高并发的场景。 worker：使用了多进程和多线程的混合模式，也同样会预派生一些子进程，然后每个子进程创建一些线程，同时包括一个监听线程，每个请求过来会被分配到一个线程来服务。占用内存少，适合高并发环境 使用线程的原因：线程比进程更加轻量级，因为线程通常会共享父进程的内存地址的，因此内存占用会减少一些。如果一个线程异常挂了，会导致父进程和它的其他正常子线程都挂了，只会影响 Apache 的一部分，而不是整个服务。 缺点：必须考虑线程安全问题，因为多个子进程时共享父进程的内存地址的。若使用 keepalive 的长连接方式，某个线程一直占据，若过多的线程被占用，会导致高并发时无服务线程可用。 event：从 Apache2.2 才被加入 MPM，Apache2.4 开始成为默认 MPM。类似 worker 模式，但解决了 keepalive 问题。有一个专门的线程来管理这些 keep-alive 线程，当有真实请求过来的时候，将请求传递给服务线程，执行完毕后，又允许它释放，这样增强了在高并发场景下的请求处理能力。 各 MPM 模式的简单优化： 若是 rpm 安装，就在httpd.conf中添加，若为源码安装，就在/usr/local/httpd-2.4/conf/extra/httpd-mpm.conf中找到对应模块修改。以下参数基本采用配置文件默认值。 worker模式 &lt;IfModule mpm_worker_module&gt; ServerLimit 25 #服务器允许配置的上限进程数 # 与ThreadLimit结合使用，可设置MaxClients允许配置的最大数值 # 在重启期间对ServerLimit的修改都会被忽略，但对MaxClients的修改可生效 ThreadLimit 200 #每个子进程可配置的线程数的上限 # 也设置了ThreadPerChild的上限，默认为64 StartServers 3 #服务器启动时建立的子进程数，默认为3 MinSpareThreads 75 #最小空闲线程数，默认75。 #若服务器中空闲进程太少，子进程会自动产生空闲进程等待 MaxSpareThreads 250 #最大空闲线程数，默认250。 #若服务器中空闲进程太多，子进程会杀死多余的空闲进程。 #取值范围：&gt;= MinSpareThreads+ThreadsPerChild MaxClients 2500 #允许同时运行的最大进程数，任何超过限制的请求进入等待队列 #默认值为ServerLimit*ThreadsPerChild。若要增加此项，则同时也要增加ServerLimit ThreadsPerChild 25 #每个子进程建立的常驻执行线程数，默认25。 #子进程创建这些线程后就不创建新线程了 MaxConnectionsPerChild 0 #处理多少个请求后子进程自动销毁，默认值0意味着永不销毁。 # 在Apache2.4版本以前，叫做MaxRequestsPerChild #当负载较高时，为了使每个进程处理更多的请求，避免销毁、创建进程的开销，一般建议设置为0或较大的数字。&lt;/IfModule&gt; prefork模式 &lt;IfModule mpm_prefork_module&gt; StartServers 5 MinSpareServers 5 #最小空闲进程数，与MinSpareThreads同理 MaxSpareServers 10 #最大空闲进程数，与MaxSpareThreads同理 ServerLimit 2000 MaxClients 1000 #默认MaxClients最多有256个线程 MaxConnectionsPerChild 0&lt;/IfModule&gt; 修改模式重启 httpd，查看进程，能看到五个子进程 ps -ef | grep httpdroot 2241 1 1 22:05 ? 00:00:00 /usr/sbin/httpd -DFOREGROUNDapache 2242 2241 0 22:05 ? 00:00:00 /usr/sbin/httpd -DFOREGROUNDapache 2243 2241 0 22:05 ? 00:00:00 /usr/sbin/httpd -DFOREGROUNDapache 2244 2241 0 22:05 ? 00:00:00 /usr/sbin/httpd -DFOREGROUNDapache 2245 2241 0 22:05 ? 00:00:00 /usr/sbin/httpd -DFOREGROUNDapache 2248 2241 0 22:05 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND event模式 &lt;IfModule mpm_event_module&gt; StartServers 3 MinSpareThreads 75 MaxSpareThreads 250 ThreadsPerChild 25 MaxRequestWorkers 400 MaxConnectionsPerChild 0&lt;/IfModule&gt; Apache 实用第三方模块若要在 httpd 上添加新的模块，可以通过修改模块配置文件增加LoalModule指令添加，也可以使用httpd-devel提供的工具apxs直接添加，需要 httpd 开启了 DSO。 apxs -n 模块名 指定模块名，可与-i和-g组合 -i 安装模块，可指定多个 -a 自动在主配置文件加上LoadModule -A 自动在主配置文件加上#LoadModule，即安装了但先不启用 -c C文件 将.c文件编译为.so文件 模块间可能存在依赖，可根据报错信息解决。 Gzip 压缩Gzip 将 Apache 网页内容压缩后传输给客户端，加快网页加载速度，建议开启。Gzip 有两个模块：mod_gzip和mod_deflate， 防 DDOS 攻击应对 DDOS 攻击的模块为mod_evasive，可通过yum install mod_evasive获取。安装完成后会生成/usr/lib64/httpd/modules/mod_evasive24.so，以及/etc/httpd/conf.d/mod_evasive.conf LAMP 环境搭建直接用yum|dnf安装 php，php 版本为 7.1。php 的核心包： php：在/etc/httpd/conf.d/创建了php.conf，在/usr/lib64/httpd/modules/创建了libphp7.so php-common：创建了大量模块存放在/usr/lib64/php/modules/，帮助文档，配置文件/etc/php.ini以及/etc/php.d/中各个配置文件。 php-fpm：创建了配置文件/etc/php-fpm.conf及/etc/php-fpm.d/ php-cli：创建了命令php、php-cgi、phar、phpize存放在/usr/bin/中 使用systemctl start php-fpm启动。 参考文章 高性能网站构建实战 Linux 系统管理与网络管理 Linux 就该这么学 Linux 运维之道（第二版） Linux 服务器架设指南（第二版） 防线-企业 Linux 安全运维理念和实战 RHCSA/RHCE 红帽 Linux 认证学习指南（第 7 版） Apache 性能优化之 MPM 选择和配置 Apache 的三种 MPM 模式比较：prefork，worker，event apache 的三种 mpm 模式 浅谈.htaccess 文件–避免滥用.htaccess 文件 Apache HTTP Server Tutorial: .htaccess files Apache httpd Tutorial: Introduction to Server Side Includes 简单说明 CGI 和动态请求是什么","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"Apache","slug":"Apache","permalink":"https://coconutmilktaro.top/tags/Apache/"},{"name":"http","slug":"http","permalink":"https://coconutmilktaro.top/tags/http/"},{"name":"LAMP","slug":"LAMP","permalink":"https://coconutmilktaro.top/tags/LAMP/"},{"name":"LAMT","slug":"LAMT","permalink":"https://coconutmilktaro.top/tags/LAMT/"}]},{"title":"NFS基础笔记","slug":"NFS笔记","date":"2018-05-02T09:18:46.000Z","updated":"2022-06-21T15:16:21.214Z","comments":true,"path":"2018/NFS笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/NFS%E7%AC%94%E8%AE%B0/","excerpt":"NFS 原理 NFS 基础配置 服务器端 NFS 客户端 nfs 挂载优化 系统安全相关 挂载性能相关","text":"NFS 原理 NFS 基础配置 服务器端 NFS 客户端 nfs 挂载优化 系统安全相关 挂载性能相关 NFS 原理NFS（Network Files Network）网络文件系统，允许网络中主机通过通过 TCP/IP 进行资源共享。采用 C/S 工作模式，NFS 服务器相当于文件服务器，将某个目录设置为输出目录，客户端可将服务器端的输出目录挂载在本地进行访问。NFSv4 基于 TCP，端口号 2049。RPC（Remote Procedure Call）远程过程调用，是一种通过网络从远程计算机程序上请求服务，跨越了传输层和应用层的协议。NFS 在传输数据时使用的端口会随机选择，而客户端是通过 RPC 知道服务器端使用的哪个端口传输数据。NFS 的 RPC 主要功能是记录每个 NFS 功能对应的端口号，并在 NFS 客户端请求时将该端口和功能信息传给该客户端，确保客户端连到正确的端口。当 NFS 服务启动时会随即取用若干端口，并主动向 RPC 服务注册这些端口，然后 RPC 使用固定的 111 端口监听 NFS 客户端请求，并将正确的端口信息返回给客户端。 注： 在启动 NFS 之前先要启动 RPC 服务，否则 NFS 无法向 RPC 注册，若 RPC 重启则原来注册的数据就全部丢失，NFS 也需重启以重新注册 RPC。 用户访问 web 页面，web 服务器上的 nfs 客户端便会向后端的 NFS 服务器通过 rpc 服务发出请求。 nfs 服务器端 rpc 找到对应注册的 nfs 端口后，通知 nfs 客户端的 rpc nfs 客户端得到了服务器端的取数据的端口，与 nfs daemon 连接存取数据 nfs 客户端在将得到的文件传给前端 NFS 局限： 存在单点故障，可通过高可用集群解决 高并发场景，NFS 性能有限 客户端认证基于 IP 和主机名，安全性一般 NFS 传输明文，不对数据完整性验证 多台客户端挂载一个 NFS 时，耦合度高，管理维护麻烦 NFS 基础配置环境： 两台虚拟机192.168.163.103/24192.168.163.104/24 系统：CentOS7 Selinux：关闭 防火墙：关闭 服务器端NFS 服务依赖于 RPC 服务与外界通信，所以需要安装 rpcbind 程序而 NFS 安装 NFS 主程序和 RPC 程序yum install nfs-utils rpcbindsystemctl enable nfs-serversystemctl start nfs-server rpcbind.service 配置文件/etc/exports 可通过rpcinfo -p localhost查看已注册的端口 # rpcinfo -p localhost program vers proto port service 100000 4 tcp 111 portmapper 100000 3 tcp 111 portmapper 100000 2 tcp 111 portmapper 100000 4 udp 111 portmapper 100000 3 udp 111 portmapper 100000 2 udp 111 portmapper 100024 1 udp 49302 status 100024 1 tcp 49199 status 100005 1 udp 20048 mountd 100005 1 tcp 20048 mountd 100005 2 udp 20048 mountd 100005 2 tcp 20048 mountd 100005 3 udp 20048 mountd 100005 3 tcp 20048 mountd 100003 3 tcp 2049 nfs 100003 4 tcp 2049 nfs nfs 相关进程 rpc.statd：检查文件一致性 rpc.idmapd：名字映射 rpc.mountd：权限管理验证（nfs mount daemon） nfsd：NFS 主进程，管理登入，ID 身份判明等 /etc/exports文件配置 # 格式为：共享目录 分享给的主机1(参数) 主机2(参数) .../var/nfsshare 192.168.163.*(rw,sync) *.example.com(rw)# 客户端地址可以是 IP、主机名、域名、网段 参数列表 参数 含义 ro 客户端只读 rw 客户端可读写 sync 数据同步写入内存与磁盘，保证数据一致性，效率低 async 异步 IO，数据先暂时存与内存，待需要时写入硬盘，效率高，但数据丢失风险高 noaccess 阻止访问该目录及其子目录 all_squash 无论 NFS 客户端使用什么用户访问，都映射为 NFS 服务器端的匿名用户，即 nfsnobody root_squash 当 NFS 客户端使用 root 访问时，映射为 NFS 服务器端的匿名用户，即 nfsnobody no_root_squash 当 NFS 客户端使用 root 访问时，仍映射为 NFS 服务器端的 root，并不安全 wdelay 为合并多次更新而延迟写入磁盘 no_wdelay 尽可能快地写入磁盘 secure 限制 nfs 服务只能使用小于 1024 的 TCP/IP 端口传输数据 insecure 可使用大于 1024 的端口 anounuid 指定 NFS 服务器中的用户为匿名用户 anoungid 指定 NFS 服务器中的用户组为匿名用户组 默认情况下，nfs 服务会禁止客户端的 root 用户对共享目录进行写操作，目的是为了保证当 nfs 以共享目录工作时，共享目录的数据不会被客户端随意修改，但是当 nfs 以远程存储工作时，这个功能就不合理，所以当 nfs 以远程存储来工作时，需要在服务端设置 no_root_squash 选项关闭该功能。 配置完后使用exportfs -rv命令，不需要重启 NFS。 exportfs -a 导出所有列在/etc/exports的目录 -v 显示所有被导出或取消导出的目录 -r 重新导出所有列在/etc/exports的目录 -u [目录] 取消指定目录的导出，与-a同时用时，会取消配置文件中所有目录的导出 nfsstat命令可查看当前 NFS 信息 nfsstat -s 显示NFS服务器信息 -c 显示NFS客户端信息 -m 显示每个NFS文件系统的统计信息（在客户端上查看） -r 显示RPC信息 若开启了 firewalld，则需要放行 nfs 和 rpcbind 还有 mountd 服务，放行端口 2049 firewall-cmd --permanent --add-service=nfsfirewall-cmd --permanent --add-service=rpc-bindfirewall-cmd --permanent --add-service=mountdfirewall-cmd --permanent --add-port=2049/tcp 文件chcon -R -t public_content_t /var/nfssharesetsebool -P nfs_export_all_rw onsetsebool -P nfs_export_all_ro on NFS 客户端 需要安装nfs-utils rpcbindyum install nfs-utils rpcbind 通过showmount查看 NFS 服务器的共享信息 showmount [options] [NFS服务器] -e 显示NFS服务器的共享列表 -a 显示本机挂载NFS资源情况 # showmount -e 192.168.163.102Export list for 192.168.163.102:/var/nfsshare 192.168.163.* 挂载到本机 # mkdir /nfsshare #创建挂载目录# mount -t nfs 192.168.163.102:/var/nfsshare /nfsshare# df -hFilesystem Size Used Avail Use% Mounted on......192.168.163.102:/var/nfsshare 17G 8.1G 9.0G 48% /nfsshare 在 mount 时也可使用-o指定文件系统的选项 rsize= 从NFS服务器读文件时每次使用的字节数，默认1024字节wsize= 向NFS服务器写文件时每次使用的字节数，默认1024字节timeo= RPC调用超时后，确定重试算法的参数soft 软挂载方式，当客户端请求得不到回应时，提示IO错误并退出hard 硬挂载方式，当客户端请求得不到回应时，提示服务器无响应，但继续请求。默认硬挂载intr NFS文件操作超时并时硬挂载时，允许中断文件操作并向调用它的程序返回EINTRro 只读方式挂载NFS文件系统rw 读写方式挂载NFS文件系统fg 在前台重试挂载bg 在后台重试挂载 也可通过配置文件/etc/fstab开机自动挂载 # vim /etc/fstab......192.168.163.102:/var/nfsshare /nfsshare nfs defaults 0 0 使用 mount 或配置文件/etc/fstab 挂载的不足：NFS 服务器与客户端的连接不是永久的，任何一方的掉线都会导致另一方等待超时。并且即使很多用户都挂载了共享目录，也会有大部分的用户在大部分时间是不会使用的，这样造成了 NFS 服务器资源的大量消耗。可通过 autofs 服务按需动态挂载解决该问题。 使用 autofs 自动挂载autofs 是一个提供按需挂载的服务，只有在用户访问该挂载点时才会动态挂载该共享目录。安装 autofs 程序，并开机自启yum install autofssystemctl enable autofs.servicesystemctl start autofs.service创建 autofs 关于 nfs 主配置文件，也可以直接在 autofs 的主配置文件/etc/auto.master中添加内容。 # vim /etc/auto.master.d/nfs.autofs主配置文件的配置格式：挂载点顶层目录 映射文件/nfs /etc/nfs.misc由于挂载点为/nfs/share，所以顶层目录为/nfs 创建 nfs 配置的映射文件/etc/nfs.misc # vim /etc/nfs.misc映射文件格式：挂载点 [-挂载选项] NFS服务器名或IP:共享目录挂载点是对于挂载点顶层目录的相对路径share -fstype=nfs,rw 192.168.163.102:/var/nfsshare 配置完成后重启 autofs 服务即可。进入/nfs目录中，查看并无内容。然后进入share便可查看到挂载目录的内容。再通过df查看，已成功挂载。 注：由于在客户端挂载时也会指定选项，若与服务器端选项不同，在执行操作时可能会报错，即：选项以服务器端配置为准。 nfs 挂载优化系统安全相关对企业一般来说，NFS 只共享静态数据，不需要 suid、exec 权限，不应允许执行命令。所以在挂载时应该指定以下参数 mount -t nfs -o nosuid,noexec,nodev,rw 10.0.0.1:/data /mnt 挂载性能相关 禁止更新目录及文件时间戳挂载mount -t nfs -o noatime,nodiratime 10.0.0.1:/data 安全加优化mount -t nfs -o nosuid,noexec,nodev,noatime,nodiratime,intr,rsize=131072,wsize=131072 10.0.0.1:/data /mnt rsize 为 nfs 客户端读 server 的数据块大小，wsize 为 nfs 客户端写 server 的数据块大小，最好设为 1024 的倍数配置这两个参数可以让一次读写更多数据包，提高访问性能 内核层面优化 net.core.wmem_default：接受套接字缓冲区大小的默认值，默认为 124928 net.core.rmem_default：发送套接字缓冲区大小的默认值，默认为 124928 net.core.rmem_max：接受套接字缓冲区大小的最大值，默认同上 net.core.wmem_max：发送套接字缓冲区大小的最大值，默认同上可以将这些参数调大","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"nfs","slug":"nfs","permalink":"https://coconutmilktaro.top/tags/nfs/"}]},{"title":"Samba基础学习笔记","slug":"samba笔记","date":"2018-05-02T09:18:06.000Z","updated":"2022-05-30T02:51:53.879Z","comments":true,"path":"2018/samba笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/samba%E7%AC%94%E8%AE%B0/","excerpt":"本篇笔记包含以下内容： Samba 原理 Samba 基础配置 服务器端 客户端","text":"本篇笔记包含以下内容： Samba 原理 Samba 基础配置 服务器端 客户端 Samba 原理Samba 最初的目的是为了 Windows 和 Linux 之间的沟通，实现不同操作系统的资源共享，如今成为了十分安全，高效的文件服务。Samba 有以下主要功能： 共享文件与打印机 提供身份认证，给不同身份的用户不同的访问权限和文件 可进行域名解析，将计算机的 NetBIOS 名解析为 IP 地址 samba 能收集局域网上用户广播的主机信息，提供检索服务，也称为浏览服务，能显示共享目录，打印机等资源 支持 SSL Samba 整合了 SMB 协议和 NetBIOS 协议，基于 TCP/IP。NetBIOS 协议NetBIOS（Network Basic Input/Output System），网络基本输入输出系统协议，属于会话层协议。能通过该协议获取计算机主机名，并解析为 IP 地址。SMB 协议SMB（Send Messsage Block），运行于 NBT 协议（NetBIOS over TCP/IP），属于表示层与应用层协议。端口号：139/TCP，137、138/UDP。 SMB 协议工作流程 协议协商客户端向 Samba 服务器发送 Negport 请求报文，列出所有支持的 SMB 版本。服务器收到后回应 Negport 报文，列出希望客户端使用的 SMB 版本。 建立连接确定了 SMB 版本，客户端会发送 Session Setup 请求报文，包含用户名与密码，建立连接。服务器收到后进行验证并回应报文，若验证通过，就返回为该用户分配的唯一 UID，若失败则返回失败信息。 访问共享资源客户端向服务器发送 Tree Connect 请求报文，包含要访问的共享资源名。服务器收到后，根据配置文件确定是否该用户能访问，返回一个响应报文，若允许访问，就给该用户与共享资源连接分配一个 TID，用户即可访问该资源。 断开连接客户端向服务器发送 Tree Disconnect 报文，请求服务器断开连接，服务器也回应一个响应，并断开连接。 Samba 守护进程 用户若要访问 Windows 上的公共资源，必须加入该 Windows 主机的群组 Workgroup，并该用户主机必须设置一个主机名（不是 hostname），该主机名是建立在 NetBIOS 协议上的，可称为 NetBIOS Name，在同一个群组中，该 NetBIOS Name 必须唯一。 用户是否能访问并对该文件进行操作不仅需要通过服务器身份认证，还需要对该文件具有权限。 Samba 服务有两个守护进程 smbd：用于管理 samba 主机共享目录、文件、打印机等，利用 TCP 传输文件，开放端口为 139/TCP 和 445/TCP nmbd：用于管理群组、NetBIOS Name 的解析，基于 UDP，开启端口 137/UDP，138/UDP 进行解析 Samba 安装包： samba：包含 samba 的守护进程文件，samba 文档，logrotate 配置文件，开机默认选项配置文件 samba-common：包含 samba 主要配置文件 smb.conf，配置文件检查程序 testparm 等 samba-client：samba 客户端程序，提供客户端操作指令集 Samba 基础配置实验环境： 两台虚拟机：samba 服务器：192.168.163.103/24samba 客户端：192.168.163.104/24 系统：CentOS7 Selinux：未开启 firewalld：未开启 服务器端安装 samba 客户端 yum install samba-common sambasystemctl enable smb nmbsystemctl start smb nmb 创建共享目录，可随意创mkdir /var/smbshare修改配置文件/etc/samba/smb.conf配置文件中可在选项前加;使其不生效，相当于#注释配置文件存在以下配置块：[global] 全局选项，对所有资源生效基础配置则不需要修改 [global] workgroup = SAMBA #设置群组 server string = Samba Server #设置服务器描述 netbios name = MYSERVER #设置NetBIOS Name interfaces = lo eth0 192.168.163.0/24 # 设置监听接口、IP地址 hosts allow = 192.168.163. #白名单，设置允许的主机网段 hosts deny = #黑名单，黑白名单设置一个即可 security = user #samba的安全模式，有三种模式：user、share、server #user模式为每次访问服务器都会登录验证，share模式为不需登录。官方仅推荐user模式。 passdb backend = tdbsam #存放用户信息，有两种选择tdbsam和lsapsam，tdbsam不需要额外配置 log file = /var/log/samba/log.%m #设置日志文件路径，%m会替换为请求连接的NetBIOS名 username map = /etc/samba/smbusers #设置用户映射，记录samba账号和虚拟账号的对应关系 #---打印配置--- printing = cups #打印配置，使用cups服务 printcap name = cups #通常设置为printcap文件 load printers = yes #自动加载打印机列表 cups options = raw #设置cups的选项，raw为允许在windows客户端上加载驱动 [homes]为特殊共享目录，表示用户主目录[printers]为特殊共享目录，表示打印机配置共享资源： [smbshare] comment = smbshare #资源描述 path = /var/smbshare #共享目录 public = no #是否允许匿名访问 guest ok = no #是否允许不输入密码访问 printable = Yes #是否可读 writable = yes #是否可写，只有该目录有写权限且此项为yes，才能写入 browseable = yes #是否可见 write list = mike #设定特定用户写权限 #若writable为no，此项仍能生效 create mask = 0600 #创建文件默认权限 directory mask = 0775 #创建目录默认权限链路 hosts allow =192.168.163. #白名单 可通过testparm检查配置文件是否正确可通过man smb.conf查看详细配置文件选项创建用户并添加到 samba由于该用户是提供给客户端用于登录 samba 的，所以在服务器端应设置为不能登陆，并且为了安全性，不要设密码。useradd mike -s /sbin/nologin注：samba 并不是将系统中的用户变为 samba 用户的，samba 的用户是独立于 linux 系统的，但必须在 linux 系统中存在，才能映射，所以 linux 系统中需要创建同名用户。将用户添加到 smb 服务器的用户列表中，并设置 smb 登录密码smbpasswd -a mike，然后输入登录密码 smbpasswd [options] [username] -a add添加 -d disable禁止用户访问 -n no password不设置密码，需要smb.conf中global设置nullpasswords=true开启 -x delete删除用户 或者使用另一条命令pdbedit，用于管理 SMB 服务的账号信息数据库pdbedit -a -u mike pdbedit [options] [username] -a 添加 -x 删除 -L 列出用户列表 -v 详细信息 注： 若安装并开启了 firewalld，需要开启端口 TCP139 端口，UDP137、138 端口，并放行服务 samba。 若安装并开启了 Selinux，需要添加上下文 chcon -R -t samba_share_t /var/smbshare或semanage fcontext -a -t samba_share_t /var/smbshare然后setsebool -P samba_export_all_rw onsetsebool -P samba_export_all_ro on若分享的是/homesetsebool -P samba_enable_home_dirs on配置完后restorecon -Rv /var/smbshare 客户端安装 samba 客户端yum install samba-client cifs-utilscifs-utils是让 Windows 系统能使用公共文件系统的工具。CIFS是微软开发的公共 Internet 文件系统协议，能够支持网上邻居。查看服务器给指定用户提供的共享目录的信息 # smbclient -L //192.168.163.103/smbshare -U mikeEnter SAMBA\\mike&#x27;s password: Sharename Type Comment --------- ---- ------- print$ Disk Printer Drivers share Disk SHARE smbshare Disk smbshare IPC$ IPC IPC Service (Samba 4.7.1) mike Disk Home DirectoriesReconnecting with SMB1 for workgroup listing. Server Comment --------- ------- Workgroup Master --------- ------- SAMBA SYSTEM3 从返回信息可得知共享资源以及群组和服务器名登录 smb 服务器，进入指定资源 # smbclient //192.168.163.103/smbshare -U mikeEnter SAMBA\\mike&#x27;s password:Try &quot;help&quot; to get a list of possible commands.smb: \\&gt; 已进入该共享目录，并进入 samba 客户端命令行模式，可通过help查看能进行的操作常用命令如下：put [本机文件路径] [资源中相对路径] 上传文件get [资源路径] 下载文件 客户端挂载 创建认证文件 # vim /root/secure/auth.smbusername=mikepassword=redhatdomain=SAMBA 设置该文件的权限，这个文件的机密性很重要chmod 700 /root/securechmod 600 /root/secure/auth.smb 挂载共享目录mount -t cifs -o rw,credentials=/root/secure/auth.smb //192.168.163.103/smbshare /shares/smbshare Windows 端登录及挂载在 Windows 端，可在文件资源管理器的地址栏输入\\\\192.168.163.103\\smbshare登录进入 smb 服务器的该资源。若要挂载，在“此电脑”中右击，选择“添加一个网络位置”，按“下一步”，进入以下界面，填入要挂载的共享目录 然后不断“下一步”，即可设置完成。在“此电脑”查看，已成功挂载。","categories":[],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"Samba","slug":"Samba","permalink":"https://coconutmilktaro.top/tags/Samba/"}]},{"title":"MySQL完全笔记","slug":"MySQL学习笔记","date":"2018-04-30T01:10:31.000Z","updated":"2022-06-21T15:16:21.212Z","comments":true,"path":"2018/MySQL学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"基于 MySQL5.7","text":"基于 MySQL5.7 本篇包含以下知识点： MySQL 体系 存储引擎 数据类型 运算符 函数 字符串函数 数值函数 日期和时间函数 系统信息函数 特殊功能函数 表操作 数据操作 多表查询 联合查询 连接查询 内连接 外连接 子查询 索引 普通索引 唯一索引 全文索引 多列索引 视图 触发器 存储过程与函数 存储过程 创建存储过程 函数 存储过程和函数表达式 事务 事务隔离级别 InnoDB 锁机制 安全 权限机制 mysql.user 表 用户机制 对用户的权限管理 日志 二进制日志 错误日志 通用查询日志 慢查询日志 维护 数据库备份与还原 数据库迁移 简单的性能优化思路 常见查看命令显示解析 SHOW TABLE STATUSMySQL 体系 MySQL 采用 C/S 体系，因此在使用时，是运行两个程序： mysqld：MySQL 服务器程序，运行在数据库服务器上，负责监听并处理请求 mysql-client：运行在客户端上，负责连接到数据库服务器并发出指令。 存储引擎MySQL 具有可替换存储引擎构架的特征。MySQL 功能分为两部分： 外层部分：完成与客户端的连接，调查 SQL 语句的内容 内层部分：即存储引擎部分，负责接收外层的数据操作指令，完成实际的数据输入输出及文件操作。MySQL 支持多种存储引擎，可通过show engines;查看 mysql 支持的存储引擎，MySQL 共支持 9 种存储引擎，其中最主要的两个引擎为 MyISAM 和 InnoDB，默认引擎为 InnoDB。 MyISAM 与 InnoDB 的区别： 特性 MyISAM InnoDB 存储限制 有 64TB 事务安全 不支持 支持 锁机制 表锁 行锁 B 树索引 支持 支持 哈希索引 不支持 不支持 全文索引 支持 不支持 集群索引 不支持 支持 数据缓存 支持 索引缓存 支持 支持 数据可压缩 支持 不支持 空间使用 低 高 内存使用 低 高 批量插入速度 高 低 外键 不支持 支持 InnoDB 只有表结构，数据全部存储在 ibdata1 文件中，算法复杂。 MyISAM 将表，数据，索引全部单独存储。 MyISAM 适合对事务完整型无要求并以访问为主的应用，访问速度快。 InnoDB 适合频繁更新、删除操作，对事务要求高，需要实现并发控制的应用。 可通过show create table 表名查询表中使用的存储引擎。也可通过alter table 表名 engine=引擎更改表的存储引擎。 数据类型 整数 tinyint：1 字节 smallint：2 字节 mediumint：3 字节 int：4 字节 bigint：8 字节 整数类型都分为有符号与无符号。默认有符号，可在类型前加上 unsigned 创建无符号类型。插入数据只能插入整数，若字段设置了是整数类型，就算插入浮点数也会转换为整数。零填充：zerofill，若数据位数不满设置位数值，则前面补充 0，且若设置零填充，数据类型自动变为无符号类型。零填充意义：保持数据格式 mysql&gt; desc my_int;+-------------+--------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------------+--------------+------+-----+---------+-------+| tinyint_1 | tinyint(4) | YES | | NULL | || smallint_1 | smallint(6) | YES | | NULL | || mediumint_1 | mediumint(9) | YES | | NULL | || int_1 | int(11) | YES | | NULL | || bigint_1 | bigint(20) | YES | | NULL | |+-------------+--------------+------+-----+---------+-------+# 括号中的数值为显示位数（宽度），可修改，不会影响数据。mysql&gt; alter table my_int modify int_1 int zerofill;# zerofill 会在显示宽度不满时用0填满mysql&gt; select int_1 from my_int;+------------+| int_1 |+------------+| 0000000004 |+------------+ 浮点数 float：4 字节，也可设置为 float(M,D) double：8 字节 decimal(M,D)：定点数，M+2 字节，取值范围与 double 一致，但有效范围由 M 与 D 决定，M 为一共的位数，D 为小数部分的位数。小数部分超出没问题，会自动四舍五入，但整数部分不能超出。 create table my_float( float_1 float(5,2), double_1 double(5,2), decimal_1 decimal(5,2));mysql&gt; insert into my_float values(1.115,1.115,1.115);# 整数部分不能超出规定长度，但小数部分可以，小数的超出部分会四舍五入。mysql&gt; select * from my_float;+---------+----------+-----------+| float_1 | double_1 | decimal_1 |+---------+----------+-----------+| 1.12 | 1.12 | 1.12 |+---------+----------+-----------+ 字符串 char(length)：定长字符串，定义时指定长度，最大 255 字节。 varchar(length)：变长字符串，最大长度 65536 个字节，一般会自动多加一个字节。实际存储从第二个字节开始，接着要用 1 到 2 个字节表示实际长度（长度超过 255 时需要 2 个字节），因此最大长度不能超过 65535。varchar 会保留字符串末尾的空格，而 char 会删除。 text：存储文字的文本字符串 blob：存储二进制的文本字符串若数据量非常大（超过 255 字节），可选用文本字符串。 枚举字符串：enum()，用于规定数据格式，节省空间（枚举实际存储的是数值）。 集合字符串：set()，集合存储的也是数值，且可以多选 存储数据 char(4) varchar(4) char 占用字节 varchar 占用字节 abcd abcd abcd 4x3 4x3+1 abcde 错误 错误 超出长度 超出长度 如何选择定长或变长字符串？ 定长字符串：磁盘空间浪费，但效率高，若数据确定长度一样，就选定长（如身份证，电话号） 变长字符串：磁盘空间节省，但效率低，若数据长度不确定，就选变长（如住址，姓名） 枚举字符串举例 mysql&gt; create table my_enum(sex enum(&#x27;m&#x27;,&#x27;f&#x27;));mysql&gt; insert into my_enum values(&#x27;m&#x27;);Query OK, 1 row affected (0.01 sec)# 字段赋值必须填枚举中的字符串mysql&gt; insert into my_enum values(&#x27;a&#x27;);ERROR 1265 (01000): Data truncated for column &#x27;sex&#x27; at row 1 在 MySQL 中，系统会自动转换数据类型。枚举中字符串是数值的证明如下： mysql&gt; select sex+0,sex from my_enum \\G*************************** 1. row ***************************sex+0: 1 sex: m*************************** 2. row ***************************sex+0: 2 sex: f# 由此可知，枚举中字符串的数值是按照枚举顺序从1开始。# 于是也可以通过数值插入mysql&gt; insert into my_enum values(1),(2);Query OK, 2 rows affected (0.00 sec) 枚举原理：枚举在进行数据规范（定义）的时候，系统会自动建立一个数字与枚举元素的对应关系（存放在日志），然后在进行数据插入时，系统自动将字符转换成对应的数字存储，在进行数据提取时，系统自动将数值转换成对应字符串显示。 集合字符串举例： mysql&gt; create table my_set(lang set(&#x27;c&#x27;,&#x27;c++&#x27;,&#x27;python&#x27;,&#x27;java&#x27;));# 与枚举类似，集合也可通过数值进行赋值 日期和时间 date：4 字节，1001 年到 9999 年的日期 datetime：8 字节，1001 年到 9999 年的日期，并能保存时间 timestamp：4 字节，1970 年 1 月 1 日到现在的秒数，最大到 2038 年 time：3 字节 year：1 字节，最小值 1901，最大值 2155MySQL 提供函数from_unixtime()将 unix 时间戳转换为时间，unix_timestamp()将日期转换为 unix 时间戳。默认情况下，如果插入时没有指定 TIMESTAMP 列的值，会将这个值设置为当前时间。应该尽量使用 TIMESTAMP，因为它比 DATETIME 空间效率更高。datetime 与时区无关，timestamp 与时区有关。 记录长度：任何一条记录的长度最长不能超过 65535 个字节，一条记录的最长字节数为 65534，但可以人为填满。MySQL 中 text 文本字符串不占用记录长度：额外存储，但 text 字符串也属于记录的一部分，所以一定要占据记录中的部分长度（10 字节，保存数据的地址与长度） 运算符算数运算符：加、减、乘、除、模 mysql&gt; select 6+4 加, 6-2 减, 6*4 乘, 6/4 除, 6 DIV 4 除, 6%4 模, 6 MOD 4 模;+----+----+----+--------+------+------+------+| 加 | 减 | 乘 | 除 | 除 | 模 | 模 |+----+----+----+--------+------+------+------+| 10 | 4 | 24 | 1.5000 | 1 | 2 | 2 |+----+----+----+--------+------+------+------+当除数为0时，结果为NULL 比较运算符：大于、小于、等于、不等于、IS NULL、BETWEEN AND、IN、LIKE、REGEXP 比较运算符 说明 &gt;或&gt;= 大于或大于等于 &lt;或&lt;= 小于或小于等于 =或&lt;=&gt; 等于 !=或&lt;&gt; 不等于 BETWEEN AND 在指定范围 IS NULL 为空 IN 在指定集合 LIKE 通配符匹配 REGEXP 正则表达式匹配 常用正则表达式 模式字符 说明 案例 ^ 匹配字符串开始 ‘^a’ $ 匹配字符串结束 ‘g$‘ . 匹配字符串中任意一个字符 ‘a.c’ [字符集合] 匹配字符集合内的任意一个字符 ‘[abc]’ [^字符集合] 匹配字符集合外的任意一个字符 ‘^abc’ str1 ｜ str2 匹配符合的字符串 ‘abc ｜ cde’ * 匹配字符，包含 0 个和 1 个 ‘a*‘ + 匹配字符，包含 1 个 ‘a+‘ 字符串{N} 字符串出现 N 次 ‘abc{2}’ 字符串(M,N) 字符串至少出现 M 次，最多 N 次 ‘abc(2,3)’ 逻辑运算符AND(&amp;&amp;)：与，OR(||)：或，NOT(!)：非，XOR：异或 位运算符&amp;：按位与，|：按位或，~：按位取反，^：按位异或，&lt;&lt;：按位左移，&gt;&gt;：按位右移可使用 BIN()函数显示二进制。 函数SQL 语句的移植性较强，而函数的移植性不强，因为各种数据库软件都有自己特有的函数。Mysql 函数分为： 字符串函数 数值函数 日期函数 系统信息函数 字符串函数 函数 功能 concat(str1,str2…) 连接字符串 insert(str,x,y,instr) 用字符串 str 的 x 位置开始 y 个字符长的子串替换字符串 instr lower(str) 将 str 的所有字符换为小写 upper(str) 将 str 的所有字符换为大写 left(str,x) 返回 str 的最左边的 x 个字符 right(str,x) 返回 str 的最右边的 x 个字符 lpad(str,n,pad) 使用 pad 字符串对 str 最左边进行填充直到长度为 n rpad(str,n,pad) 使用 pad 字符串对 str 最右边进行填充直到长度为 n ltrim(str) 去掉 str 左边的空格 rtrim(str) 去掉 str 右边的空格 trim(str) 去除 str 行头和行尾的空格 repeat(str,x) 返回 str 重复 x 次的结果 replace(str,a,b) 使用字符串 b 替换 str 中所有字符串 a strcmp(str1,str2) 比较字符串 substring(str,x,y) 返回 str 中从 x 位置起 y 个长度的字符串 数值函数 函数 功能 abs(x) 返回 x 的绝对值 ceil(x) 返回大于 x 的最小整数值 floor(x) 返回小于 x 的最大整数值 mod(x) 返回 x%y rand() 返回 0-1 的随机数 rand(x) 返回 0-1 的随机数，x 对应的随机数是固定的 round(x,y) 返回 x 的四舍五入后 y 位小数的值（y 可选） truncate(x,y) 返回 x 截断为 y 位小数的值 日期和时间函数 函数 功能 curdate() 获取当前日期 curtime() 获取当前时间 now() 获取当前日期和时间 unix_timestamp(date) 获取 date 的 unix 时间戳 from_unixtime(timestamp) 获取 unix 时间戳 week(date) 返回 date 为一年中的第几周 year(date) 返回 date 的年份 monthname(date) 返回 date 的月份 hour(time) 返回 time 的小时值 minute(time) 返回 time 的分钟值 系统信息函数 函数 功能 version() 返回版本号 database() 返回当前数据库名 user() 返回当前用户 last_insert_id() 返回最近生成的 Auto_Increment 值 特殊功能函数 函数 功能 password(str) 对 str 加密 format(x,n) 对 x 格式化，保留 n 位小数 inet_aton(ip) 将 IP 地址转换为数字 inet_ntoa(x) 将数字转换为 IP 地址 get_loct(name,time) 创建一个持续时间 time 的名为 name 的锁 release_loct(name) 对名字为 name 的锁解锁 benchmark(count,expr) 将表达式 expr 执行 count 次 convert(s USING cs) 将字符串 s 的字符集变为 cs convert(x,type) 将 x 转为 type 类型 表操作创建表 create table 表名( 字段1 数据类型, 字段2 数据类型, ......);也可直接create table 数据库名.表名(); # 这样不需要先进入库，直接建表。 表创建后，数据库文件下会生成对应表的结构文件.frm（与存储引擎有关）。 查看创建语句show create table 表名;查看表结构desc/show 表名; mysql&gt; desc user;# field：字段名# type：字段类型# null：（列属性）是否允许为空，null不是数据类型# key：索引：pri主键，uni唯一键# defalut：（列属性）默认值# extra：（列属性）扩充属性 更改表名rename table 表名 to 新表名;更改表属性 alter table 表名 表选项 参数表选项:add column 字段名 数据类型 [位置]; # 新增字段modify 字段名 数据类型 [属性] [位置]; # 修改字段change 旧字段 新字段 数据类型 [属性] [位置]; # 重命名字段drop 字段名; # 删除字段位置：first：第一个，after 字段：在字段后 删除表drop table 表名; 若要删除多张表，用,分隔表名 表约束：保证数据的合法性 空属性：NULL（默认），NOT NULL（不为空）要做到数据不为空，空就没有意义，空数据无法参与运算，所以定义字段时就要设置 not null，若字段未指定该选项，当字段为空时，MySQL 会用 NULL 填充，而 NULL 会占用一个字节，当指定了 not null 后，该字段必须有值，确保数据准确性。 列描述 comment：无实际含义，描述字段 默认值 default：可在字段设置时添加 default ，在插入字段时不赋初值就会使用默认值 主键 primary key：一张表只有一个字段可以使用对应键，用来唯一的约束该字段里的数据，不能重复，一张表最多只有一个主键，主键默认不为空（not null）。 增加主键： 法一：在创建字段时就添加primary key 关键字create table user( id int primary key, name varchar(20));法二：在创建表时，在所有字段后使用primary key(字段名) 创建主键，如有多个字段作为主键，可以是复合主键create table user( id int, name varchar(20), primary key(id,name));mysql&gt; desc user1;+-------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+-------------+------+-----+---------+-------+| id | int(11) | NO | PRI | NULL | || name | varchar(20) | NO | PRI | NULL | |+-------+-------------+------+-----+---------+-------+法三：追加主键alter table 表名 add primary key(字段); 或alter table 表名 modify 字段名 类型 primary key;# 前提：字段对应数据是独立的（不重复） 主键约束：主键字段数据不允许相同，若相同则数据操作失败主键删除：无法更新主键，只有删除了以后才能再添加 alter table 表名 drop primary key; 分类 逻辑主键：字段无业务含义（如 id），一般以此类字段做主键 业务主键：字段存放业务数据 自增长 auto-increment：若该字段未赋值或仅有默认值，会自动触发，会给字段值不断+1（当前字段中最大值），形成新字段，常与主键搭配。注： 字段做自增长的前提：本身是一个索引（key 属性有值），字段值必须是整型数字。一张表最多只有一个字段自增长。修改自增长：修改的值必须比该字段当前最大值大(小的话不生效) alter table 表 auto_increment = x; 查看自增长变量 mysql&gt; show variables like &#x27;auto_increment%&#x27;;+--------------------------+-------+ | Variable_name | Value | +--------------------------+-------+ | auto_increment_increment | 1 | | auto_increment_offset | 1 | +--------------------------+-------+ increment 为自增长步数 offset 为自增长起始值修改：set auto_increment_increment = x; ,修改是对整个数据库，且仅是会话级，通过 alter table 表 modify即可修改 唯一键 unique key：数据不能重复，允许为空，也可多个为空，空字段不参与唯一键比较。 数据操作数据插入 insert into 表名 values(字段1,字段2,...),(字段1,字段2,...),...;# 插入数据（直接插数据，字段要一一对应）insert into 表名 (字段1名,字段2名,...) values(字段1,字段2,字段3),...;# 可指定插入字段，就不用对齐了 若主键冲突，即主键对应的值已存在，插入就会失败。有以下两种解决方法。法一：更新insert into 表名(字段(要包含主键)) values() on duplicate key update 字段 = 值;法二：替换replace into 表名(字段(包含主键)) values(); 蠕虫复制：将已有的数据进行新增，数据成倍增加用法 1：从已有表创建新表（仅仅复制表结构）create table 表名 like 库名.表名;例：mysql&gt; create table user_worm like user;用法 2：将查出的数据复制到一张表insert into 表名(字段) select 字段 from 表名;例：mysql&gt; insert into user_worm (id,name,sex,age) select id,name,sex,age from user;蠕虫复制的意义： 可以快速让表中数据膨胀到一定数量级以测试表的压力与效率 数据更新update 表名 set 字段 = 值 [where] [limit 限制更新数量（前几行）]; 数据删除delete from 表名 [where];数据删除不会改变表的结构，如自增长不会归零，只能删除表后再重建truncate 表名; # 先删除该表后再创建该表 数据查询 select [选项] 字段[别名] from 表名 [where][group by][having][order by][limit];选项： all/* ：保留所有结果，默认（尽量不要打印所有） distinct：去重 别名： 字段名 as 别名 常用关键字： where：where 子句用于过滤满足条件的数据。子句返回结果为 0 或 1。where 是唯一一个直接从磁盘读取数据时就开始判断的条件（从读取到第一条数据时就进行判断，成立就保存在内存）。where 后的参数 参数 说明 between…and… 介于某个范围之内（闭区间） not between…and… 不在某个范围之内 in(项 1,项 2…) 在指定项内 not in(项 1,项 2…) 不在指定项内 like 搜索匹配，常与模式匹配符配合使用 not like like 的反义 is null 空值判断符 is not null 非空判断符 not/and/or 逻辑运算符，分别表示否、并且、或，用于多个逻辑连接 % 模式匹配符，表示任意字串 优先级：NOT &gt; AND &gt; OR group by：根据某字段分组，用于按组统计数据常用统计函数： count()：统计分组后的记录数max()：每组中最大值min()：每组中最小值avg()：求平均值sum()：求和 可在group by后加上asc或desc，分别表示升序或降序。若只是分类，并不会显示所有数据，仅仅是分组，列出有哪些组。 可以设置多个字段进行排序，会按照字段的书写顺序进行先后排序。例如，group by age,score会先对 age 进行排序，然后对结果再进行 score 的排序。函数group_concat(字段名)可对分组结果中的某个字段进行字符串的连接。 with rollup：回溯统计，根据当前分组字段向上级分组汇报多字段回溯：考虑第一层分组会有回溯，第二层要看第一层分组的组数，组数是多少就回溯几次 having：进行条件判断在 where 判断后，由于数据已进入内存，所以不能再用 where 判断了，要对 where 判断的结果再次判断，就要用 having。having 能做 where 做到几乎所有事情，而 where 不能做 having 能做的很多事情。 分组统计的结果只能 having 使用 mysql&gt; select id,score,count(*),group_concat(name) from user group by score having count(*)&gt;=1;+-------+-------+----------+--------------------+| id | score | count(*) | group_concat(name) |+-------+-------+----------+--------------------+| 10002 | 68 | 2 | mike,jessie || 10001 | 78 | 3 | jack,kate,lisi || 10006 | 86 | 2 | zhangsan,wangwu || 10005 | 97 | 1 | jason |+-------+-------+----------+--------------------+ order by：排序，依赖校对集，显示所有记录，默认升序排序。多字段排序：根据某个字段排序，然后对排序好的结果再按某字段排序 limit：限制数量 用法1：limit 长度 限制记录数（排名前N个）mysql&gt; select * from user order by score desc limit 3;+-------+----------+------+------+-------+| id | name | sex | age | score |+-------+----------+------+------+-------+| 10005 | jason | m | 22 | 97 || 10008 | wangwu | m | 20 | 86 || 10006 | zhangsan | m | 21 | 86 |+-------+----------+------+------+-------+用法2：limit 起始,长度 从某起始位置（最小为0）开始限制（实现分页）mysql&gt; select * from user order by score desc limit 4,8;+-------+--------+------+------+-------+| id | name | sex | age | score |+-------+--------+------+------+-------+| 10003 | kate | f | 19 | 78 || 10007 | lisi | f | 19 | 78 || 10002 | mike | m | 21 | 68 || 10004 | jessie | f | 20 | 68 |+-------+--------+------+------+-------+ 多表查询关系分为： 一对一：一张表的一条记录最多只能与另一张表的一条数据对应 一对多：一张表的一条记录可与另一张表的多条数据对应 多对多：两张表互相存在一对多关系 联合查询也称“并”（UNOIN），多次查询（多条 select），在记录上进行拼接。每一条 select 获取的字段数必须一致，字段名可以不一致，但字段数一定一致。会自动删除重复的记录（所有字段和值全部一致的记录）。 select 语句1 union select 语句2union选项： all # 保留所有 distinct # 去重 联合查询的意义： 查询同一张表，但需求不同 多表查询：多张表结构完全一样，保存数据类型也一致 在联合查询中，order by 不能直接使用，必须搭配 limit 限定最大数 例： mysql&gt; (select id,name,score from user order by score) union (select id,name,score from stu order by score);+-------+------------+-------+| id | name | score |+-------+------------+-------+| 10001 | jack | 78 || 10002 | mike | 68 || 10003 | kate | 78 || 10004 | jessie | 68 || 10005 | jason | 97 || 10006 | zhangsan | 86 |并没有排序，当加上limit 后即可实现排序mysql&gt; (select id,name,score from user order by score desc limit 999) union (select id,name,score from stu order by score desc limit 999);是两张表分别进行排序，然后合并 连接查询将多张表进行数据的拼接。分类：内连接（Inner Join），外连接（Outer Join），交叉连接（Cross Join）。连接查询的速度很慢，通常使用子查询。 内连接从左表中读取每一条记录与右表中所有记录匹配，只保留匹配的数据 语法：select 字段 from 左表 inner join 右表 on 左表.字段 = 右表.字段; 若这两张表要查询的字段唯一，就不需要加表名。字段别名及表别名的使用：查询数据时，不同表有同名字段，可使用别名。 若内连接不指定 on ，效果会和交叉连接一样。可用 where 代替 on（但 where 没 on 效率高） 内连接根据不同的实现作用又分为： 自然连接：natural join，仅进行匹配以及去重。不能指定执行过程中的匹配条件。 等值连接：用=匹配字段值相等的记录 不等连接：用!=匹配字段值不相等的记录 注：内连接和外连接都可以模拟自然连接，只要在连接后面加 using(字段名)，就可使用同名字段作为连接条件，自动合并 select * from stu join user using(id,name,score); 外连接从主表中读取每一条记录与另一张表中所有记录匹配，会保留所有记录以一张表为主，称为主表，根据主表的位置，外连接又分为左连接和右连接。 左连接 left join：以左表为主表 右连接 right join：以右表为主表 全外连接 full outer join：除了匹配的记录，还包括不匹配的记录结果记录数至少为主表的总记录数，副表的为匹配的记录会显示为 null显示仍为左连接在表的靠左部分，右连接在表的靠右部分 语法：左连接select 字段 from 左表 left|right join 右表 on 左表.字段 = 右表.字段; 子查询虽然可通过连接查询实现多表查询，但性能很慢，因此推荐使用子查询进行多表查询。 子查询分类： 按位置分类：子查询在外部查询中出现的位置 from 子查询：子查询在 from 之后 where 子查询：在 where 中 exist 子查询：在 exists 中 按结果分类：根据子查询得到的结果查询 标量子查询：子查询得到的结果是一行一列 列子查询：结果是一列多行 行子查询：结果是多列一行（也可以多行多列） 表子查询：子查询得到的结果是多行多列（出现在 from 后） 标量子查询 mysql&gt; select * from stu_info where id = (select id from stu where id = 20001);+-------+------------+------------+| id | birthday | birthplace |+-------+------------+------------+| 20001 | 1998-07-03 | 河南 |+-------+------------+------------+ 列子查询 关键字IN的子查询mysql&gt; select * from stu_info where birthplace in (select birthplace from stu_info where birthplace = &#x27;江苏&#x27;);+-------+------------+------------+| id | birthday | birthplace |+-------+------------+------------+| 20003 | 1998-10-03 | 江苏 || 20005 | 1998-10-02 | 江苏 || 20007 | 1997-11-24 | 江苏 |+-------+------------+------------+关键字ANY（或SOME）的子查询 三种匹配规则： 1. = ANY ，与关键字IN作用相同 2. &gt; ANY（或&gt;=） ，比子查询中记录的最小值大的即可 3. &lt; ANY（或&lt;=） ，比子查询中记录的最大值小的即可mysql&gt; select * from stu where score &gt;= ANY ( select score from stu where age = 19);+-------+----------+------+------+-------+| id | name | sex | age | score |+-------+----------+------+------+-------+| 20001 | chenning | f | 20 | 98 || 20004 | yunlu | f | 19 | 93 || 20007 | chenliu | m | 20 | 94 |+-------+----------+------+------+-------+关键字ALL的子查询 两种匹配规则： 1. &gt; ALL ，比子查询中记录的最大值还要大 2. &lt; ALL ，比子查询中记录的最小值还要小mysql&gt; select * from stu where score &gt; ALL ( select score from stu where age = 19);+-------+----------+------+------+-------+| id | name | sex | age | score |+-------+----------+------+------+-------+| 20001 | chenning | f | 20 | 98 || 20007 | chenliu | m | 20 | 94 |+-------+----------+------+------+-------+关键字EXISTS的子查询用于判断是否满足（跨表），接在where后，exists返回值只有0和1 两种匹配： 1. EXISTS，存在 2. NOT EXISTS，不存在mysql&gt; select * from stu where not exists(select * from stu_info where birthplace = &#x27;河南&#x27;);解释：只要不存在河南的学生，就将所有学生信息打印出。EXISTS语句仅仅是判断where中的条件，并不会进行select的输出控制。 行子查询 mysql&gt; select * from stu where (age,score) = ( select age,score from stu where id = 20001);+-------+----------+------+------+-------+| id | name | sex | age | score |+-------+----------+------+------+-------+| 20001 | chenning | f | 20 | 98 |+-------+----------+------+------+-------+ 索引系统通过算法将已有的数据单独建立一个文件，文件能实现快速查找匹配数据作用： 1.提高查询数据效率 2.约束数据的有效性增加索引的前提条件： 因为索引本身会产生文件（较大），所以若某个数据经常使用时就可使用索引。 根据存储类型，可将索引分为：B 树索引（默认索引）和哈希索引。InnoDB 和 MyISAM 引擎都支持 B 树索引，Memory 引擎支持哈希索引 Mysql 支持六种索引： 普通索引 index 唯一索引 unique 全文索引 fulltext index 单列索引 多列索引 空间索引 以下情况时适合创建索引： 经常被查询的字段，即 where 子句出现的字段 在分组的字段，即 group by 子句出现的字段 存在依赖关系的子表和父表间的联合查询，即主键和外键字段 设置唯一完整性约束的字段 不适合创建索引的情况： 查询中很少被使用的字段 拥有许多重复值的字段 普通索引在创建索引时，不附加任何限制条件，可创建在任何数据类型上 1. 创建表时创建普通索引create table 表名( 字段 类型, index|key 索引名(字段1(长度) &#123;ASC|DESC&#125;));2. 在已存在的表上创建普通索引create index 索引名 on 表名 (字段(长度) &#123;ASC|DESC&#125;);3. 修改表创建索引alter table 表名 add index|key 索引名(字段(长度) &#123;ASC|DESC&#125;); 用 INDEX 或 KEY 参数都可创建索引。索引名与字段关联，可设置索引长度（因为不同存储引擎定义了表的最大索引数和最大索引长度），还可设置升降序。 Mysql 支持的存储引擎对每个表支持至少 16 个索引，总索引长度至少为 256 字节。 唯一索引创建索引时，限制索引的值必须唯一。根据创建索引的方式分为：自动索引和手动索引。自动索引：在数据库表中设置完整性约束时，该表会被系统自动创建索引。当设置表中的某个字段设置主键或唯一键完整性约束时，系统会自动关联该字段的唯一索引。 1. 在创建表时创建唯一索引create table 表名( 字段 类型, unique index|key 索引名(字段(长度) &#123;ASC|DESC&#125;);2. 在已存在的表上创建唯一索引create unique index 索引名 on 表名(字段(长度) &#123;ASC|DESC&#125;);3. 修改表创建索引alter table 表名 add unique index|key 索引名(字段(长度) &#123;ASC|DESC&#125;); 全文索引针对文章内部的关键字进行索引，表引擎必须为 MyISAM。主要用于关联数据类型为 char、varchar、text 的字段，以便能够更加快速地查询数据量较大的字符串类型的字段。默认情况全文索引搜索不区分大小写，若全文索引所关联的字段为二进制类型，则以区分大小写搜索。 注：不要在导入数据时使用 fulltext，应该在导入后使用 1. 创建表时创建全文索引create table 表名( 字段 属性, fulltext index|key 索引名(字段(长度) &#123;ASC|DESC&#125;))engine=MYISAM;2. 在已存在的表上创建全文索引create fulltext index 索引名 on 表名(字段(长度) &#123;ASC|DESC&#125;);3. 修改表创建索引alter table 表名 add fulltext index|key 索引名(字段(长度) &#123;ASC|DESC&#125;); 全文索引操作符： 操作符 说明 + 包含 - 排除 &lt; 包含且增加等级 &gt; 包含且减少等级 ( ) 表达式 * 词尾通配符 “ “ 字符串 多列索引在创建索引时所关联的字段不是一个字段，而是多个字段。只有查询条件使用了关联字段的第一个字段，多列字段才会被使用。 1. 创建表时创建多列索引create table 表名( 字段 类型, index|key 索引名( 字段1(长度) &#123;ASC|DESC&#125;, ... 字段n(长度) &#123;ASC|DESC&#125;);2. 在已存在的表上创建多列索引create index 索引名 on 表名 ( 字段1(长度) &#123;ASC|DESC&#125;, ... 字段n(长度) &#123;ASC|DESC&#125;);3. 修改表创建索引alter table 表名 add index|key 索引名( 字段1(长度) &#123;ASC|DESC&#125;, ... 字段n(长度) &#123;ASC|DESC&#125;); 视图本质是一种虚拟表，内容与真实表相似，但并不在数据库中以存储的数据值形式存在，数据来自自定义视图的查询所引用基本表，并在具体引用视图时动态生成。 创建视图：create view 视图名 as select语句;注：有多张基表时，要保证字段名不同，可用别名区分修改视图： 1. 使用ALTER语句修改视图alter view 视图名 as select语句;2. 使用CREATE OR REPLACE语句修改视图create or replace view 视图名 as select 语句;这个方式修改视图会在视图存在的情况下直接修改，而若不存在就创建视图。 删除视图：drop view 视图名; 可以向单表中插数据，但不能向多表插数据，且插入数据只能插视图中有的字段 查看视图show tables;查看视图详细信息show table status (from 数据库);查看视图定义信息show create view 视图名;查看视图设计信息desc 视图名; 触发器触发器的执行是由事件来触发、激活从而实现执行。为某张表绑定一段代码，当对表操作时，就会触发代码执行。触发器由三部分组成： 事件类型：增删改–insert、delete、update 触发时间：before、after 触发对象：表中记录 创建触发器：create trigger 触发器名 before|after 触发事件 on 表名 for each row 触发后动作;# for each row 表示任何一条记录上的操作满足触发事件# 后面跟上的是激活触发器后执行的语句创建包含多条执行语句的触发器：create trigger 触发器名 before|after 触发事件 on 表名 for each row begin 触发后动作 end# 在BEGIN和END间因为有多条语句需要使用分号隔开# 而在mysql中默认分号为结束，因此需要在创建触发器前将结束符重新设置，并在创建完成后再将触发器设置回分号。delimiter 结束符删除触发器drop trigger 触发器名;查看触发器show triggers;mysql&gt; show triggers\\G*************************** 1. row *************************** Trigger: my_trigger1 # 触发器名 Event: INSERT # 触发事件 Table: stu # 操作表 Statement: begin # 激活触发器后的动作insert into stu_journal values(&#x27;insert&#x27;,now());end Timing: AFTER # 触发器执行的时间 Created: 2018-07-08 14:10:54.01 sql_mode: STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION Definer: root@localhostcharacter_set_client: gbkcollation_connection: gbk_chinese_ci Database Collation: utf8_general_ci查看系统表triggers中所有记录select * from information_schema.triggers\\Gmysql&gt; select * from information_schema.triggers where trigger_name=&#x27;my_trigger1&#x27;\\G*************************** 1. row *************************** TRIGGER_CATALOG: def TRIGGER_SCHEMA: test TRIGGER_NAME: my_trigger1 EVENT_MANIPULATION: INSERT EVENT_OBJECT_CATALOG: def EVENT_OBJECT_SCHEMA: test EVENT_OBJECT_TABLE: stu ACTION_ORDER: 1 ACTION_CONDITION: NULL ACTION_STATEMENT: begininsert into stu_journal values(&#x27;insert&#x27;,now());end ACTION_ORIENTATION: ROW ACTION_TIMING: AFTERACTION_REFERENCE_OLD_TABLE: NULLACTION_REFERENCE_NEW_TABLE: NULL ACTION_REFERENCE_OLD_ROW: OLD ACTION_REFERENCE_NEW_ROW: NEW CREATED: 2018-07-08 14:10:54.01 SQL_MODE: STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION DEFINER: root@localhost CHARACTER_SET_CLIENT: gbk COLLATION_CONNECTION: gbk_chinese_ci DATABASE_COLLATION: utf8_general_ci 存储过程与函数一个完整的操作会包含 多条 SQL 语句，在执行过程中需要根据前面的 SQL 语句的执行结果有选择的执行后面的 SQL 语句。存储过程与函数可理解为一条或多条 SQL 语句的集合，且也是事先经过编译并存储在数据库中的一段 SQL 语句集合，是一种没有返回值的函数。 存储过程与函数的优点： 允许表春组件式编程，提高了 SQL 语句的重用性、共享性、可移植性 实现较快执行速度，减少网络流量 可被作为一种安全机制 缺点： 编写复杂 需要创建数据库对象的权限 存储过程和函数的区别： 函数必须有返回值，存储过程没有 存储过程创建存储过程delimiter 结束符create procedure 过程名(procedure_parameter参数) characteristic特性 begin 过程体 end 结束符delimiter ;与触发器类似，同样需要先用delimiter修改结束符 其中 procedure_parameter 参数的格式如下输入/输出类型 参数名 参数类型 输入输出类型有三种 IN：输入类型，数据只从外部传入内部，可是数值也可是变量。存储过程可能会修改这个值，但是对于调用者来说，在存储过程返回结果时，所做的修改是不可见的。 OUT：输出类型，只允许过程使用内部数据，外部传入内部只能是变量。其初始值为 NULL，当存储过程返回时，这个值对调用者来说是可见的。 INOUT：输入输出类型，外部可在内部使用，内部修改也可在外部使用，只能传变量，存储过程可能会修改这个值，当存储过程返回的时候，所做的修改对调用者来说是可见的。 参数类型可为 Mysql 支持的任何类型 characteristic 特性的可选参数 [NOT] DETERMINSTIC：存储过程的执行结果是否确定# DETERMINSTIC表示确认，加NOT则为不确认&#123;CONTAINS SQL|NO SQL|READS SQL DATA|MODIFIES SQL DATA&#125;：表示使用SQL语句的限制# CONTAINS SQL：表示可包含SQL，但不包含读或写数据的语句# NO SQL：表示不包含SQL语句# READS SQL DATA：表示包含读数据语句# MODIFIES SQL DATA：表示包含写数据语句# 默认为CONTAINS SQLSQL SECURITY &#123;DEFINER|INVOKER&#125;：表示谁有权限执行# DEFINER：表示只有定义者自己能执行# INVOKER：表示调用者都可执行# 默认为DEFINERCOMMENT 注释 示例： delimiter $create procedure proce_sel_stu(in stuid int)comment &#x27;显示stu表指定学号的学生姓名和成绩&#x27;bgein select name,score from stu where id=stuid;end$$delimiter ; 使用call 存储过程名(参数);对存储过程的调用。 查看存储过程查看存储过程创建语句show create procedure 存储过程名\\G查看存储过程状态信息show procedure status like &#39;过程名&#39;\\G 在information_schema库中存在一张存储所有存储过程和函数的表routines，因此此表也可查看存储过程和函数。 修改存储过程 alter procedure 过程名 特性 存储过程不能修改过程体，只能删除后重新创。删除存储过程drop procedure 存储过程名; 函数创建函数delimiter 结束符create function 函数名(参数) returns 返回数据类型 特性 begin 函数体 end结束符delimiter ;# 特性与存储过程一致# 参数不用指定输入输出 注：函数不存在“重写”，即函数名不能相同。并且推荐函数名的格式为 func_XXX 或 function_XXX示例： delimiter $$create function func_sel_stu(stuid int) returns int begin return (select score from stu where id=stuid); end $$delimiter ; 使用select 函数(参数);调用函数。 查看函数查看函数创建函数show create function 函数名\\G查看函数状态信息show function status like &#39;函数名&#39;\\G修改函数 alter function 函数名 特性 存储过程不能修改函数体，只能删除后重新创。删除函数drop function 函数名; 存储过程和函数表达式 变量使用declare 变量名（可多个，逗号分隔） 类型 [默认值]声明变量使用set 变量名=XX（可以是值，也可以是赋值表达式，可多个，逗号分隔）;赋值变量也可以通过select 字段 into 变量（可多个） from ...;将查询结果赋给变量。注：将查询结果赋值给变量时，该查询语句的返回结果只能是单行 条件条件用于提高安全性。条件用于定义在处理过程中遇到问题时相应的处理步骤。 定义条件declare 条件名 condition for condition_value状态值状态值：mysql_error_code mysql错误值SQLSTATE[VALUE] sqlstate_value 指定sql状态不要使用mysql error code 0或以‘00’开头的code或一个SQLSTATE，因为这些指示成功而不是一个错误条件。定义处理delimiter 结束符declare 处理类型 handler for 状态值（可多个） begin 处理 end 结束符delimiter ;处理类型，即当handler被触发后需要执行什么动作1. CONTINUE：继续执行2. EXIT：终止程序3. UNDO状态值，handler触发的条件：1. mysql error code或SQLSTATE value2. 定义条件时的条件名3. SQLWARNING：代表所有以01开头的SQLSTATE4. NOT FOUND：代表所有以02开头的SQLSTATE5. SQLEXCEPTION：代表除01和02开头的SQLSTATE 详细 SQLSTATE 表 示例： DECLARE no_such_table CONDITION FOR 1051;DECLARE CONTINUE HANDLER FOR no_such_table BEGIN -- body of handler END; 游标指定由 select 语句返回的行集合结果集，并遍历该结果集，可看做一种数据类型，类似指针或数组下标。使用declare 游标名 cursor for select语句;声明游标使用open 游标名打开游标。打开时，游标指向的是第一条数据的前一位。使用fetch 游标名 into 变量名（可多个，逗号分隔）使用游标，遍历赋值给变量。使用close 游标名关闭游标 流程控制条件控制 if条件分支if 条件 then 执行语句elseif 执行语句else 执行语句end if;case条件分支case 条件判断的变量 when 条件 then 执行语句 when 条件 then 执行语句end case 注：创建条件控制需要修改语句结束符循环控制 [标签:]where 条件 do 执行语句end where[标签];循环控制：循环内部进行循环判断和控制interate：迭代，类似continueleave：离开，类似break[标签:]where 条件 do 执行语句 leave | interate 循环名;end where[标签];[标签:]loop 执行语句end loop[标签][标签:]repeat 条件 do 执行语句end repeat[标签]可使用标签，两个标签分别代表循环的开始和结束，但必须一致，也可省略若要退出循环，使用leave 标签 事务为保证数据库记录的更新从一个一致性状态变更为另一个一致性状态。事务的四个特性： 原子性：事务中所有操作视为一个原子单元，对事务所进行的数据修改等操作只能是完全提交或完全回滚。 一致性：事务完成时，所有变更必须应用于事务的修改 隔离性：一个事务中的操作必须与其他事务所做的修改隔离，当前事务不会查看由另一个并发事务正在修改的数据（通过锁机制实现） 持久性：事务完成后，所做的所有修改对数据的影响是永久的 InnoDB 支持事务，而 MyISAM 不支持事务事务安全：保护连续操作同时满足。意义：保证数据操作的完整性事务操作分为：自动事务（默认），手动事务 手动事务： 开启事务：告诉系统以下所有操作不直接写入数据表，先放到事务日志中。start transaction;或begin;此后的操作会保存在事务日志中，并不是真的对操作了数据表，所以若再通过另一个命令行用户登录查看时，该数据是未被操作的。 关闭事务：选择性的将日志文件中操作的结果同步到数据表包含两个操作： 提交事务commit：同步数据表，操作成功 回滚事务rollback：直接清空日志表，操作失败 自动事务：通过 autocommit 变量控制查看自动事务状态show variables like &#39;autocommit&#39;;默认开启set autocommit = off;关闭事务自动提交。关闭自动后，需要手动选择处理提交或回滚 事务原理：事务开启后，所有操作临时保存在事务日志，只有在 commit 时才会同步到数据表，其他情况都会导致清空。其中日志文件分为两个： REDO 日志：记录事务日志。每条 SQL 进行数据库更新操作时，首先将 REDO 日志写入到日志缓存区中。当客户端执行 COMMIT 命令提交时，日志缓冲区的内容被刷新到磁盘。REDO 日志对应ib_logfile文件，默认大小 5MB，建议设置为 512MB 以便容纳较大的事务。在 Mysql 崩溃恢复时，会重新执行 REDO 日志记录。 UNDO 日志：也称为回滚段。用于事务异常时的回滚处理，复制事务前得到数据库内容到 UNDO 缓冲区，然后在合适的时间将内容刷新到磁盘。磁盘上不存在单独的 UNDO 日志文件，而是存放在表空间对应的.ibd数据文件中。 回滚点：在某个成功的操作完成后，后续的操作可能成功可能失败，可以自当前成功的位置设置一个点，可以供后续失败操作返回到该位置，而不是返回所有操作。savepoint 回滚点名;rollback 回滚点名; # 使用start transaction 或 begin开启事务mysql&gt; start transaction;mysql&gt; update stu set age=22 where id=20007;mysql&gt; commit; 或mysql&gt; rollback; 事务隔离级别SQL 定义了 4 种隔离级别，指定了事务中哪些数据改变其他事务可见，哪些数据改变其他事务不可见。低隔离级别可支持更高并发处理，同时占用的系统资源更少。可通过show variables like &#39;tx_isolation&#39;查看当前事务隔离级别。 READ-UNCOMMITTED：读取未提交内容，所有事务都可以看到其他未提交事务的执行结果。读取未提交的数据称为脏读。开启 A 与 B 事务，A 更新，B 不操作，但 A 在提交前，B 能读到更新后的数据，而此时 A 回滚了，也就是 B 还是读到了错误的数据。 READ-COMMITTED：读取提交内容，一个事务从开始到提交前所做的任何改变都是不可见的，事务只能看见已经提交的变化。同一事务的其他实例在该实例处理时可能会有新的数据提交导致数据改变，所以同一查询可能返回不同结果。 REPEATABLE-READ：可重读，Mysql 默认事务隔离级别。确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。存在问题：A 的操作对表中所有行，B 的操作是添加一行，于是 A 会发现有一行没有被修改。这个问题称为幻读。解决：InnoDB 的多版本并发控制 MVCC 机制。InnoDB 通过为每个数据行增加两个隐含值的方式实现，两个隐含值记录行的创建时间和过期时间。每行记录事件发生时的系统版本号。每一次开始一个新事务时版本号会自动加 1，每个事务保存开始时的版本号，每个查询根据事务的版本号查询结果。 SERIALIZABLE：可串行化。最高的隔离级别。通过强制事务排序，使各事务不可能冲突。通过在每个读的数据行上加上共享锁实现。不推荐使用。 InnoDB 锁机制锁机制：为解决数据库并发控制问题，保证数据一致性，需要对并发操作控制，并实现 Mysql 各个隔离级别。有以下类型： 共享锁：S（Share），锁粒度是单行或多行。一个事务获取了共享锁后，可对锁定范围内的数据执行读操作。事务 A 与 B，若 A 获取了共享锁，B 仍可获得共享锁，但不能获得排他锁。若 A 获得了排他锁，B 不能获得共享锁和排他锁。 排他锁：X（eXclusive），排他锁的粒度与共享锁相同。事务获取排他锁后，可对锁定范围的数据执行写操作。 意向锁：一种表锁，粒度为整张表。分为意向共享锁 IS 和意向排他锁 IX。表示一个事务有意对数据上共享锁或排他锁。锁与锁之间的关系，要么相容，要么互斥。相容：事务 A 获得了锁 a，事务 B 还可获得锁 b互斥：事务 A 获得了锁 a，事务 B 在 A 释放 a 之前不能获得锁 b 锁粒度锁粒度分为表锁和行锁。innodb 默认是行锁，但如果在事务操作的过程中，没有使用索引，那么系统会自动全表检索数据，自动升级为表锁。行锁：只有当前行被锁住，别的用户不能操作。行锁支持最大并发。InnoDB 使用行锁。支持并发读写。表锁：整张表被锁住，别的用户不能操作。开销最小，允许的并发量也最小。MyISAM 使用表锁。当行或表被锁住时，若另一用户也要更改就只能等待锁被解除（commit 或 rollback），否则无法操作成功。 安全权限机制三张关于权限的表，存放在mysql库中。 user db host mysql.user 表一共有 45 个字段，可分为 4 类：用户字段、权限字段、安全字段、资源控制字段 用户字段三个字段：host 主机名，user 用户名，password 密码 权限字段一系列以_priv结尾的字段，这些字段决定了权限。两个返回值，Y 和 N，默认为 N。 字段 权限名 权限范围 Select_priv select 查询表 Insert_priv insert 插入表 Update_priv update 更新表 Delete_priv delete 删除表 Create_priv create 库、表、索引 Drop_priv drop 库、表 Reload_priv reload 库、表 Shutdown_priv shutdown 关闭服务器 Process_priv process 服务器管理 File_priv file 加载服务器主机的文件 Grant_priv grant 库、表、存储过程、函数 References_priv references 库、表 Index_priv index 用索引查表 Alter_priv alter 修改表 Show_db_priv show databases 服务器 Super_priv super 超级权限 Create_tmp_table_priv create temporary tables 临时表 Lock_tables_priv lock tables 锁定表 Execute_priv execute 执行存储过程或函数 Repl_slave_priv replication slave 服务器管理 Repl_client_priv replication client 服务器管理 Create_view_priv create view 创建视图 Show_view_priv show view 查看视图 Create_routine_priv create routine 创建存储过程或函数 Alter_routine_priv alter routine 修改存储过程或函数 Create_user_priv create user 创建用户 Event_priv event 计时器 Trigger_priv create trigger 触发器 Create_tablespace_priv create tablespace 创建表空间 安全字段用于判断用户是否能够登录成功 字段 说明 ssl_type 支持 ssl 加密的安全字段 ssl_cipher 支持 ssl 加密的安全字段 x509_issuer 支持 x509 的字段 x509_subject 支持 x509 的字段 可通过以下方式查看是否字段支持 ssl 加密 mysql&gt; show variables like &#x27;have_openssl&#x27;;+---------------+----------+| Variable_name | Value |+---------------+----------+| have_openssl | DISABLED |+---------------+----------+ 资源控制字段 字段 说明 max_questions 每小时允许执行多少次查询 max_updates 每小时允许执行多少次更新 max_connections 每小时允许建立多少次连接 max_user_connections 单个用户可同时具有的连接数 所有资源控制字段的默认值为 0，表示是没有限制。 用户机制包括：登录和退出 Mysql，创建用户，删除用户，修改用户密码，修改用户权限等。 连接 Mysql 服务器的命令： mysql -h Mysql服务器的地址，可用域名，也可用IP地址 -p 指定所连接Mysql服务器的端口，默认3306 -u 登录Mysql使用的用户 -p 将提示输入密码 DBname 指定登录到的库 -e 指定执行的SQL语句 对用户的操作： 创建用户： 1. create user 用户名[@主机] [identified by &quot;密码&quot;];2. insert into mysql.user(Host,User,Password) values(主机名,用户名,PASSWORD(&quot;密码&quot;));# 要使用PASSWORD()对密码加密3. grant 权限 on 库.表 to 用户名[@主机] [identified by &quot;密码&quot;];在赋予权限后，要flush privileges;刷新权限 修改用户账户密码： 1. mysqladmin -u 用户名 -p 原密码 &quot;新密码&quot;# 新密码必须用双引号括起来2. set password=PASSWORD(&quot;新密码&quot;);# 修改当前登录用户的密码（即只修改自己的密码）3. update mysql.user set password=PASSWORD(&quot;新密码&quot;) where user=&quot;用户名&quot; and host=&quot;localhost&quot;; 修改普通用户账户密码： 1. grant 权限 on 库.表 to 用户名 [identified by &quot;密码&quot;];2. set password for 用户名[@主机]=PASSWORD(&quot;新密码&quot;);3. update mysql.user set password=PASSWORD(&quot;新密码&quot;) where user=&quot;用户名&quot; and host=&quot;主机名&quot;;4. set password=PASSWORD(&quot;新密码&quot;);# 修改当前登录用户的密码（即只修改自己的密码） 删除普通用户账号： 1. drop user 用户名1,用户名2....2. delete from mysql.user where user=&quot;用户名&quot; and host=&quot;主机&quot;; 对用户的权限管理 对用户授权： grant 权限 on 库.表 to 用户 [identified by &quot;密码&quot;] with 选项;# with后有以下选项：GRANT OPTION：被授权用户可将权限授权给其他用户MAX_QUERIES_PER_HOUR count：设置每小时可执行count次查询MAX_UPDATES_PER_HOUR count：设置每小时可执行count次更新MAX_CONNECTIONS_PER_HOUR count：设置每小时可建立count次查询MAX_USER_CONNECTIONS count：设置单个用户可同时具有count个连接 查看用户拥有权限：show grant for 用户名[@主机]; 收回用户拥有权限： revoke 权限 on 库.表 from 用户名 [identified by &quot;密码&quot;];若要直接回收全部权限，可使用以下语句revoke all privileges,grant option from 用户名 [identified by &quot;密码&quot;]; 日志Mysql 日志分为： 二进制日志：以二进制形式记录数据库的各种操作，但不记录查询语句 错误日志：记录 Mysql 服务器启动、关闭、运行时的错误信息 通用查询日志：记录 Mysql 启动和关闭信息、客户端连接信息、更新数据 SQL 语句、查询 SQL 语句 慢查询日志：记录执行时间超过指定时间的各种操作，可用于定位 Mysql 性能瓶颈 二进制日志二进制日志默认关闭。可通过 mysql 配置文件my.ini的log-bin参数，将注释去掉即可开启二进制日志。log-bin = 二进制日志路径路径是可选。若没指定路径，会使用默认名主机名-bin.number，number 格式为 000001 开始的计数，并保存到默认目录：数据库的数据文件目录，即C:\\ProgramData\\MySQL\\MySQL Server 5.7\\Data。 每次重启 Mysql 服务器都会生成一个新的二进制日志文件，number 会递增 可通过mysqlbinlog 二进制日志查看。不能直接打开，否则是乱码。 若要停止二进制日志，只要将my.ini中的 log-bin 恢复注释或删除即可。或者在数据库中通过对变量的设置实现开启或关闭二进制日志。set SQL_LOG_BIN=若为 1 表示开启，若为 0 表示关闭 只有有 super 权限的用户才能执行 set 语句 删除二进制日志reset master;可删除所有二进制日志文件purge master logs to 日志文件可删除 number 所有小于该日志的日志purge master logs before &#39;yyyy-mm-dd hh:MM:ss&#39;删除指定日期前创建的二进制日志 错误日志Mysql 默认开启错误日志，也无法被禁止。同样该日志默认也存放在C:\\ProgramData\\MySQL\\MySQL Server 5.7\\Data中，文件名称格式为Mysql主机名.err。可修改my.ini的error-bin修改日志的路径。 错误日志以文本文件形式存储信息，可直接打开。命令mysqladmin -u root -p flush-logs会先创建一个新的错误日志，然后将旧的错误日志改名为原文件名-old。 通用查询日志由于该日志记录了客户端 Mysql 的所有请求，若实例的访问量较大，则此日志会急剧增大，影响 Mysql 性能，一般建议关闭。 若要开启通用查询日志，设置my.ini的general-log=1，默认未开启。general_log_file设置通用查询日志的路径，格式为文件名.log，默认为主机名.log。 也可通过设置环境变量开启或关闭，set global general_log = on;开启通用查询日志。若要关闭，设为 off 即可。通过show variables like &#39;%general_log%&#39;;查看相关变量（只有是否开启和文件路径）。 同样可以使用mysqladmin -u root -p flush-logs删除日志，但 Mysql 会创建一个新日志覆盖旧日志。 慢查询日志默认慢查询日志是关闭的。可通过my.ini的slow-query-log=1开启。可通过slow_query_log_file设置慢查询日志的路径，文件格式为文件名-slow.log，默认为主机名-slow.log。默认存放在C:\\ProgramData\\MySQL\\MySQL Server 5.7\\Data。可通过long_query_time设置超时时间，默认为 10s。修改配置后需要重启 Mysql 才能生效。所以最好通过修改环境变量动态开启关闭。set global slow_query_log=on;开启慢查询日志set global long_query_time=3;设置超时时间，对设置后的新连接有效，可重新连接 Mysql。 Mysql 提供工具mysqldumpslow.pl对慢查询日志文件进行分析，该工具在C:\\Program Files\\MySQL\\MySQL Server 5.7\\bin中。该工具由 perl 语言编写，因此需要 perl 环境 mysqldumpslow.pl -s 分析慢查询日志时指定排序参数，有以下可选参数 al 平均锁定时间 ar 平均返回记录数 at 平均查询时间 -t 只显示指定的行数 若要停止慢查询日志，可将my.ini的slow-query-log与long_query_time注释即可。或通过修改环境变量slow-query-log=off关闭。若要删除慢查询日志，可通过命令mysqladmin -u root -p flush-logs创建新的日志，会覆盖旧日志。 维护数据库备份与还原使用mysqldump命令进行数据备份mysqldump -u [username] -p [dbname] [table1]... &gt; [path]/[filename].sql备份单个数据库，可指定表（可多张），若不指定，就备份整个库。导出的 sql 文件路径与名称都可自定义。mysqldump -u [username] -p --databases [dbname]... &gt; [path]/[filename].sql备份多个数据库mysqldump -u [username] -p --all -databases &gt; [path]/[filename].sql备份所有数据库 还原数据需要先在 mysql 中创建对应库，然后在数据库外执行命令。mysql -u [username] -p [dbname] &lt; [path]/[filename].sql可指定数据库，指定就还原该数据库下的表，不指定就还原所有库。 若要通过复制对数据恢复，则需要保证两个 Mysql 的版本号一致，且只能对存储引擎为 MYISAM 的表有效。 将数据库表与文本文件互相导入导出导出有三种方法： select ...into outfile...;命令 mysqldump命令 mysql命令 1. select 字段名 from 表名 过滤条件 # 第一部分是普通的查询语句 into outfile 文件名 选项; # 设置要导出到的文件以及文件的参数选项有六种选项： fields terminated by 字符串：用于设置字段的分隔符，默认为&#x27;\\t&#x27; fields enclosed by 字符：用于设置括上字段值的字符符号，默认不使用任何符号 fields optionally enclosed by 字符：用于设置括上char、varchar、text等字段值的字符符号，默认不使用任何符号 fields escaped by 字符：用于设置转义字符的字符符号，默认为&#x27;\\&#x27; lines starting by 字符：用于设置每行开头的字符符号，默认不使用任何符号 lines terminated by 字符串：用于设置每行结束时的字符串符号，默认为&#x27;\\n&#x27;例：select * from user into outfile &#x27;.\\user.txt&#x27; fields terminated by &#x27;\\,&#x27; optionally enclosed by &#x27;\\&quot;&#x27; lines terminated by &#x27;\\r\\n&#x27;;2. mysqldump -u 用户名 -p -T 文件目录 数据库 表名 选项有四种选项： --fields-terminated-by=字符串 ：设置字段的分隔符，默认为&#x27;\\t&#x27; --fields-enclosed-by=字符 ：设置括上字段值的字符符号，默认不使用任何符号 --fields-optionally-enclosed-by=字符 ：设置括上char、varchar、text等字段值的字符符号，默认不使用任何符号 --lines-terminated-by=字符串 ：设置每行结束时的字符串符号，默认为&#x27;\\n&#x27;例：mysqldump -u 用户名 -p -T &#x27;.\\&#x27; test user &quot;--fields-terminated-by=,&quot; &quot;--lines-terminated-by=\\r\\n&quot;使用mysqldump命令不仅会在指定目录中生成[表名].txt文件，还会生成[表名].sql文件。3. mysql -u 用户名 -p -e &quot;select 字段 from 表名&quot; 数据库名 &gt; 文件名 -e选项用于执行查询语句例：mysql -u root -p -e &quot;select * from user&quot; test &gt; .\\user.txt 导入有两种方法： load data infile命令 mysqlimport命令 1. load data infile 文件名 into table 表名 选项;有九种选项。前六种与导出的select六种一致，后三种为： ignore N lines ：忽视文件的前N行数据 字段列表：实现根据字段列表中的字段和顺序加载记录 set column=EXPR ：设置列的转换条件EXPR，即所指定的列经过相应转换后才会被加载例：load data infile &#x27;.\\uesr.txt&#x27; into table user into outfile &#x27;.\\user.txt&#x27; fields terminated by &#x27;\\,&#x27; optionally enclosed by &#x27;\\&quot;&#x27; lines terminated by &#x27;\\r\\n&#x27;;2. mysqlimport -u 用户名 -p 数据库名 文件名 选项有六种选项。其中四种与导出的mysqldump选项一致，其余两种为： --fields-escaped-by=字符 ：设置转移字符 --ignore-lines=N ：忽略文件的前N行记录例：mysqlimport -u root -p test &quot;.\\user.txt&quot; &quot;--fields-terminated-by=,&quot; &quot;--lines-terminated-by=\\r\\n&quot; 数据库迁移分为三种情况： 相同版本间迁移：使用 mysqldump 和 mysql 进行备份与恢复 案例：mysqldump -h 主机A -u root -p=密码 -all-databases|mysql -h 主机B -u root -p=密码# 其中 | 即为管道符 不同版本间迁移：又分为高版本向低版本迁移和低版本向高版本迁移 高版本向低版本迁移：高版本会兼容低版本若表的存储引擎为MYISAM，可直接复制或使用命令mysqlhotcopy。若表的存储引擎为InnoDB，可使用mysqldump与mysql的组合进行备份与恢复而低版本并不兼容高版本，所以迁移会较困难 不同数据库间迁移若从 MYSQL 迁移到 SQL SERVER，可通过 MyODBC 实现。若从 MYSQL 迁移到 ORACLE，可先导出 sql 文件，然后手动修改 create 语句。 简单的性能优化思路 可通过show variables和show status查看修改配置和变量参数进行调优 若多个任务中一个执行缓慢，会影响其他任务。可通过show processlist显示所有活动进程，或执行kill终结消耗资源过多的进程 最好多次试验连接或子查询，找到效率最高的搜索方法。在 select 时可通过explain语句查看 select 的执行情况 使用存储过程的速度会提高 若不必要，不要直接执行select *语句 使用 UNION 连接 select 语句，比一系列 OR 条件的 select 语句效率高 对象索引可改善数据检索的性能，但会损失插入、更新、删除的性能。对于不常查询的表最好不要创建索引 关键字 like 的执行效率很低，一般会通过full text代替 like 常见查看命令显示解析SHOW TABLE STATUSshow table status (from 数据库名) (like 表达式);会直接显示该数据库中所有表的状态信息。 Name: stu # 表名或视图名 Engine: InnoDB # 存储引擎 Version: 10 # .frm文件版本 Row_format: Dynamic # 行存储格式 Rows: 7 # 行数目 Avg_row_length: 2340 # 行平均长度 Data_length: 16384 # 文件长度Max_data_length: 0 # 文件最大长度 Index_length: 0 # 索引文件长度 Data_free: 0 # 表被整序后，但未使用的字节数目 Auto_increment: NULL # 下一个Auto_increment值 Create_time: 2018-07-07 08:58:34 # 表的创建时间 Update_time: 2018-07-07 09:01:00 # 最后一次更新时间 Check_time: NULL # 最后一次检查时间 Collation: utf8_general_ci # 字符集 Checksum: NULL # 表的活性校验 Create_options: # 表的额外选项 Comment: # 表的注释 参考资料 MYSQL 数据库应用从入门到精通（第二版） Mysql 异常处理–condition 和 handler Mysql 系列–骏马金龙","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://coconutmilktaro.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://coconutmilktaro.top/tags/MySQL/"}]},{"title":"YAML学习笔记","slug":"YAML学习笔记","date":"2018-04-29T07:28:40.000Z","updated":"2022-05-30T02:51:53.877Z","comments":true,"path":"2018/YAML学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/YAML%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"本篇介绍 YAML 的语法，包括以下部分： YAML 简介 规范 数据结构","text":"本篇介绍 YAML 的语法，包括以下部分： YAML 简介 规范 数据结构 YAML 简介YAML 是一个类似 XML、JSON 的标记性语言。YAML 强调以数据为中心，并不是以标识语言为重点。因而 YAML 本身的定义比较简单，号称“一种人性化的数据格式语言”。实质上是一种通用的数据串行化格式，专门用于写配置文件。 YAML 与 XML、JSON 的区别： YAML 比 XML 和 JSON 都简单，易阅读，并且能表示出更加复杂的结构，运行效率也很高。 规范 大小写敏感 使用缩进表示层级关系 缩进时不能使用 Tab 键，只能用空格 缩进空格数不重要，只要相同层级左侧对齐即可（类似 Python） #表示注解 数据结构 对象：键值对的集合 数组：按次序排列的值，也称序列、列表 纯量：单个，不可再分的值 对象通过冒号分隔键、值person: zhangsan 也可通过行内表示法，写成行内对象person: &#123; name: zhangsan, age: 20 &#125; 数组一组连词线开头的行，构成一个数组 - zhangsan- lisi- wangwu 也可通过行内表示法 person: [zhangsan, lisi, wangwu] 复合结构对象和数组可以结合使用，形成复合结构。 对象嵌套对象 age: zhangsan: 20 lisi: 21 wangwu: 19 对象嵌套数组 lang: - c - python - java 数组嵌套数组数据结构的子成员是一个数组，则可以在该项下面缩进一个空格，yaml 语法会自动只空一格 - - c - java - python还可这样表示-- c - java - python还可这样表示- [c, java, python] 数组嵌套对象 - id: 1 name: zhangsan- id: 2 name: lisi 综合实例 ports: - port: 80 targetPort: 8082 - port: 81 targetPort: 8083 纯量纯量的数据类型与 js 相同 字符串 布尔值 true和false 整数 浮点数 Null ~ 时间：采用 ISO8601 标准 time: 2018-04-29T17:30:08+08:00 日期：采用 ISO8601 标准 date: 2018-04-29 integer: 12345 # 整数标准形式octal: 0o34 # 八进制表示，第二个是字母 ohex: 0xFF # 十六进制表示float: 1.23e+3 # 浮点数fixed: 13.67 # 固定小数minmin: -.inf # 表示负无穷notNumber: .NaN # 无效数字null: # 空值boolean: [true, false] # 布尔值 YAML 允许使用两个感叹号，强制转换数据类型 下面是可转换的类型（内置类型） !!int # 整数类型 !!float # 浮点类型 !!bool # 布尔类型 !!str # 字符串类型 !!binary # 也是字符串类型 !!timestamp # 日期时间类型 !!null # 空值 !!set # 集合 !!omap, !!pairs # 键值列表或对象列表 !!seq # 序列，也是列表 !!map # 键值表 例：obj1: !!str 123obj2: !!str trueomap: !!omap - Mark: 65 - Sammy: 63 - Key: 58set: !!set # ? 表示键为数组，在这里数组为 null ? Mark ? Sammy ? Key 字符串字符串默认不使用引号表示 str: hello 如果字符串之中包含空格或特殊字符，需要放在引号之中。 str: &quot;hello,world&quot; 单引号和双引号都可以使用，双引号不会对特殊字符转义。 str1: &quot;hello\\n&quot; #会将\\n识别为换行str2: &#x27;hello\\n&#x27; #会认为字符串就是hello\\n 字符串可以写成多行，从第二行开始，必须有一个单空格缩进。换行符会被转为空格。 str: 这是 多行 字符串 多行字符串可以使用|保留换行符，也可以使用&gt;折叠换行。 str1: | abc def # &quot;会保存\\n&quot;str2: &gt; abd def # &quot;不会保存\\n&quot; +表示保留文字块末尾的换行，-表示删除字符串末尾的换行。 s1: | abc# 即表示s1: &#x27;abc\\\\n&#x27;s2: |+ abc# 即表示s2: &#x27;abc\\\\n\\\\n&#x27;s3: |- abc# 即表示s3: &#x27;abc&#x27; 字符串之中可以插入 HTML 标记。 page: | &lt;div&gt;abcabc&lt;/div&gt; 引用锚点&amp;和别名*，可以用来引用。&amp;用来建立锚点（defaults），&lt;&lt;表示合并到当前数据，*用来引用锚点。 student: &amp;stu # &amp;用于起别名 zhangsan: 1 lisi: 2class1: &lt;&lt;: *stu等同于class1: zhangsan: 1 lisi: 2 文件YAML 文件可以由一或多个文档组成（也即相对独立的组织结构组成），文档间使用---在每文档开始作为分隔符。同时，文档也可以使用...作为结束符（可选，但标明后会有利于网络上传输）。 ---文件1---文件2 参考文章：YAML 语言教程-阮一峰 http://www.ruanyifeng.com/blog/2016/07/yaml.htmlYAML 书写规范 http://www.cnblogs.com/wdliu/p/7080305.htmlYAML 最最基础语法 https://blog.csdn.net/vincent_hbl/article/details/75411243yaml1.2 官方文档 http://yaml.org/spec/1.2/spec.html","categories":[],"tags":[{"name":"Lang","slug":"Lang","permalink":"https://coconutmilktaro.top/tags/Lang/"},{"name":"YAML","slug":"YAML","permalink":"https://coconutmilktaro.top/tags/YAML/"}]},{"title":"网络I/O模型","slug":"网络IO模型","date":"2018-04-29T01:26:12.000Z","updated":"2022-05-30T02:51:54.004Z","comments":true,"path":"2018/网络IO模型/","link":"","permalink":"https://coconutmilktaro.top/2018/%E7%BD%91%E7%BB%9CIO%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"同步与异步同步和异步描述的是用户线程和内核的交互方式。 同步： 指的是在两个或多个数据库、文件、模块、线程之间用来保持数据内容一致性的机制。用户线程发起I/O请求后需要等待，或者轮询内核I/O操作完成后才能继续执行。 同步处理过程：提交请求-&gt;等待内核处理（期间该线程不能干任何事）-&gt;处理完毕返回 异步： 指用户线程发起I/O请求后仍继续执行，不用阻塞当前线程来等待处理完成，当内核I/O操作完成后会通知用户线程，或者调用用户线程注册的回调函数，回调通知此线程。 异步处理过程：请求通过事件触发-&gt;内核处理（这时该线程仍然可以作其他事情）-&gt;处理完毕 同步、异步关注的是消息通知机制，针对的是客户端。 阻塞与非阻塞阻塞和非阻塞描述的是用户线程调用内核I/O操作的方式 阻塞：调用结果返回之前，调用者会被挂起（不可中断），调用者只有在得到返回结果后才能继续。即I/O操作需要彻底完成后才返回用户空间。 非阻塞：调用结果返回前，不会被挂起，调用不会阻塞调用者。在内核的数据还未准备好时，会立即返回，进程可以去干其他事情。即I/O操作被调用后立即返回给用户一个状态值，无须等到I/O操作彻底完成。 阻塞、非阻塞关注的是调用者等待被调用者返回结果时的状态，针对的是服务器端。 阻塞、非阻塞与同步、异步的区别一个I/O操作实际是分为两个步骤：发起I/O请求和实际的I/O请求。 阻塞和非阻塞在于第一步，即发起I/O请求是否被阻塞。 同步和非同步在于第二步，即实际I/O请求是否被阻塞。 同步是在 I/O 中的一系列操作都是调用者（用户进程）自己完成（自己去问内核）。而异步是调用者在发起调用后，自己不管了，等内核数据准备好了以后，内核自己告诉进程，即让内核去通知进程，实现回调。 至于阻塞与非阻塞，是决定是否让调用者挂起。 网络 I/O 的本质是 socket 的读取，socket 在 linux 系统被抽象为流，I/O 可以理解为对流的操作。这个操作又分为两个阶段： 1.等待流数据准备，即等待网络上的数据分组到达，然后被复制到内核的某个缓冲区 2.从内核向进程复制数据，把数据从内核缓冲区复制到应用进程缓冲区 五种 Unix I/O 模型I/O 模型：进程是无法直接操作 I/O 设备的，其必须通过系统调用请求内核来协助完成 I/O 动作，而内核会为每个 I/O 设备维护一个 buffer。 用户进程发起请求，内核接受到请求后，从 I/O 设备中获取数据到 buffer 中，再将 buffer 中的数据 copy 到用户进程的地址空间，该用户进程获取到数据后再响应客户端。如下图中，真正称为 I/O 的就是内核内存与与进程内存间的过程 同步 I/O 模型阻塞 I/O（Blocking I/O）： 当用户进程进行系统调用 read()时，进程发起 recvform 系统调用，内核就开始了 I/O 的第一个阶段，准备数据到缓冲区中，当数据都准备完成后，则将数据从内核缓冲区中拷贝到用户进程的内存中，这时用户进程才解除 block 的状态重新运行。整个过程中用户进程都是阻塞的。不会消耗 CPU 时间，执行效率高。 非阻塞 I/O（Non-Blocking I/O）： 用户进程只有在第二个阶段被阻塞了，而第一个阶段没有阻塞。在第一个阶段中，recvform 系统调用调用之后，内核马上返回给进程，如果数据还没准备好，此时会返回一个 error，进程在返回之后，可以干点别的事情，然后再发起 recvform 系统调用，用户进程需要盲等，不停的去轮询内核，看数据是否准备好了。在拷贝数据整个过程，进程仍然是属于阻塞的状态。由于用户进程轮询内核，所以该模型是比较消耗 CPU 的，效率较低。 ** I/O 复用（I/O Multiplexing）：** I/O 执行的两个阶段都是用户进程都是阻塞的，但是两个阶段是独立的，在一次完整的 I/O 操作中，该用户进程是发起了两次系统调用。使用 select()、poll()或 epoll()（poll 的改进版）进行调用，可支持两路调用。相比于多进程和多线程技术，I/O 复用不需要进程线程创建和切换的开销，系统开销更小。 select 调用是内核级别的，select 轮询可以等待多个 socket，当其中任何一个 socket 的数据准备好了（通过内核监视），就能返回进行可读，然后进程再进行 recvform 系统调用。select 在此模式下最多只支持 1024 个并发。 I/O 复用应用场景： 服务器需要同时处理多个处于监听状态或多个连接状态的套接字。 服务器需要同时处理多种网络协议的套接字。 信号驱动 I/O（Signal Driven I/O）： 也称基于事件的 I/O。只有在 I/O 执行的第二阶段阻塞了用户进程，而在第一阶段是没有阻塞的。在 I/O 执行的第一阶段，当数据准备完成之后，内核会主动的通知用户进程数据已经准备完成（通过返回一个 SIGIO 信号），即对用户进程做一个回调。该通知分为两种，一为水平触发，即如果用户进程不响应则会一直发送通知，二为边缘触发，即只通知一次。 注：需要先开启套接字的信号驱动 I/O 功能，并使系统调用 sigaction 安装一个信号处理函数 异步 I/O 模型异步 I/O（Asynchrnous I/O）： 当用户进程发起系统调用后，立刻就可以开始去做其它的事情，然后直到 I/O 执行的两个阶段都完成之后，内核会给用户进程发送通知，告诉用户进程操作已经完成了。由于在调用后进程会立刻返回，所以在整个输入操作的等待和复制期间，进程都不会阻塞。 异步 I/O 不需要 select 或 poll 主动询问，也没有询问描述符的数量限制。 参考文章：简明网络 I/O 模型—同步异步阻塞非阻塞之惑 浅谈 Linux 下的五种 I/O 模型 Linux 网络 I/O 模型简介（图文） socket 和 网络 I/O 模型","categories":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"I/O","slug":"I-O","permalink":"https://coconutmilktaro.top/tags/I-O/"}]},{"title":"Docker网络学习笔记","slug":"Docker网络学习笔记","date":"2018-04-27T05:04:48.000Z","updated":"2022-05-30T02:51:53.789Z","comments":true,"path":"2018/Docker网络学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Docker%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"主要是对 docker 文档(v18.03)的翻译以及自己的学习笔记","text":"主要是对 docker 文档(v18.03)的翻译以及自己的学习笔记 Docker 网络模式目前 Docker 容器共有 5 种网络模式： 桥接模式（bridge） 主机模式（host） 容器模式（container） 无网络模式（none） 用户自定义模式（user-defined） 当安装完 docker 后，会默认创建三个网络，可通过docker network ls查看 # docker network lsNETWORK ID NAME DRIVER SCOPE01ec14a3a84c bridge bridge local56d5a7a9b06c host host local9cbd2d449df7 none null local 用户可在运行容器时通过--network=指定网络。 桥接模式 bridgebridge 是 docker 默认选择的网络，而网桥就是 docker0 通过ifconfig即可看到。 # ifconfigdocker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 0.0.0.0 ...... 创建容器时若未指定 network 或网桥，就会默认挂到 docker0 网桥上。docker0 的网段为 172.17.0.0/16，网关地址为 172.17.0.1，可通过docker inspect bridge查看。 # docker inspect bridge[ &#123; &quot;Name&quot;: &quot;bridge&quot;, ...... &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot; &#125; ] &#125;, ...... docker daemon 会创建一对对等接口：虚拟网桥上的 vethxxx 和容器的 eth0。veth 放置在宿主机的命名空间中，将宿主机上的所有网络为 bridge 的容器都连接到这个内部网络中，同时 daemon 会从网桥的私有地址空间中分配一个 IP 地址和子网给该容器。 # ifconfig......veth7576df5: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet6 fe80::9023:faff:fe28:14b3 prefixlen 64 scopeid 0x20&lt;link&gt; ether 92:23:fa:28:14:b3 txqueuelen 0 (Ethernet) ......vethab244c0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet6 fe80::d047:b2ff:fee4:89c8 prefixlen 64 scopeid 0x20&lt;link&gt; ether d2:47:b2:e4:89:c8 txqueuelen 0 (Ethernet) ............ 连接同一个 bridge 网络的容器间能够通过 IP 地址互相通信。由于运行容器默认使用 bridge 网络，所以若要运行对外提供访问的服务，如 web 服务，就必须暴露端口，通过-p（指定容器暴露端口）或-P（发布容器所有端口）发布容器暴露的端口。 默认情况下，创建容器时不会将任何端口发布到外部。若通过-p或--publish发布端口，会创建一个防火墙规则，将容器端口映射到宿主机上的端口。 Docker 在 bridge 网络上不支持服务自动发现。如果需要通过容器名实现容器间的互相通信，就要设置连接属性--link=容器名:别名（官方并不推荐，已快被淘汰，官方推荐用user-definded网络实现互通）。容器内所有的环境变量都可供连接到它的容器使用，可能会造成安全隐患。 主机模式 host在此模式下，容器网络与宿主机网络间的隔离将被禁止，容器共享宿主机的网络命名空间，使容器直接暴露在公共网络中。因此，需要通过端口映射（Port Mapping）进行协调。 # docker run -it --network=host alpine/ # ifconfigbr-7673688a6ae1 Link encap:Ethernet HWaddr 02:42:DC:D4:64:FA inet addr:172.22.0.1 Bcast:172.22.255.255 Mask:255.255.0.0 ......docker0 Link encap:Ethernet HWaddr 02:42:81:58:18:0C inet addr:172.17.0.1 Bcast:172.17.255.255 Mask:255.255.0.0 ......ens33 Link encap:Ethernet HWaddr 00:0C:29:58:0C:12 inet addr:192.168.163.101 Bcast:192.168.163.255 Mask:255.255.255.0 ............ 由此可知，当使用 host 模式网络时，容器实际上继承了宿主机的 IP 地址，并且在容器中可以看到宿主机的所有网卡。 因为没有路由开销，因此主机模式会比 bridge 模式更快。但是由于容器直接被暴露在公共网络中，会有安全隐患。 容器网络 container在该模式，新创建的容器和已经存在的一个容器共享一个网络命名空间。两个容器除了网络的命名空间，其他的如文件系统、进程列表等仍然是隔离的。两个容器可以通过环回口进行设备通信。该模式也是 Kubernetes 使用的网络模式。 该模式通过--network=container:另一个已存在的容器实现。 # docker run -it --name container_A alpine/ # ifconfigeth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:05 inet addr:172.17.0.5 Bcast:172.17.255.255 Mask:255.255.0.0 ......# docker run -it --name container_B --network=container:container_A alpine/ # ifconfigeth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:05 inet addr:172.17.0.5 Bcast:172.17.255.255 Mask:255.255.0.0 ...... 无网络模式 none该模式关闭了容器的网络功能，容器处于自己独立的网络命名空间中，且不进行任何配置。 使用场景：1.容器并不需要网络（例如只需要写磁盘卷的批处理任务，或生成随机密钥）2.自定义网络 用户自定义模式 user-defined本模式使用户可自定义网络中的参数，以满足特定需求，例如 DNS 服务器。同一个自定义网络中，可以使用对方容器的容器名、服务名、网络别名来找到对方。这个时候帮助进行服务发现的是 Docker 内置的 DNS。所以，无论容器是否重启、更换 IP，内置的 DNS 都能正确指定到对方的位置。 docker 内嵌 DNS Server，但只有在用户自定义模式才能使用。 docker 提供的网络驱动：User-defined bridge，overlay，macvlan，host，Third-party network plugins。 可通过docker network create --driver=[driver] [network-name] [--subnet] [--gateway]指定网络驱动创建网络，并指定网段和网关。 bridge用于在同一主机内的通信。macvlan和overlay用于跨主机通信。 macvlan：当从 VM 设置迁移或需要容器看起来像网络上的物理主机时使用，可以使每个容器都具有唯一的 MAC 地址。 overlay：当需要在不同 Docker 主机上运行的容器进行通信，或者当多个应用程序使用 swarm 服务一起工作时使用。 host和网络模式对应的作用相同，host 驱动仅可用于 v17.06 版本以上的 docker swarm 集群。network plugins是第三方为 docker 制作的网络插件。 bridge 驱动: 用于创建类似 bridge 网络模式的网络，加入该网络的容器必须在同一台宿主机，仅适合一台主机上的小型网络。 # docker network create --driver=bridge mybridge --subnet=10.1.1.0/24 --gateway=10.1.1.1# docker inspect mybridge[ &#123; &quot;Name&quot;: &quot;mybridge&quot;, ...... &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;10.1.1.0/24&quot;, &quot;Gateway&quot;: &quot;10.1.1.1&quot; &#125; ]......# docker run -it --network=mybridge alpine/ # ifconfigeth0 Link encap:Ethernet HWaddr 02:42:0A:01:01:02 inet addr:10.1.1.2 Bcast:10.1.1.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 ...... 同主机容器间通信，分为：IP 通信，DNS Server，Joined 容器。 IP 通信： 容器处于两个不同网络中，通过docker network connect [对端容器所处网络名][本端容器]连接容器。 --alias 设置对端网络别名--ip 指定对端网络上该IP地址的容器--ip6 同上，为IPv6地址--link 指定连接的容器名--link-local-ip 为容器添加一个连接本地的地址 # docker run -it --name container_A alpine# docker network connect mybridge container_A# docker run -it --network=mybridge --name=container_B alpine# docker inspect container_B -f &#x27;&#123;&#123;.NetworkSettings.Networks.mybridge.IPAddress&#125;&#125;&#x27;10.1.1.4# docker inspect container_A -f &#x27;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&#x27;172.17.0.3# docker attach container_A/ # ping 10.1.1.4PING 10.1.1.4 (10.1.1.4): 56 data bytes64 bytes from 10.1.1.4: seq=0 ttl=64 time=0.294 ms/ # ifconfigeth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:03 inet addr:172.17.0.3 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 ......eth1 Link encap:Ethernet HWaddr 02:42:0A:01:01:03 inet addr:10.1.1.3 Bcast:10.1.1.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 ...... 原理如图（图为 docker 文档的，ip 地址有偏差），docker 会在本容器上创建一个新的网卡，由指定的网络分配 IP 地址，实现与指定的网桥中容器的连接（如上面 ifconfig 中 eth1）。 DNS： 使用对方容器的容器名、服务名、网络别名来找到对方，无论容器是否重启、更换 IP，内置的 DNS 都能正确指定到对方的位置。 Joined 容器： 即 container 网络模式，使两个及以上的容器共享一个网络栈，共享网卡和配置。 容器访问外网容器默认就能访问外网，这里外网指容器外的网络，不只是互联网。 处理流程：1.容器发数据包，docker0 收到数据包后查看 IP 头，发现是发往外网的，交给 NAT 处理。2.NAT 将源地址转为宿主机 IP 地址，并从主机网卡发出。 外网访问容器外网通过端口映射访问容器，每个映射的端口，宿主机都会启动一个 docker-proxy 进程处理访问容器的流量，可在宿主机通过ps -ef | grep docker-proxy查看端口映射情况下图为内外网的完整访问流程图 跨主机网络docker 容器有多种访问外网的方案，其中 docker 提供两个原生方案：overlay 和 macvlan。还可选择第三方方案：flannel，weave，calico。 众多的 docker 网络方案通过 libnetwork 与容器网络模型（Container Network Model）集成在一起，其中 libnetwork 为 docker 容器网络库，而其核心即为容器网络模型，对容器网络进行了抽象，由以下三个组件组成： Sandbox：容器的网络栈，包含容器接口、路由表和 DNS 设置。Sandbox 的实现标准为 Linux Network Namespace，可以包含来自不同 Network 的 Endpoint。 Endpoint：将 Sandbox 接入 Network，典型实现为 Veth Pair。一个 Endpoint 只属于一个网络，也只属于一个 Sandbox。 Network：包含一组 Endpoint，同一 Network 的 Endpoint 可以直接通信。 OverlayOverlay 网络驱动创建了多 docker 主机间的分布式网络，允许与其连接的容器互相安全地通信，服务和容器能同时连接多个网络，但也仅能在连接的网络间通信。虽然可以将集群服务和单独的容器都连入一个 overlay 网络，但 overlay 对于集群与单独容器的默认配置是不同的，对于不同对象有不同的选项。 在创建 overlay 网络之前，需要使用 docker swarm 初始化作为 swarm manager，或者使用docker swarm join将其加入到现有 swarm 中。这两者都创建缺省的 swarm 服务使用的默认 overlay 网络ingress。 实验环境：swarm manager：192.168.163.102swarm worker：192.168.163.103 在 manager 上初始化 docker swarmdocker swarm initworker 上加入 docker swarm（将 manager 上生成的命令复制粘贴到 worker 上运行） # docker swarm join --token SWMTKN-1-077i43tqnp5df8y29nrrh8apm9y2a4khzggg8nydd2yy8nzzjw-0i1qu8z1xl2s7ngy8y1gcnfnb 192.168.163.102:2377This node joined a swarm as a worker. 创建 overlay 网络 my_overlay： # docker network create -d overlay my_overlay# docker network lsNETWORK ID NAME DRIVER SCOPE8cc4b6f2ddfa bridge bridge local5db2b494b1af docker_gwbridge bridge local5a1cfdddfd60 host host localy6l4bambmqoj ingress overlay swarmbdv2xdmkujbu my_overlay overlay swarm7ec96e1718f1 none null local 若要创建一个 overlay 网络供群集服务或独立容器与其他 Docker 守护程序上运行的其他独立容器进行通信，要添加--attachable参数。ingress网络创建时没有--attachable选项，说明只有 swarm 服务可以使用它，而不是独立的容器。通过在创建网络时添加--attachable选项使得运行在不同 Docker 守护进程上的独立容器能够进行通信，而无需在各个 Docker 守护进程主机上设置路由。 容器发现对于大多数情况，应该连接到服务名，而不是单独容器，因为服务是负载均衡的且是由所有服务后的容器（即任务 task）处理的。要获取支持该服务的所有任务（task）的列表，可以执行 DNS 查找tasks.&lt;service-name&gt; overlay 网络加密默认 docker 对 swarm 服务在 GCM 模式下使用 AES 算法加密，集群中的 manager 节点每 12 小时就轮换用于加密 gossip（反熵算法）数据的密钥。 要加密应用程序数据，需要在创建 overlay 网络时添加--opt encrypted。这使得 vxlan 级别的 IPSEC 加密成为可能。这种加密会带来不可忽视的性能损失，所以应该在生产中使用它之前对其进行测试。 当启用 overlay 加密时，Docker 会在所有节点之间创建 IPSEC 隧道，在这些节点上调度连接到 overlay 网络服务的任务。这些通道在 GCM 模式下也使用 AES 算法，manager 节点每 12 小时自动轮换一次密钥。 不要将 Windows 节点添加到加密的 overlay 网络。Windows 上不支持 overlay 网络加密。如果 Windows 节点尝试连接到加密的 overlay 网络，虽不会报告错误，但节点无法通信。 默认 ingress 网络默认 overlay 网络 ingress 的作用：当自动选择的子网与网络中已存在的子网冲突或需要自定义某项低层的网络配置（例如 MTU）时，默认的 ingress 网络会很有用。通常在 Swarm 中创建服务前对 ingress 网络进行删除或重建操作。如果已有发布端口的服务，在删除 ingress 网络前必须先删除这些服务。若没有 ingress 网络且不发布端口的服务在运行却没有进行负载均衡，那么那些发布端口的服务会受到影响。 可在创建网络时加上--ingress选项创建 ingress 网络并自定义网络参数。只能创建一个 ingress 网络。 默认 docker_gwbridge 网络docker_gwbridge 是一个虚拟网桥，它将 overlay 网络（包括 ingress 网络）连接到单独的 Docker 守护进程的物理网络。初始化群集或将 Docker 主机加入群集时，Docker 会自动创建它，但它不是 Docker 设备。它存在于 Docker 主机的内核中。如果需要自定义其设置，则必须在将 Docker 主机加入集群之前或临时从集群中暂时删除主机之后执行该自定义操作。 若要删除 docker_gwbridge 网络，需要先停止 docker，再删除 docker_gwbridge 网络，由于停止了 docker，所以要通过ip link删除该网络。 # ip link set docker_gwbridge down# ip link del docker_gwbridge 再启动 docker，但不要加入或初始化 swarm。重建一个 docker_gwbridge 网络，然后再加入或初始化 swarm。 在 overlay 网络发布端口连接在同一个 overlay 网络的集群服务可以有效地互相发布所有端口。若要使一个端口能在服务外能访问，必须在docker service create或docker service update后加上-p选项发布指定端口。支持冒号分隔的旧语法-p 8080:80/tcp和逗号分隔的新语法-p published=8080,target=80,protocol=tcp。 绕过集群服务的路由网格（routing mesh）默认情况下，发布端口的群集服务使用路由网格来完成。当连接到任何 swarm 节点上的已发布端口（无论是否运行给定服务）时，都会透明地重定向到正在运行该服务的 worker。实际上，Docker 充当群集服务的负载平衡器。使用路由网格的服务以虚拟 IP（VIP）模式运行。即使在每个节点上运行的服务（通过--global标志）也使用路由网格。使用路由网格时，不能保证哪个 Docker 节点服务客户端会请求。 要绕过路由网格，可以通过设置选项--endpoint-mode dnsrr来使用DNS Round Robin（DNSRR）模式启动服务且必须在服务前运行自定义的负载均衡器。对 Docker 主机上服务名的 DNS 查询会返回运行该服务的节点的 IP 地址列表，可以通过配置负载均衡器使用此列表并且平衡各节点间的流量。 单独控制和数据默认情况下，尽管群集控制流量是加密的，但群集管理和应用程序之间的控制流量运行在同一个网络上，可以配置 Docker 使用单独的网络接口来处理两种不同类型的流量。初始化或加入群集时，分别指定--advertise-addr和--datapath-addr，加入集群的每个节点都要执行此操作。 实验（根据官方文档的实验）Manager（system2）:192.168.163.102Worker-1（system3）：192.192.168.163.103Worker-2（system4）：192.192.168.163.104 在 manager 上初始化 swarm，worker 节点加入 swarm。在 manager 上查看节点。 # docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONohfkwg8uu4zkjtyk1l1nbze4p * system2.example.com Ready Active Leader 18.03.1-ce6dbboj25t5tws0ohd1pdhsahl system3.example.com Ready Active 18.03.1-ceaug4gnqnm0na4pwu835dku51x system4.example.com Ready Active 18.03.1-ce 可通过--filter role=worker|manager过滤节点信息 在 manager 上创建 overlay 网络。不需要在其他节点上创建 overlay 网络，当其中一个节点开始运行需要 overlay 网络的服务时，它将自动创建。docker network create -d overlay nginx-net在 manager 上创建一个 nginx 服务（只能在 manager 上创建服务） # docker service create \\--name my-nginx \\-p 80:80 \\--replicas=5 \\ #设置创建的任务个数--network nginx-net \\nginx# docker service lsID NAME MODE REPLICAS IMAGE PORTSnvxzb5kzihl6 my-nginx replicated 5/5 nginx:latest *:80-&gt;80/tcp 在 manager 和 worker 上查看 nginx-net 网络情况，以及容器情况docker inspect nginx-net 新建 overlay 网络，将服务更新到新的网络上docker network create -d overlay nginx-net-2 # docker service update \\--network-add nginx-net-2 \\ # 将my-nginx添加进nginx-net-2网络中--network-rm nginx-net \\ # 将my-nginx从nginx-net网络中删除my-nginx 注：overlay 网络会因为需要自动创建，但不会自动删除（当服务不需要该网络后）。需要手动删除服务和网络。docker service rm my-nginx &gt; docker network rm nginx-net nginx-net-2 Macvlan一些应用类似传统应用或监视网络流量的应用，希望直接连接到物理网络，可以使用 macvlan 网络驱动为每个容器的虚拟网络接口分配 MAC 地址，需要指定宿主机上的物理接口用于 Macvlan，以及 Macvlan 的子网和网关。可以使用不同的物理网络接口来隔离 Macvlan 网络。网络设备需要能够处理“混杂模式”，其中一个物理接口可以分配多个 MAC 地址。网络模式最好是bridge或overlay。可以在 bridge 模式或 vlan 的 trunk 模式中创建 macvlan 网络。 在 bridge 网络模式中创建 macvlan 网络在docker network create后添加-d macvlan，也可再指定流量通过的实际网卡-o parent=ens33。若要排除在 macvlan 中使用的 IP 地址，可添加选项--aux-addresses=&quot;aux-addr=&quot;，参数值为一组键值对。一张网卡仅能被一个 macvlan 网络设为parent。 在 vlan trunk 模式中创建 macvlan 网络通过在网卡名后加.[数字]创建网卡子接口，例如-o parent=ens33.10。 使用 IPvlan可以使用三层 IPvlan 取代二层 Macvlan。通过指定-o ipvlan_mode=l2 macvlan 网络的注意条件 大多数云提供商会阻塞访问 macvlan 网络，可能需要物理访问网络设备。 macvlan 网络驱动程序仅适用于 Linux 主机，而 Mac 的 Docker 桌面，Windows 的 Docker 桌面或 Windows Server 的 Docker EE 不支持。 至少需要 3.9 版的 Linux 内核，建议使用 4.0 或更高版本。 补充知识点支持 IPv6只有 Linux 主机的 docker 支持 IPv6。修改/etc/docker/daemon.json，添加&#123;&quot;ipv6&quot;: true&#125;开启 IPv6。然后重新加载配置文件systemctl daemon-reload或systemctl reload docker。在创建网络的时候可以加上--ipv6选项，在创建容器时加上--ip6选项 配置 iptablesDocker 通过 iptables 规则来提供网络隔离，不应修改 Docker 已设置的 iptables 规则。所有 Docker 的 iptables 规则都被添加到DOCKER链中，不要手动操作此表。如果需要添加能在加载 Docker 规则之前加载的规则，应该将它们添加到DOCKER-USER链中，这些规则在 Docker 自动创建任何规则之前加载。 默认情况下，所有外部源 IP 都被允许连接到 Docker 守护进程。若要只允许特定的 IP 或网络访问容器，可在DOCKER过滤器链的顶部插入否定规则。 例：iptables -I DOCKER-USER -i ens33 ! --source 192.168.163.0/24 -j DROP为防止 Docker 修改 iptables 策略，在/etc/docker/daemon.json中设置&#123;&quot;iptables&quot;: false&#125;，官方不推荐这样设置，因为这样所有关于 docker 的 iptables 配置都要手动管理。 DNS 选项 --dns 指定一个或多个DNS服务器--dns-search 设置dns搜索域--dns-opt 键值对，可参考/etc/resolv.conf--hostname 设置容器的主机名，默认就是容器名 docker 代理在 Docker 客户端上编辑启动容器的用户主目录~/.docker/config.json文件。添加字段，可用httpsProxy或ftpProxy指定代理服务器的类型，并指定代理服务器的地址和端口，可以同时配置多个代理服务器。 通过将noProxy键设置为一个或多个逗号分隔的 IP 地址或主机，指定排除的代理服务器，支持*字符作为通配符。 &#123; &quot;proxies&quot;: &#123; &quot;default&quot;: &#123; &quot;httpProxy&quot;: &quot;http://127.0.0.1:3001&quot;, &quot;noProxy&quot;: &quot;*.test.example.com,.example.com&quot; &#125; &#125;&#125; 在创建容器时可通过--env设置环境变量，通过环境变量指定代理服务器。 --env HTTP_PROXY=&quot;&quot;--env HTTPS_PROXY=&quot;&quot;--env FTP_PROXY=&quot;&quot;--env NO_PROXY=&quot;&quot; 参考文档Cloudman 《每天五分钟玩转 docker 容器技术》docker 官方文档-网络板块 &gt; docker 网络模式","categories":[{"name":"云计算","slug":"云计算","permalink":"https://coconutmilktaro.top/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"docker","slug":"docker","permalink":"https://coconutmilktaro.top/tags/docker/"}]},{"title":"磁盘管理与文件系统学习笔记","slug":"磁盘管理与文件系统学习笔记","date":"2018-04-01T13:22:44.000Z","updated":"2022-05-30T02:51:54.000Z","comments":true,"path":"2018/磁盘管理与文件系统学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E4%B8%8E%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Chrony/NTP学习笔记","slug":"Chrony学习笔记","date":"2018-01-31T14:42:55.000Z","updated":"2022-05-30T02:51:53.783Z","comments":true,"path":"2018/Chrony学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/Chrony%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"本篇包含以下内容 NTP 协议介绍 Chrony 基础搭建 ntpd 基础搭建","text":"本篇包含以下内容 NTP 协议介绍 Chrony 基础搭建 ntpd 基础搭建 NTP 协议介绍NTP 全称 Network Time Protocol 网络时间协议，用于同步计算机时间。保证局域网服务器与时间服务器的时间保持一致，并支持使用加密确认的方式防止恶意协议攻击。 自 CentOS7.2 后，chronyd 服务代替原来的 ntpd 服务，性能提高且配置简单。 根据红帽文档，chronyd 与 ntpd 的区别在于： chronyd 使用更好的算法，同步精度、速度与对系统的影响都比 ntpd 更好。 chronyd 可以在更大的范围内调整系统时间速率，且能在时钟损坏或不稳定的计算机上正常工作。 当网络故障时，chronyd 仍能很好地工作，而 ntpd 必须定时轮询时间参考才能正常工作。 chronyd 可以快速适应时钟速率的突然变化，ntpd 则需要一段时间才能稳定。 chronyd 提供对孤立网络的支持，手动输入校准时间，并通过算法计算实时时间，估计计算机增减时间的速率，从而调整时间。 Chrony 基础搭建环境 CentOS7.4 步骤 安装 chrony 服务（默认已安装）yum install chrony安装完后会有两个程序，一个 chronyd 服务，一个 chronyc 监控配置程序。 启动服务，设置开机自启systemctl start chronydsystemctl enable chronyd chrony 的配置文件/etc/chrony.conf server ntp.sjtu.edu.cn iburstserver s1a.time.edu.cn iburstserver s1b.time.edu.cn iburstserver s1d.time.edu.cn iburst//server 添加时间服务器，能添加很多driftfile /var/lib/chrony/drift//chronyd中的校准文件，根据实际时间计算出计算机增减时间的比率，能在重启后做出补偿makestep 1.0 3//当系统时钟漂移过快后，会通过很长的调整期纠正，该命令指定在调整期大于某阈值时才调整//此处是当偏移大于1秒，系统时钟调整3次。rtcsync//启用内核模式，系统时间每11分钟拷贝到实时时钟#allow 192.168.0.0/16//允许指定网段或主机使用服务#keyfile /etc/chrony.keys//设置密钥文件，可做NTP加密logdir /var/log/chrony//设置日志文件 防火墙放行并重启服务firewall-cmd --permanent --add-service=ntpfirewall-cmd --permanent --add-rich-rule=&#39;rule family=ipv4 port port=123 protocol=udp accept&#39;firewall-cmd --reloadsystemctl restart chronyd 查看同步源信息 chronyc sourcestats//查看同步源状态210 Number of sources = 4Name/IP Address NP NR Span Frequency Freq Skew Offset Std Dev==============================================================================202.120.2.100.dns.sjtu.e&gt; 0 0 0 +0.000 2000.000 +0ns 4000ms10.112.202.in-addr.arpa.&gt; 0 0 0 +0.000 2000.000 +0ns 4000msntpa.nic.edu.cn 0 0 0 +0.000 2000.000 +0ns 4000mstime.njnet.edu.cn 4 3 10 +810.140 43784.844 +7121us 14mschronyc sources //查看同步源，结果与上一条类似 自动同步时间chronyc sources -v 若要局域网内同步时间，只要客户端都安装 chrony，且配置文件的 server 设置为此服务器 ip 即可。 ntpd 基础搭建 安装 ntpd 服务yum install ntp 修改配置文件/etc/ntp.conf在 restrict 段添加允许的主机网段restrict 192.168.163.0 mask 255.255.255.0允许指定网段或主机使用服务（类似 chrony 的 allow）server 字段与 chrony 类似，指定上游 ntp 服务器。 重启 ntpdsystemctl restart ntpd.service 在 ntpd 服务未开启时，可用命令ntpdate 0.centos.pool.ntp.org手动同步。这条命令只能在 ntpd 未开启时才有效。 命令ntpq -p列出 NTP 服务器与上游服务器的连接状态 # ntpq -p remote refid st t when poll reach delay offset jitter==============================================================================*static-5-103-13 .GPS. 1 u 19 64 1 777.937 -99.910 133.624+mx.comglobalit. 128.227.205.3 2 u 19 64 1 413.258 84.278 15.570-ntp6.flashdance 192.36.143.130 2 u 18 64 1 438.957 196.165 32.565+119.79-161-57.c 129.242.4.241 2 u 50 64 1 670.566 58.678 51.049remote：上层ntp的IP地址或主机名，&#x27;+&#x27;表示优先，&#x27;*&#x27;表示次优先refid：参考的上一层NTP主机的地址st：stratum阶层poll：下次更新在几秒后offset：时间补偿的结果 扩展内容 系统时间与BIOS时间不一定相同。查看硬件BIOS时间：# hwclock -rWed 02 May 2018 05:00:32 PM CST -0.854732 seconds将当前系统时间写入BIOS中# hwclock -w","categories":[{"name":"系统运维","slug":"系统运维","permalink":"https://coconutmilktaro.top/categories/%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"Chrony","slug":"Chrony","permalink":"https://coconutmilktaro.top/tags/Chrony/"},{"name":"NTP","slug":"NTP","permalink":"https://coconutmilktaro.top/tags/NTP/"}]},{"title":"iSCSI学习笔记","slug":"iSCSI学习笔记","date":"2018-01-29T00:32:43.000Z","updated":"2022-05-30T02:51:53.878Z","comments":true,"path":"2018/iSCSI学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/iSCSI%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"iSCSI 笔记包含以下内容 iSCSI 介绍 常见存储方式 SCSI 与 iSCSI iSCSI 基础搭建","text":"iSCSI 笔记包含以下内容 iSCSI 介绍 常见存储方式 SCSI 与 iSCSI iSCSI 基础搭建 iSCSI 介绍常见存储方式 DAS 直接附加存储：外接存储设备直接连在服务器内部总线上，数据存储设备是整个服务器结构的一部分。 NAS 网络接入存储：存储设备独立于服务器，作为文件服务器独立存在与网络中。存储设备通过标准的网络拓扑（如以太网）添加到一群计算机。 SAN 存储区域网络：创造了存储的网络化。包含两种部署方式 FCSAN：使用光纤通道 IPSAN：使用 IP 通道（如以太网线与 iSCSI 技术） SCSI 与 iSCSI SCSI 小型计算机系统接口：一种通用接口标准，多用于服务器系统级接口。SCSI 结构基于 C/S 模式，其通常应用环境是：设备互相靠近，并且这些设备由 SCSI 总线连接。 iSCSI Internet 小型计算机系统接口：一种基于 TCP/IP 的协议，用于建立管理 IP 存储设备、主机与客户机之间的连接，并创建 SAN。SAN 使得 SCSI 协议应用于高速数据传输网络成为可能，这种传输以数据块级别（block-level）在多个数据存储网络间进行。 iSCSI 的主要功能是在 TCP/IP 网络上的主机系统（启动器 initiator）和存储设备（目标器 target）之间进行大量数据的封装和可靠传输。此外，iSCSI 提供了在 IP 网络封装 SCSI 命令，且运行在 TCP 上。 服务器：target 客户端：initiator iSCSI 基础搭建环境： CentOS7，内核 3.10 两台虚拟机，system1 与 system2，system1 为服务器，system2 为客户端 虚拟机网段 192.168.163.0/24 system1 IP：192.168.163.100/24 system2 IP：192.168.163.102/24 首先搭建服务器 安装 target 与 targetcli 并开机启动服务器端要安装的服务为 targetd，还需安装 targetcli 程序进行配置 yum install targetd targetclisystemctl enable targetd targetsystemctl start targetd target 确保系统有空闲可用的裸磁盘本处选择/dev/sdc1，大小 5G 进入targetcli程序配置 目录结构o- / o- backstores | o- block | o- fileio | o- pscsi | o- ramdisk o- iscsi o- loopback 进入 block，创建设备 disk1create dev=/dev/sdc1 name=disk1另一种写法create disk1 /dev/sdc1然后进入 iscsi 目录，设置服务器端识别号 识别号规范：iqn.年-月.域名反置:服务器主机名 create wwn=iqn.2018-01.com.example:system1该标识符可以自己设定如上配置，也可让系统自动生成直接在 iscsi 目录中create设置后 iscsi 目录结构如下 o- iscsi o- iqn.2018-01.com.example:system1 o- tpg1 o- acls o- luns o- portals o- 0.0.0.0:3260 进入 acls/ 添加客户端身份标识create wwn=iqn.2018-01.com.example:system2进入 luns/ 给该组设置可用的存储设备create /backstores/block/disk1disk1 就是 block 中创建的设备名此时 iscsi 目录结构 o- iscsi o- iqn.2018-01.com.example:system1 o- tpg1 o- acls | o- iqn.2018-01.com.example:system2 | o- mapped_lun0 o- luns | o- lun0 o- portals o- 0.0.0.0:3260 进入 portals/ 修改服务端端口号有可能里面没有默认配置，直接创建。若有默认配置就先删除再创建delete 0.0.0.0 ip_port=3260create 192.168.163.100 3260此处 ip 地址为服务器端 IP，端口号为 3260全部配置完 exit 退出配置程序 防火墙放行对应端口号，重启服务端口号 3260，且 tcp 端口放行 firewall-cmd --permanent --add-port=3260/tcpfirewall-cmd --reloadsystemctl restart targetd target 然后是客户端搭建 安装客户端软件yum install iscsi-initiator-utils 设置客户端识别号该文件需要自己输入配置，输入客户端的识别码vim /etc/iscsi/initiatorname.iscsiinitiatorname=iqn.2018-01.com.example:system2 启动服务systemctl enable iscsi iscsidsystemctl start iscsi iscsid 获取硬盘iscsiadm -m discovery -t st -p 192.168.163.100 -l iscsiadm 用于管理 iSCSI 数据库配置文件的命令行工具-m discovery 表示发现查找-t senbtargets 表示发布的 target，简写 st-p IP 指定服务器地址-l 表示 login，可不加 连接成功信息:Logging in to [iface: default, target: iqn.2018-01.com.example:system1, portal: 192.168.163.100,3260] (multiple)Login to [iface: default, target: iqn.2018-01.com.example:system1, portal: 192.168.163.100,3260] successful. 通过命令lsblk查看是否拿到 [root@system2 ~]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 20G 0 disk├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 19G 0 part ├─centos-root 253:0 0 17G 0 lvm / └─centos-swap 253:1 0 2G 0 lvm [SWAP]sdb 8:16 0 5G 0 disksr0 11:0 1 1024M 0 rom 发现出现了 sdb，大小为 5G，已成功获取","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"iSCSI","slug":"iSCSI","permalink":"https://coconutmilktaro.top/tags/iSCSI/"}]},{"title":"DNS学习笔记","slug":"DNS学习笔记","date":"2018-01-28T12:22:42.000Z","updated":"2022-05-30T02:51:53.784Z","comments":true,"path":"2018/DNS学习笔记/","link":"","permalink":"https://coconutmilktaro.top/2018/DNS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"本篇 DNS 笔记包含以下内容 DNS 简介 DNS 解析原理 DNS 域名解析原理（迭代） 域名解析方法 DNS 深入理解 基础配置 正向解析 反向解析 常用命令","text":"本篇 DNS 笔记包含以下内容 DNS 简介 DNS 解析原理 DNS 域名解析原理（迭代） 域名解析方法 DNS 深入理解 基础配置 正向解析 反向解析 常用命令 DNS 简介DNS 全称 Domain Name System 域名解析服务，用于解析域名与 IP 地址对应关系。 DNS 基于 UDP，UDP 端口号 53，也会使用 TCP 的 53 端口，先会用 UDP 查找，若 UDP 查不到或请求大于 512 字节时才用 TCP 查找。同时，UDP 进行名称解析，TCP 进行区域解析。 DNS 组成： 域名服务器：提供域名解析服务的软件。DNS 域名解析服务中最高效的是 Bind。Bind 的程序名叫 named。 解析器：访问域名服务器的客户端，负责解析从域名服务器获取的响应。如 nslookup。 目前互联网的命名方式为层次树状结构，任何互联网上的主机或路由器都有唯一层次结构的名字，即域名。 功能 正向解析：根据域名查找对应 IP 反向解析：根据 IP 查找对应域名 DNS 解析原理DNS 域名解析原理（迭代） 客户端将域名解析请求发给本地域名服务器或/etc/resolv.conf中列出的服务器 本地域名服务器收到后，查询本地缓存，若有就返回 若没有就把请求发给根域名服务器，迭代查询 本地域名服务器会将最终的结果存入缓存，同时将结果返回给主机。 /etc/resolv.conf用于定义 dns 查询指向的服务器以及解析顺序文件结构 domain domain_name # 声明本地域名，即解析时自动隐式补齐的域名search domain_name_list # 指定域名搜索顺序(最多6个)，和domain不能共存，若共存了，则后面的行生效nameserver IP-Address # 设置DNS指向，最多3个options timeout:n attempts:n # 指定解析超时时间(默认5秒)和解析次数(默认2次) 域名解析方法 递归：服务器收到请求时，若不能解析，则把请求转发到下一台服务器直到有一台解析成功。注：是收到请求的服务器去问，一个问下一个，最后解析完成后原路返回。 迭代：服务器收到请求时，若不能解析，则按根域-&gt;一级域名-&gt;二级域名-&gt;三级域名依次询问，直到解析成功。注：是本地服务器去不断问。 禁止递归查询的原因：对于授权域名服务器，若打开了递归查询，相当于配置为开放 DNS 服务器，会造成大量数据流量。所以在授权域名服务器上，应该禁用递归查询。recursion no; 反向解析：根据 IP 查找对应域名。将创建一个 in-addr.arpa 的专门的域处理。 高速缓存：DNS 会将解析的信息保存在高速缓存中。每条记录都对应一个 TTL，设置在缓存中保存的时间。 DNS 深入理解DNS 报文解析 分类 主域名服务器 master server：在特定区域内具有唯一性的域名解析服务器 辅域名服务器 slave server：从主服务器获取域名解析信息并维护，以防主服务器宕机。会通过 TCP 与主域名服务器通信，获取 zone 数据。 缓存服务器 Caching-Only Server：向其他服务器查询域名解析信息，每获取一个就放在高速缓存中，提高重复查询效率。 域名服务器类型 根域名服务器：最高层次的域名服务器，存放所有顶级域名服务器的 IP 地址与域名。当一个本地域名服务器对一个域名无法解析时，就会直接找根域名服务器，根域名服务器会告知应该去哪个顶级域名服务器查询。全球共 13 个根域名服务器。可通过dig查看。 顶级域名服务器：负责管理在该服务器上注册的二级域名服务器的 IP 地址和域名。 授权域名服务器：DNS 采用分区方式设置域名服务器。一个服务器所管理的范围为区。区的范围小于等于域的范围，每个区都设有一台权限域名服务器，负责将其分区的主机域名解析。由专业域名服务公司维护，若授权域名服务器出现故障，域名将不能被解析。 本地域名服务器：也称默认域名服务器，当主机发出 DNS 查询报文时，会最先询问本地域名服务器。 域名结构每一级域名都由英文字母与数字组成，不超过 63 字符，且不区分大小写，完整域名不超过 255 字符。目前顶级域名 TLD（Top Level Domain）三大类：国家顶级域名、国际顶级域名、通用顶级域名。互联网的命名空间是按照机构的组织划分的，与物理网络无关，与 IP 子网无关。 . 根域，管理一级域名 com、or、gov、cn 等一级域名，管理二级域名 baidu、google 等二级域名，管理三级域名，当国家为一级域名时，com 等一级域名便会下降一级别，依次类推 依次会有三级，四级 www、ftp、mail 等主机域名，为提供服务的主机名 DNS 系统采用阶层式管理，上一级的服务器只记录下一层的主机名（服务器名） 资源记录语法&#123;name&#125; &#123;TTL&#125; class record-type record-specfic-data name：域记录名。通常只有第一个 DNS 资源记录会配置，其他资源记录的 name 可能为空，那么其他资源记录会接受先前资源记录的名字。 TTL：生存时间。指定该数据在数据库中保存的时间，此栏若为空，表示默认生存时间在授权资源记录中指定。 class：记录的类。大范围用于 Internet 地址和其他信息地址类为 IN（基本都是 IN）。 record-type：记录类型。常为 A、NS、MX、CNAME record-specfic-data：记录指定数据。 记录类型： A：IPv4 地址记录，将主机映射到 IPv4 地址 # host -v -t A baidu.com;; ANSWER SECTION:baidu.com. 5 IN A 123.125.115.110baidu.com. 5 IN A 220.181.57.216 AAAA：IPv6 地址记录，将主机名映射到 IPv6 地址 CNAME：规范名称记录，将一个记录别名化为另一个记录，其中应具有 A 或 AAAA 记录。就是实际主机名当 DNS 解析器收到 CNAME 记录为查询响应时，DNS 解析器会使用规范名称重新发出查询。CNAME 记录数据可指向 DNS 中任何位置的名称，无论在区域内还是区域外。应避免将 CNAME 记录指向其他 CNAME 记录以避免 CNAME 循环。CNAME 记录链必须以 A 或 AAAA 记录结束。当使用 CDN 时，也可使用 CNAME 链。NS 和 MX 记录不可指向 CNAME 记录。 # host -v -t CNAME baidu.com;; ANSWER SECTION:www.baidu.com. 5 IN CNAME www.a.shifen.com. PTR：指针记录，将 IPv4 或 IPv6 地址映射到主机名，用于反向 DNS 解析。对行为类似于主机名的 IP 进行编码。 # host -v -t PTR 202.108.22.220;; ANSWER SECTION:220.22.108.202.in-addr.arpa. 5 IN PTR xd-22-220-a8.bta.net.cn. NS：名称服务器记录，将域名映射到 DNS 名称服务器。区域的每个公开授权名称服务器必须具有 NS 记录。 # host -v -t ns baidu.com;; ANSWER SECTION:baidu.com. 5 IN NS ns7.baidu.com. SOA：授权起始记录，提供有关 DNS区域工作方式的信息。每个区域正好有一个 SOA 记录，指定主服务器，以及辅（从）服务器更新副本的方式。 # host -v -t SOA baidu.com;; AUTHORITY SECTION:baidu.com. 5 IN SOA dns.baidu.com. sa.baidu.com. 2012138777 300 300 2592000 7200# dns.baidu.com. 主名称服务器（Master）# sa.baidu.com. DNS区域负责人的邮箱地址，因为@在zone文件中有意义，所以改为了.# 2012138777 区域版本号（序列号）# 300 检查区域更新频率（单位s）（refresh）# 300 在重试失败的刷新前应等待的时间（单位s）（retry）# 2592000 刷新失败，在停止使用其旧区域副本前等待的时间（单位s）（expire）# 7200 若解析器查询某个名称并该名称不存在，解析器将“记录不存在”信息进行缓存的时间（单位s）值的设置范围：刷新频率（refresh）&gt;= 2 × 重试刷新时间（retry）refresh+retry &lt; 超时时间（expire）expire &gt;= retry × 10expire &gt;= 7天 MX：邮件交换记录，将域名映射到邮件交换。邮件交换将接收该名称的电子邮件。数据为优先级，用于在多个 MX 记录间确定顺序，以及用于该名称的邮件交换的主机名。 # dig -t mx google.com;; ANSWER SECTION:google.com. 5 IN MX 40 alt3.aspmx.l.google.com.google.com. 5 IN MX 50 alt4.aspmx.l.google.com.google.com. 5 IN MX 20 alt1.aspmx.l.google.com.google.com. 5 IN MX 30 alt2.aspmx.l.google.com.google.com. 5 IN MX 10 aspmx.l.google.com.#邮件域名前的数字，为优先级，值越低越优先，具有优先的邮件处理权 TXT：文本记录，将名称映射到文本。通常用于提供发送方策略框架 SPF、域密钥识别邮件 DKIM、基于域的消息身份验证报告一致性 DMARC 等数据。 # dig -t txt google.com;; ANSWER SECTION:google.com. 5 IN TXT &quot;v=spf1 include:_spf.google.com ~all&quot;google.com. 5 IN TXT &quot;facebook-domain-verification=22rm551cu4k0ab0bxsw536tlds4h95&quot;google.com. 5 IN TXT &quot;docusign=05958488-4752-4ef2-95eb-aa7ba8a3bd0e&quot;# txt记录是按一定格式编写的，最常用的是SPF（sender policy framework），登记某个域名拥有的用来外发邮件的所有IP# SPF就是用于反垃圾邮件，阻止发件人发送假冒域中发件人地址的电子邮件# v=spf1 include:_spf.google.com ~all# 其中v标识spf版本，include指定spf标识，~all表示其余都不认可 SPF 防止伪造邮件的过程： 邮件服务器收到邮件后，先检查哪个域声明发送了该邮件，并检查该域名的 SPF 记录的 DNS 确定发送服务器的 IP 地址是否与 SPF 记录中已发布的 IP 地址匹配 对该邮件打分，若匹配，则通过验证并打一个正分，否则不通过并打一个负分 SRV：服务记录，用于查找支持域的特定服务的主机。使用格式设置为包含服务和协议名称的域名。如_service._protocol.domainname，SRV 记录可记录为域提供服务的主机名和服务端口号，还包括优先级和权重值。 名称服务器 Name Server：存储域名资源信息的程序，会响应解析器的请求。利用该服务器，整个网络可划分为一个域的分层结构。整个域名空间可划分为多个区域 zone，zone 通常表示管理界限的划分，也就是 DNS 树状结构上的一点。每个 zone 都有一个主域名服务器，还可有多个辅域名服务器。 反向解析：IP 是倒过来写的。 # dig -x 8.8.4.4;; ANSWER SECTION:4.4.8.8.in-addr.arpa. 5 IN PTR google-public-dns-b.google.com. 基础配置环境 CentOS7，内核 3.10 虚拟机网段：192.168.163.0/24 DNS 服务器 IP 地址：192.168.163.102/24 DNS 服务器主机名：system2.example.com 网关：192.168.163.254 客户端主机名：system3.example.com DNS 服务相关配置文件 /etc/named.conf：主配置文件 /etc/named.rfc1912.zones：区域配置文件 /etc/reslov.conf：本地的 DNS 服务器 /etc/nsswitch.conf：优先级配置文件 /var/named/目录：存放区域（zone）文件 步骤 安装 bind 服务yum groupinstall DNS\\ Server 开启 named 服务防火墙放行 dns，rich rules 放行 UDP 和 TCP 的 53 端口systemctl enable named.servicefirewall-cmd --add-service=dns --permanentfirewall-cmd --permanent --add-rich-rule=&#39;rule family=ipv4 port port=53 protocol=udp protocol=tcp accept&#39;firewall-cmd --reload 修改配置文件修改配置文件最好先做备份 cp -a XX XX.bak首先修改/etc/named.conf **注：注释需要用; ** 只摘取部分options &#123; //指定bind服务参数 //listen-on 指定bind侦听的本机IP及端口 listen-on port 53 &#123; any; &#125;; //要将&#123;&#125;中改为any，本机的任意IP都监听（一台服务器可能有多个IP） listen-on-v6 port 53 &#123; ::1; &#125;; //directory 指定区域配置文件的路径 //若使用chroot则该路径是相对路径，对应/var/named/chroot/var/named/ directory &quot;/var/named&quot;; //改为any，接受任意IP的DNS查询请求 //也可指定网段，只给该网段做DNS allow-query &#123; any; &#125;; ;forward only; #只做转发 forwarders &#123;114.114.114.114; 8.8.8.8;&#125;; #上层的DNS服务器&#125;;zone &quot;.&quot; IN &#123; //指定当前bind可管辖的区域 type hint; //指定区域类型 file &quot;named.ca&quot;;//指定区域配置文件&#125;;//以下是区域配置文件和密钥文件include &quot;/etc/named.rfc1912.zones&quot;;include &quot;/etc/named.root.key&quot;; 修改后直接重新加载配置文件rndc reload，之后测试是否生效 # dig www.google.com @127.0.0.1#@127.0.0.1是用于指定本地解析.......;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;www.google.com. IN A;; ANSWER SECTION:www.google.com. 113 IN A 208.101.60.87# 域名（FQDN） TTL值 关键词INTERNET RR类型 IPv4地址 正向解析修改/etc/named.rfc1912.zones配置正向解析区域文件named.conf中也能写区域配置，但为了安全和管理，将主配置和区域配置分开为两个文件 zone &quot;example.com&quot; IN &#123; type master; //指定区域类型master file &quot;example.com.zone&quot;; //指定区域配置文件路径（相对路径，相对于named.conf中directory指定路径） //表示若要解析example.com的域名就要去该文件找 allow-update &#123; none; &#125;; //不允许客户机动态更新解析信息&#125; DNS 区域类型 master：主要区域，拥有该区域数据文件，并对此区域提供管理数据 slave：辅助区域，拥有主要区域数据的只读副本，从主区域同步所有区域数据 hint：dns 启动时，使用 hint 区域的信息查找最近的根域名服务器，没有就使用默认根服务器 然后配置解析数据信息文件/var/named/example.com.zone配置文件有模板，可复制/var/named/named.localhost并改名最好将文件名改为域名.zone $TTL 1D@ IN SOA example.com. root.example.com. ( 0 ; serial //更新序列号 1D ; refresh //更新时间 1H ; retry //重试延时 1W ; expire //失效时间 3H ) ; minimum //无效解析记录的缓存时间 NS ns.example.com.ns IN A 192.168.163.102 IN MX 10 mail.example.com.mail IN A 192.168.163.102www IN A 192.168.163.102// TTL 指定资源记录存放在缓存中的时间，单位秒，一般直接调用$TTL的值// @为当前域，根据主配置文件的zone区域决定// IN是网络类型，表示自身// SOA记录：起始授权记录，在一个区域一定是唯一的，定义区域的全局参数// example.com. DNS区域地址（完整的，要加.根域）// root.example.com. 服务器邮箱地址（完整）//NS记录：名称服务器记录，在一个区域至少一条，记录区域的授权服务器，一般就指定为主机名ns//该记录下一行就指定该服务器的ip地址//ns 为主机名 A为地址记录，写域名对应IP//MX邮件交换记录：指定邮件服务器，用于根据收件人地址后缀定位邮件服务器，为管理员自己接收邮件的域名//其他主机名也是一样 主和辅服务器都应该列在上级域的NS记录中，才能形成一个正式的授权。也应该列在自己主机的域文件中，任何列在NS记录中的服务器必须配置为那个域的授权域名服务器。 反向解析修改/etc/named.rfc1912.zones，添加 zone &quot;163.168.192.in-addr.arpa&quot; IN &#123; type master; # 服务类型master file &quot;192.168.163.arpa&quot;;&#125; 反向解析的参考配置文件为/var/named/named.loopback 可通过named-checkconf或named-checkzone检查配置文件语法是否正确。至此基础配置完成，重启服务 使用命令nslookup输入域名测试 常用命令**rndc**：bind 的管理配置工具 rndc COMMANDS reload 重载主配置文件和区域解析库文件 reload zone 重载区域解析库文件 retransfer zone 手动启动区域传输过程 notify zone 重新对区域传送通知 reconfig 重载主配置文件和区域解析库文件 querylog 开启或关闭查询日志 trace 递增debug级别rndc-confgen 生成rndc配置文件 host：查询某个域名或主机名所对应的所有 IP 地址 # host baidu.combaidu.com has address 123.125.115.110baidu.com has address 220.181.57.216 nslookup：查询一台主机 IP 及对应的域名 两种模式：交互式：不加参数 非交互式：加参数# nslookup baidu.comServer: 192.168.163.254Address: 192.168.163.254#53# nslookup&gt; baidu.comServer: 192.168.163.254Address: 192.168.163.254#53 **dig**：查询 DNS 服务器 dig -t 指定查询类型 例：dig -t mx|soa|nsdig -x 反向解析 参考书籍：骏马金龙 DNS &amp; bind 从基础到深入http://www.cnblogs.com/f-ck-need-u/p/7367503.html骏马金龙 Linux 的网络管理http://www.cnblogs.com/f-ck-need-u/p/7074594.htmlLinux 就该这么学Linux 运维最佳实践Linux 系统管理与网络管理Linux 服务器架设指南Linux 服务器架设、性能调优、集群管理教程","categories":[{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"DNS","slug":"DNS","permalink":"https://coconutmilktaro.top/tags/DNS/"}]}],"categories":[{"name":"Python开发","slug":"Python开发","permalink":"https://coconutmilktaro.top/categories/Python%E5%BC%80%E5%8F%91/"},{"name":"架构","slug":"架构","permalink":"https://coconutmilktaro.top/categories/%E6%9E%B6%E6%9E%84/"},{"name":"Java","slug":"Java","permalink":"https://coconutmilktaro.top/categories/Java/"},{"name":"数据库","slug":"数据库","permalink":"https://coconutmilktaro.top/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"应用运维","slug":"应用运维","permalink":"https://coconutmilktaro.top/categories/%E5%BA%94%E7%94%A8%E8%BF%90%E7%BB%B4/"},{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/categories/%E7%BD%91%E7%BB%9C/"},{"name":"云计算","slug":"云计算","permalink":"https://coconutmilktaro.top/categories/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"后端开发","slug":"后端开发","permalink":"https://coconutmilktaro.top/categories/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"系统运维","slug":"系统运维","permalink":"https://coconutmilktaro.top/categories/%E7%B3%BB%E7%BB%9F%E8%BF%90%E7%BB%B4/"},{"name":"Linux原理","slug":"Linux原理","permalink":"https://coconutmilktaro.top/categories/Linux%E5%8E%9F%E7%90%86/"},{"name":"虚拟化","slug":"虚拟化","permalink":"https://coconutmilktaro.top/categories/%E8%99%9A%E6%8B%9F%E5%8C%96/"},{"name":"大数据","slug":"大数据","permalink":"https://coconutmilktaro.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"日志","slug":"大数据/日志","permalink":"https://coconutmilktaro.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%97%A5%E5%BF%97/"},{"name":"架构","slug":"大数据/日志/架构","permalink":"https://coconutmilktaro.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%97%A5%E5%BF%97/%E6%9E%B6%E6%9E%84/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://coconutmilktaro.top/tags/kubernetes/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://coconutmilktaro.top/tags/zookeeper/"},{"name":"API","slug":"API","permalink":"https://coconutmilktaro.top/tags/API/"},{"name":"python","slug":"python","permalink":"https://coconutmilktaro.top/tags/python/"},{"name":"ansible","slug":"ansible","permalink":"https://coconutmilktaro.top/tags/ansible/"},{"name":"分布式","slug":"分布式","permalink":"https://coconutmilktaro.top/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"JVM","slug":"JVM","permalink":"https://coconutmilktaro.top/tags/JVM/"},{"name":"Java","slug":"Java","permalink":"https://coconutmilktaro.top/tags/Java/"},{"name":"influxdb","slug":"influxdb","permalink":"https://coconutmilktaro.top/tags/influxdb/"},{"name":"数据库","slug":"数据库","permalink":"https://coconutmilktaro.top/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Supervisor","slug":"Supervisor","permalink":"https://coconutmilktaro.top/tags/Supervisor/"},{"name":"sre","slug":"sre","permalink":"https://coconutmilktaro.top/tags/sre/"},{"name":"网络","slug":"网络","permalink":"https://coconutmilktaro.top/tags/%E7%BD%91%E7%BB%9C/"},{"name":"5G","slug":"5G","permalink":"https://coconutmilktaro.top/tags/5G/"},{"name":"监控","slug":"监控","permalink":"https://coconutmilktaro.top/tags/%E7%9B%91%E6%8E%A7/"},{"name":"云计算","slug":"云计算","permalink":"https://coconutmilktaro.top/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://coconutmilktaro.top/tags/Prometheus/"},{"name":"PowerShell","slug":"PowerShell","permalink":"https://coconutmilktaro.top/tags/PowerShell/"},{"name":"华三","slug":"华三","permalink":"https://coconutmilktaro.top/tags/%E5%8D%8E%E4%B8%89/"},{"name":"路由","slug":"路由","permalink":"https://coconutmilktaro.top/tags/%E8%B7%AF%E7%94%B1/"},{"name":"园区网","slug":"园区网","permalink":"https://coconutmilktaro.top/tags/%E5%9B%AD%E5%8C%BA%E7%BD%91/"},{"name":"VLAN","slug":"VLAN","permalink":"https://coconutmilktaro.top/tags/VLAN/"},{"name":"PPP","slug":"PPP","permalink":"https://coconutmilktaro.top/tags/PPP/"},{"name":"IPv6","slug":"IPv6","permalink":"https://coconutmilktaro.top/tags/IPv6/"},{"name":"QoS","slug":"QoS","permalink":"https://coconutmilktaro.top/tags/QoS/"},{"name":"高可用","slug":"高可用","permalink":"https://coconutmilktaro.top/tags/%E9%AB%98%E5%8F%AF%E7%94%A8/"},{"name":"TCP/IP","slug":"TCP-IP","permalink":"https://coconutmilktaro.top/tags/TCP-IP/"},{"name":"安全","slug":"安全","permalink":"https://coconutmilktaro.top/tags/%E5%AE%89%E5%85%A8/"},{"name":"无线","slug":"无线","permalink":"https://coconutmilktaro.top/tags/%E6%97%A0%E7%BA%BF/"},{"name":"渗透","slug":"渗透","permalink":"https://coconutmilktaro.top/tags/%E6%B8%97%E9%80%8F/"},{"name":"Python","slug":"Python","permalink":"https://coconutmilktaro.top/tags/Python/"},{"name":"lang","slug":"lang","permalink":"https://coconutmilktaro.top/tags/lang/"},{"name":"Django","slug":"Django","permalink":"https://coconutmilktaro.top/tags/Django/"},{"name":"Whoosh","slug":"Whoosh","permalink":"https://coconutmilktaro.top/tags/Whoosh/"},{"name":"全文检索","slug":"全文检索","permalink":"https://coconutmilktaro.top/tags/%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2/"},{"name":"运维","slug":"运维","permalink":"https://coconutmilktaro.top/tags/%E8%BF%90%E7%BB%B4/"},{"name":"namespace","slug":"namespace","permalink":"https://coconutmilktaro.top/tags/namespace/"},{"name":"Vultr","slug":"Vultr","permalink":"https://coconutmilktaro.top/tags/Vultr/"},{"name":"Shadowsocks","slug":"Shadowsocks","permalink":"https://coconutmilktaro.top/tags/Shadowsocks/"},{"name":"存储","slug":"存储","permalink":"https://coconutmilktaro.top/tags/%E5%AD%98%E5%82%A8/"},{"name":"MFS","slug":"MFS","permalink":"https://coconutmilktaro.top/tags/MFS/"},{"name":"文件系统","slug":"文件系统","permalink":"https://coconutmilktaro.top/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"},{"name":"HAProxy","slug":"HAProxy","permalink":"https://coconutmilktaro.top/tags/HAProxy/"},{"name":"翻译","slug":"翻译","permalink":"https://coconutmilktaro.top/tags/%E7%BF%BB%E8%AF%91/"},{"name":"nfs","slug":"nfs","permalink":"https://coconutmilktaro.top/tags/nfs/"},{"name":"LDAP","slug":"LDAP","permalink":"https://coconutmilktaro.top/tags/LDAP/"},{"name":"验证","slug":"验证","permalink":"https://coconutmilktaro.top/tags/%E9%AA%8C%E8%AF%81/"},{"name":"自动化","slug":"自动化","permalink":"https://coconutmilktaro.top/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/"},{"name":"SaltStack","slug":"SaltStack","permalink":"https://coconutmilktaro.top/tags/SaltStack/"},{"name":"Memcached","slug":"Memcached","permalink":"https://coconutmilktaro.top/tags/Memcached/"},{"name":"缓存","slug":"缓存","permalink":"https://coconutmilktaro.top/tags/%E7%BC%93%E5%AD%98/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://coconutmilktaro.top/tags/MongoDB/"},{"name":"docker","slug":"docker","permalink":"https://coconutmilktaro.top/tags/docker/"},{"name":"KVM","slug":"KVM","permalink":"https://coconutmilktaro.top/tags/KVM/"},{"name":"虚拟化","slug":"虚拟化","permalink":"https://coconutmilktaro.top/tags/%E8%99%9A%E6%8B%9F%E5%8C%96/"},{"name":"Puppet","slug":"Puppet","permalink":"https://coconutmilktaro.top/tags/Puppet/"},{"name":"MySQL","slug":"MySQL","permalink":"https://coconutmilktaro.top/tags/MySQL/"},{"name":"集群","slug":"集群","permalink":"https://coconutmilktaro.top/tags/%E9%9B%86%E7%BE%A4/"},{"name":"同步","slug":"同步","permalink":"https://coconutmilktaro.top/tags/%E5%90%8C%E6%AD%A5/"},{"name":"DRBD","slug":"DRBD","permalink":"https://coconutmilktaro.top/tags/DRBD/"},{"name":"server","slug":"server","permalink":"https://coconutmilktaro.top/tags/server/"},{"name":"heartbeat","slug":"heartbeat","permalink":"https://coconutmilktaro.top/tags/heartbeat/"},{"name":"Linux","slug":"Linux","permalink":"https://coconutmilktaro.top/tags/Linux/"},{"name":"sed","slug":"sed","permalink":"https://coconutmilktaro.top/tags/sed/"},{"name":"awk","slug":"awk","permalink":"https://coconutmilktaro.top/tags/awk/"},{"name":"zabbix","slug":"zabbix","permalink":"https://coconutmilktaro.top/tags/zabbix/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://coconutmilktaro.top/tags/Elasticsearch/"},{"name":"ELK","slug":"ELK","permalink":"https://coconutmilktaro.top/tags/ELK/"},{"name":"Kafka","slug":"Kafka","permalink":"https://coconutmilktaro.top/tags/Kafka/"},{"name":"Kibana","slug":"Kibana","permalink":"https://coconutmilktaro.top/tags/Kibana/"},{"name":"Logstash","slug":"Logstash","permalink":"https://coconutmilktaro.top/tags/Logstash/"},{"name":"日志","slug":"日志","permalink":"https://coconutmilktaro.top/tags/%E6%97%A5%E5%BF%97/"},{"name":"Filebeat","slug":"Filebeat","permalink":"https://coconutmilktaro.top/tags/Filebeat/"},{"name":"数据结构","slug":"数据结构","permalink":"https://coconutmilktaro.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"算法","slug":"算法","permalink":"https://coconutmilktaro.top/tags/%E7%AE%97%E6%B3%95/"},{"name":"Cacti","slug":"Cacti","permalink":"https://coconutmilktaro.top/tags/Cacti/"},{"name":"SQLite","slug":"SQLite","permalink":"https://coconutmilktaro.top/tags/SQLite/"},{"name":"IS-IS","slug":"IS-IS","permalink":"https://coconutmilktaro.top/tags/IS-IS/"},{"name":"Postfix","slug":"Postfix","permalink":"https://coconutmilktaro.top/tags/Postfix/"},{"name":"邮件服务","slug":"邮件服务","permalink":"https://coconutmilktaro.top/tags/%E9%82%AE%E4%BB%B6%E6%9C%8D%E5%8A%A1/"},{"name":"sendmail","slug":"sendmail","permalink":"https://coconutmilktaro.top/tags/sendmail/"},{"name":"Tomcat","slug":"Tomcat","permalink":"https://coconutmilktaro.top/tags/Tomcat/"},{"name":"web","slug":"web","permalink":"https://coconutmilktaro.top/tags/web/"},{"name":"java","slug":"java","permalink":"https://coconutmilktaro.top/tags/java/"},{"name":"OpenSSL","slug":"OpenSSL","permalink":"https://coconutmilktaro.top/tags/OpenSSL/"},{"name":"SSL","slug":"SSL","permalink":"https://coconutmilktaro.top/tags/SSL/"},{"name":"SSH","slug":"SSH","permalink":"https://coconutmilktaro.top/tags/SSH/"},{"name":"无人值守","slug":"无人值守","permalink":"https://coconutmilktaro.top/tags/%E6%97%A0%E4%BA%BA%E5%80%BC%E5%AE%88/"},{"name":"PXE","slug":"PXE","permalink":"https://coconutmilktaro.top/tags/PXE/"},{"name":"Kickstart","slug":"Kickstart","permalink":"https://coconutmilktaro.top/tags/Kickstart/"},{"name":"Cobbler","slug":"Cobbler","permalink":"https://coconutmilktaro.top/tags/Cobbler/"},{"name":"Rsync","slug":"Rsync","permalink":"https://coconutmilktaro.top/tags/Rsync/"},{"name":"性能","slug":"性能","permalink":"https://coconutmilktaro.top/tags/%E6%80%A7%E8%83%BD/"},{"name":"LVM","slug":"LVM","permalink":"https://coconutmilktaro.top/tags/LVM/"},{"name":"RAID","slug":"RAID","permalink":"https://coconutmilktaro.top/tags/RAID/"},{"name":"组播","slug":"组播","permalink":"https://coconutmilktaro.top/tags/%E7%BB%84%E6%92%AD/"},{"name":"OSPF","slug":"OSPF","permalink":"https://coconutmilktaro.top/tags/OSPF/"},{"name":"BGP","slug":"BGP","permalink":"https://coconutmilktaro.top/tags/BGP/"},{"name":"docker-compose","slug":"docker-compose","permalink":"https://coconutmilktaro.top/tags/docker-compose/"},{"name":"容器编排","slug":"容器编排","permalink":"https://coconutmilktaro.top/tags/%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://coconutmilktaro.top/tags/Kubernetes/"},{"name":"vpn","slug":"vpn","permalink":"https://coconutmilktaro.top/tags/vpn/"},{"name":"ftp","slug":"ftp","permalink":"https://coconutmilktaro.top/tags/ftp/"},{"name":"iptables","slug":"iptables","permalink":"https://coconutmilktaro.top/tags/iptables/"},{"name":"Selinux","slug":"Selinux","permalink":"https://coconutmilktaro.top/tags/Selinux/"},{"name":"防火墙","slug":"防火墙","permalink":"https://coconutmilktaro.top/tags/%E9%98%B2%E7%81%AB%E5%A2%99/"},{"name":"firewalld","slug":"firewalld","permalink":"https://coconutmilktaro.top/tags/firewalld/"},{"name":"dhcp","slug":"dhcp","permalink":"https://coconutmilktaro.top/tags/dhcp/"},{"name":"http","slug":"http","permalink":"https://coconutmilktaro.top/tags/http/"},{"name":"HAproxy","slug":"HAproxy","permalink":"https://coconutmilktaro.top/tags/HAproxy/"},{"name":"代理","slug":"代理","permalink":"https://coconutmilktaro.top/tags/%E4%BB%A3%E7%90%86/"},{"name":"负载均衡","slug":"负载均衡","permalink":"https://coconutmilktaro.top/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"name":"varnish","slug":"varnish","permalink":"https://coconutmilktaro.top/tags/varnish/"},{"name":"squid","slug":"squid","permalink":"https://coconutmilktaro.top/tags/squid/"},{"name":"Nagios","slug":"Nagios","permalink":"https://coconutmilktaro.top/tags/Nagios/"},{"name":"LVS","slug":"LVS","permalink":"https://coconutmilktaro.top/tags/LVS/"},{"name":"keepalived","slug":"keepalived","permalink":"https://coconutmilktaro.top/tags/keepalived/"},{"name":"wireshark","slug":"wireshark","permalink":"https://coconutmilktaro.top/tags/wireshark/"},{"name":"Redis","slug":"Redis","permalink":"https://coconutmilktaro.top/tags/Redis/"},{"name":"linux","slug":"linux","permalink":"https://coconutmilktaro.top/tags/linux/"},{"name":"Nginx","slug":"Nginx","permalink":"https://coconutmilktaro.top/tags/Nginx/"},{"name":"LNMP","slug":"LNMP","permalink":"https://coconutmilktaro.top/tags/LNMP/"},{"name":"Apache","slug":"Apache","permalink":"https://coconutmilktaro.top/tags/Apache/"},{"name":"LAMP","slug":"LAMP","permalink":"https://coconutmilktaro.top/tags/LAMP/"},{"name":"LAMT","slug":"LAMT","permalink":"https://coconutmilktaro.top/tags/LAMT/"},{"name":"Samba","slug":"Samba","permalink":"https://coconutmilktaro.top/tags/Samba/"},{"name":"Lang","slug":"Lang","permalink":"https://coconutmilktaro.top/tags/Lang/"},{"name":"YAML","slug":"YAML","permalink":"https://coconutmilktaro.top/tags/YAML/"},{"name":"I/O","slug":"I-O","permalink":"https://coconutmilktaro.top/tags/I-O/"},{"name":"Chrony","slug":"Chrony","permalink":"https://coconutmilktaro.top/tags/Chrony/"},{"name":"NTP","slug":"NTP","permalink":"https://coconutmilktaro.top/tags/NTP/"},{"name":"iSCSI","slug":"iSCSI","permalink":"https://coconutmilktaro.top/tags/iSCSI/"},{"name":"DNS","slug":"DNS","permalink":"https://coconutmilktaro.top/tags/DNS/"}]}